{"custom_id": "keras#fd23f6671b088f19acdb5d148c8af1cd63583c9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 1348 | Contributors (this commit): 27 | Commits (past 90d): 4 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -591,10 +591,7 @@ def get(identifier):\n   \"\"\"\n   if identifier is None:\n     return linear\n-  if isinstance(identifier, str):\n-    identifier = str(identifier)\n-    return deserialize(identifier)\n-  elif isinstance(identifier, dict):\n+  if isinstance(identifier, (str,dict)):\n     return deserialize(identifier)\n   elif callable(identifier):\n     return identifier\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7fda375bb5c26683e199859fedd1c9f900cb23ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 87 | Churn Cumulative: 2949 | Contributors (this commit): 3 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,15 +18,16 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n-\n+import contextlib\n import functools\n from keras.engine import base_layer\n-from keras.utils import tf_inspect\n from keras.utils import layer_utils\n+from keras.utils import tf_inspect\n \n-from tensorflow.python.ops import variable_scope as vs\n-from tensorflow.python.platform import tf_logging as logging\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.ops import variable_scope as vs  # pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.platform import tf_logging as logging  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n \n@@ -150,6 +151,11 @@ class _EagerVariableStore(tf.Module):\n     self._regularizers = {}  # A dict mapping var names to their regularizers.\n     self._store_eager_variables = True\n \n+  @contextlib.contextmanager\n+  def scope(self):\n+    with vs.with_variable_store(self):\n+      yield\n+\n   def get_variable(\n       self,\n       name,\n@@ -739,7 +745,7 @@ def track_tf1_style_variables(method):\n       self._tf1_style_var_store = var_store  # pylint: disable=protected-access\n \n     existing_regularized_variables = set(var_store._regularizers.keys())  # pylint: disable=protected-access\n-    with vs.with_variable_store(var_store):\n+    with var_store.scope():\n       out = method(self, *args, **kwargs)\n \n     # If this is a layer method, add the regularization losses\n\n@@ -969,6 +969,75 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     tmp_dir = self.get_temp_dir()\n     tf.saved_model.save(layer, tmp_dir)\n \n+  def test_variable_store_scope_get_variable(self):\n+    # Test the module shim when using `get_variable` (and regularizers) directly\n+\n+    class WrappedDenseLayer(tf.Module):\n+\n+      def __init__(self, units, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.units = units\n+        self._variable_store = variable_scope_shim._EagerVariableStore()\n+\n+      def get_compat_v1_regularization_losses(self):\n+        \"\"\"Dict w/ regularization losses from `get_variable`.\"\"\"\n+        return {name: regularizer() for name, regularizer\n+                in self._variable_store._regularizers.items()}  # pylint: disable=protected-access\n+\n+      def __call__(self, inputs, training=None):\n+        with self._variable_store.scope():\n+          out = inputs\n+          with tf.compat.v1.variable_scope(\"dense_one\"):\n+            # The weights are created with a `regularizer`,\n+            # so the layer should track their regularization losses\n+            kernel = tf.compat.v1.get_variable(\n+                shape=[out.shape[-1], self.units],\n+                regularizer=regularizers.L2(),\n+                initializer=tf.compat.v1.ones_initializer(),\n+                name=\"kernel\")\n+            bias = tf.compat.v1.get_variable(\n+                shape=[self.units,],\n+                initializer=tf.compat.v1.zeros_initializer(),\n+                name=\"bias\")\n+            out = tf.matmul(out, kernel)\n+            out = tf.nn.bias_add(out, bias)\n+          with tf.compat.v1.variable_scope(\"nested_scope\"):\n+            with tf.compat.v1.variable_scope(\"dense_two\"):\n+              kernel = tf.compat.v1.get_variable(\n+                  shape=[out.shape[-1], self.units],\n+                  regularizer=regularizers.L2(),\n+                  initializer=tf.compat.v1.ones_initializer(),\n+                  name=\"kernel\")\n+              bias = tf.compat.v1.get_variable(\n+                  shape=[self.units,],\n+                  initializer=tf.compat.v1.zeros_initializer(),\n+                  name=\"bias\")\n+              out = tf.matmul(out, kernel)\n+              out = tf.nn.bias_add(out, bias)\n+          return out\n+\n+    layer = WrappedDenseLayer(10)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    weights = {x.name: x for x in layer.variables}\n+\n+    # Verify the correct output, regularization losses, + variables were made\n+    self.assertEqual(weights.keys(), {\"dense_one/bias:0\",\n+                                      \"dense_one/kernel:0\",\n+                                      \"nested_scope/dense_two/bias:0\",\n+                                      \"nested_scope/dense_two/kernel:0\"})\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 50)\n+    self.assertAllEqual(\n+        tf.add_n(layer.get_compat_v1_regularization_losses().values()), 1.5)\n+\n+    # Verify reuse by updating the variables then re-running\n+    weights[\"dense_one/kernel:0\"].assign(tf.ones(shape=(5, 10)) * 2)\n+    weights[\"nested_scope/dense_two/kernel:0\"].assign(\n+        tf.ones(shape=(10, 10)) * 2)\n+    out = layer(tf.ones(shape=(5, 5)))\n+    self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 200)\n+    self.assertAllEqual(\n+        tf.add_n(layer.get_compat_v1_regularization_losses().values()), 6)\n+\n   def test_module_get_variable(self):\n     # Test the module shim when using `get_variable` (and regularizers) directly\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97bc1e1415319bc9460b8b12408498c134aaefa0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2036 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -999,14 +999,14 @@ class SymbolicSupportTest(keras_parameterized.TestCase):\n     with tf.Graph().as_default():\n       x1 = tf.ones((3, 3))\n     x2 = tf.ones((3, 3))\n-    with self.assertRaisesRegex(TypeError, 'Graph tensors'):\n+    with self.assertRaises(TypeError):\n       tf.matmul(x1, x2)\n \n   def test_mixing_numpy_arrays_and_graph_tensors(self):\n     with tf.Graph().as_default():\n       x1 = tf.ones((3, 3))\n     x2 = np.ones((3, 3), dtype='float32')\n-    with self.assertRaisesRegex(TypeError, 'Graph tensors'):\n+    with self.assertRaises(TypeError):\n       tf.matmul(x1, x2)\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#451055a178cd4cf88d4b7604e6a969f63449f554", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 31 | Files Changed: 1 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 73 | Churn Cumulative: 16027 | Contributors (this commit): 114 | Commits (past 90d): 37 | Contributors (cumulative): 114 | DMM Complexity: 0.0\n\nDIFF:\n@@ -843,11 +843,18 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access\n       return outputs\n \n-    def train_function(iterator, steps_per_execution):\n+    if (self._steps_per_execution is None or\n+        self._steps_per_execution.numpy().item() == 1):\n+\n+      def train_function(iterator):\n+        \"\"\"Runs a training execution with one step.\"\"\"\n+        return step_function(self, iterator)\n+\n+    else:\n+\n+      def train_function(iterator):\n         \"\"\"Runs a training execution with multiple steps.\"\"\"\n-      outputs = step_function(self, iterator)\n-      if steps_per_execution > 1:\n-        for _ in tf.range(steps_per_execution - 1):\n+        for _ in tf.range(self._steps_per_execution):\n           outputs = step_function(self, iterator)\n         return outputs\n \n@@ -856,14 +863,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           train_function, experimental_relax_shapes=True)\n       self.train_tf_function = train_function\n \n+    self.train_function = train_function\n+\n     if self._cluster_coordinator:\n-      self.train_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n-          train_function,\n-          args=(it, self._steps_per_execution.numpy().item()))\n-    else:\n-      self.train_function = lambda it: train_function(  # pylint: disable=g-long-lambda\n-          it,\n-          self._steps_per_execution.numpy().item())\n+      self.train_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+          train_function, args=(iterator,))\n \n     return self.train_function\n \n@@ -1327,11 +1331,18 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           outputs, self.distribute_strategy, reduction='first')\n       return outputs\n \n-    def test_function(iterator, steps_per_execution):\n+    if (self._steps_per_execution is None or\n+        self._steps_per_execution.numpy().item() == 1):\n+\n+      def test_function(iterator):\n+        \"\"\"Runs an evaluation execution with one step.\"\"\"\n+        return step_function(self, iterator)\n+\n+    else:\n+\n+      def test_function(iterator):\n         \"\"\"Runs an evaluation execution with multiple steps.\"\"\"\n-      outputs = step_function(self, iterator)\n-      if steps_per_execution > 1:\n-        for _ in tf.range(steps_per_execution - 1):\n+        for _ in tf.range(self._steps_per_execution):\n           outputs = step_function(self, iterator)\n         return outputs\n \n@@ -1339,14 +1350,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       test_function = tf.function(\n           test_function, experimental_relax_shapes=True)\n \n+    self.test_function = test_function\n+\n     if self._cluster_coordinator:\n-      self.test_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n-          test_function,\n-          args=(it, self._steps_per_execution.numpy().item()))\n-    else:\n-      self.test_function = lambda it: test_function(  # pylint: disable=g-long-lambda\n-          it,\n-          self._steps_per_execution.numpy().item())\n+      self.test_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+          test_function, args=(iterator,))\n \n     return self.test_function\n \n@@ -1578,11 +1586,19 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           outputs, self.distribute_strategy, reduction='concat')\n       return outputs\n \n-    def predict_function(iterator, steps_per_execution):\n+    if (self._steps_per_execution is None or\n+        self._steps_per_execution.numpy().item() == 1):\n+\n+      def predict_function(iterator):\n+        \"\"\"Runs an evaluation execution with one step.\"\"\"\n+        return step_function(self, iterator)\n+\n+    else:\n+\n+      def predict_function(iterator):\n         \"\"\"Runs an evaluation execution with multiple steps.\"\"\"\n         outputs = step_function(self, iterator)\n-      if steps_per_execution > 1:\n-        for _ in tf.range(steps_per_execution - 1):\n+        for _ in tf.range(self._steps_per_execution - 1):\n           tf.autograph.experimental.set_loop_options(\n               shape_invariants=[(\n                   t, tf_utils.get_tensor_spec(t, dynamic_batch=True).shape)\n@@ -1596,12 +1612,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       predict_function = tf.function(\n           predict_function, experimental_relax_shapes=True)\n \n-    steps_per_execution = (\n-        lambda: self._steps_per_execution.numpy().item()  # pylint: disable=g-long-lambda\n-        if self._steps_per_execution is not None else 1)\n-    self.predict_function = lambda it: predict_function(  # pylint: disable=g-long-lambda\n-        it, steps_per_execution())\n-\n+    self.predict_function = predict_function\n     return self.predict_function\n \n   @traceback_utils.filter_traceback\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89fd0f5bd1595e2697faa427f2c7485b10e7cc02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 11 | Churn Cumulative: 426 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,13 +21,6 @@ from keras.utils import control_flow_util\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n-# TODO(b/168039935): track dropout rate to decide whether/how to make a\n-# dropout rate fastpath.\n-keras_temporary_dropout_rate = tf.__internal__.monitoring.BoolGauge(\n-    '/tensorflow/api/keras/dropout/temp_rate_is_zero',\n-    'Temporarily record if Keras dropout layer was created w/'\n-    'constant rate = 0')\n-\n \n @keras_export('keras.layers.Dropout')\n class Dropout(Layer):\n@@ -87,10 +80,6 @@ class Dropout(Layer):\n       raise ValueError(f'Invalid value {rate} received for '\n                        f'`rate`, expected a value between 0 and 1.')\n     self.rate = rate\n-    if isinstance(rate, (int, float)) and not rate:\n-      keras_temporary_dropout_rate.get_cell().set(True)\n-    else:\n-      keras_temporary_dropout_rate.get_cell().set(False)\n     self.noise_shape = noise_shape\n     self.seed = seed\n     self.supports_masking = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a2d73dc96bd002225704dbb638518b4f632637a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 9880 | Contributors (this commit): 21 | Commits (past 90d): 17 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -935,6 +935,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       outputs = tf.nest.map_structure(\n           keras_tensor.keras_tensor_from_tensor, outputs)\n \n+    self._set_save_spec(inputs, args, kwargs)\n     if hasattr(self, '_set_inputs') and not self.inputs:\n       # TODO(kaftan): figure out if we need to do this at all\n       # Subclassed network: explicitly set metadata normally set by\n\n@@ -1476,6 +1476,26 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     y = fn(x)\n     self.assertEqual(y.shape.as_list(), [10, 1])\n \n+  def test_save_spec(self):\n+    \"\"\"Tests that functional model generates the correct save spec.\"\"\"\n+\n+    class MultiInputModel(training_lib.Model):\n+\n+      def call(self, x, y):\n+        return x\n+\n+    inp = input_layer_lib.Input(shape=(1,))\n+    inp2 = input_layer_lib.Input(shape=(1,), batch_size=5, dtype=tf.int32)\n+    out = MultiInputModel()(inp, inp2)\n+    m = training_lib.Model(inputs={'x': inp, 'y': inp2}, outputs=out)\n+    input_spec = m.save_spec(dynamic_batch=False)[0][0]\n+    self.assertIn('x', input_spec)\n+    self.assertIn('y', input_spec)\n+    self.assertAllEqual([None, 1], input_spec['x'].shape.as_list())\n+    self.assertAllEqual(tf.float32, input_spec['x'].dtype)\n+    self.assertAllEqual([5, 1], input_spec['y'].shape.as_list())\n+    self.assertAllEqual(tf.int32, input_spec['y'].dtype)\n+\n \n class DeferredModeTest(keras_parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a38a5ab069ffbd4a1d96b0c24eeecb02a3a4632b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1440 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -513,7 +513,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     lookup_data = self._lookup_layer(inputs)\n \n     # For any non-int output, we can return directly from the underlying layer.\n-    if self._output_mode is not INT:\n+    if self._output_mode != INT:\n       return lookup_data\n \n     if self._ragged:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa6bc44e9ac14534d1f9ebb092164b22dec35f87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 648 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -238,6 +238,7 @@ class MultiHeadAttention(Layer):\n     self._bias_initializer = initializers.get(bias_initializer)\n     self._kernel_regularizer = regularizers.get(kernel_regularizer)\n     self._bias_regularizer = regularizers.get(bias_regularizer)\n+    self._activity_regularizer = regularizers.get(activity_regularizer)\n     self._kernel_constraint = constraints.get(kernel_constraint)\n     self._bias_constraint = constraints.get(bias_constraint)\n     if attention_axes is not None and not isinstance(attention_axes,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28a9f02b7f14841a2f6ea0cb485659f6ef039631", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 11757 | Contributors (this commit): 21 | Commits (past 90d): 20 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2173,7 +2173,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     \"\"\"\n     if not self._inbound_nodes:\n       raise AttributeError(f'The layer \"{self.name}\" has never been called '\n-                           'and thus has no defined input shape.')\n+                           'and thus has no defined input shape. Note that the '\n+                           '`input_shape` property is only available for '\n+                           'Functional and Sequential models.')\n     all_input_shapes = set(\n         [str(node.input_shapes) for node in self._inbound_nodes])\n     if len(all_input_shapes) == 1:\n\n@@ -715,6 +715,16 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     self.assertTrue(layer.built)\n     self.assertEqual([None, 3], layer._build_input_shape.as_list())\n \n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_layer_input_shape_raises_error(self):\n+    layer = layers.Dense(3)\n+    with self.assertRaisesRegex(AttributeError, 'no defined input shape'):\n+      _ = layer.input_shape\n+\n+    layer(tf.ones((10, 1)))\n+    with self.assertRaisesRegex(AttributeError, 'no defined input shape'):\n+      _ = layer.input_shape\n+\n   @combinations.generate(combinations.combine(mode=['eager']))\n   def test_custom_layer_training_arg(self):\n     class CustomLayerNoTrainingArg(base_layer.Layer):\n\n@@ -1576,8 +1576,10 @@ class Layer(base_layer.Layer):\n         RuntimeError: if called in Eager mode.\n     \"\"\"\n     if not self._inbound_nodes:\n-      raise AttributeError('The layer has never been called '\n-                           'and thus has no defined input shape.')\n+      raise AttributeError(f'The layer \"{self.name}\" has never been called '\n+                           'and thus has no defined input shape. Note that the '\n+                           '`input_shape` property is only available for '\n+                           'Functional and Sequential models.')\n     all_input_shapes = set(\n         [str(node.input_shapes) for node in self._inbound_nodes])\n     if len(all_input_shapes) == 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a6dea64b66b799a195195dfcd8763d05614c5cbb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 146 | Files Changed: 2 | Hunks: 21 | Methods Changed: 20 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 188 | Churn Cumulative: 7157 | Contributors (this commit): 6 | Commits (past 90d): 18 | Contributors (cumulative): 11 | DMM Complexity: 0.28421052631578947\n\nDIFF:\n@@ -425,45 +425,10 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n \n     oov_start = self._oov_start_index()\n     token_start = self._token_start_index()\n-    should_have_mask = (oov_start > 0)\n-    has_mask = should_have_mask and vocabulary[0] == self.mask_token\n-\n-    should_have_oov = (self.num_oov_indices > 0)\n-    expected_oov = [self.oov_token] * self.num_oov_indices\n-    found_oov = vocabulary[oov_start:token_start]\n-    has_oov = should_have_oov and np.array_equal(found_oov, expected_oov)\n-\n-    if all([should_have_mask, has_mask, should_have_oov]) and not has_oov:\n-      raise ValueError(\n-          \"Invalid vocabulary format. The layer was created with \"\n-          \"`mask_token={mask}` and `oov_token={oov}`. These tokens should be \"\n-          \"included in the provided vocabulary. The passed vocabulary has the \"\n-          \"correct mask token `{mask}` at index 0, but does not have the OOV \"\n-          \"token `{oov}` in indices [{start}:{end}]. Instead, we found \"\n-          \"`{found}`. Was this vocabulary generated by a layer with \"\n-          \"incompatible settings?\".format(\n-              mask=self.mask_token,\n-              oov=self.oov_token,\n-              start=oov_start,\n-              end=token_start,\n-              found=found_oov))\n-\n-    if all([should_have_oov, has_oov, should_have_mask]) and not has_mask:\n-      raise ValueError(\n-          \"Invalid vocabulary format. The layer was created with \"\n-          \"`mask_token={mask}` and `oov_token={oov}`. These tokens should be \"\n-          \"included in the provided vocabulary. The passed vocabulary has the \"\n-          \"correct OOV token `{oov}` at indices [{start}:{end}], but does not \"\n-          \"have the mask token `{mask}` in index 0. Instead, we found \"\n-          \"`{found}`. Was this vocabulary generated by a layer with \"\n-          \"incompatible settings?\".format(\n-              mask=self.mask_token,\n-              oov=self.oov_token,\n-              start=oov_start,\n-              end=token_start,\n-              found=vocabulary[0]))\n-\n-    found_special_tokens = has_oov or has_mask\n+    special_tokens = (\n+        [self.mask_token] * oov_start + [self.oov_token] * self.num_oov_indices)\n+    found_special_tokens = np.array_equal(\n+        special_tokens, vocabulary[:token_start])\n     if found_special_tokens:\n       tokens = vocabulary[token_start:]\n     else:\n@@ -475,20 +440,26 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n                        \"term. Please uniquify your dataset. The repeated terms \"\n                        \"are {}\".format(repeated_tokens))\n \n-    if self.mask_token in tokens:\n-      mask_index = np.argwhere(tokens == self.mask_token)[0]\n-      raise ValueError(\"Reserved mask token {} was found in the passed \"\n-                       \"vocabulary at index {}. Please either remove the \"\n-                       \"reserved token from the vocabulary or change the \"\n-                       \"mask token for this layer.\".format(\n-                           self.mask_token, mask_index))\n-    if self.oov_token in tokens:\n-      oov_index = np.argwhere(tokens == self.oov_token)[0]\n-      raise ValueError(\"Reserved OOV token {} was found in the passed \"\n-                       \"vocabulary at index {}. Please either remove the \"\n-                       \"reserved token from the vocabulary or change the \"\n-                       \"OOV token for this layer.\".format(\n-                           self.oov_token, oov_index))\n+    if self.mask_token is not None and self.mask_token in tokens:\n+      mask_index = np.argwhere(vocabulary == self.mask_token)[-1]\n+      raise ValueError(\n+          \"Found reserved mask token at unexpected location in `vocabulary`. \"\n+          \"Note that passed `vocabulary` does not need to include the OOV and \"\n+          \"mask tokens. Either remove all mask and OOV tokens, or include them \"\n+          \"only at the start of the vocabulary in precisely this order: \"\n+          f\"{special_tokens}. Received: mask_token={self.mask_token} at \"\n+          f\"vocabulary index {mask_index}\")\n+    # Only error out for oov_token when invert=True. When invert=False,\n+    # oov_token is unused during lookup.\n+    if self.oov_token is not None and self.invert and self.oov_token in tokens:\n+      oov_index = np.argwhere(vocabulary == self.oov_token)[-1]\n+      raise ValueError(\n+          \"Found reserved OOV token at unexpected location in `vocabulary`. \"\n+          \"Note that passed `vocabulary` does not need to include the OOV and \"\n+          \"mask tokens. Either remove all mask and OOV tokens, or include them \"\n+          \"only at the start of the vocabulary in precisely this order: \"\n+          f\"{special_tokens}. Received: oov_token={self.oov_token} at \"\n+          f\"vocabulary index {oov_index}\")\n \n     new_vocab_size = token_start + len(tokens)\n     if self.max_tokens is not None and (new_vocab_size > self.max_tokens):\n\n@@ -1628,7 +1628,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n \n   def test_non_unique_vocab_fails(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\", \"fire\"]\n-    with self.assertRaisesRegex(ValueError, \".*repeated term.*fire.*\"):\n+    with self.assertRaisesRegex(ValueError, \"repeated term.*fire\"):\n       _ = index_lookup.IndexLookup(\n           vocabulary=vocab_data,\n           max_tokens=None,\n@@ -1637,39 +1637,6 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n           oov_token=\"[OOV]\",\n           dtype=tf.string)\n \n-  def test_vocab_with_oov_and_wrong_mask_fails(self):\n-    vocab_data = [\"custom_mask\", \"[OOV]\", \"earth\", \"wind\", \"and\", \"fire\"]\n-    layer = index_lookup.IndexLookup(\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        mask_token=\"\",\n-        oov_token=\"[OOV]\",\n-        dtype=tf.string)\n-    with self.assertRaisesRegex(ValueError, \".*does not have the mask token.*\"):\n-      layer.set_vocabulary(vocab_data)\n-\n-  def test_vocab_with_oov_and_no_mask_fails(self):\n-    vocab_data = [\"[OOV]\", \"earth\", \"wind\", \"and\", \"fire\"]\n-    layer = index_lookup.IndexLookup(\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        mask_token=\"\",\n-        oov_token=\"[OOV]\",\n-        dtype=tf.string)\n-    with self.assertRaisesRegex(ValueError, \".*Reserved OOV.*\"):\n-      layer.set_vocabulary(vocab_data)\n-\n-  def test_vocab_with_mask_but_no_oov_fails(self):\n-    vocab_data = [\"\", \"earth\", \"wind\", \"and\", \"fire\"]\n-    layer = index_lookup.IndexLookup(\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        mask_token=\"\",\n-        oov_token=\"[OOV]\",\n-        dtype=tf.string)\n-    with self.assertRaisesRegex(ValueError, \".*does not have the OOV token.*\"):\n-      layer.set_vocabulary(vocab_data)\n-\n   def test_vocab_with_repeated_element_fails(self):\n     vocab_data = [\"earth\", \"earth\", \"wind\", \"and\", \"fire\"]\n     layer = index_lookup.IndexLookup(\n@@ -1678,18 +1645,19 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         dtype=tf.string)\n-    with self.assertRaisesRegex(ValueError, \".*repeated term.*earth.*\"):\n+    with self.assertRaisesRegex(ValueError, \"repeated term.*earth\"):\n       layer.set_vocabulary(vocab_data)\n \n-  def test_vocab_with_reserved_oov_element_fails(self):\n+  def test_vocab_with_reserved_oov_element_and_invert_true_fails(self):\n     vocab_data = [\"earth\", \"test\", \"[OOV]\", \"wind\", \"and\", \"fire\"]\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n+        invert=True,\n         dtype=tf.string)\n-    with self.assertRaisesRegex(ValueError, \".*Reserved OOV.*\"):\n+    with self.assertRaisesRegex(ValueError, \"reserved OOV\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_vocab_with_reserved_mask_element_fails(self):\n@@ -1700,7 +1668,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         mask_token=\"mask_token\",\n         oov_token=\"[OOV]\",\n         dtype=tf.string)\n-    with self.assertRaisesRegex(ValueError, \".*Reserved mask.*\"):\n+    with self.assertRaisesRegex(ValueError, \"reserved mask\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_vocab_size_changed_pad_to_max_false_fails(self):\n@@ -1776,59 +1744,16 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n           oov_token=-1,\n           dtype=tf.int64)\n \n-  def test_int_vocab_with_oov_and_wrong_mask_fails(self):\n-    vocab_data = [1234, -1, 11, 21, 13, 14]\n-    layer = index_lookup.IndexLookup(\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        mask_token=0,\n-        oov_token=-1,\n-        dtype=tf.int64)\n-    with self.assertRaisesRegex(ValueError, \"does not have the mask token `0`\"):\n-      layer.set_vocabulary(vocab_data)\n-\n-  def test_int_vocab_with_oov_and_no_mask_fails(self):\n-    vocab_data = [-1, 11, 12, 13, 14]\n-    layer = index_lookup.IndexLookup(\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        mask_token=0,\n-        oov_token=-1,\n-        dtype=tf.int64)\n-    with self.assertRaisesRegex(ValueError, \"Reserved OOV\"):\n-      layer.set_vocabulary(vocab_data)\n-\n-  def test_int_vocab_with_mask_but_no_oov_fails(self):\n-    vocab_data = [0, 11, 12, 13, 14]\n-    layer = index_lookup.IndexLookup(\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        mask_token=0,\n-        oov_token=-1,\n-        dtype=tf.int64)\n-    with self.assertRaisesRegex(ValueError, \"does not have the OOV token `-1`\"):\n-      layer.set_vocabulary(vocab_data)\n-\n-  def test_int_vocab_with_repeated_element_fails(self):\n-    vocab_data = [11, 11, 34, 23, 124]\n-    layer = index_lookup.IndexLookup(\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        mask_token=0,\n-        oov_token=-1,\n-        dtype=tf.int64)\n-    with self.assertRaisesRegex(ValueError, \"repeated term.*11\"):\n-      layer.set_vocabulary(vocab_data)\n-\n-  def test_int_vocab_with_reserved_oov_element_fails(self):\n+  def test_int_vocab_with_reserved_oov_element_and_invert_true_fails(self):\n     vocab_data = [14, 38, -1, 34, 3, 84]\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1,\n+        invert=True,\n         dtype=tf.int64)\n-    with self.assertRaisesRegex(ValueError, \"Reserved OOV\"):\n+    with self.assertRaisesRegex(ValueError, \"reserved OOV\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_int_vocab_with_reserved_mask_element_fails(self):\n@@ -1839,11 +1764,11 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         mask_token=0,\n         oov_token=-1,\n         dtype=tf.int64)\n-    with self.assertRaisesRegex(ValueError, \"Reserved mask\"):\n+    with self.assertRaisesRegex(ValueError, \"reserved mask\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_no_vocab_file_string_fails(self):\n-    with self.assertRaisesRegex(ValueError, \".*non_existent_file.*\"):\n+    with self.assertRaisesRegex(ValueError, \"non_existent_file\"):\n       _ = index_lookup.IndexLookup(\n           vocabulary=\"non_existent_file\",\n           max_tokens=None,\n@@ -1906,7 +1831,7 @@ class IndexLookupInverseVocabularyTest(\n \n   def test_non_unique_vocab_fails(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\", \"fire\"]\n-    with self.assertRaisesRegex(ValueError, \".*repeated term.*fire.*\"):\n+    with self.assertRaisesRegex(ValueError, \"repeated term.*fire\"):\n       _ = index_lookup.IndexLookup(\n           vocabulary=vocab_data,\n           max_tokens=None,\n@@ -1936,7 +1861,7 @@ class IndexLookupInverseVocabularyTest(\n         oov_token=\"[OOV]\",\n         dtype=tf.string,\n         invert=True)\n-    with self.assertRaisesRegex(ValueError, \".*repeated term.*earth.*\"):\n+    with self.assertRaisesRegex(ValueError, \"repeated term.*earth\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_vocab_with_reserved_mask_element_fails(self):\n@@ -1948,12 +1873,12 @@ class IndexLookupInverseVocabularyTest(\n         oov_token=\"[OOV]\",\n         dtype=tf.string,\n         invert=True)\n-    with self.assertRaisesRegex(ValueError, \".*Reserved mask.*\"):\n+    with self.assertRaisesRegex(ValueError, \"reserved mask\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_non_unique_int_vocab_fails(self):\n     vocab_data = [12, 13, 14, 15, 15]\n-    with self.assertRaisesRegex(ValueError, \".*repeated term.*15.*\"):\n+    with self.assertRaisesRegex(ValueError, \"repeated term.*15\"):\n       _ = index_lookup.IndexLookup(\n           vocabulary=vocab_data,\n           max_tokens=None,\n@@ -1972,7 +1897,7 @@ class IndexLookupInverseVocabularyTest(\n         oov_token=-1,\n         dtype=tf.int64,\n         invert=True)\n-    with self.assertRaisesRegex(ValueError, \".*repeated term.*11.*\"):\n+    with self.assertRaisesRegex(ValueError, \"repeated term.*11\"):\n       layer.set_vocabulary(vocab_data)\n \n \n@@ -1990,11 +1915,11 @@ class IndexLookupErrorTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         dtype=tf.string)\n     with self.assertRaisesRegex(ValueError,\n-                                \"vocabulary larger than the maximum vocab.*\"):\n+                                \"vocabulary larger than the maximum vocab\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_zero_max_tokens_fails(self):\n-    with self.assertRaisesRegex(ValueError, \".*max_tokens.*\"):\n+    with self.assertRaisesRegex(ValueError, \"max_tokens\"):\n       _ = index_lookup.IndexLookup(\n           max_tokens=0,\n           num_oov_indices=1,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bb7798f87205058a26a7f96092e458e32a7fe69a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 59 | Churn Cumulative: 17773 | Contributors (this commit): 116 | Commits (past 90d): 46 | Contributors (cumulative): 122 | DMM Complexity: None\n\nDIFF:\n@@ -72,15 +72,43 @@ class Functional(training_lib.Model):\n \n   Example:\n \n-  ```\n+  ```python\n   inputs = keras.Input(shape=(10,))\n   x = keras.layers.Dense(1)(inputs)\n   outputs = tf.nn.relu(x)\n   model = keras.Model(inputs, outputs)\n   ```\n \n+  A new `Functional` model can also be created by using the\n+  intermediate tensors. This enables you to quickly extract sub-components\n+  of the model.\n+\n+  Example:\n+\n+  ```python\n+  inputs = keras.Input(shape=(None, None, 3))\n+  processed = keras.layers.RandomCrop(width=32, height=32)(inputs)\n+  conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)\n+  pooling = keras.layers.GlobalAveragePooling2D()(conv)\n+  feature = keras.layers.Dense(10)(pooling)\n+\n+  full_model = keras.Model(inputs, feature)\n+  backbone = keras.Model(processed, conv)\n+  activations = keras.Model(conv, feature)\n+  ```\n+\n+  Note that the `backbone` and `activations` models are not\n+  created with `keras.Input` objects, but with the tensors that are originated\n+  from `keras.Inputs` objects. Under the hood, the layers and weights will\n+  be shared across these models, so that user can train the `full_model`, and\n+  use `backbone` or `activations` to do feature extraction.\n+  The inputs and outputs of the model can be nested structures of tensors as\n+  well, and the created models are standard `Functional` model that support\n+  all the existing API.\n+\n   Args:\n-    inputs: List of input tensors (must be created via `tf.keras.Input()`).\n+    inputs: List of input tensors (must be created via `tf.keras.Input()` or\n+      originated from `tf.keras.Input()`).\n     outputs: List of output tensors.\n     name: String, optional. Name of the model.\n     trainable: Boolean, optional. If the model's variables should be trainable.\n\n@@ -90,6 +90,33 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   Note: Only dicts, lists, and tuples of input tensors are supported. Nested\n   inputs are not supported (e.g. lists of list or dicts of dict).\n \n+  A new Functional API model can also be created by using the\n+  intermediate tensors. This enables you to quickly extract sub-components\n+  of the model.\n+\n+  Example:\n+\n+  ```python\n+  inputs = keras.Input(shape=(None, None, 3))\n+  processed = keras.layers.RandomCrop(width=32, height=32)(inputs)\n+  conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)\n+  pooling = keras.layers.GlobalAveragePooling2D()(conv)\n+  feature = keras.layers.Dense(10)(pooling)\n+\n+  full_model = keras.Model(inputs, feature)\n+  backbone = keras.Model(processed, conv)\n+  activations = keras.Model(conv, feature)\n+  ```\n+\n+  Note that the `backbone` and `activations` models are not\n+  created with `keras.Input` objects, but with the tensors that are originated\n+  from `keras.Inputs` objects. Under the hood, the layers and weights will\n+  be shared across these models, so that user can train the `full_model`, and\n+  use `backbone` or `activations` to do feature extraction.\n+  The inputs and outputs of the model can be nested structures of tensors as\n+  well, and the created models are standard Functional API models that support\n+  all the existing APIs.\n+\n   2 - By subclassing the `Model` class: in that case, you should define your\n   layers in `__init__()` and you should implement the model's forward pass\n   in `call()`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#54738c9de91a8ca443030afb2642fde5ac996c91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 145 | Lines Deleted: 46 | Files Changed: 5 | Hunks: 48 | Methods Changed: 20 | Complexity Δ (Sum/Max): 9/3 | Churn Δ: 191 | Churn Cumulative: 11726 | Contributors (this commit): 22 | Commits (past 90d): 24 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1734,7 +1734,7 @@ def disable_generator_for_rng():\n   _USE_GENERATOR_FOR_RNG = False\n \n \n-class RandomGenerator:\n+class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n   \"\"\"Random generator that selects appropriate random ops.\n \n   This class contains the logic for legacy stateful random ops, as well as the\n\n@@ -13,9 +13,8 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Tests for Keras core layers.\"\"\"\n-# pylint: disable=g-bad-import-order\n-import tensorflow.compat.v2 as tf\n \n+import os\n import textwrap\n \n import keras\n@@ -25,6 +24,8 @@ from keras.layers import core\n from keras.mixed_precision import policy\n import numpy as np\n \n+import tensorflow.compat.v2 as tf\n+\n \n @keras_parameterized.run_all_keras_modes\n class DropoutLayersTest(keras_parameterized.TestCase):\n@@ -89,6 +90,44 @@ class DropoutLayersTest(keras_parameterized.TestCase):\n     # Test that dropout mask is shared across second dim.\n     self.assertAllClose(out_np[:, 0, :], out_np[:, 1, :])\n \n+  def test_dropout_with_savemodel(self):\n+    inputs = keras.Input(shape=(5, 10))\n+    layer = keras.layers.Dropout(0.5)\n+    layer._random_generator._force_generator = True\n+    outputs = layer(inputs)\n+    model = keras.Model(inputs, outputs)\n+    train = model(np.ones((20, 5, 10)), training=True)\n+    predict = model(np.ones((20, 5, 10)))\n+    # Make sure the weights from tf.random.Generator is not present in the model\n+    # which will cause weight loading issue for existing application models if\n+    # it contains dropout layer.\n+    self.assertEmpty(layer.get_weights())\n+    self.assertEmpty(model.get_weights())\n+\n+    # Make sure the layer does dropout value when training\n+    self.assertNotAllClose(train, predict)\n+\n+    model.save(os.path.join(self.get_temp_dir(), 'savedmodel'),\n+               save_format='tf')\n+    loaded_model = keras.models.load_model(\n+        os.path.join(self.get_temp_dir(), 'savedmodel'))\n+    predict2 = loaded_model(np.ones((20, 5, 10)))\n+\n+    self.assertAllClose(predict, predict2)\n+    # Make sure the model droput different value after loading\n+    train2 = loaded_model(np.ones((20, 5, 10)), training=True)\n+    self.assertNotAllClose(train, train2)\n+    self.assertIsNotNone(loaded_model.layers[1]._random_generator)\n+\n+    # Also make sure the checkpoint doesn't contain any variable from the\n+    # dropout layer, to keep the backward compatibility.\n+    checkpoint = tf.train.Checkpoint(model)\n+    save_path = checkpoint.save(os.path.join(self.get_temp_dir(), 'checkpoint'))\n+    checkpoint_var_names = [name_value_tuple[0] for name_value_tuple in\n+                            tf.train.list_variables(save_path)]\n+    for name in checkpoint_var_names:\n+      self.assertNotIn('dropout', name)\n+\n \n @keras_parameterized.run_all_keras_modes\n class LambdaLayerTest(keras_parameterized.TestCase):\n\n@@ -74,7 +74,17 @@ class Dropout(Layer):\n       training mode (adding dropout) or in inference mode (doing nothing).\n   \"\"\"\n \n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n+    # Note that the constructor is annotated with\n+    # @no_automatic_dependency_tracking. This is to skip the auto\n+    # tracking of self._random_generator instance, which is an AutoTrackable.\n+    # The backend.RandomGenerator could contain a tf.random.Generator instance\n+    # which will have tf.Variable as the internal state. We want to avoid saving\n+    # that state into model.weights and checkpoints for backward compatibility\n+    # reason. In the meantime, we still need to make them visible to SavedModel\n+    # when it is tracing the tf.function for the `call()`.\n+    # See _list_extra_dependencies_for_serialization below for more details.\n     super(Dropout, self).__init__(**kwargs)\n     if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n       raise ValueError(f'Invalid value {rate} received for '\n@@ -83,6 +93,10 @@ class Dropout(Layer):\n     self.noise_shape = noise_shape\n     self.seed = seed\n     self.supports_masking = True\n+    self._random_generator = backend.RandomGenerator(seed)\n+\n+  def build(self, input_shape):\n+    self._random_generator._maybe_init()  # pylint: disable=protected-access\n \n   def _get_noise_shape(self, inputs):\n     # Subclasses of `Dropout` may implement `_get_noise_shape(self, inputs)`,\n@@ -102,11 +116,8 @@ class Dropout(Layer):\n       training = backend.learning_phase()\n \n     def dropped_inputs():\n-      return tf.nn.dropout(\n-          inputs,\n-          noise_shape=self._get_noise_shape(inputs),\n-          seed=self.seed,\n-          rate=self.rate)\n+      return self._random_generator.dropout(\n+          inputs, self.rate, noise_shape=self._get_noise_shape(inputs))\n \n     output = control_flow_util.smart_cond(training, dropped_inputs,\n                                           lambda: tf.identity(inputs))\n@@ -124,3 +135,10 @@ class Dropout(Layer):\n     base_config = super(Dropout, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n+  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n+    # This method exposes the self._random_generator to SavedModel only\n+    # (not layer.weights and checkpoint).\n+    deps = super()._list_extra_dependencies_for_serialization(\n+        serialization_cache)\n+    deps['_random_generator'] = self._random_generator\n+    return deps\n\n@@ -13,19 +13,46 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Layers that operate regularization via the addition of noise.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n from keras import backend\n-from keras.engine.base_layer import Layer\n+from keras.engine import base_layer\n from keras.utils import tf_utils\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n \n+class BaseRandomLayer(base_layer.Layer):\n+  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, seed=None, **kwargs):\n+    # Note that the constructor is annotated with\n+    # @no_automatic_dependency_tracking. This is to skip the auto\n+    # tracking of self._random_generator instance, which is an AutoTrackable.\n+    # The backend.RandomGenerator could contain a tf.random.Generator instance\n+    # which will have tf.Variable as the internal state. We want to avoid saving\n+    # that state into model.weights and checkpoints for backward compatibility\n+    # reason. In the meantime, we still need to make them visible to SavedModel\n+    # when it is tracing the tf.function for the `call()`.\n+    # See _list_extra_dependencies_for_serialization below for more details.\n+    super().__init__(**kwargs)\n+    self._random_generator = backend.RandomGenerator(seed)\n+\n+  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n+    # This method exposes the self._random_generator to SavedModel\n+    # only (not layer.weights and checkpoint).\n+    deps = super()._list_extra_dependencies_for_serialization(\n+        serialization_cache)\n+    deps['_random_generator'] = self._random_generator\n+    return deps\n+\n+\n @keras_export('keras.layers.GaussianNoise')\n-class GaussianNoise(Layer):\n+class GaussianNoise(BaseRandomLayer):\n   \"\"\"Apply additive zero-centered Gaussian noise.\n \n   This is useful to mitigate overfitting\n@@ -54,11 +81,10 @@ class GaussianNoise(Layer):\n   \"\"\"\n \n   def __init__(self, stddev, seed=None, **kwargs):\n-    super(GaussianNoise, self).__init__(**kwargs)\n+    super(GaussianNoise, self).__init__(seed=seed, **kwargs)\n     self.supports_masking = True\n     self.stddev = stddev\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed)\n \n   def call(self, inputs, training=None):\n \n@@ -82,7 +108,7 @@ class GaussianNoise(Layer):\n \n \n @keras_export('keras.layers.GaussianDropout')\n-class GaussianDropout(Layer):\n+class GaussianDropout(BaseRandomLayer):\n   \"\"\"Apply multiplicative 1-centered Gaussian noise.\n \n   As it is a regularization layer, it is only active at training time.\n@@ -108,11 +134,10 @@ class GaussianDropout(Layer):\n   \"\"\"\n \n   def __init__(self, rate, seed=None, **kwargs):\n-    super(GaussianDropout, self).__init__(**kwargs)\n+    super(GaussianDropout, self).__init__(seed=seed, **kwargs)\n     self.supports_masking = True\n     self.rate = rate\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed)\n \n   def call(self, inputs, training=None):\n     if 0 < self.rate < 1:\n@@ -139,7 +164,7 @@ class GaussianDropout(Layer):\n \n \n @keras_export('keras.layers.AlphaDropout')\n-class AlphaDropout(Layer):\n+class AlphaDropout(BaseRandomLayer):\n   \"\"\"Applies Alpha Dropout to the input.\n \n   Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n@@ -169,12 +194,11 @@ class AlphaDropout(Layer):\n   \"\"\"\n \n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n-    super(AlphaDropout, self).__init__(**kwargs)\n+    super(AlphaDropout, self).__init__(seed=seed, **kwargs)\n     self.rate = rate\n     self.noise_shape = noise_shape\n     self.seed = seed\n     self.supports_masking = True\n-    self._random_generator = backend.RandomGenerator(seed)\n \n   def _get_noise_shape(self, inputs):\n     return self.noise_shape if self.noise_shape else tf.shape(inputs)\n\n@@ -189,9 +189,35 @@ class CenterCrop(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n+class BaseRandomLayer(base_layer.Layer):\n+  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, seed=None, **kwargs):\n+    # Note that the constructor is annotated with\n+    # @no_automatic_dependency_tracking. This is to skip the auto\n+    # tracking of self._random_generator instance, which is an AutoTrackable.\n+    # The backend.RandomGenerator could contain a tf.random.Generator instance\n+    # which will have tf.Variable as the internal state. We want to avoid saving\n+    # that state into model.weights and checkpoints for backward compatibility\n+    # reason. In the meantime, we still need to make them visible to SavedModel\n+    # when it is tracing the tf.function for the `call()`.\n+    # See _list_extra_dependencies_for_serialization below for more details.\n+    super().__init__(**kwargs)\n+    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n+\n+  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n+    # This method exposes the self._random_generator to SavedModel only\n+    # (not layer.weights and checkpoint).\n+    deps = super()._list_extra_dependencies_for_serialization(\n+        serialization_cache)\n+    deps['_random_generator'] = self._random_generator\n+    return deps\n+\n+\n @keras_export('keras.layers.RandomCrop',\n               'keras.layers.experimental.preprocessing.RandomCrop')\n-class RandomCrop(base_layer.Layer):\n+class RandomCrop(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly crops images during training.\n \n   During training, this layer will randomly choose a location to crop images\n@@ -223,11 +249,10 @@ class RandomCrop(base_layer.Layer):\n \n   def __init__(self, height, width, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomCrop').set(True)\n-    super(RandomCrop, self).__init__(**kwargs, autocast=False)\n+    super(RandomCrop, self).__init__(**kwargs, autocast=False, seed=seed)\n     self.height = height\n     self.width = width\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -329,7 +354,7 @@ HORIZONTAL_AND_VERTICAL = 'horizontal_and_vertical'\n \n @keras_export('keras.layers.RandomFlip',\n               'keras.layers.experimental.preprocessing.RandomFlip')\n-class RandomFlip(base_layer.Layer):\n+class RandomFlip(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly flips images during training.\n \n   This layer will flip the images horizontally and or vertically based on the\n@@ -359,7 +384,7 @@ class RandomFlip(base_layer.Layer):\n                mode=HORIZONTAL_AND_VERTICAL,\n                seed=None,\n                **kwargs):\n-    super(RandomFlip, self).__init__(**kwargs)\n+    super(RandomFlip, self).__init__(seed=seed, **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomFlip').set(True)\n     self.mode = mode\n     if mode == HORIZONTAL:\n@@ -375,7 +400,6 @@ class RandomFlip(base_layer.Layer):\n       raise ValueError('RandomFlip layer {name} received an unknown mode '\n                        'argument {arg}'.format(name=self.name, arg=mode))\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -421,7 +445,7 @@ class RandomFlip(base_layer.Layer):\n # TODO(tanzheny): Add examples, here and everywhere.\n @keras_export('keras.layers.RandomTranslation',\n               'keras.layers.experimental.preprocessing.RandomTranslation')\n-class RandomTranslation(base_layer.Layer):\n+class RandomTranslation(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly translates images during training.\n \n   This layer will apply random translations to each image during training,\n@@ -483,7 +507,7 @@ class RandomTranslation(base_layer.Layer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomTranslation').set(\n         True)\n-    super(RandomTranslation, self).__init__(**kwargs)\n+    super(RandomTranslation, self).__init__(seed=seed, **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n       self.height_lower = height_factor[0]\n@@ -518,7 +542,6 @@ class RandomTranslation(base_layer.Layer):\n     self.fill_value = fill_value\n     self.interpolation = interpolation\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -748,7 +771,7 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n \n @keras_export('keras.layers.RandomRotation',\n               'keras.layers.experimental.preprocessing.RandomRotation')\n-class RandomRotation(base_layer.Layer):\n+class RandomRotation(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly rotates images during training.\n \n   This layer will apply random rotations to each image, filling empty space\n@@ -804,7 +827,7 @@ class RandomRotation(base_layer.Layer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomRotation').set(\n         True)\n-    super(RandomRotation, self).__init__(**kwargs)\n+    super(RandomRotation, self).__init__(seed=seed, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.lower = factor[0]\n@@ -820,7 +843,6 @@ class RandomRotation(base_layer.Layer):\n     self.fill_value = fill_value\n     self.interpolation = interpolation\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -875,7 +897,7 @@ class RandomRotation(base_layer.Layer):\n \n @keras_export('keras.layers.RandomZoom',\n               'keras.layers.experimental.preprocessing.RandomZoom')\n-class RandomZoom(base_layer.Layer):\n+class RandomZoom(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly zooms images during training.\n \n   This layer will randomly zoom in or out on each axis of an image\n@@ -942,7 +964,7 @@ class RandomZoom(base_layer.Layer):\n                fill_value=0.0,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomZoom').set(True)\n-    super(RandomZoom, self).__init__(**kwargs)\n+    super(RandomZoom, self).__init__(seed=seed, **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n       self.height_lower = height_factor[0]\n@@ -974,7 +996,6 @@ class RandomZoom(base_layer.Layer):\n     self.fill_value = fill_value\n     self.interpolation = interpolation\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -1082,7 +1103,7 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n \n @keras_export('keras.layers.RandomContrast',\n               'keras.layers.experimental.preprocessing.RandomContrast')\n-class RandomContrast(base_layer.Layer):\n+class RandomContrast(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n   This layer will randomly adjust the contrast of an image or images by a random\n@@ -1115,7 +1136,7 @@ class RandomContrast(base_layer.Layer):\n   def __init__(self, factor, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomContrast').set(\n         True)\n-    super(RandomContrast, self).__init__(**kwargs)\n+    super(RandomContrast, self).__init__(seed=seed, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.lower = factor[0]\n@@ -1126,7 +1147,6 @@ class RandomContrast(base_layer.Layer):\n       raise ValueError('Factor cannot have negative values or greater than 1.0,'\n                        ' got {}'.format(factor))\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -1161,7 +1181,7 @@ class RandomContrast(base_layer.Layer):\n \n @keras_export('keras.layers.RandomHeight',\n               'keras.layers.experimental.preprocessing.RandomHeight')\n-class RandomHeight(base_layer.Layer):\n+class RandomHeight(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image height during training.\n \n   This layer adjusts the height of a batch of images by a random factor.\n@@ -1202,7 +1222,7 @@ class RandomHeight(base_layer.Layer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomHeight').set(True)\n-    super(RandomHeight, self).__init__(**kwargs)\n+    super(RandomHeight, self).__init__(seed=seed, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.height_lower = factor[0]\n@@ -1220,7 +1240,6 @@ class RandomHeight(base_layer.Layer):\n     self.interpolation = interpolation\n     self._interpolation_method = get_interpolation(interpolation)\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -1264,7 +1283,7 @@ class RandomHeight(base_layer.Layer):\n \n @keras_export('keras.layers.RandomWidth',\n               'keras.layers.experimental.preprocessing.RandomWidth')\n-class RandomWidth(base_layer.Layer):\n+class RandomWidth(BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image width during training.\n \n   This layer will randomly adjusts the width of a batch of images of a\n@@ -1305,7 +1324,7 @@ class RandomWidth(base_layer.Layer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomWidth').set(True)\n-    super(RandomWidth, self).__init__(**kwargs)\n+    super(RandomWidth, self).__init__(seed=seed, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.width_lower = factor[0]\n@@ -1322,7 +1341,6 @@ class RandomWidth(base_layer.Layer):\n     self.interpolation = interpolation\n     self._interpolation_method = get_interpolation(interpolation)\n     self.seed = seed\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n \n   def call(self, inputs, training=True):\n     if training is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
