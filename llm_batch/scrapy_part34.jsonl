{"custom_id": "scrapy#baf9a8d846d779b4c1c81fa860f741a019859f48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 411 | Contributors (this commit): 3 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         self.prefix = settings['S3_PREFIX']\n         self.access_key = settings['AWS_ACCESS_KEY_ID']\n         self.secret_key = settings['AWS_SECRET_ACCESS_KEY']\n-        self.image_refresh_days = settings.getint('IMAGES_REFRESH_DAYS', 90)\n+        self.image_refresh_days = settings.getint('IMAGE_EXPIRES', 90)\n         MediaPipeline.__init__(self)\n \n     def s3_request(self, key, method, body=None, headers=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7be6ff0727ac3793fb7add22d131301f1d5355bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 413 | Contributors (this commit): 3 | Commits (past 90d): 17 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         self.prefix = settings['S3_PREFIX']\n         self.access_key = settings['AWS_ACCESS_KEY_ID']\n         self.secret_key = settings['AWS_SECRET_ACCESS_KEY']\n-        self.image_refresh_days = settings.getint('IMAGE_EXPIRES', 90)\n+        self.image_refresh_days = settings.getint('IMAGES_EXPIRES', 90)\n         MediaPipeline.__init__(self)\n \n     def s3_request(self, key, method, body=None, headers=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8fc4719d0c2abe76c00c0f757c97a908115e7737", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 63 | Churn Cumulative: 153 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,52 @@\n+\"\"\"\n+Dns cache module. \n+This module implements a dns cache to improve the performance of the \n+crawler, reducing the dns lookups.\n+\"\"\"\n+import socket\n+from scrapy.core import signals\n+from pydispatch import dispatcher\n+\n+class DNSCache(object):\n+    \"\"\"\n+    DNSCache.\n+\n+    Impelements a DNS Cache to improve the performance of the \n+    DNS lookup that the crawler request when it's running.\n+    Use:\n+     Create a single instance:\n+       >>> import dnscache\n+       >>> cachedns = dnscache.DNSCache()\n+     \n+     To get (and set a new host if not exists) we only call \n+       >> ip = cachedns.get('python.org')\n+       >>> print ip\n+       '82.94.164.162'\n+       >>> \n+    \"\"\"\n+    def __init__(self):\n+        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        self._cache = {}\n+    \n+    def get(self, host):\n+        \"\"\"\n+        Returns the ip associated with the host and save it in\n+        the cache.\n+        \"\"\"\n+        try:\n+            ips = self._cache[host]\n+        except KeyError:\n+            # The socket.gethostbyname_ex throw an\n+            # exception when it can't get the host \n+            # ip. So we save the hostname anyway.\n+            # we use socket.getaddrinfo that support IP4/IP6\n+            try:\n+                ips = list(set([x[4][0] for x in socket.getaddrinfo(host,None)])) \n+            except socket.gaierror:\n+                ips = [host]\n+            self._cache[host] = ips\n+        return ips[0]\n+\n+    def domain_closed(self, domain, spider, status):\n+        if domain in self._cache:\n+            del self._cache[domain]\n\n@@ -16,6 +16,11 @@ from scrapy.core.exceptions import UsageError, HttpException\n from scrapy.utils.defer import defer_succeed\n from scrapy.conf import settings\n \n+from scrapy.core.downloader.dnscache import DNSCache\n+\n+# Cache for dns lookups.\n+dnscache = DNSCache()\n+\n def download_any(request, spider):\n     u = urlparse.urlparse(request.url)\n     if u.scheme == 'file':\n@@ -62,12 +67,14 @@ def download_http(request, spider):\n     factory.deferred.addCallbacks(_on_success, _on_error)\n \n     u = urlparse.urlparse(request.url)\n+    ip = dnscache.get(u.hostname)\n+    port = u.port\n     if u.scheme == 'https' :\n         from twisted.internet import ssl\n         contextFactory = ssl.ClientContextFactory()\n-        reactor.connectSSL(u.hostname, u.port or 443, factory, contextFactory)\n+        reactor.connectSSL(ip, port or 443, factory, contextFactory)\n     else:\n-        reactor.connectTCP(u.hostname, u.port or 80, factory)\n+        reactor.connectTCP(ip, port or 80, factory)\n     return factory.deferred\n \n def download_file(request, spider) :\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1272b138ea3dd028846ab209cdfcaed5c66ff63f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 22 | Files Changed: 4 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): -3/1 | Churn Δ: 54 | Churn Cumulative: 466 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,17 @@\n+\"\"\"\n+HTTP basic auth downloader middleware\n+\n+See documentation in docs/ref/downloader-middleware.rst\n+\"\"\"\n+\n+from scrapy.utils.request import request_authenticate\n+\n class HttpAuthMiddleware(object):\n     \"\"\"This middleware allows spiders to use HTTP auth in a cleaner way\n     (http_user and http_pass spider class attributes)\"\"\"\n \n     def process_request(self, request, spider):\n-        if getattr(spider, 'http_user', None) or getattr(spider, 'http_pass', None):\n-            request.httpauth(spider.http_user, spider.http_pass)\n-\n+        http_user = getattr(spider, 'http_user', '')\n+        http_pass = getattr(spider, 'http_pass', '')\n+        if http_user or http_pass:\n+            request_authenticate(request, http_user, http_pass)\n\n@@ -1,19 +1,18 @@\n import urllib\n from copy import copy\n-from base64 import urlsafe_b64encode\n \n from twisted.internet import defer\n \n from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n from scrapy.utils.url import safe_url_string\n-from scrapy.utils.c14n import canonicalize\n from scrapy.utils.defer import chain_deferred\n \n class Request(object):\n-    def __init__(self, url, callback=None, context=None, method=None, body=None, headers=None, cookies=None,\n-            referer=None, url_encoding='utf-8', link_text='', http_user='', http_pass='', dont_filter=None, \n-            domain=None):\n+\n+    def __init__(self, url, callback=None, context=None, method=None,\n+        body=None, headers=None, cookies=None, referer=None,\n+        url_encoding='utf-8', link_text='', dont_filter=None, domain=None):\n \n         self.encoding = url_encoding  # this one has to be set first\n         self.set_url(url)\n@@ -22,7 +21,8 @@ class Request(object):\n         if method is None and body is not None:\n             method = 'POST' # backwards compatibility\n         self.method = method.upper() if method else 'GET'\n-        assert isinstance(self.method, basestring), 'Request method argument must be str or unicode, got %s: %s' % (type(method), method)\n+        assert isinstance(self.method, basestring), \\\n+             'Request method argument must be str or unicode, got %s: %s' % (type(method), method)\n \n         # body\n         if isinstance(body, dict):\n@@ -45,9 +45,6 @@ class Request(object):\n         # shortcut for setting referer\n         if referer is not None:\n             self.headers['referer'] = referer\n-        # http auth\n-        if http_user or http_pass:\n-            self.httpauth(http_user, http_pass)\n         self.depth = 0\n         self.link_text = link_text\n         #allows to directly specify the spider for the request\n@@ -74,13 +71,6 @@ class Request(object):\n         self._url = Url(safe_url_string(decoded_url, self.encoding))\n     url = property(lambda x: x._url, set_url)\n \n-    def httpauth(self, http_user, http_pass):\n-        if not http_user:\n-            http_user = ''\n-        if not http_pass:\n-            http_pass = ''\n-        self.headers['Authorization'] = 'Basic ' + urlsafe_b64encode(\"%s:%s\" % (http_user, http_pass))\n-\n     def __str__(self):\n         if self.method != 'GET':\n             return \"<(%s) %s>\" % (self.method, self.url)\n\n@@ -1,6 +1,6 @@\n import unittest\n from scrapy.http import Request\n-from scrapy.utils.request import request_fingerprint\n+from scrapy.utils.request import request_fingerprint, request_authenticate\n \n class UtilsRequestTest(unittest.TestCase):\n \n@@ -55,7 +55,10 @@ class UtilsRequestTest(unittest.TestCase):\n         fp2 = request_fingerprint(r2)\n         self.assertNotEqual(fp1, fp2)\n \n+    def test_request_authenticate(self):\n+        r = Request(\"http://www.example.com\")\n+        request_authenticate(r, 'someuser', 'somepass')\n+        self.assertEqual(r.headers['Authorization'], 'Basic c29tZXVzZXI6c29tZXBhc3M=')\n+\n if __name__ == \"__main__\":\n     unittest.main()\n-\n-\n\n@@ -4,6 +4,7 @@ scrapy.http.Request objects\n \"\"\"\n \n import hashlib\n+from base64 import urlsafe_b64encode\n \n from scrapy.utils.url import canonicalize_url\n \n@@ -56,6 +57,13 @@ def request_fingerprint(request, include_headers=()):\n         request._cache[cachekey] = fphash\n         return fphash\n \n+def request_authenticate(request, username, password):\n+    \"\"\"Autenticate the given request (in place) using the HTTP basic access\n+    authentication mechanism (RFC 2617) and the given username and password\n+    \"\"\"\n+    b64userpass = urlsafe_b64encode(\"%s:%s\" % (username, password))\n+    request.headers['Authorization'] = 'Basic ' + b64userpass\n+\n def request_info(request):\n     \"\"\"Return a short string with request info including method, url and\n     fingeprint. Mainly used for debugging\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f89fc10b70e252dd063ff17f9413aec580efa81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 306 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,12 +61,14 @@ class Request(object):\n     def prepend_callback(self, func, *args, **kwargs):\n         if callable(func):\n             func = defer.Deferred().addCallback(func, *args, **kwargs)\n-        assert isinstance(func, defer.Deferred), 'prepend_callback expects a callable or defer.Deferred instance, got %s' % type(func)\n+        assert isinstance(func, defer.Deferred), \\\n+            'prepend_callback expects a callable or defer.Deferred instance, got %s' % type(func)\n         self.deferred = chain_deferred(func, self.deferred)\n         return self.deferred\n \n     def set_url(self, url):\n-        assert isinstance(url, basestring), 'Request url argument must be str or unicode, got %s:' % type(url).__name__\n+        assert isinstance(url, basestring), \\\n+            'Request url argument must be str or unicode, got %s:' % type(url).__name__\n         decoded_url = url if isinstance(url, unicode) else url.decode(self.encoding)\n         self._url = Url(safe_url_string(decoded_url, self.encoding))\n     url = property(lambda x: x._url, set_url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae95c1df68486f7e897f8b57b5f88a97fcbad568", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 15 | Churn Cumulative: 420 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,3 +1,7 @@\n+\"\"\"\n+WARNING: This Scheduler code is obsolete and needs to be rewritten \n+\"\"\"\n+\n from datetime import datetime\n \n from twisted.internet import defer\n@@ -90,6 +94,7 @@ class RulesScheduler(Scheduler):\n \n         d = defer.Deferred()\n         d.addCallbacks(callback, errback)\n-        request.prepend_callback(d)\n+        # prepend_callback Request method was removed (it never worked properly anyways) \n+        #request.prepend_callback(d)\n \n         return request\n\n@@ -58,14 +58,6 @@ class Request(object):\n             return chain_deferred(self.deferred, callback)\n         return self.deferred.addCallback(callback, *args, **kwargs)\n \n-    def prepend_callback(self, func, *args, **kwargs):\n-        if callable(func):\n-            func = defer.Deferred().addCallback(func, *args, **kwargs)\n-        assert isinstance(func, defer.Deferred), \\\n-            'prepend_callback expects a callable or defer.Deferred instance, got %s' % type(func)\n-        self.deferred = chain_deferred(func, self.deferred)\n-        return self.deferred\n-\n     def set_url(self, url):\n         assert isinstance(url, basestring), \\\n             'Request url argument must be str or unicode, got %s:' % type(url).__name__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eef01a9fdd9bb820e5edf1780f4b950d439e5085", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 9 | Churn Cumulative: 323 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,19 +10,14 @@ from scrapy.utils.defer import chain_deferred\n \n class Request(object):\n \n-    def __init__(self, url, callback=None, context=None, method=None,\n+    def __init__(self, url, callback=None, context=None, method='GET',\n         body=None, headers=None, cookies=None, referer=None,\n         url_encoding='utf-8', link_text='', dont_filter=None, domain=None):\n \n         self.encoding = url_encoding  # this one has to be set first\n         self.set_url(url)\n \n-        # method\n-        if method is None and body is not None:\n-            method = 'POST' # backwards compatibility\n-        self.method = method.upper() if method else 'GET'\n-        assert isinstance(self.method, basestring), \\\n-             'Request method argument must be str or unicode, got %s: %s' % (type(method), method)\n+        self.method = method.upper()\n \n         # body\n         if isinstance(body, dict):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9513b1f465899ea1b4208b0cff5897091e056ca6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 7 | Hunks: 8 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1889 | Contributors (this commit): 3 | Commits (past 90d): 34 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ class ValidationError(DropItem):\n             return '%s' % (self.problem)\n \n class ValidationPipeline(object):\n-    def process_item(self, domain, response, item):\n+    def process_item(self, domain, item):\n         item.validate()\n         return item\n \n\n@@ -51,7 +51,7 @@ class ItemSamplerPipeline(object):\n         dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n-    def process_item(self, domain, response, item):\n+    def process_item(self, domain, item):\n         sampled = stats.getpath(\"%s/items_sampled\" % domain, 0)\n         if sampled < items_per_domain:\n             self.items[item.guid] = item\n\n@@ -26,7 +26,7 @@ class MediaPipeline(object):\n     def close_domain(self, domain):\n         del self.domaininfo[domain]\n \n-    def process_item(self, domain, response, item):\n+    def process_item(self, domain, item):\n         info = self.domaininfo[domain]\n         requests = self.get_media_requests(item, info)\n         assert requests is None or hasattr(requests, '__iter__'), \\\n\n@@ -27,7 +27,7 @@ class ShoveItemPipeline(object):\n         dispatcher.connect(self.domain_open, signal=signals.domain_open)\n         dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n \n-    def process_item(self, domain, response, item):\n+    def process_item(self, domain, item):\n         guid = str(item.guid)\n \n         if guid in self.stores[domain]:\n\n@@ -10,6 +10,6 @@ class ShowItemPipeline(object):\n         if not settings['DEBUG_SHOWITEM']:\n             raise NotConfigured\n \n-    def process_item(self, domain, response, item):\n+    def process_item(self, domain, item):\n         log.msg(\"Scraped: \\n%s\" % repr(item), log.DEBUG, domain=domain)\n         return item\n\n@@ -244,7 +244,7 @@ class ExecutionEngine(object):\n                 if isinstance(item, ScrapedItem):\n                     log.msg(\"Scraped %s in <%s>\" % (item, request.url), log.DEBUG, domain=domain)\n                     signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, item=item, spider=spider, response=response)\n-                    piped = self.pipeline.pipe(item, spider, response) # TODO: remove response\n+                    piped = self.pipeline.pipe(item, spider)\n                     piped.addBoth(_onpipelinefinish, item)\n                 elif isinstance(item, Request):\n                     signals.send_catch_log(signal=signals.request_received, sender=self.__class__, request=item, spider=spider, response=response)\n\n@@ -46,7 +46,7 @@ class ItemPipelineManager(object):\n     def domain_is_idle(self, domain):\n         return not self.domaininfo.get(domain)\n \n-    def pipe(self, item, spider, response):\n+    def pipe(self, item, spider):\n         \"\"\"\n         item pipelines are instanceable classes that defines a `pipeline` method\n         that takes ScrapedItem as input and returns ScrapedItem.\n@@ -74,7 +74,7 @@ class ItemPipelineManager(object):\n             current_stage = pipeline.pop(0)\n             log.msg(\"_%s_ Pipeline stage: %s\" % (item, type(current_stage).__name__), log.TRACE, domain=domain)\n \n-            d = mustbe_deferred(current_stage.process_item, domain, response, item)\n+            d = mustbe_deferred(current_stage.process_item, domain, item)\n             d.addCallback(_next_stage)\n             return d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2a7b41cdb2b5dd41391eb2fccdd675cdbd8af833", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 709 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 0.5\n\nDIFF:\n@@ -11,7 +11,7 @@ from scrapy.utils.defer import chain_deferred\n class Request(object):\n \n     def __init__(self, url, callback=None, context=None, method='GET',\n-        body=None, headers=None, cookies=None, referer=None,\n+        body=None, headers=None, cookies=None,\n         url_encoding='utf-8', link_text='', dont_filter=None, domain=None):\n \n         self.encoding = url_encoding  # this one has to be set first\n@@ -37,9 +37,6 @@ class Request(object):\n         self.context = context or {}\n         # dont_filter be filtered by scheduler\n         self.dont_filter = dont_filter\n-        # shortcut for setting referer\n-        if referer is not None:\n-            self.headers['referer'] = referer\n         self.depth = 0\n         self.link_text = link_text\n         #allows to directly specify the spider for the request\n\n@@ -42,9 +42,6 @@ class RequestTest(unittest.TestCase):\n             self.assert_(isinstance(k, str))\n             self.assert_(isinstance(v, str))\n \n-        r = Request(url=\"http://www.example.org/1\", referer=u\"http://www.example.org\")\n-        self.assert_(isinstance(r.headers['referer'], str))\n-\n     def test_eq(self):\n         url = 'http://www.scrapy.org'\n         r1 = Request(url=url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d26a54f54113af552f98ab6a3ecfff28302e54e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 57 | Churn Cumulative: 57 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,32 @@\n+import unittest\n+\n+from BeautifulSoup import BeautifulSoup\n+\n+from scrapy.http import Response\n+from scrapy.contrib.response.soup import ResponseSoup\n+\n+class ResponseSoupTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        ResponseSoup()\n+\n+    def test_response_soup(self):\n+        r1 = Response('example.com', 'http://www.example.com', body='')\n+\n+        soup1 = r1.getsoup()\n+        soup2 = r1.getsoup()\n+\n+        assert isinstance(r1.soup, BeautifulSoup)\n+        assert isinstance(soup2, BeautifulSoup)\n+        # make sure it's cached\n+        assert soup1 is soup2\n+\n+    def test_response_soup_caching(self):\n+        r1 = Response('example.com', 'http://www.example.com', body='')\n+        soup1 = r1.getsoup()\n+        r2 = r1.copy()\n+        soup2 = r1.getsoup()\n+        soup3 = r2.getsoup()\n+\n+        assert soup1 is soup2\n+        assert soup1 is not soup3\n\n@@ -0,0 +1,25 @@\n+import unittest\n+\n+from scrapy.http import Response\n+from scrapy.xpath.extension import ResponseLibxml2\n+\n+class ResponseLibxml2Test(unittest.TestCase):\n+\n+    def setUp(self):\n+        ResponseLibxml2()\n+\n+    def test_response_libxml2_caching(self):\n+        r1 = Response('example.com', 'http://www.example.com', body='<html><head></head><body></body></html>')\n+        r2 = r1.copy()\n+\n+        doc1 = r1.getlibxml2doc()\n+        doc2 = r1.getlibxml2doc()\n+        doc3 = r2.getlibxml2doc()\n+\n+        # make sure it's cached\n+        assert doc1 is doc2\n+        assert doc1 is not doc3\n+\n+        # don't leave libxml2 documents in memory to avoid wrong libxml2 leaks reports\n+        del doc1, doc2, doc3\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ba6238c83a8c1abd4b7ded833d3eafc7e3265a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 31 | Files Changed: 10 | Hunks: 27 | Methods Changed: 14 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 110 | Churn Cumulative: 1412 | Contributors (this commit): 3 | Commits (past 90d): 40 | Contributors (cumulative): 15 | DMM Complexity: 0.9130434782608695\n\nDIFF:\n@@ -16,7 +16,7 @@ class ResponseSoup(object):\n \n def getsoup(response, **kwargs):\n     # TODO: use different cache buckets depending on constructor parameters\n-    if not hasattr(response, '_soup'):\n+    if 'soup' not in response.cache:\n         body = response.body.to_string() if response.body is not None else \"\"\n-        setattr(response, '_soup', BeautifulSoup(body, **kwargs))\n-    return response._soup\n+        response.cache['soup'] = BeautifulSoup(body, **kwargs)\n+    return response.cache['soup']\n\n@@ -20,17 +20,19 @@ class DepthMiddleware(object):\n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n             if isinstance(request, Request):\n-                request.depth = response.request.depth + 1\n-                if self.maxdepth and request.depth > self.maxdepth:\n+                depth = response.request.meta['depth'] + 1\n+                request.meta['depth'] = depth\n+                if self.maxdepth and depth > self.maxdepth:\n                     log.msg(\"Ignoring link (depth > %d): %s \" % (self.maxdepth, request.url), level=log.DEBUG, domain=spider.domain_name)\n                     return False\n                 elif self.stats:\n-                    stats.incpath('%s/request_depth_count/%s' % (spider.domain_name, request.depth))\n-                    if request.depth > stats.getpath('%s/request_depth_max' % spider.domain_name, 0):\n-                        stats.setpath('%s/request_depth_max' % spider.domain_name, request.depth)\n+                    stats.incpath('%s/request_depth_count/%s' % (spider.domain_name, depth))\n+                    if depth > stats.getpath('%s/request_depth_max' % spider.domain_name, 0):\n+                        stats.setpath('%s/request_depth_max' % spider.domain_name, depth)\n             return True\n \n-        if self.stats and response.request.depth == 0: # otherwise we loose stats for depth=0 \n+        if self.stats and 'depth' not in response.request.meta: # otherwise we loose stats for depth=0 \n+            response.request.meta['depth'] = 0\n             stats.incpath('%s/request_depth_count/0' % spider.domain_name)\n \n         return (r for r in result or () if _filter(r))\n\n@@ -90,7 +90,8 @@ class CrawlSpider(BaseSpider):\n                 links = rule.process_links(links)\n             seen = seen.union(links)\n             for link in links:\n-                r = Request(url=link.url, link_text=link.text)\n+                r = Request(url=link.url)\n+                r.meta['link_text'] = link.text\n                 r.append_callback(self._response_downloaded, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n                 requests.append(r)\n         return requests\n\n@@ -1,5 +1,12 @@\n+\"\"\"\n+This module implements the Request class which is used to represent HTTP\n+requests in Scrapy.\n+\n+See documentation in docs/ref/request-response.rst\n+\"\"\"\n+\n import urllib\n-from copy import copy\n+import copy\n \n from twisted.internet import defer\n \n@@ -12,7 +19,7 @@ class Request(object):\n \n     def __init__(self, url, callback=None, context=None, method='GET',\n         body=None, headers=None, cookies=None,\n-        url_encoding='utf-8', link_text='', dont_filter=None, domain=None):\n+        url_encoding='utf-8', dont_filter=None, domain=None):\n \n         self.encoding = url_encoding  # this one has to be set first\n         self.set_url(url)\n@@ -37,13 +44,11 @@ class Request(object):\n         self.context = context or {}\n         # dont_filter be filtered by scheduler\n         self.dont_filter = dont_filter\n-        self.depth = 0\n-        self.link_text = link_text\n         #allows to directly specify the spider for the request\n         self.domain = domain\n \n-        # bucket to store cached data such as fingerprint and others\n-        self._cache = {}\n+        self.meta = {}\n+        self.cache = {}\n         \n     def append_callback(self, callback, *args, **kwargs):\n         if isinstance(callback, defer.Deferred):\n@@ -79,12 +84,12 @@ class Request(object):\n \n     def copy(self):\n         \"\"\"Clone request except `context` attribute\"\"\"\n-        new = copy(self)\n-        new._cache = {}\n+        new = copy.copy(self)\n+        new.cache = {}\n         for att in self.__dict__:\n-            if att not in ['_cache', 'context', 'url', 'deferred']:\n+            if att not in ['cache', 'context', 'url', 'deferred']:\n                 value = getattr(self, att)\n-                setattr(new, att, copy(value))\n+                setattr(new, att, copy.copy(value))\n         new.deferred = defer.Deferred()\n         new.context = self.context # requests shares same context dictionary\n         return new\n\n@@ -1,3 +1,10 @@\n+\"\"\"\n+This module implements the Response class which is used to represent HTTP\n+responses in Scrapy.\n+\n+See documentation in docs/ref/request-response.rst\n+\"\"\"\n+\n import re\n import hashlib\n import copy\n@@ -34,6 +41,8 @@ class Response(object) :\n         self.body = ResponseBody(body, self.headers_encoding())\n         self.cached = False\n         self.request = None # request which originated this response\n+        self.meta = {}\n+        self.cache = {}\n \n     def version(self):\n         \"\"\"A hash of the contents of this response\"\"\"\n@@ -85,6 +94,7 @@ class Response(object) :\n         # Response.__init__ forbids the use of ResponseBody instances\n         if 'body' not in kw:\n             newresp.body = samebody()\n+        newresp.meta = self.meta.copy()\n         return newresp\n \n     def to_string(self):\n\n@@ -79,5 +79,20 @@ class RequestTest(unittest.TestCase):\n         self.assert_(isinstance(r.url, str))\n         self.assertEqual(r.url, \"http://www.scrapy.org/price/%C2%A3\")\n \n+    def test_copy(self):\n+        \"\"\"Test Request copy\"\"\"\n+        \n+        r1 = Request(\"http://www.example.com\")\n+        r1.meta['foo'] = 'bar'\n+        r1.cache['lala'] = 'lolo'\n+        r2 = r1.copy()\n+\n+        assert r1.cache\n+        assert not r2.cache\n+\n+        # make sure meta dict is shallow copied\n+        assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n+        self.assertEqual(r1.meta, r2.meta)\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,7 +1,7 @@\n-from unittest import TestCase, main\n+import unittest\n from scrapy.http import Response, ResponseBody\n \n-class ResponseTest(TestCase):\n+class ResponseTest(unittest.TestCase):\n     def test_init(self):\n         # Response requires domain and url\n         self.assertRaises(Exception, Response)\n@@ -14,7 +14,22 @@ class ResponseTest(TestCase):\n         # test presence of all optional parameters\n         self.assertTrue(isinstance(Response('example.com', 'http://example.com/', original_url='http://example.com/None', headers={}, status=200, body=None), Response))\n \n-class ResponseBodyTest(TestCase):\n+    def test_copy(self):\n+        \"\"\"Test Response copy\"\"\"\n+        \n+        r1 = Response('example.com', \"http://www.example.com\")\n+        r1.meta['foo'] = 'bar'\n+        r1.cache['lala'] = 'lolo'\n+        r2 = r1.copy()\n+\n+        assert r1.cache\n+        assert not r2.cache\n+\n+        # make sure meta dict is shallow copied\n+        assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n+        self.assertEqual(r1.meta, r2.meta)\n+\n+class ResponseBodyTest(unittest.TestCase):\n     unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n \n     def test_encoding(self):\n@@ -34,4 +49,4 @@ class ResponseBodyTest(TestCase):\n         self.assertEqual(cp1251_body.to_string('utf-8'), self.unicode_string.encode('utf-8'))\n \n if __name__ == \"__main__\":\n-    main()\n+    unittest.main()\n\n@@ -15,7 +15,7 @@ class UtilsRequestTest(unittest.TestCase):\n         self.assertEqual(request_fingerprint(r1), request_fingerprint(r2))\n \n         # make sure caching is working\n-        self.assertEqual(request_fingerprint(r1), r1._cache['fingerprint'])\n+        self.assertEqual(request_fingerprint(r1), r1.cache['fingerprint'])\n \n         r1 = Request(\"http://www.example.com/members/offers.html\")\n         r2 = Request(\"http://www.example.com/members/offers.html\")\n\n@@ -43,7 +43,7 @@ def request_fingerprint(request, include_headers=()):\n         cachekey = 'fingerprint'\n \n     try:\n-        return request._cache[cachekey]\n+        return request.cache[cachekey]\n     except KeyError:\n         fp = hashlib.sha1()\n         fp.update(request.method)\n@@ -54,7 +54,7 @@ def request_fingerprint(request, include_headers=()):\n                 fp.update(hdr)\n                 fp.update(request.headers.get(hdr, ''))\n         fphash = fp.hexdigest()\n-        request._cache[cachekey] = fphash\n+        request.cache[cachekey] = fphash\n         return fphash\n \n def request_authenticate(request, username, password):\n\n@@ -12,9 +12,9 @@ class ResponseLibxml2(object):\n         setattr(Response, 'getlibxml2doc', getlibxml2doc)\n \n def getlibxml2doc(response, constructor=xmlDoc_from_html):\n-    attr = '_lx2doc_%s' % constructor.__name__\n-    if not hasattr(response, attr):\n+    cachekey = 'lx2doc_%s' % constructor.__name__\n+    if cachekey not in response.cache:\n         lx2doc = Libxml2Document(response, constructor=constructor)\n-        setattr(response, attr, lx2doc)\n-    return getattr(response, attr)\n+        response.cache[cachekey] = lx2doc\n+    return response.cache[cachekey]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b545381bd7bd51c6102920e2f67ab9671e7ca82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 631 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -250,7 +250,7 @@ class ExecutionEngine(object):\n                     signals.send_catch_log(signal=signals.request_received, sender=self.__class__, request=item, spider=spider, response=response)\n                     self.crawl(request=item, spider=spider, priority=priority)\n                 else:\n-                    log.msg('Garbage found in spider output while processing %s, got type %s' % (request, type(item)), log.DEBUG, domain=domain)\n+                    log.msg(\"Spider must return Request or ScrapedItem objects, got '%s' while processing %s\" % (type(item).__name__, request), log.WARNING, domain=domain)\n \n             class _ResultContainer(object):\n                 def append(self, item):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b1745f49f1b2cfd3078508a30a6f5c55d1f50603", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 20 | Files Changed: 4 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 27 | Churn Cumulative: 744 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -150,11 +150,10 @@ class Cache(object):\n             responseheaders = f.read()\n \n         url = metadata['url']\n-        original_url = metadata.get('original_url', url)\n         headers = Headers(responseheaders)\n         status = metadata['status']\n \n-        response = Response(domain=domain, url=url, original_url=original_url, headers=headers, status=status, body=responsebody)\n+        response = Response(domain=domain, url=url, headers=headers, status=status, body=responsebody)\n         response.cached = True\n         return response\n \n@@ -168,7 +167,6 @@ class Cache(object):\n                 'method': request.method,\n                 'status': response.status,\n                 'domain': response.domain,\n-                'original_url': response.original_url,\n                 'timestamp': datetime.datetime.utcnow(),\n             }\n \n\n@@ -49,7 +49,7 @@ def download_http(request, spider):\n         body = body or ''\n         status = factory.status\n         headers = Headers(factory.response_headers)\n-        r = Response(domain=spider.domain_name, url=request.url, headers=headers, status=status, body=body)\n+        r = Response(domain=spider.domain_name, url=request.url, status=status, headers=headers, body=body)\n         signals.send_catch_log(signal=signals.request_uploaded, sender='download_http', request=request, spider=spider)\n         signals.send_catch_log(signal=signals.response_downloaded, sender='download_http', response=r, spider=spider)\n         return r\n\n@@ -1,6 +1,6 @@\n \"\"\"\n This module implements the Response class which is used to represent HTTP\n-responses in Scrapy.\n+esponses in Scrapy.\n \n See documentation in docs/ref/request-response.rst\n \"\"\"\n@@ -18,22 +18,12 @@ from twisted.web import http\n reason_phrases = http.RESPONSES \n \n class Response(object):\n-    \"\"\"HTTP responses\n \n-    Arguments:\n-        * Domain - the spider domain for the page\n-        * url - the final url for the resource\n-        * original_url - the url requested\n-        * headers - HTTP headers\n-        * status - HTTP status code\n-        * body - Body object containing the content of the response\n-    \"\"\"\n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n \n-    def __init__(self, domain, url, original_url=None, headers=None, status=200, body=None):\n+    def __init__(self, domain, url, status=200, headers=None, body=None):\n         self.domain = domain\n         self.url = Url(url)\n-        self.original_url = Url(original_url) if original_url else url # different if redirected or escaped\n         self.headers = Headers(headers or {})\n         self.status = status\n         # ResponseBody is not meant to be used directly (use .replace instead)\n@@ -58,8 +48,8 @@ class Response(object) :\n                 return encoding.group(1)\n \n     def __repr__(self):\n-        return \"Response(domain=%s, url=%s, original_url=%s, headers=%s, status=%s, body=%s)\" % \\\n-                (repr(self.domain), repr(self.url), repr(self.original_url), repr(self.headers), repr(self.status), repr(self.body))\n+        return \"Response(domain=%s, url=%s, headers=%s, status=%s, body=%s)\" % \\\n+                (repr(self.domain), repr(self.url), repr(self.headers), repr(self.status), repr(self.body))\n \n     def __str__(self):\n         version = '%s..%s' % (self.version()[:4], self.version()[-4:])\n@@ -87,7 +77,6 @@ class Response(object) :\n             return copy.deepcopy(self.body)\n         newresp = Response(kw.get('domain', self.domain),\n                            kw.get('url', self.url),\n-                           original_url=kw.get('original_url', self.original_url),\n                            headers=kw.get('headers', sameheaders()),\n                            status=kw.get('status', self.status),\n                            body=kw.get('body'))\n\n@@ -12,7 +12,7 @@ class ResponseTest(unittest.TestCase):\n         self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body='body'), Response))\n         self.assertRaises(AssertionError, Response, 'example.com', 'http://example.com/', body=ResponseBody('body', 'utf-8'))\n         # test presence of all optional parameters\n-        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', original_url='http://example.com/None', headers={}, status=200, body=None), Response))\n+        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', headers={}, status=200, body=None), Response))\n \n     def test_copy(self):\n         \"\"\"Test Response copy\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da6a24b66281bb9a9ebbd0a0bded6521b2166d24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 117 | Lines Deleted: 67 | Files Changed: 14 | Hunks: 51 | Methods Changed: 40 | Complexity Δ (Sum/Max): 7/2 | Churn Δ: 184 | Churn Cumulative: 3393 | Contributors (this commit): 4 | Commits (past 90d): 66 | Contributors (cumulative): 26 | DMM Complexity: 0.5909090909090909\n\nDIFF:\n@@ -15,6 +15,14 @@ from scrapy.core.exceptions import NotConfigured, HttpException, IgnoreRequest\n from scrapy.utils.request import request_fingerprint\n from scrapy.conf import settings\n \n+class CachedResponse(Response):\n+\n+    def __init__(self, *args, **kwargs):\n+        Response.__init__(self, *args, **kwargs)\n+        self.meta['cached'] = True\n+\n+    def __str__(self):\n+        return \"(cached) \" + Response.__str__(self)\n \n class CacheMiddleware(object):\n     def __init__(self):\n@@ -39,7 +47,6 @@ class CacheMiddleware(object):\n             log.msg(\"Corrupt cache for %s\" % request.url, log.WARNING)\n             response = False\n         if response:\n-            response.cached = True\n             if not 200 <= int(response.status) < 300:\n                 raise HttpException(response.status, None, response)\n             return response\n@@ -50,7 +57,7 @@ class CacheMiddleware(object):\n         if not is_cacheable(request):\n             return response\n \n-        if isinstance(response, Response) and not response.cached:\n+        if isinstance(response, Response) and not response.meta.get('cached'):\n             key = request_fingerprint(request)\n             domain = spider.domain_name\n             self.cache.store(domain, key, request, response)\n@@ -153,8 +160,7 @@ class Cache(object):\n         headers = Headers(responseheaders)\n         status = metadata['status']\n \n-        response = Response(domain=domain, url=url, headers=headers, status=status, body=responsebody)\n-        response.cached = True\n+        response = CachedResponse(domain=domain, url=url, headers=headers, status=status, body=responsebody)\n         return response\n \n     def store(self, domain, key, request, response):\n\n@@ -1,20 +1,19 @@\n from scrapy import log\n+from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n \n class CrawlDebug(object):\n+\n     def __init__(self):\n-        self.enabled = settings.getbool('CRAWL_DEBUG')\n+        raise NotConfigured\n     \n     def process_request(self, request, spider):\n-        if self.enabled:\n         log.msg(\"Crawling %s\" % repr(request), domain=spider.domain_name, level=log.DEBUG)\n \n     def process_exception(self, request, exception, spider):\n-        if self.enabled:\n         log.msg(\"Crawl exception %s in %s\" % (exception, repr(request)), domain=spider.domain_name, level=log.DEBUG)\n \n     def process_response(self, request, response, spider):\n-        if self.enabled:\n-            log.msg(\"Fetched %s from %s\" % (response.info(), repr(request)), domain=spider.domain_name, level=log.DEBUG)\n+        log.msg(\"Fetched %s from %s\" % (response, repr(request)), domain=spider.domain_name, level=log.DEBUG)\n         return response\n \n\n@@ -35,7 +35,7 @@ class HistoryMiddleware(object):\n \n     def process_response(self, request, response, spider):\n         version = request.context.get('history_response_version')\n-        if version == response.version():\n+        if version == self.get_version(response):\n             del request.content['history_response_version']\n             hist = self.historydata.version_info(domain, version)\n             if hist:\n@@ -66,11 +66,13 @@ class HistoryMiddleware(object):\n         if response:\n             redirect_url = response.url\n             parentkey = urlkey(response.request.headers.get('referer')) if response.request else None\n-            version = response.version()\n+            version = self.get_version(response)\n         else:\n             redirect_url, parentkey, version = url, None, None\n         self.historydata.store(domain, key, url, parentkey, version, post_version)\n \n+    def get_version(self, response):\n+        key = hashlib.sha1(response.body.to_string()).hexdigest()\n \n def urlkey(url):\n     \"\"\"Generate a 'key' for a given url\n\n@@ -2,6 +2,7 @@\n WARNING: This Scheduler code is obsolete and needs to be rewritten \n \"\"\"\n \n+import hashlib\n from datetime import datetime\n \n from twisted.internet import defer\n@@ -72,7 +73,8 @@ class RulesScheduler(Scheduler):\n \n         def callback(pagedata):\n             \"\"\"process other callback if we pass the checks\"\"\"\n-            if version == pagedata.version():\n+            \n+            if version == self.get_version(pagedata):\n                 hist = self.historydata.version_info(domain, version)\n                 if hist:\n                     versionkey, created = hist\n@@ -83,7 +85,7 @@ class RulesScheduler(Scheduler):\n                         message = \"skipping %s: unchanged for %s\" % (pagedata.url, delta)\n                         raise IgnoreRequest(message)\n             self.record_visit(domain, request.url, pagedata.url,\n-                              pagedata.parent,  pagedata.version(),\n+                              pagedata.parent,  self.get(pagedata),\n                               post_version)\n             return pagedata\n \n@@ -98,3 +100,6 @@ class RulesScheduler(Scheduler):\n         #request.prepend_callback(d)\n \n         return request\n+\n+    def get_version(self, response):\n+        key = hashlib.sha1(response.body.to_string()).hexdigest()\n\n@@ -47,7 +47,7 @@ def download_http(request, spider):\n \n     def _response(body):\n         body = body or ''\n-        status = factory.status\n+        status = int(factory.status)\n         headers = Headers(factory.response_headers)\n         r = Response(domain=spider.domain_name, url=request.url, status=status, headers=headers, body=body)\n         signals.send_catch_log(signal=signals.request_uploaded, sender='download_http', request=request, spider=spider)\n\n@@ -353,7 +353,7 @@ class ExecutionEngine(object):\n             if isinstance(response, Response):\n                 response.request = request # tie request to obtained response\n                 cached = 'cached' if response.cached else 'live'\n-                log.msg(\"Crawled %s <%s> from <%s>\" % (cached, response.url, request.headers.get('referer')), level=log.DEBUG, domain=domain)\n+                log.msg(\"Crawled %s from <%s>\" % (response, request.headers.get('referer')), level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n                 redirected = response # proper alias\n\n@@ -2,10 +2,10 @@\n Module containing all HTTP related classes\n \n Use this module (instead of the more specific ones) when importing Headers,\n-Request, Response, ResponseBody and Url outside this module.\n+Request, Response and Url outside this module.\n \"\"\"\n \n from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n from scrapy.http.request import Request\n-from scrapy.http.response import Response, ResponseBody\n+from scrapy.http.response import Response\n\n@@ -63,9 +63,10 @@ class Request(object):\n     url = property(lambda x: x._url, set_url)\n \n     def __str__(self):\n-        if self.method != 'GET':\n-            return \"<(%s) %s>\" % (self.method, self.url)\n+        if self.method == 'GET':\n             return \"<%s>\" % self.url\n+        else:\n+            return \"<%s %s>\" % (self.method, self.url)\n \n     def __len__(self):\n         \"\"\"Return raw HTTP request size\"\"\"\n@@ -103,6 +104,7 @@ class Request(object):\n \n         s  = \"%s %s HTTP/1.1\\r\\n\" % (self.method, self.url)\n         s += \"Host: %s\\r\\n\" % self.url.hostname\n+        if self.headers:\n             s += self.headers.to_string() + \"\\r\\n\"\n         s += \"\\r\\n\"\n         if self.body:\n\n@@ -6,17 +6,15 @@ See documentation in docs/ref/request-response.rst\n \"\"\"\n \n import re\n-import hashlib\n import copy\n+from types import NoneType\n \n+from twisted.web.http import RESPONSES\n from BeautifulSoup import UnicodeDammit\n \n from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n \n-from twisted.web import http\n-reason_phrases = http.RESPONSES \n-\n class Response(object):\n \n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n@@ -26,20 +24,17 @@ class Response(object):\n         self.url = Url(url)\n         self.headers = Headers(headers or {})\n         self.status = status\n-        # ResponseBody is not meant to be used directly (use .replace instead)\n-        assert(isinstance(body, basestring) or body is None)\n-        self.body = ResponseBody(body, self.headers_encoding())\n+        if body is not None:\n+            assert isinstance(body, basestring), \\\n+                \"body must be basestring, got %s\" % type(body).__name__\n+            self.body = _ResponseBody(body, self.headers_encoding())\n+        else:\n+            self.body = None\n         self.cached = False\n-        self.request = None # request which originated this response\n+        self.request = None\n         self.meta = {}\n         self.cache = {}\n \n-    def version(self):\n-        \"\"\"A hash of the contents of this response\"\"\"\n-        if not hasattr(self, '_version'):\n-            self._version = hashlib.sha1(self.body.to_string()).hexdigest()\n-        return self._version\n-\n     def headers_encoding(self):\n         content_type = self.headers.get('Content-Type')\n         if content_type:\n@@ -52,39 +47,36 @@ class Response(object):\n                 (repr(self.domain), repr(self.url), repr(self.headers), repr(self.status), repr(self.body))\n \n     def __str__(self):\n-        version = '%s..%s' % (self.version()[:4], self.version()[-4:])\n-        return \"<Response: %s %s (%s)>\" % (self.status, self.url, version)\n+        if self.status == 200:\n+            return \"<%s>\" % (self.url)\n+        else:\n+            return \"<%d %s>\" % (self.status, self.url)\n \n     def __len__(self):\n         \"\"\"Return raw HTTP response size\"\"\"\n         return len(self.to_string())\n \n-    def info(self):\n-        return \"<Response status=%s domain=%s url=%s headers=%s\" % (self.status, self.domain, self.url, self.headers)\n-\n     def copy(self):\n         \"\"\"Create a new Response based on the current one\"\"\"\n         return self.replace()\n \n-    def replace(self, **kw):\n-        \"\"\"Create a new Response with the same attributes except for those given new values.\n+    def replace(self, domain=None, url=None, status=None, headers=None, body=None):\n+        \"\"\"Create a new Response with the same attributes except for those\n+        given new values.\n \n-        Example: newresp = oldresp.replace(body=\"New body\")\n+        Example:\n+\n+        >>> newresp = oldresp.replace(body=\"New body\")\n         \"\"\"\n-        def sameheaders():\n-            return copy.deepcopy(self.headers)\n-        def samebody():\n-            return copy.deepcopy(self.body)\n-        newresp = Response(kw.get('domain', self.domain),\n-                           kw.get('url', self.url),\n-                           headers=kw.get('headers', sameheaders()),\n-                           status=kw.get('status', self.status),\n-                           body=kw.get('body'))\n-        # Response.__init__ forbids the use of ResponseBody instances\n-        if 'body' not in kw:\n-            newresp.body = samebody()\n-        newresp.meta = self.meta.copy()\n-        return newresp\n+        new = self.__class__(domain=domain or self.domain,\n+                             url=url or self.url,\n+                             status=status or self.status,\n+                             headers=headers or copy.deepcopy(self.headers),\n+                             body=body)\n+        if body is None:\n+            new.body = copy.deepcopy(self.body)\n+        new.meta = self.meta.copy()\n+        return new\n \n     def to_string(self):\n         \"\"\"\n@@ -93,7 +85,8 @@ class Response(object):\n         received (that's not exposed by Twisted).\n         \"\"\"\n \n-        s  = \"HTTP/1.1 %s %s\\r\\n\" % (self.status, reason_phrases[int(self.status)])\n+        s  = \"HTTP/1.1 %s %s\\r\\n\" % (self.status, RESPONSES[self.status])\n+        if self.headers:\n             s += self.headers.to_string() + \"\\r\\n\"\n         s += \"\\r\\n\"\n         if self.body:\n@@ -101,10 +94,15 @@ class Response(object):\n             s += \"\\r\\n\"\n         return s\n \n-class ResponseBody(object):\n-    \"\"\"The body of an HTTP response\n+class _ResponseBody(object):\n+    \"\"\"The body of an HTTP response. \n     \n-    This handles conversion to unicode and various character encodings.\n+    WARNING: This is a private class and could be removed in the future without\n+    previous notice. Do not use it this class from outside this module, use\n+    the Response class instead.\n+\n+    Currently, the main purpose of this class is to handle conversion to\n+    unicode and various character encodings.\n     \"\"\"\n \n     _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n@@ -182,7 +180,7 @@ class ResponseBody(object):\n         return proposed\n \n     def __repr__(self):\n-        return \"ResponseBody(content=%s, declared_encoding=%s)\" % (repr(self._content), repr(self.declared_encoding))\n+        return \"_ResponseBody(content=%s, declared_encoding=%s)\" % (repr(self._content), repr(self.declared_encoding))\n \n     def __str__(self):\n         return self.to_string()\n\n@@ -6,6 +6,7 @@ import shutil\n import tempfile\n import tarfile\n import copy\n+import hashlib\n \n from pydispatch import dispatcher\n \n@@ -132,7 +133,7 @@ class Replay(object):\n             self.passed_new[str(item.guid)] = item\n \n     def response_received(self, response, spider):\n-        key = response.version()\n+        key = hashlib.sha1(response.body.to_string()).hexdigest()\n         if (self.recording or self.updating) and key:\n             self.responses_old[key] = response.copy()\n         elif key:\n\n@@ -42,7 +42,7 @@ class AdaptorsTestCase(unittest.TestCase):\n     def get_selector(self, domain, url, sample_filename, headers=None, selector=HtmlXPathSelector):\n         sample_filename = os.path.join(self.samplesdir, sample_filename)\n         body = file(sample_filename).read()\n-        response = Response(domain=domain, url=url, headers=Headers(headers), status='200', body=body)\n+        response = Response(domain=domain, url=url, headers=Headers(headers), status=200, body=body)\n         return selector(response)\n \n     def test_extract(self):\n\n@@ -149,9 +149,9 @@ class EngineTest(unittest.TestCase):\n \n         for response, spider in session.respplug:\n             if session.getpath(response.url) == '/item999.html':\n-                self.assertEqual('404', response.status)\n+                self.assertEqual(404, response.status)\n             if session.getpath(response.url) == '/redirect':\n-                self.assertEqual('302', response.status)\n+                self.assertEqual(302, response.status)\n             self.assertEqual(response.domain, spider.domain_name)\n \n     def test_item_data(self):\n\n@@ -94,5 +94,23 @@ class RequestTest(unittest.TestCase):\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n         self.assertEqual(r1.meta, r2.meta)\n \n+    def test_copy_inherited_classes(self):\n+        \"\"\"Test Request children copies preserve their class\"\"\"\n+\n+        class CustomRequest(Request):\n+            pass\n+\n+        r1 = CustomRequest('example.com', 'http://www.example.com')\n+        r2 = r1.copy()\n+\n+        assert type(r2) is CustomRequest\n+\n+    def test_to_string(self):\n+        r1 = Request(\"http://www.example.com\")\n+        self.assertEqual(r1.to_string(), 'GET http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n+\n+        r1 = Request(\"http://www.example.com\", method='POST', headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n+        self.assertEqual(r1.to_string(), 'POST http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\\r\\n')\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,7 +1,9 @@\n import unittest\n-from scrapy.http import Response, ResponseBody\n+from scrapy.http import Response\n+from scrapy.http.response import _ResponseBody\n \n class ResponseTest(unittest.TestCase):\n+\n     def test_init(self):\n         # Response requires domain and url\n         self.assertRaises(Exception, Response)\n@@ -10,7 +12,6 @@ class ResponseTest(unittest.TestCase):\n         # body can be str or None but not ResponseBody\n         self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body=None), Response))\n         self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body='body'), Response))\n-        self.assertRaises(AssertionError, Response, 'example.com', 'http://example.com/', body=ResponseBody('body', 'utf-8'))\n         # test presence of all optional parameters\n         self.assertTrue(isinstance(Response('example.com', 'http://example.com/', headers={}, status=200, body=None), Response))\n \n@@ -29,12 +30,30 @@ class ResponseTest(unittest.TestCase):\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n         self.assertEqual(r1.meta, r2.meta)\n \n+    def test_copy_inherited_classes(self):\n+        \"\"\"Test Response children copies preserve their class\"\"\"\n+\n+        class CustomResponse(Response):\n+            pass\n+\n+        r1 = CustomResponse('example.com', 'http://www.example.com')\n+        r2 = r1.copy()\n+\n+        assert type(r2) is CustomResponse\n+\n+    def test_to_string(self):\n+        r1 = Response('example.com', \"http://www.example.com\")\n+        self.assertEqual(r1.to_string(), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n+\n+        r1 = Response('example.com', \"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n+        self.assertEqual(r1.to_string(), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\\r\\n')\n+        \n class ResponseBodyTest(unittest.TestCase):\n     unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n \n     def test_encoding(self):\n         original_string = self.unicode_string.encode('cp1251')\n-        cp1251_body     = ResponseBody(original_string, 'cp1251')\n+        cp1251_body     = _ResponseBody(original_string, 'cp1251')\n \n         # check to_unicode\n         self.assertTrue(isinstance(cp1251_body.to_unicode(), unicode))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
