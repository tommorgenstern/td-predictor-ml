{"custom_id": "keras#1c7a0248b90da029f8cf0be504f7262b960c8fc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 809 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -473,6 +473,8 @@ class Graph(Model):\n         x = self._get_x(data)\n         output_list = super(Graph, self).predict(x, batch_size=batch_size,\n                                                  verbose=verbose)\n+        if not isinstance(output_list, list):\n+            output_list = [output_list]\n         return dict(zip(self._graph_outputs, output_list))\n \n     def train_on_batch(self, data,\n@@ -528,6 +530,8 @@ class Graph(Model):\n \n     def predict_on_batch(self, data):\n         output_list = super(Graph, self).predict_on_batch(data)\n+        if not isinstance(output_list, list):\n+            output_list = [output_list]\n         return dict(zip(self._graph_outputs, output_list))\n \n     def fit_generator(self, generator, samples_per_epoch, nb_epoch,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f576f37801af6760931ca0de094535fc884181cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 14 | Churn Cumulative: 1552 | Contributors (this commit): 16 | Commits (past 90d): 13 | Contributors (cumulative): 16 | DMM Complexity: 0.1111111111111111\n\nDIFF:\n@@ -430,8 +430,11 @@ class TensorBoard(Callback):\n         histogram_freq: frequency (in epochs) at which to compute activation\n             histograms for the layers of the model. If set to 0,\n             histograms won't be computed.\n+        write_graph: whether to visualize the graph in tensorboard. The log file can\n+            become quite large when write_graph is set to True.\n     '''\n-    def __init__(self, log_dir='./logs', histogram_freq=0):\n+\n+    def __init__(self, log_dir='./logs', histogram_freq=0, write_graph=True):\n         super(Callback, self).__init__()\n         if K._BACKEND != 'tensorflow':\n             raise Exception('TensorBoard callback only works '\n@@ -439,6 +442,7 @@ class TensorBoard(Callback):\n         self.log_dir = log_dir\n         self.histogram_freq = histogram_freq\n         self.merged = None\n+        self.write_graph = write_graph\n \n     def _set_model(self, model):\n         import tensorflow as tf\n@@ -457,8 +461,16 @@ class TensorBoard(Callback):\n                     tf.histogram_summary('{}_out'.format(layer),\n                                          layer.output)\n         self.merged = tf.merge_all_summaries()\n+        if self.write_graph:\n+            tf_version = tuple(int(i) for i in tf.__version__.split('.'))\n+            if tf_version >= (0, 8, 0):\n+                self.writer = tf.train.SummaryWriter(self.log_dir,\n+                                                     self.sess.graph)\n+            else:\n                 self.writer = tf.train.SummaryWriter(self.log_dir,\n                                                      self.sess.graph_def)\n+        else:\n+            self.writer = tf.train.SummaryWriter(self.log_dir)\n \n     def on_epoch_end(self, epoch, logs={}):\n         import tensorflow as tf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f7e78df65a865084a34fc54a6cb4f78c1592137", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 36 | Files Changed: 1 | Hunks: 14 | Methods Changed: 14 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 88 | Churn Cumulative: 1165 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,6 +29,11 @@ class Optimizer(object):\n             when their absolute value exceeds this value.\n     '''\n     def __init__(self, **kwargs):\n+        allowed_kwargs = {'clipnorm', 'clipvalue'}\n+        for k in kwargs:\n+            if k not in allowed_kwargs:\n+                raise Exception('Unexpected keyword argument '\n+                                'passed to optimizer: ' + str(k))\n         self.__dict__.update(kwargs)\n         self.updates = []\n         self.weights = []\n@@ -89,7 +94,12 @@ class Optimizer(object):\n         return weights\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__}\n+        config = {'name': self.__class__.__name__}\n+        if hasattr(self, 'clipnorm'):\n+            config['clipnorm'] = self.clipnorm\n+        if hasattr(self, 'clipvalue'):\n+            config['clipvalue'] = self.clipvalue\n+        return config\n \n \n class SGD(Optimizer):\n@@ -102,8 +112,8 @@ class SGD(Optimizer):\n         decay: float >= 0. Learning rate decay over each update.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n     '''\n-    def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False,\n-                 *args, **kwargs):\n+    def __init__(self, lr=0.01, momentum=0., decay=0.,\n+                 nesterov=False, **kwargs):\n         super(SGD, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.iterations = K.variable(0.)\n@@ -135,11 +145,12 @@ class SGD(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"lr\": float(K.get_value(self.lr)),\n-                \"momentum\": float(K.get_value(self.momentum)),\n-                \"decay\": float(K.get_value(self.decay)),\n-                \"nesterov\": self.nesterov}\n+        config = {'lr': float(K.get_value(self.lr)),\n+                  'momentum': float(K.get_value(self.momentum)),\n+                  'decay': float(K.get_value(self.decay)),\n+                  'nesterov': self.nesterov}\n+        base_config = super(SGD, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class RMSprop(Optimizer):\n@@ -157,7 +168,7 @@ class RMSprop(Optimizer):\n         rho: float >= 0.\n         epsilon: float >= 0. Fuzz factor.\n     '''\n-    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n+    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, **kwargs):\n         super(RMSprop, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.lr = K.variable(lr)\n@@ -183,10 +194,11 @@ class RMSprop(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"lr\": float(K.get_value(self.lr)),\n-                \"rho\": float(K.get_value(self.rho)),\n-                \"epsilon\": self.epsilon}\n+        config = {'lr': float(K.get_value(self.lr)),\n+                  'rho': float(K.get_value(self.rho)),\n+                  'epsilon': self.epsilon}\n+        base_config = super(RMSprop, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Adagrad(Optimizer):\n@@ -199,7 +211,7 @@ class Adagrad(Optimizer):\n         lr: float >= 0. Learning rate.\n         epsilon: float >= 0.\n     '''\n-    def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n+    def __init__(self, lr=0.01, epsilon=1e-6, **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.lr = K.variable(lr)\n@@ -222,9 +234,10 @@ class Adagrad(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"lr\": float(K.get_value(self.lr)),\n-                \"epsilon\": self.epsilon}\n+        config = {'lr': float(K.get_value(self.lr)),\n+                  'epsilon': self.epsilon}\n+        base_config = super(Adagrad, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Adadelta(Optimizer):\n@@ -242,7 +255,7 @@ class Adadelta(Optimizer):\n     # References\n         - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n     '''\n-    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n+    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.lr = K.variable(lr)\n@@ -275,10 +288,11 @@ class Adadelta(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"lr\": float(K.get_value(self.lr)),\n-                \"rho\": self.rho,\n-                \"epsilon\": self.epsilon}\n+        config = {'lr': float(K.get_value(self.lr)),\n+                  'rho': self.rho,\n+                  'epsilon': self.epsilon}\n+        base_config = super(Adadelta, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Adam(Optimizer):\n@@ -294,8 +308,8 @@ class Adam(Optimizer):\n     # References\n         - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n     '''\n-    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8,\n-                 *args, **kwargs):\n+    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n+                 epsilon=1e-8, **kwargs):\n         super(Adam, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.iterations = K.variable(0)\n@@ -331,11 +345,12 @@ class Adam(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"lr\": float(K.get_value(self.lr)),\n-                \"beta_1\": float(K.get_value(self.beta_1)),\n-                \"beta_2\": float(K.get_value(self.beta_2)),\n-                \"epsilon\": self.epsilon}\n+        config = {'lr': float(K.get_value(self.lr)),\n+                  'beta_1': float(K.get_value(self.beta_1)),\n+                  'beta_2': float(K.get_value(self.beta_2)),\n+                  'epsilon': self.epsilon}\n+        base_config = super(Adam, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class Adamax(Optimizer):\n@@ -352,8 +367,8 @@ class Adamax(Optimizer):\n     # References\n         - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n     '''\n-    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-8,\n-                 *args, **kwargs):\n+    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n+                 epsilon=1e-8, **kwargs):\n         super(Adamax, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.iterations = K.variable(0.)\n@@ -392,11 +407,12 @@ class Adamax(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"lr\": float(K.get_value(self.lr)),\n-                \"beta_1\": float(K.get_value(self.beta_1)),\n-                \"beta_2\": float(K.get_value(self.beta_2)),\n-                \"epsilon\": self.epsilon}\n+        config = {'lr': float(K.get_value(self.lr)),\n+                  'beta_1': float(K.get_value(self.beta_1)),\n+                  'beta_2': float(K.get_value(self.beta_2)),\n+                  'epsilon': self.epsilon}\n+        base_config = super(Adamax, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n # aliases\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#078b20169b1e7bfe86e5ce5f691b75f2b9de127a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 29 | Churn Cumulative: 6049 | Contributors (this commit): 30 | Commits (past 90d): 105 | Contributors (cumulative): 49 | DMM Complexity: 1.0\n\nDIFF:\n@@ -602,6 +602,16 @@ def get_value(x):\n     return x.eval(session=get_session())\n \n \n+def batch_get_value(xs):\n+    '''Returns the value of more than one tensor variable,\n+    as a list of Numpy arrays.\n+    '''\n+    if xs:\n+        return get_session().run(xs)\n+    else:\n+        return []\n+\n+\n def set_value(x, value):\n     '''Sets the value of a tensor variable,\n     from a Numpy array.\n\n@@ -484,6 +484,13 @@ def get_value(x):\n     return x.get_value()\n \n \n+def batch_get_value(xs):\n+    '''Returns the value of more than one tensor variable,\n+    as a list of Numpy arrays.\n+    '''\n+    return [get_value(x) for x in xs]\n+\n+\n def set_value(x, value):\n     x.set_value(np.asarray(value, dtype=x.dtype))\n \n\n@@ -847,10 +847,11 @@ class Layer(object):\n         if not params:\n             return\n         weight_value_tuples = []\n-        for p, w in zip(params, weights):\n-            if K.get_value(p).shape != w.shape:\n+        param_values = K.batch_get_value(params)\n+        for pv, p, w in zip(param_values, params, weights):\n+            if pv.shape != w.shape:\n                 raise Exception('Layer weight shape ' +\n-                                str(K.get_value(p).shape) +\n+                                str(pv.shape) +\n                                 ' not compatible with '\n                                 'provided weight shape ' + str(w.shape))\n             weight_value_tuples.append((p, w))\n@@ -861,10 +862,7 @@ class Layer(object):\n         as a list of numpy arrays.\n         '''\n         params = self.trainable_weights + self.non_trainable_weights\n-        weights = []\n-        for p in params:\n-            weights.append(K.get_value(p))\n-        return weights\n+        return K.batch_get_value(params)\n \n     def get_config(self):\n         '''Returns a Python dictionary (serializable)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8864bfe48d64c15dc70f13e46c7e08772811fd9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 11 | Churn Cumulative: 9424 | Contributors (this commit): 49 | Commits (past 90d): 67 | Contributors (cumulative): 52 | DMM Complexity: 0.0\n\nDIFF:\n@@ -686,7 +686,7 @@ class Model(Container):\n \n     def _make_predict_function(self):\n         if not hasattr(self, 'predict_function'):\n-            raise Exception('You must compile your model before using it.')\n+            self.predict_function = None\n         if self.predict_function is None:\n             if self.uses_learning_phase:\n                 inputs = self.inputs + [K.learning_phase()]\n@@ -694,10 +694,11 @@ class Model(Container):\n                 inputs = self.inputs\n             # returns network outputs. Does not update weights.\n             # Does update the network states.\n+            kwargs = getattr(self, '_function_kwargs', {})\n             self.predict_function = K.function(inputs,\n                                                self.outputs,\n                                                updates=self.state_updates,\n-                                               **self._function_kwargs)\n+                                               **kwargs)\n \n     def _fit_loop(self, f, ins, out_labels=[], batch_size=32,\n                   nb_epoch=100, verbose=1, callbacks=[],\n\n@@ -452,7 +452,7 @@ class Sequential(Model):\n             A Numpy array of predictions.\n         '''\n         if self.model is None:\n-            raise Exception('The model needs to be compiled before being used.')\n+            self.build()\n         return self.model.predict(x, batch_size=batch_size, verbose=verbose)\n \n     def predict_on_batch(self, x):\n@@ -534,8 +534,6 @@ class Sequential(Model):\n         # Returns\n             A Numpy array of probability predictions.\n         '''\n-        if self.model is None:\n-            raise Exception('The model needs to be compiled before being used.')\n         preds = self.predict(x, batch_size, verbose)\n         if preds.min() < 0. or preds.max() > 1.:\n             warnings.warn('Network returning invalid probability values. '\n@@ -557,8 +555,6 @@ class Sequential(Model):\n         # Returns\n             A numpy array of class predictions.\n         '''\n-        if self.model is None:\n-            raise Exception('The model needs to be compiled before being used.')\n         proba = self.predict(x, batch_size=batch_size, verbose=verbose)\n         if proba.shape[-1] > 1:\n             return proba.argmax(axis=-1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18504bcc86c670f41d6ca89a3a5f6e5fed839b20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 142 | Lines Deleted: 29 | Files Changed: 3 | Hunks: 15 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 171 | Churn Cumulative: 4504 | Contributors (this commit): 23 | Commits (past 90d): 36 | Contributors (cumulative): 29 | DMM Complexity: 0.782608695652174\n\nDIFF:\n@@ -0,0 +1,76 @@\n+'''Compare LSTM implementations on the IMDB sentiment classification task.\n+\n+consume_less='cpu' preprocesses input to the LSTM which typically results in\n+faster computations at the expense of increased peak memory usage as the\n+preprocessed input must be kept in memory.\n+\n+consume_less='mem' does away with the preprocessing, meaning that it might take\n+a little longer, but should require less peak memory.\n+\n+consume_less='gpu' concatenates the input, output and forget gate's weights\n+into one, large matrix, resulting in faster computation time as the GPU can\n+utilize more cores, at the expense of reduced regularization because the same\n+dropout is shared across the gates. It should require similar memory usage as\n+consume_less='mem'.\n+'''\n+\n+import time\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+from keras.preprocessing import sequence\n+from keras.models import Sequential\n+from keras.layers import Embedding, BatchNormalization, Dense, LSTM\n+from keras.datasets import imdb\n+\n+max_features = 20000\n+max_length = 80\n+embedding = 400\n+batch_size = 256\n+epochs = 10\n+modes = ['cpu', 'mem', 'gpu']\n+\n+print('Loading data...')\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n+X_train = sequence.pad_sequences(X_train, max_length)\n+X_test = sequence.pad_sequences(X_test, max_length)\n+\n+# Compile and train different models while meauring performance.\n+results = []\n+for mode in modes:\n+    print(\"Testing mode: consume_less='{}'\".format(mode))\n+\n+    model = Sequential()\n+    model.add(Embedding(max_features, embedding, input_length=max_length, dropout=0.2))\n+    model.add(BatchNormalization())\n+    model.add(LSTM(embedding, dropout_W=0.2, dropout_U=0.2, consume_less=mode))\n+    model.add(Dense(1, activation='sigmoid'))\n+    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n+\n+    start_time = time.time()\n+    history = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_data=(X_test, y_test))\n+    average_time_per_epoch = (time.time() - start_time) / epochs\n+\n+    results.append((history, average_time_per_epoch))\n+\n+# Compare models' accuracy, loss and elapsed time per epoch.\n+plt.style.use('ggplot')\n+ax1 = plt.subplot2grid((2,2), (0,0))\n+ax1.set_title('Accuracy')\n+ax1.set_ylabel('Validation Accuracy')\n+ax1.set_xlabel('Epochs')\n+ax2 = plt.subplot2grid((2,2), (1,0))\n+ax2.set_title('Loss')\n+ax2.set_ylabel('Validation Loss')\n+ax2.set_xlabel('Epochs')\n+ax3 = plt.subplot2grid((2,2), (0,1), rowspan=2)\n+ax3.set_title('Time')\n+ax3.set_ylabel('Seconds')\n+for mode, result in zip(modes, results):\n+    ax1.plot(result[0].epoch, result[0].history['val_acc'], label=mode)\n+    ax2.plot(result[0].epoch, result[0].history['val_loss'], label=mode)\n+ax1.legend()\n+ax2.legend()\n+ax3.bar(np.arange(len(results)), [x[1] for x in results], tick_label=modes, align='center')\n+plt.tight_layout()\n+plt.show()\n\n@@ -81,12 +81,20 @@ class Recurrent(Layer):\n             is always unrolled, so this argument does not do anything.\n             Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n             Unrolling is only suitable for short sequences.\n-        consume_less: one of \"cpu\", \"mem\". If set to \"cpu\", the RNN will use\n+        consume_less: one of \"cpu\", \"mem\", or \"gpu\" (LSTM only).\n+            If set to \"cpu\", the RNN will use\n             an implementation that uses fewer, larger matrix products,\n-            thus running faster (at least on CPU) but consuming more memory.\n+            thus running faster on CPU but consuming more memory.\n+\n             If set to \"mem\", the RNN will use more matrix products,\n             but smaller ones, thus running slower (may actually be faster on GPU)\n             while consuming less memory.\n+\n+            If set to \"gpu\" (LSTM only), the LSTM will combine the input gate,\n+            the forget gate and the output gate into a single matrix,\n+            enabling more time-efficient parallelization on the GPU. Note: RNN\n+            dropout must be shared for all gates, resulting in a slightly\n+            reduced regularization.\n         input_dim: dimensionality of the input (integer).\n             This argument (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n@@ -637,8 +645,7 @@ class LSTM(Recurrent):\n \n     def build(self, input_shape):\n         self.input_spec = [InputSpec(shape=input_shape)]\n-        input_dim = input_shape[2]\n-        self.input_dim = input_dim\n+        self.input_dim = input_shape[2]\n \n         if self.stateful:\n             self.reset_states()\n@@ -646,56 +653,64 @@ class LSTM(Recurrent):\n             # initial states: 2 all-zero tensors of shape (output_dim)\n             self.states = [None, None]\n \n-        self.W_i = self.init((input_dim, self.output_dim),\n+        if self.consume_less == 'gpu':\n+            self.W = self.init((self.input_dim, 4*self.output_dim),\n+                               name='{}_W'.format(self.name))\n+            self.U = self.inner_init((self.output_dim, 4*self.output_dim),\n+                                     name='{}_U'.format(self.name))\n+\n+            self.b = K.variable(np.hstack((np.zeros(self.output_dim),\n+                                           K.get_value(self.forget_bias_init(self.output_dim)),\n+                                           np.zeros(self.output_dim),\n+                                           np.zeros(self.output_dim))),\n+                                name='{}_b'.format(self.name))\n+            self.trainable_weights = [self.W, self.U, self.b]\n+        else:\n+            self.W_i = self.init((self.input_dim, self.output_dim),\n                                  name='{}_W_i'.format(self.name))\n             self.U_i = self.inner_init((self.output_dim, self.output_dim),\n                                        name='{}_U_i'.format(self.name))\n             self.b_i = K.zeros((self.output_dim,), name='{}_b_i'.format(self.name))\n \n-        self.W_f = self.init((input_dim, self.output_dim),\n+            self.W_f = self.init((self.input_dim, self.output_dim),\n                                  name='{}_W_f'.format(self.name))\n             self.U_f = self.inner_init((self.output_dim, self.output_dim),\n                                        name='{}_U_f'.format(self.name))\n             self.b_f = self.forget_bias_init((self.output_dim,),\n                                              name='{}_b_f'.format(self.name))\n \n-        self.W_c = self.init((input_dim, self.output_dim),\n+            self.W_c = self.init((self.input_dim, self.output_dim),\n                                  name='{}_W_c'.format(self.name))\n             self.U_c = self.inner_init((self.output_dim, self.output_dim),\n                                        name='{}_U_c'.format(self.name))\n             self.b_c = K.zeros((self.output_dim,), name='{}_b_c'.format(self.name))\n \n-        self.W_o = self.init((input_dim, self.output_dim),\n+            self.W_o = self.init((self.input_dim, self.output_dim),\n                                  name='{}_W_o'.format(self.name))\n             self.U_o = self.inner_init((self.output_dim, self.output_dim),\n                                        name='{}_U_o'.format(self.name))\n             self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n \n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(K.concatenate([self.W_i,\n-                                                        self.W_f,\n-                                                        self.W_c,\n-                                                        self.W_o]))\n-            self.regularizers.append(self.W_regularizer)\n-        if self.U_regularizer:\n-            self.U_regularizer.set_param(K.concatenate([self.U_i,\n-                                                        self.U_f,\n-                                                        self.U_c,\n-                                                        self.U_o]))\n-            self.regularizers.append(self.U_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(K.concatenate([self.b_i,\n-                                                        self.b_f,\n-                                                        self.b_c,\n-                                                        self.b_o]))\n-            self.regularizers.append(self.b_regularizer)\n-\n             self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                       self.W_c, self.U_c, self.b_c,\n                                       self.W_f, self.U_f, self.b_f,\n                                       self.W_o, self.U_o, self.b_o]\n \n+            self.W = K.concatenate([self.W_i, self.W_f, self.W_c, self.W_o])\n+            self.U = K.concatenate([self.U_i, self.U_f, self.U_c, self.U_o])\n+            self.b = K.concatenate([self.b_i, self.b_f, self.b_c, self.b_o])\n+\n+        self.regularizers = []\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+        if self.U_regularizer:\n+            self.U_regularizer.set_param(self.U)\n+            self.regularizers.append(self.U_regularizer)\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -743,16 +758,31 @@ class LSTM(Recurrent):\n         B_U = states[2]\n         B_W = states[3]\n \n+        if self.consume_less == 'gpu':\n+            z = K.dot(x * B_W[0], self.W) + K.dot(h_tm1 * B_U[0], self.U) + self.b\n+\n+            z0 = z[:, :self.output_dim]\n+            z1 = z[:, self.output_dim: 2 * self.output_dim]\n+            z2 = z[:, 2 * self.output_dim: 3 * self.output_dim]\n+            z3 = z[:, 3 * self.output_dim:]\n+\n+            i = self.inner_activation(z0)\n+            f = self.inner_activation(z1)\n+            c = f * c_tm1 + i * self.activation(z2)\n+            o = self.inner_activation(z3)\n+        else:\n             if self.consume_less == 'cpu':\n                 x_i = x[:, :self.output_dim]\n                 x_f = x[:, self.output_dim: 2 * self.output_dim]\n                 x_c = x[:, 2 * self.output_dim: 3 * self.output_dim]\n                 x_o = x[:, 3 * self.output_dim:]\n-        else:\n+            elif self.consume_less == 'mem':\n                 x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n                 x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n                 x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n                 x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n+            else:\n+                raise Exception('Unknown `consume_less` mode.')\n \n             i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n             f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n\n@@ -32,6 +32,13 @@ def _runner(layer_class):\n                        'dropout_W': 0.1},\n                input_shape=(3, 2, 3))\n \n+    # check implementation modes\n+    for mode in ['cpu', 'mem', 'gpu']:\n+        layer_test(layer_class,\n+                   kwargs={'output_dim': output_dim,\n+                           'consume_less': mode},\n+                   input_shape=(3, 2, 3))\n+\n     # check statefulness\n     model = Sequential()\n     model.add(embeddings.Embedding(embedding_num, embedding_dim,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c57aff85b7d4b1c60c94bea0e06025de2eace5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 43 | Files Changed: 2 | Hunks: 12 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 95 | Churn Cumulative: 4279 | Contributors (this commit): 22 | Commits (past 90d): 32 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -10,8 +10,10 @@ a little longer, but should require less peak memory.\n consume_less='gpu' concatenates the input, output and forget gate's weights\n into one, large matrix, resulting in faster computation time as the GPU can\n utilize more cores, at the expense of reduced regularization because the same\n-dropout is shared across the gates. It should require similar memory usage as\n-consume_less='mem'.\n+dropout is shared across the gates.\n+\n+Note that the relative performance of the different `consume_less` modes\n+can vary depending on your device, your model and the size of your data.\n '''\n \n import time\n@@ -20,13 +22,13 @@ import matplotlib.pyplot as plt\n \n from keras.preprocessing import sequence\n from keras.models import Sequential\n-from keras.layers import Embedding, BatchNormalization, Dense, LSTM\n+from keras.layers import Embedding, Dense, LSTM\n from keras.datasets import imdb\n \n max_features = 20000\n max_length = 80\n-embedding = 400\n-batch_size = 256\n+embedding_dim = 256\n+batch_size = 128\n epochs = 10\n modes = ['cpu', 'mem', 'gpu']\n \n@@ -38,17 +40,21 @@ X_test = sequence.pad_sequences(X_test, max_length)\n # Compile and train different models while meauring performance.\n results = []\n for mode in modes:\n-    print(\"Testing mode: consume_less='{}'\".format(mode))\n+    print('Testing mode: consume_less=\"{}\"'.format(mode))\n \n     model = Sequential()\n-    model.add(Embedding(max_features, embedding, input_length=max_length, dropout=0.2))\n-    model.add(BatchNormalization())\n-    model.add(LSTM(embedding, dropout_W=0.2, dropout_U=0.2, consume_less=mode))\n+    model.add(Embedding(max_features, embedding_dim, input_length=max_length, dropout=0.2))\n+    model.add(LSTM(embedding_dim, dropout_W=0.2, dropout_U=0.2, consume_less=mode))\n     model.add(Dense(1, activation='sigmoid'))\n-    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n+    model.compile(loss='binary_crossentropy',\n+                  optimizer='adam',\n+                  metrics=['accuracy'])\n \n     start_time = time.time()\n-    history = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_data=(X_test, y_test))\n+    history = model.fit(X_train, y_train,\n+                        batch_size=batch_size,\n+                        nb_epoch=epochs,\n+                        validation_data=(X_test, y_test))\n     average_time_per_epoch = (time.time() - start_time) / epochs\n \n     results.append((history, average_time_per_epoch))\n@@ -71,6 +77,7 @@ for mode, result in zip(modes, results):\n     ax2.plot(result[0].epoch, result[0].history['val_loss'], label=mode)\n ax1.legend()\n ax2.legend()\n-ax3.bar(np.arange(len(results)), [x[1] for x in results], tick_label=modes, align='center')\n+ax3.bar(np.arange(len(results)), [x[1] for x in results],\n+        tick_label=modes, align='center')\n plt.tight_layout()\n plt.show()\n\n@@ -81,7 +81,9 @@ class Recurrent(Layer):\n             is always unrolled, so this argument does not do anything.\n             Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n             Unrolling is only suitable for short sequences.\n-        consume_less: one of \"cpu\", \"mem\", or \"gpu\" (LSTM only).\n+        consume_less: one of \"cpu\", \"mem\", or \"gpu\".\n+            Note that \"gpu\" mode is only available for LSTM.\n+\n             If set to \"cpu\", the RNN will use\n             an implementation that uses fewer, larger matrix products,\n             thus running faster on CPU but consuming more memory.\n@@ -391,15 +393,15 @@ class SimpleRNN(Recurrent):\n         return constants\n \n     def get_config(self):\n-        config = {\"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"inner_init\": self.inner_init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"dropout_W\": self.dropout_W,\n-                  \"dropout_U\": self.dropout_U}\n+        config = {'output_dim': self.output_dim,\n+                  'init': self.init.__name__,\n+                  'inner_init': self.inner_init.__name__,\n+                  'activation': self.activation.__name__,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'dropout_W': self.dropout_W,\n+                  'dropout_U': self.dropout_U}\n         base_config = super(SimpleRNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -574,16 +576,16 @@ class GRU(Recurrent):\n         return constants\n \n     def get_config(self):\n-        config = {\"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"inner_init\": self.inner_init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"dropout_W\": self.dropout_W,\n-                  \"dropout_U\": self.dropout_U}\n+        config = {'output_dim': self.output_dim,\n+                  'init': self.init.__name__,\n+                  'inner_init': self.inner_init.__name__,\n+                  'activation': self.activation.__name__,\n+                  'inner_activation': self.inner_activation.__name__,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'dropout_W': self.dropout_W,\n+                  'dropout_U': self.dropout_U}\n         base_config = super(GRU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -814,16 +816,16 @@ class LSTM(Recurrent):\n         return constants\n \n     def get_config(self):\n-        config = {\"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"inner_init\": self.inner_init.__name__,\n-                  \"forget_bias_init\": self.forget_bias_init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"dropout_W\": self.dropout_W,\n-                  \"dropout_U\": self.dropout_U}\n+        config = {'output_dim': self.output_dim,\n+                  'init': self.init.__name__,\n+                  'inner_init': self.inner_init.__name__,\n+                  'forget_bias_init': self.forget_bias_init.__name__,\n+                  'activation': self.activation.__name__,\n+                  'inner_activation': self.inner_activation.__name__,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'U_regularizer': self.U_regularizer.get_config() if self.U_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'dropout_W': self.dropout_W,\n+                  'dropout_U': self.dropout_U}\n         base_config = super(LSTM, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5749f1b971c7a7daa019eea3696b9f8c30c3cb98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1741 | Contributors (this commit): 24 | Commits (past 90d): 38 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,10 @@ from theano import tensor as T\n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from theano.tensor.signal import pool\n from theano.tensor.nnet import conv3d2d\n-from theano.sandbox import softsign as T_softsign\n+try:\n+    from theano.tensor.nnet.nnet import softsign as T_softsign\n+except ImportError:\n+    from theano.sandbox.softsign import softsign as T_softsign\n import inspect\n import numpy as np\n from .common import _FLOATX, _EPSILON\n@@ -734,7 +737,7 @@ def softplus(x):\n \n \n def softsign(x):\n-    return T_softsign.softsign(x)\n+    return T_softsign(x)\n \n \n def categorical_crossentropy(output, target, from_logits=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b4e157356a60986f2a38d26d00beb19d76dbbd3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 7577 | Contributors (this commit): 44 | Commits (past 90d): 36 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,10 @@ from .legacy.models import Graph\n \n def model_from_config(config, custom_objects={}):\n     from keras.utils.layer_utils import layer_from_config\n+    if isinstance(config, list):\n+        raise Exception('model_fom_config expects a dictionary.'\n+                        'To load an old-style config use the appropiate'\n+                        '`load_config` method on Sequential or Graph')\n     return layer_from_config(config, custom_objects=custom_objects)\n \n \n@@ -699,7 +703,7 @@ class Sequential(Model):\n \n     def get_config(self):\n         '''Returns the model configuration\n-        as a Python dictionary.\n+        as a Python list.\n         '''\n         config = []\n         if self.layers[0].__class__.__name__ == 'Merge':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c460e1e08ab2ff65678ca0d3b97b578e5afd931", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 979 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,6 @@\n from __future__ import absolute_import\n import sys\n from six.moves import cPickle\n-from six.moves import range\n \n \n def load_batch(fpath, label_key='labels'):\n\n@@ -2,7 +2,6 @@ from __future__ import absolute_import\n import copy\n import inspect\n import types\n-import numpy as np\n \n from ..utils.np_utils import to_categorical\n from ..models import Sequential\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b24f6d7b19c82181b20475087416959077a2190", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 981 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,7 @@\n from __future__ import absolute_import\n import sys\n from six.moves import cPickle\n+from six.moves import range\n \n \n def load_batch(fpath, label_key='labels'):\n\n@@ -2,6 +2,7 @@ from __future__ import absolute_import\n import copy\n import inspect\n import types\n+import numpy as np\n \n from ..utils.np_utils import to_categorical\n from ..models import Sequential\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba8fefa8ec43de03e4fb3b26e77009c3645dd205", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 44 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 119 | Churn Cumulative: 4291 | Contributors (this commit): 23 | Commits (past 90d): 31 | Contributors (cumulative): 23 | DMM Complexity: 0.6818181818181818\n\nDIFF:\n@@ -81,7 +81,7 @@ class Recurrent(Layer):\n             is always unrolled, so this argument does not do anything.\n             Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n             Unrolling is only suitable for short sequences.\n-        consume_less: one of \"cpu\", \"mem\", or \"gpu\" (LSTM only).\n+        consume_less: one of \"cpu\", \"mem\", or \"gpu\" (LSTM/GRU only).\n             If set to \"cpu\", the RNN will use\n             an implementation that uses fewer, larger matrix products,\n             thus running faster on CPU but consuming more memory.\n@@ -90,7 +90,7 @@ class Recurrent(Layer):\n             but smaller ones, thus running slower (may actually be faster on GPU)\n             while consuming less memory.\n \n-            If set to \"gpu\" (LSTM only), the LSTM will combine the input gate,\n+            If set to \"gpu\" (LSTM/GRU only), the RNN will combine the input gate,\n             the forget gate and the output gate into a single matrix,\n             enabling more time-efficient parallelization on the GPU. Note: RNN\n             dropout must be shared for all gates, resulting in a slightly\n@@ -452,53 +452,66 @@ class GRU(Recurrent):\n \n     def build(self, input_shape):\n         self.input_spec = [InputSpec(shape=input_shape)]\n-        input_dim = input_shape[2]\n-        self.input_dim = input_dim\n+        self.input_dim = input_shape[2]\n \n-        self.W_z = self.init((input_dim, self.output_dim),\n-                             name='{}_W_z'.format(self.name))\n-        self.U_z = self.inner_init((self.output_dim, self.output_dim),\n-                                   name='{}_U_z'.format(self.name))\n-        self.b_z = K.zeros((self.output_dim,), name='{}_b_z'.format(self.name))\n-\n-        self.W_r = self.init((input_dim, self.output_dim),\n-                             name='{}_W_r'.format(self.name))\n-        self.U_r = self.inner_init((self.output_dim, self.output_dim),\n-                                   name='{}_U_r'.format(self.name))\n-        self.b_r = K.zeros((self.output_dim,), name='{}_b_r'.format(self.name))\n-\n-        self.W_h = self.init((input_dim, self.output_dim),\n-                             name='{}_W_h'.format(self.name))\n-        self.U_h = self.inner_init((self.output_dim, self.output_dim),\n-                                   name='{}_U_h'.format(self.name))\n-        self.b_h = K.zeros((self.output_dim,), name='{}_b_h'.format(self.name))\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(K.concatenate([self.W_z,\n-                                                        self.W_r,\n-                                                        self.W_h]))\n-            self.regularizers.append(self.W_regularizer)\n-        if self.U_regularizer:\n-            self.U_regularizer.set_param(K.concatenate([self.U_z,\n-                                                        self.U_r,\n-                                                        self.U_h]))\n-            self.regularizers.append(self.U_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(K.concatenate([self.b_z,\n-                                                        self.b_r,\n-                                                        self.b_h]))\n-            self.regularizers.append(self.b_regularizer)\n-\n-        self.trainable_weights = [self.W_z, self.U_z, self.b_z,\n-                                  self.W_r, self.U_r, self.b_r,\n-                                  self.W_h, self.U_h, self.b_h]\n         if self.stateful:\n             self.reset_states()\n         else:\n             # initial states: all-zero tensor of shape (output_dim)\n             self.states = [None]\n \n+        if self.consume_less == 'gpu':\n+\n+            self.W = self.init((self.input_dim, 3 * self.output_dim),\n+                               name='{}_W'.format(self.name))\n+            self.U = self.inner_init((self.output_dim, 3 * self.output_dim),\n+                                     name='{}_U'.format(self.name))\n+\n+            self.b = K.variable(np.hstack((np.zeros(self.output_dim),\n+                                           np.zeros(self.output_dim),\n+                                           np.zeros(self.output_dim))),\n+                                name='{}_b'.format(self.name))\n+\n+            self.trainable_weights = [self.W, self.U, self.b]\n+        else:\n+\n+            self.W_z = self.init((self.input_dim, self.output_dim),\n+                                 name='{}_W_z'.format(self.name))\n+            self.U_z = self.inner_init((self.output_dim, self.output_dim),\n+                                       name='{}_U_z'.format(self.name))\n+            self.b_z = K.zeros((self.output_dim,), name='{}_b_z'.format(self.name))\n+\n+            self.W_r = self.init((self.input_dim, self.output_dim),\n+                                 name='{}_W_r'.format(self.name))\n+            self.U_r = self.inner_init((self.output_dim, self.output_dim),\n+                                       name='{}_U_r'.format(self.name))\n+            self.b_r = K.zeros((self.output_dim,), name='{}_b_r'.format(self.name))\n+\n+            self.W_h = self.init((self.input_dim, self.output_dim),\n+                                 name='{}_W_h'.format(self.name))\n+            self.U_h = self.inner_init((self.output_dim, self.output_dim),\n+                                       name='{}_U_h'.format(self.name))\n+            self.b_h = K.zeros((self.output_dim,), name='{}_b_h'.format(self.name))\n+\n+            self.trainable_weights = [self.W_z, self.U_z, self.b_z,\n+                                      self.W_r, self.U_r, self.b_r,\n+                                      self.W_h, self.U_h, self.b_h]\n+\n+            self.W = K.concatenate([self.W_z, self.W_r, self.W_h])\n+            self.U = K.concatenate([self.U_z, self.U_r, self.U_h])\n+            self.b = K.concatenate([self.b_z, self.b_r, self.b_h])\n+\n+        self.regularizers = []\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+        if self.U_regularizer:\n+            self.U_regularizer.set_param(self.U)\n+            self.regularizers.append(self.U_regularizer)\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -536,15 +549,33 @@ class GRU(Recurrent):\n         B_U = states[1]  # dropout matrices for recurrent units\n         B_W = states[2]\n \n+        if self.consume_less == 'gpu':\n+\n+            matrix_x = K.dot(x * B_W[0], self.W) + self.b\n+            matrix_inner = K.dot(h_tm1 * B_U[0], self.U[:, :2 * self.output_dim])\n+\n+            x_z = matrix_x[:, :self.output_dim]\n+            x_r = matrix_x[:, self.output_dim: 2 * self.output_dim]\n+            inner_z = matrix_inner[:, :self.output_dim]\n+            inner_r = matrix_inner[:, self.output_dim: 2 * self.output_dim]\n+\n+            z = self.inner_activation(x_z + inner_z)\n+            r = self.inner_activation(x_r + inner_r)\n+\n+            x_h = matrix_x[:, 2 * self.output_dim:]\n+            inner_h = K.dot(r * h_tm1 * B_U[0], self.U[:, 2 * self.output_dim:])\n+            hh = self.activation(x_h + inner_h)\n+        else:\n             if self.consume_less == 'cpu':\n                 x_z = x[:, :self.output_dim]\n                 x_r = x[:, self.output_dim: 2 * self.output_dim]\n                 x_h = x[:, 2 * self.output_dim:]\n-        else:\n+            elif self.consume_less == 'mem':\n                 x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n                 x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n                 x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n-\n+            else:\n+                raise Exception('Unknown `consume_less` mode.')\n             z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n             r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
