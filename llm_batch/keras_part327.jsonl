{"custom_id": "keras#9c266106163390f173625c4e7b1ccb03ae145ffc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 37 | Files Changed: 7 | Hunks: 37 | Methods Changed: 37 | Complexity Δ (Sum/Max): 17/8 | Churn Δ: 162 | Churn Cumulative: 31838 | Contributors (this commit): 172 | Commits (past 90d): 65 | Contributors (cumulative): 201 | DMM Complexity: 0.6379310344827587\n\nDIFF:\n@@ -4066,7 +4066,8 @@ def function(inputs, outputs, updates=None, name=None, **kwargs):\n       outs = model(model_inputs)\n       if wrap_outputs:\n         outs = [outs]\n-      return tf_utils.to_numpy_or_python_type(outs)\n+      return tf_utils.sync_to_numpy_or_python_type(outs)\n+\n     return func\n \n   if kwargs:\n\n@@ -222,6 +222,8 @@ class CallbackList:\n         cb._implements_predict_batch_hooks() for cb in self.callbacks)\n     # pylint: enable=protected-access\n \n+    self._disallow_batch_hooks_in_ps_strategy()\n+\n     # Performance check: Check batch hooks for slowness compared to batch time.\n     # Only run check for custom callbacks (i.e. not present in this file).\n     self._check_timing = any(\n@@ -336,7 +338,7 @@ class CallbackList:\n         hook(batch, logs)\n       else:\n         if numpy_logs is None:  # Only convert once.\n-          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n+          numpy_logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         hook(batch, numpy_logs)\n \n     if self._check_timing:\n@@ -387,7 +389,7 @@ class CallbackList:\n         callback.on_epoch_begin(epoch, logs)\n       else:\n         if numpy_logs is None:  # Only convert once.\n-          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n+          numpy_logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         callback.on_epoch_begin(epoch, numpy_logs)\n \n   def on_epoch_end(self, epoch, logs=None):\n@@ -408,7 +410,7 @@ class CallbackList:\n         callback.on_epoch_end(epoch, logs)\n       else:\n         if numpy_logs is None:  # Only convert once.\n-          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n+          numpy_logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         callback.on_epoch_end(epoch, numpy_logs)\n \n   def on_train_batch_begin(self, batch, logs=None):\n@@ -491,7 +493,7 @@ class CallbackList:\n         callback.on_train_begin(logs)\n       else:\n         if numpy_logs is None:  # Only convert once.\n-          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n+          numpy_logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         callback.on_train_begin(numpy_logs)\n \n   def on_train_end(self, logs=None):\n@@ -508,7 +510,7 @@ class CallbackList:\n         callback.on_train_end(logs)\n       else:\n         if numpy_logs is None:  # Only convert once.\n-          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n+          numpy_logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         callback.on_train_end(numpy_logs)\n \n   def on_test_begin(self, logs=None):\n@@ -525,7 +527,7 @@ class CallbackList:\n         callback.on_test_begin(logs)\n       else:\n         if numpy_logs is None:  # Only convert once.\n-          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n+          numpy_logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         callback.on_test_begin(numpy_logs)\n \n   def on_test_end(self, logs=None):\n@@ -542,7 +544,7 @@ class CallbackList:\n         callback.on_test_end(logs)\n       else:\n         if numpy_logs is None:  # Only convert once.\n-          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n+          numpy_logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         callback.on_test_end(numpy_logs)\n \n   def on_predict_begin(self, logs=None):\n@@ -559,7 +561,7 @@ class CallbackList:\n         callback.on_predict_begin(logs)\n       else:\n         if numpy_logs is None:  # Only convert once.\n-          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n+          numpy_logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         callback.on_predict_begin(numpy_logs)\n \n   def on_predict_end(self, logs=None):\n@@ -576,12 +578,32 @@ class CallbackList:\n         callback.on_predict_end(logs)\n       else:\n         if numpy_logs is None:  # Only convert once.\n-          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n+          numpy_logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         callback.on_predict_end(numpy_logs)\n \n   def __iter__(self):\n     return iter(self.callbacks)\n \n+  def _disallow_batch_hooks_in_ps_strategy(self):\n+    \"\"\"Error out if batch-level callbacks are passed with PSStrategy.\"\"\"\n+    # pylint: disable=protected-access\n+    strategy = tf.distribute.get_strategy()\n+    if strategy._should_use_with_coordinator:\n+      unsupported_callbacks = []\n+      for cb in self.callbacks:\n+        # These Callbacks can accept RemoteValues directly.\n+        if getattr(cb, '_supports_tf_logs', False):\n+          continue\n+        if (cb._implements_train_batch_hooks() or\n+            cb._implements_test_batch_hooks() or\n+            cb._implements_predict_batch_hooks()):\n+          unsupported_callbacks.append(cb)\n+      if unsupported_callbacks:\n+        raise ValueError('Batch-level `Callback`s are not supported with '\n+                         '`ParameterServerStrategy`. Found unsupported '\n+                         'callbacks: {}'.format(unsupported_callbacks))\n+    # pylint: enable=protected-access\n+\n \n @keras_export('keras.callbacks.Callback')\n class Callback:\n@@ -933,7 +955,7 @@ class TerminateOnNaN(Callback):\n     logs = logs or {}\n     loss = logs.get('loss')\n     if loss is not None:\n-      loss = tf_utils.to_numpy_or_python_type(loss)\n+      loss = tf_utils.sync_to_numpy_or_python_type(loss)\n       if np.isnan(loss) or np.isinf(loss):\n         print('Batch %d: Invalid loss, terminating training' % (batch))\n         self.model.stop_training = True\n@@ -1083,11 +1105,11 @@ class ProgbarLogger(Callback):\n \n     if self.verbose == 1:\n       # Only block async when verbose = 1.\n-      logs = tf_utils.to_numpy_or_python_type(logs)\n+      logs = tf_utils.sync_to_numpy_or_python_type(logs)\n       self.progbar.update(self.seen, list(logs.items()), finalize=False)\n \n   def _finalize_progbar(self, logs, counter):\n-    logs = tf_utils.to_numpy_or_python_type(logs or {})\n+    logs = tf_utils.sync_to_numpy_or_python_type(logs or {})\n     if self.target is None:\n       if counter is not None:\n         counter = counter.numpy()\n@@ -1382,7 +1404,7 @@ class ModelCheckpoint(Callback):\n     if isinstance(self.save_freq,\n                   int) or self.epochs_since_last_save >= self.period:\n       # Block only when saving interval is reached.\n-      logs = tf_utils.to_numpy_or_python_type(logs)\n+      logs = tf_utils.sync_to_numpy_or_python_type(logs)\n       self.epochs_since_last_save = 0\n       filepath = self._get_file_path(epoch, logs)\n \n\n@@ -100,10 +100,16 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n                  x=None,\n                  steps_per_epoch=10,\n                  run_eagerly=False,\n-                 with_normalization_layer=False):\n-    model, callbacks = self._model_compile(strategy, steps_per_execution,\n+                 with_normalization_layer=False,\n+                 callbacks=None):\n+    if callbacks is None:\n+      callbacks = []\n+\n+    model, default_callbacks = self._model_compile(strategy,\n+                                                   steps_per_execution,\n                                                    run_eagerly,\n                                                    with_normalization_layer)\n+    callbacks += default_callbacks\n \n     def dataset_fn(input_context):\n       del input_context\n@@ -118,7 +124,6 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n         x,\n         epochs=10,\n         steps_per_epoch=steps_per_epoch,\n-        verbose=0,\n         callbacks=callbacks,\n         validation_data=validation_data)\n     return model\n@@ -200,6 +205,53 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n     self.assertIs(strategy._cluster_coordinator,\n                   tf.distribute.experimental.coordinator.ClusterCoordinator(strategy))\n \n+  def testModelFitErrorOnBatchLevelCallbacks(self, strategy):\n+\n+    class BatchLevelCallback(callbacks_lib.Callback):\n+\n+      def on_train_batch_end(self, batch, logs=None):\n+        pass\n+\n+    with self.assertRaisesRegex(ValueError,\n+                                \"Batch-level `Callback`s are not supported\"):\n+      callbacks = [BatchLevelCallback()]\n+      self._model_fit(strategy, callbacks=callbacks)\n+\n+  def testModelFitCallbackSupportsTFLogs(self, strategy):\n+\n+    class MyCallback(callbacks_lib.Callback):\n+\n+      def __init__(self):\n+        super(MyCallback, self).__init__()\n+        # Fetches the RemoteValues if necessary.\n+        self._supports_tf_logs = True\n+\n+      def on_train_batch_end(self, batch, logs=None):\n+        assert isinstance(logs, tf.distribute.experimental.coordinator.RemoteValue)\n+\n+    my_callback = MyCallback()\n+    callbacks = [my_callback]\n+    self._model_fit(strategy, callbacks=callbacks)\n+\n+  def testModelFitVerbosity(self, strategy):\n+\n+    class MyCallback(callbacks_lib.Callback):\n+      pass\n+\n+    my_callback = MyCallback()\n+    callbacks = [my_callback]\n+    self._model_fit(strategy, callbacks=callbacks)\n+    # PSStrategy should default to epoch-level logging.\n+    self.assertEqual(my_callback.params[\"verbose\"], 2)\n+\n+  def testModelFitTensorBoardEpochLevel(self, strategy):\n+    log_dir = self.get_temp_dir()\n+    callbacks = [callbacks_lib.TensorBoard(log_dir)]\n+    self._model_fit(strategy, callbacks=callbacks)\n+    self.assertTrue(tf.compat.v1.gfile.Exists(log_dir))\n+    files = tf.compat.v1.gfile.ListDirectory(log_dir)\n+    self.assertGreaterEqual(len(files), 1)\n+\n \n if __name__ == \"__main__\":\n   tf.compat.v1.enable_v2_behavior()\n\n@@ -1313,9 +1313,6 @@ class DataHandler(object):\n           \"`steps_per_execution > 1`, you must specify the number of steps \"\n           \"to run.\")\n \n-  def resolve_logs(self, logs):\n-    return logs\n-\n \n class _ClusterCoordinatorDataHandler(DataHandler):\n   \"\"\"A `DataHandler` that is compatible with `ClusterCoordinator`.\"\"\"\n@@ -1344,9 +1341,6 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n   def sync(self):\n     self._model._cluster_coordinator.join()  # pylint: disable=protected-access\n \n-  def resolve_logs(self, logs):\n-    return logs.fetch()\n-\n \n def get_data_handler(*args, **kwargs):\n   if getattr(kwargs[\"model\"], \"_cluster_coordinator\", None):\n\n@@ -854,7 +854,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           y=None,\n           batch_size=None,\n           epochs=1,\n-          verbose=1,\n+          verbose='auto',\n           callbacks=None,\n           validation_split=0.,\n           validation_data=None,\n@@ -915,11 +915,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             The model is not trained for a number of iterations\n             given by `epochs`, but merely until the epoch\n             of index `epochs` is reached.\n-        verbose: 0, 1, or 2. Verbosity mode.\n+        verbose: 'auto', 0, 1, or 2. Verbosity mode.\n             0 = silent, 1 = progress bar, 2 = one line per epoch.\n-            Note that the progress bar is not particularly useful when\n-            logged to a file, so verbose=2 is recommended when not running\n-            interactively (eg, in a production environment).\n+            'auto' defaults to 1 for most cases, but 2 when used with\n+            `ParameterServerStrategy`. Note that the progress bar is not\n+            particularly useful when logged to a file, so verbose=2 is\n+            recommended when not running interactively (eg, in a production\n+            environment).\n         callbacks: List of `keras.callbacks.Callback` instances.\n             List of callbacks to apply during training.\n             See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n@@ -927,6 +929,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             and need not be passed into `model.fit`.\n             `tf.keras.callbacks.ProgbarLogger` is created or not based on\n             `verbose` argument to `model.fit`.\n+            Callbacks with batch-level calls are currently unsupported with\n+            `tf.distribute.experimental.ParameterServerStrategy`, and users are\n+            advised to implement epoch-level calls instead with an appropriate\n+            `steps_per_epoch` value.\n         validation_split: Float between 0 and 1.\n             Fraction of the training data to be used as validation data.\n             The model will set apart this fraction of the training data,\n@@ -1075,6 +1081,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self._check_call_args('fit')\n     _disallow_inside_tf_function('fit')\n \n+    if verbose == 'auto':\n+      if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n+        verbose = 2  # Default to epoch-level logging for PSStrategy.\n+      else:\n+        verbose = 1  # Default to batch-level logging otherwise.\n+\n     if validation_split:\n       # Create the validation data using the training data. Only supported for\n       # `Tensor` and `NumPy` input.\n@@ -1152,7 +1164,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               if self.stop_training:\n                 break\n \n-        logs = data_handler.resolve_logs(logs)\n+        logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         if logs is None:\n           raise ValueError('Expect x to be a non-empty array or dataset.')\n         epoch_logs = copy.copy(logs)\n@@ -1455,7 +1467,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               logs = tmp_logs  # No error, now safe to assign to logs.\n               end_step = step + data_handler.step_increment\n               callbacks.on_test_batch_end(end_step, logs)\n-      logs = tf_utils.to_numpy_or_python_type(logs)\n+      logs = tf_utils.sync_to_numpy_or_python_type(logs)\n       callbacks.on_test_end(logs=logs)\n \n       if return_dict:\n@@ -1705,7 +1717,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         raise ValueError('Expect x to be a non-empty array or dataset.')\n       callbacks.on_predict_end()\n     all_outputs = tf.__internal__.nest.map_structure_up_to(batch_outputs, concat, outputs)\n-    return tf_utils.to_numpy_or_python_type(all_outputs)\n+    return tf_utils.sync_to_numpy_or_python_type(all_outputs)\n \n   def reset_metrics(self):\n     \"\"\"Resets the state of all the metrics in the model.\n@@ -1789,7 +1801,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     if reset_metrics:\n       self.reset_metrics()\n-    logs = tf_utils.to_numpy_or_python_type(logs)\n+    logs = tf_utils.sync_to_numpy_or_python_type(logs)\n     if return_dict:\n       return logs\n     else:\n@@ -1847,7 +1859,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     if reset_metrics:\n       self.reset_metrics()\n-    logs = tf_utils.to_numpy_or_python_type(logs)\n+    logs = tf_utils.sync_to_numpy_or_python_type(logs)\n     if return_dict:\n       return logs\n     else:\n@@ -1877,7 +1889,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x)\n       self.predict_function = self.make_predict_function()\n       outputs = self.predict_function(iterator)\n-    return tf_utils.to_numpy_or_python_type(outputs)\n+    return tf_utils.sync_to_numpy_or_python_type(outputs)\n \n   def fit_generator(self,\n                     generator,\n\n@@ -151,4 +151,4 @@ def strategy_supports_no_merge_call():\n   if not tf.distribute.has_strategy():\n     return True\n   strategy = tf.distribute.get_strategy()\n-  return not getattr(strategy.extended, \"_use_merge_call\", True)\n+  return not strategy.extended._use_merge_call()  # pylint: disable=protected-access\n\n@@ -473,8 +473,8 @@ def get_tensor_spec(t, dynamic_batch=False, name=None):\n   # pylint: enable=protected-access\n \n \n-def to_numpy_or_python_type(tensors):\n-  \"\"\"Converts a structure of `Tensor`s to `NumPy` arrays or Python scalar types.\n+def sync_to_numpy_or_python_type(tensors):\n+  \"\"\"Syncs and converts a structure of `Tensor`s to `NumPy` arrays or Python scalar types.\n \n   For each tensor, it calls `tensor.numpy()`. If the result is a scalar value,\n   it converts it to a Python type, such as a float or int, by calling\n@@ -484,6 +484,10 @@ def to_numpy_or_python_type(tensors):\n   with. This is especially useful for bfloat16 Numpy scalars, which don't\n   support as many operations as other Numpy values.\n \n+  Async strategies (such as `TPUStrategy` and `ParameterServerStrategy`) are\n+  forced to\n+  sync during this process.\n+\n   Args:\n     tensors: A structure of tensors.\n \n@@ -491,6 +495,9 @@ def to_numpy_or_python_type(tensors):\n     `tensors`, but scalar tensors are converted to Python types and non-scalar\n     tensors are converted to Numpy arrays.\n   \"\"\"\n+  if isinstance(tensors, tf.distribute.experimental.coordinator.RemoteValue):\n+    return tensors.fetch()\n+\n   def _to_single_numpy_or_python_type(t):\n     if isinstance(t, tf.Tensor):\n       x = t.numpy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1c2a085ea8cb7a63ce02ad32a5efe7c16151a3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 107 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,7 @@ DOCLINES = __doc__.split('\\n')\n # This version string is semver compatible, but incompatible with pip.\n # For pip, we will remove all '-' characters from this string, and use the\n # result for pip.\n-_VERSION = '2.5.0'\n+_VERSION = '2.6.0'\n \n REQUIRED_PACKAGES = [\n     # We depend on TensorFlow's declared pip dependencies.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#88f9cd3210598fbd5b8644144075dbd5de7a0306", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 105 | Files Changed: 19 | Hunks: 89 | Methods Changed: 29 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 185 | Churn Cumulative: 84975 | Contributors (this commit): 215 | Commits (past 90d): 167 | Contributors (cumulative): 313 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,7 +43,6 @@ from keras.utils import object_identity\n from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training import moving_averages\n from tensorflow.python.util import keras_deps\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n@@ -1899,9 +1898,13 @@ def moving_average_update(x, value, momentum):\n   Returns:\n       The updated variable.\n   \"\"\"\n-  zero_debias = not tf.__internal__.tf2.enabled()\n-  return moving_averages.assign_moving_average(\n-      x, value, momentum, zero_debias=zero_debias)\n+  if tf.__internal__.tf2.enabled():\n+    momentum = tf.cast(momentum, x.dtype)\n+    value = tf.cast(value, x.dtype)\n+    return x.assign(x * momentum + value * (1 - momentum))\n+  else:\n+    return tf.__internal__.train.assign_moving_average(\n+        x, value, momentum, zero_debias=True)\n \n \n # LINEAR ALGEBRA\n\n@@ -40,7 +40,6 @@ from keras.optimizer_v2 import gradient_descent\n from keras.optimizer_v2 import learning_rate_schedule\n from keras.utils import np_utils\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training import checkpoint_management\n \n try:\n   import h5py  # pylint:disable=g-import-not-at-top\n@@ -2735,7 +2734,7 @@ class MostRecentlyModifiedFileMatchingPatternTest(tf.test.TestCase):\n     ckpt_file_path = os.path.join(test_dir, ckpt_file_name)\n     with open(ckpt_file_path, 'w') as f:\n       f.write('dummy ckpt')\n-    checkpoint_management.update_checkpoint_state_internal(\n+    tf.__internal__.train.update_checkpoint_state(\n         test_dir, ckpt_file_path)\n \n     file_paths = [\n\n@@ -296,13 +296,9 @@ class TestDistributionStrategyDnnCorrectness(tf.test.TestCase,\n         sync_batchnorm=sync_batchnorm,\n         jit_compile=False)\n \n-    error_margin = 1e-2 if jit_compile else 1e-3\n-    loss_error_margin = 5e-2 if jit_compile else 1e-3\n-\n-    self.assertAllClose(wts, wts_with_ds, atol=error_margin, rtol=error_margin)\n-    self.assertAllClose(\n-        loss, loss_with_ds, atol=loss_error_margin, rtol=loss_error_margin)\n-    self.assertAllClose(acc, acc_with_ds, atol=error_margin, rtol=error_margin)\n+    self.assertAllClose(wts, wts_with_ds, atol=1e-3, rtol=1e-3)\n+    self.assertAllClose(loss, loss_with_ds, atol=1e-3, rtol=1e-3)\n+    self.assertAllClose(acc, acc_with_ds, atol=1e-3, rtol=1e-3)\n \n \n if __name__ == '__main__':\n\n@@ -51,7 +51,6 @@ from keras.utils import version_utils\n from keras.utils.generic_utils import to_snake_case  # pylint: disable=unused-import\n from keras.utils.tf_utils import is_tensor_or_tensor_list  # pylint: disable=unused-import\n from tensorflow.python.platform import tf_logging\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import get_canonical_name_for_symbol\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n@@ -296,7 +295,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         keras_layers_gauge.get_cell(self._get_cell_name()).set(True)\n         self._instrumented_keras_layer_class = True\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self,\n                trainable=True,\n                name=None,\n@@ -430,7 +429,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # a list with one element.\n     self._preserve_input_structure_in_config = False\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   @generic_utils.default\n   def build(self, input_shape):\n     \"\"\"Creates the variables of the layer (optional, for subclass implementers).\n@@ -1249,7 +1248,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n   @input_spec.setter\n   # Must be decorated to prevent tracking, since the input_spec can be nested\n   # InputSpec objects.\n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def input_spec(self, value):\n     for v in tf.nest.flatten(value):\n       if v is not None and not isinstance(v, InputSpec):\n@@ -2204,7 +2203,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     return self._inbound_nodes_value\n \n   @_inbound_nodes.setter\n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _inbound_nodes(self, value):\n     self._inbound_nodes_value = value\n \n@@ -2213,7 +2212,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     return self._outbound_nodes_value\n \n   @_outbound_nodes.setter\n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _outbound_nodes(self, value):\n     self._outbound_nodes_value = value\n \n@@ -2650,7 +2649,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                                  object_identity.ObjectIdentityDictionary())\n     return self._obj_reference_counts_dict\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _maybe_create_attribute(self, name, default_value):\n     \"\"\"Create the attribute with the default value if it hasn't been created.\n \n@@ -2957,7 +2956,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n   # SavedModel properties. Please see keras/saving/saved_model for details.\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _set_save_spec(self, inputs):\n     if self._saved_model_inputs_spec is not None:\n       return  # Already set.\n@@ -3050,7 +3049,7 @@ class TensorFlowOpLayer(Layer):\n       effect on this class, however is used in `get_config`.\n   \"\"\"\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self,\n                node_def,\n                name,\n\n@@ -44,7 +44,6 @@ from keras.utils import tf_utils\n from keras.utils.generic_utils import to_snake_case  # pylint: disable=unused-import\n from keras.utils.tf_utils import is_tensor_or_tensor_list  # pylint: disable=unused-import\n from tensorflow.python.platform import tf_logging\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.tools.docs import doc_controls\n \n \n@@ -124,7 +123,7 @@ class Layer(base_layer.Layer):\n       tf.Module._TF_MODULE_IGNORED_PROPERTIES\n   ))\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, trainable=True, name=None, dtype=None, dynamic=False,\n                **kwargs):\n     self._instrument_layer_creation()\n@@ -239,7 +238,7 @@ class Layer(base_layer.Layer):\n     # a list with one element.\n     self._preserve_input_structure_in_config = False\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   @generic_utils.default\n   def build(self, input_shape):\n     \"\"\"Creates the variables of the layer (optional, for subclass implementers).\n@@ -881,7 +880,7 @@ class Layer(base_layer.Layer):\n   @input_spec.setter\n   # Must be decorated to prevent tracking, since the input_spec can be nested\n   # InputSpec objects.\n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def input_spec(self, value):\n     for v in tf.nest.flatten(value):\n       if v is not None and not isinstance(v, base_layer.InputSpec):\n@@ -1713,7 +1712,7 @@ class Layer(base_layer.Layer):\n     return self._inbound_nodes_value\n \n   @_inbound_nodes.setter\n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _inbound_nodes(self, value):\n     self._inbound_nodes_value = value\n \n@@ -1722,7 +1721,7 @@ class Layer(base_layer.Layer):\n     return self._outbound_nodes_value\n \n   @_outbound_nodes.setter\n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _outbound_nodes(self, value):\n     self._outbound_nodes_value = value\n \n@@ -2114,7 +2113,7 @@ class Layer(base_layer.Layer):\n                                  object_identity.ObjectIdentityDictionary())\n     return self._obj_reference_counts_dict\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _maybe_create_attribute(self, name, default_value):\n     \"\"\"Create the attribute with the default value if it hasn't been created.\n \n\n@@ -27,7 +27,6 @@ from keras.engine import data_adapter\n from keras.engine.base_layer import Layer\n from keras.utils import tf_utils\n from keras.utils import version_utils\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -243,7 +242,7 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n     self._reset_state_impl()\n     self._is_adapted = False\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _configure_steps_per_execution(self, steps_per_execution):\n     self._steps_per_execution = tf.Variable(\n         steps_per_execution,\n@@ -294,7 +293,7 @@ class CombinerPreprocessingLayer(PreprocessingLayer):\n   def reset_state(self):\n     self._adapt_accumulator = None\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def update_state(self, data):\n     if self._adapt_accumulator is None:\n       self._adapt_accumulator = self._get_accumulator()\n\n@@ -34,7 +34,6 @@ from keras.utils import generic_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.tools.docs import doc_controls\n \n \n@@ -95,7 +94,7 @@ class Functional(training_lib.Model):\n       training_lib.Model._TF_MODULE_IGNORED_PROPERTIES\n   ))\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, inputs, outputs, name=None, trainable=True,\n                **kwargs):\n     # This is used by the Model class, since we have some logic to swap the\n@@ -109,7 +108,7 @@ class Functional(training_lib.Model):\n     super(Functional, self).__init__(name=name, trainable=trainable)\n     self._init_graph_network(inputs, outputs)\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _init_graph_network(self, inputs, outputs):\n     base_layer.keras_api_gauge.get_cell('Functional').set(True)\n     # This method is needed for Sequential to reinitialize graph network when\n\n@@ -30,7 +30,6 @@ from keras.utils import layer_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -97,7 +96,7 @@ class Sequential(functional.Functional):\n   ```\n   \"\"\"\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, layers=None, name=None):\n     \"\"\"Creates a `Sequential` model instance.\n \n@@ -147,7 +146,7 @@ class Sequential(functional.Functional):\n       return layers[1:]\n     return layers[:]\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def add(self, layer):\n     \"\"\"Adds a layer instance on top of the layer stack.\n \n@@ -235,7 +234,7 @@ class Sequential(functional.Functional):\n \n     self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def pop(self):\n     \"\"\"Removes the last layer in the model.\n \n@@ -260,7 +259,7 @@ class Sequential(functional.Functional):\n       self._init_graph_network(self.inputs, self.outputs)\n       self.built = True\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _build_graph_network_for_inferred_shape(self,\n                                               input_shape,\n                                               input_dtype=None):\n\n@@ -47,8 +47,6 @@ from keras.utils.io_utils import ask_to_proceed_with_overwrite\n from keras.utils.io_utils import path_to_string\n from keras.utils.mode_keys import ModeKeys\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training import checkpoint_management\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.training.tracking import util as trackable_utils\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n@@ -194,7 +192,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     else:\n       return super(Model, cls).__new__(cls, *args, **kwargs)\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, *args, **kwargs):\n     self._is_model_for_instrumentation = True\n     base_layer.keras_api_gauge.get_cell('model').set(True)\n@@ -293,7 +291,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self._init_batch_counters()\n     self._base_model_initialized = True\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _init_batch_counters(self):\n     # Untracked Variables, used to keep track of mini-batches seen in `fit`,\n     # `evaluate`, and `predict`.\n@@ -585,7 +583,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     return tf.nest.map_structure(_get_single_optimizer, optimizer)\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _reset_compile_cache(self):\n     self.train_function = None\n     self.test_function = None\n@@ -594,7 +592,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     # Used to cache `trainable` attr of `Layer`s for `fit`.\n     self._compiled_trainable_state = self._get_trainable_state()\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _configure_steps_per_execution(self, steps_per_execution):\n     self._steps_per_execution = tf.Variable(\n         steps_per_execution,\n@@ -2200,7 +2198,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         session = backend.get_session()\n       self._trackable_saver.save(filepath, session=session, options=options)\n       # Record this checkpoint so it's visible from tf.train.latest_checkpoint.\n-      checkpoint_management.update_checkpoint_state_internal(\n+      tf.__internal__.train.update_checkpoint_state(\n           save_dir=os.path.dirname(filepath),\n           model_checkpoint_path=filepath,\n           save_relative_paths=True,\n@@ -2501,7 +2499,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       raise ValueError('No such layer: ' + name + '.')\n     raise ValueError('Provide either a layer name or layer index.')\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _set_save_spec(self, inputs):\n     if self._saved_model_inputs_spec is not None:\n       return  # Already set.\n\n@@ -47,7 +47,6 @@ from keras.utils import tf_inspect\n from keras.utils import tf_utils\n from keras.utils.mode_keys import ModeKeys\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training.tracking import base as trackable\n \n try:\n   from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n@@ -144,7 +143,7 @@ class Model(training_lib.Model):\n   def _init_batch_counters(self):\n     pass  # Batch counters should not be created in legacy graph mode.\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _set_strategy(self, strategy):\n     self._compile_time_distribution_strategy = strategy\n \n@@ -214,7 +213,7 @@ class Model(training_lib.Model):\n                          'with steps_per_run greater than 1.')\n     return super(Model, self).load_weights(filepath, by_name, skip_mismatch)\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def compile(self,\n               optimizer='rmsprop',\n               loss=None,\n@@ -468,7 +467,7 @@ class Model(training_lib.Model):\n                 '  model=_create_model()\\n'\n                 '  model.compile(...)'% (v, strategy))\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _init_distributed_function_cache_if_not_compiled(self):\n     if not hasattr(self, '_distributed_function_cache'):\n       self._distributed_function_cache = {}\n@@ -1494,7 +1493,7 @@ class Model(training_lib.Model):\n       self._compile_weights_loss_and_weighted_metrics()\n     return recompile\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _compile_weights_loss_and_weighted_metrics(self, sample_weights=None):\n     \"\"\"Compiles the model loss and weighted metric sub-graphs.\n \n@@ -1658,7 +1657,7 @@ class Model(training_lib.Model):\n       return self.callback_model\n     return self\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _make_callback_model(self, grouped_model):\n     first_replicated_model = self._distribution_strategy.unwrap(\n         grouped_model)[0]\n@@ -2619,7 +2618,7 @@ class Model(training_lib.Model):\n \n     self._set_output_attrs(outputs)\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _set_input_attrs(self, inputs):\n     \"\"\"Sets attributes related to the inputs of the Model.\"\"\"\n     if self.inputs:\n@@ -2664,7 +2663,7 @@ class Model(training_lib.Model):\n \n     return inputs\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _set_output_attrs(self, outputs):\n     \"\"\"Sets attributes related to the outputs of the Model.\"\"\"\n     # NOTE(taylorrobie): This convention cannot be changed without updating the\n\n@@ -39,7 +39,6 @@ from keras.utils import generic_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import get_canonical_name_for_symbol\n from tensorflow.python.util.tf_export import get_symbol_from_name\n from tensorflow.python.util.tf_export import keras_export\n@@ -831,7 +830,7 @@ class Lambda(Layer):\n     Specified by `output_shape` argument\n   \"\"\"\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, function, output_shape=None, mask=None, arguments=None,\n                **kwargs):\n     super(Lambda, self).__init__(**kwargs)\n@@ -1312,7 +1311,7 @@ class TFOpLambda(Layer):\n   out = x * tf_variable\n   \"\"\"\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, function, **kwargs):\n     self.function = function\n     self.symbol = (\n@@ -1498,7 +1497,7 @@ class SlicingOpLambda(TFOpLambda):\n   out = x * tf_variable\n   \"\"\"\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, function, **kwargs):\n     super(SlicingOpLambda, self).__init__(function, **kwargs)\n \n@@ -1576,7 +1575,7 @@ class InstanceProperty(Layer):\n   out = x.flat_values\n   \"\"\"\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, attr_name, **kwargs):\n     self.attr_name = attr_name\n \n@@ -1723,7 +1722,7 @@ class ClassMethod(Layer):\n   out = tf.RaggedTensor.from_row_splits(x, y)\n   \"\"\"\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, cls_ref, method_name, **kwargs):\n     self.cls_ref = cls_ref\n     self.method_name = method_name\n\n@@ -34,7 +34,6 @@ from keras.utils import control_flow_util\n from keras.utils import generic_utils\n from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n@@ -452,7 +451,7 @@ class RNN(Layer):\n   @states.setter\n   # Automatic tracking catches \"self._states\" which adds an extra weight and\n   # breaks HDF5 checkpoints.\n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def states(self, states):\n     self._states = states\n \n@@ -1104,7 +1103,7 @@ class DropoutRNNCellMixin(object):\n     self._create_non_trackable_mask_cache()\n     super(DropoutRNNCellMixin, self).__init__(*args, **kwargs)\n \n-  @trackable.no_automatic_dependency_tracking\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _create_non_trackable_mask_cache(self):\n     \"\"\"Create the cache for dropout and recurrent dropout mask.\n \n\n@@ -225,7 +225,7 @@ class LossFunctionWrapper(Loss):\n       name: (Optional) name for the loss.\n       **kwargs: The keyword arguments that are passed on to `fn`.\n     \"\"\"\n-    super(LossFunctionWrapper, self).__init__(reduction=reduction, name=name)\n+    super().__init__(reduction=reduction, name=name)\n     self.fn = fn\n     self._fn_kwargs = kwargs\n \n@@ -249,7 +249,7 @@ class LossFunctionWrapper(Loss):\n     config = {}\n     for k, v in self._fn_kwargs.items():\n       config[k] = backend.eval(v) if tf_utils.is_tensor_or_variable(v) else v\n-    base_config = super(LossFunctionWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -308,8 +308,7 @@ class MeanSquaredError(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'mean_squared_error'.\n     \"\"\"\n-    super(MeanSquaredError, self).__init__(\n-        mean_squared_error, name=name, reduction=reduction)\n+    super().__init__(mean_squared_error, name=name, reduction=reduction)\n \n \n @keras_export('keras.losses.MeanAbsoluteError')\n@@ -367,8 +366,7 @@ class MeanAbsoluteError(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'mean_absolute_error'.\n     \"\"\"\n-    super(MeanAbsoluteError, self).__init__(\n-        mean_absolute_error, name=name, reduction=reduction)\n+    super().__init__(mean_absolute_error, name=name, reduction=reduction)\n \n \n @keras_export('keras.losses.MeanAbsolutePercentageError')\n@@ -428,7 +426,7 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n       name: Optional name for the op. Defaults to\n         'mean_absolute_percentage_error'.\n     \"\"\"\n-    super(MeanAbsolutePercentageError, self).__init__(\n+    super().__init__(\n         mean_absolute_percentage_error, name=name, reduction=reduction)\n \n \n@@ -489,7 +487,7 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n       name: Optional name for the op. Defaults to\n         'mean_squared_logarithmic_error'.\n     \"\"\"\n-    super(MeanSquaredLogarithmicError, self).__init__(\n+    super().__init__(\n         mean_squared_logarithmic_error, name=name, reduction=reduction)\n \n \n@@ -583,7 +581,7 @@ class BinaryCrossentropy(LossFunctionWrapper):\n             more details.\n       name: (Optional) Name for the op. Defaults to 'binary_crossentropy'.\n     \"\"\"\n-    super(BinaryCrossentropy, self).__init__(\n+    super().__init__(\n         binary_crossentropy,\n         name=name,\n         reduction=reduction,\n@@ -662,7 +660,7 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'categorical_crossentropy'.\n     \"\"\"\n-    super(CategoricalCrossentropy, self).__init__(\n+    super().__init__(\n         categorical_crossentropy,\n         name=name,\n         reduction=reduction,\n@@ -739,7 +737,7 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n       name: Optional name for the op. Defaults to\n         'sparse_categorical_crossentropy'.\n     \"\"\"\n-    super(SparseCategoricalCrossentropy, self).__init__(\n+    super().__init__(\n         sparse_categorical_crossentropy,\n         name=name,\n         reduction=reduction,\n@@ -802,7 +800,7 @@ class Hinge(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'hinge'.\n     \"\"\"\n-    super(Hinge, self).__init__(hinge, name=name, reduction=reduction)\n+    super().__init__(hinge, name=name, reduction=reduction)\n \n \n @keras_export('keras.losses.SquaredHinge')\n@@ -863,8 +861,7 @@ class SquaredHinge(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'squared_hinge'.\n     \"\"\"\n-    super(SquaredHinge, self).__init__(\n-        squared_hinge, name=name, reduction=reduction)\n+    super().__init__(squared_hinge, name=name, reduction=reduction)\n \n \n @keras_export('keras.losses.CategoricalHinge')\n@@ -923,8 +920,7 @@ class CategoricalHinge(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'categorical_hinge'.\n     \"\"\"\n-    super(CategoricalHinge, self).__init__(\n-        categorical_hinge, name=name, reduction=reduction)\n+    super().__init__(categorical_hinge, name=name, reduction=reduction)\n \n \n @keras_export('keras.losses.Poisson')\n@@ -980,7 +976,7 @@ class Poisson(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'poisson'.\n     \"\"\"\n-    super(Poisson, self).__init__(poisson, name=name, reduction=reduction)\n+    super().__init__(poisson, name=name, reduction=reduction)\n \n \n @keras_export('keras.losses.LogCosh')\n@@ -1037,7 +1033,7 @@ class LogCosh(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'log_cosh'.\n     \"\"\"\n-    super(LogCosh, self).__init__(log_cosh, name=name, reduction=reduction)\n+    super().__init__(log_cosh, name=name, reduction=reduction)\n \n \n @keras_export('keras.losses.KLDivergence')\n@@ -1097,8 +1093,7 @@ class KLDivergence(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'kl_divergence'.\n     \"\"\"\n-    super(KLDivergence, self).__init__(\n-        kl_divergence, name=name, reduction=reduction)\n+    super().__init__(kl_divergence, name=name, reduction=reduction)\n \n \n @keras_export('keras.losses.Huber')\n@@ -1165,8 +1160,7 @@ class Huber(LossFunctionWrapper):\n             more details.\n       name: Optional name for the op. Defaults to 'huber_loss'.\n     \"\"\"\n-    super(Huber, self).__init__(\n-        huber, name=name, reduction=reduction, delta=delta)\n+    super().__init__(huber, name=name, reduction=reduction, delta=delta)\n \n \n @keras_export('keras.metrics.mean_squared_error', 'keras.metrics.mse',\n@@ -1966,7 +1960,7 @@ class CosineSimilarity(LossFunctionWrapper):\n                axis=-1,\n                reduction=losses_utils.ReductionV2.AUTO,\n                name='cosine_similarity'):\n-    super(CosineSimilarity, self).__init__(\n+    super().__init__(\n         cosine_similarity, reduction=reduction, name=name, axis=axis)\n \n \n@@ -2065,11 +2059,10 @@ def get(identifier):\n     return deserialize(identifier)\n   if isinstance(identifier, dict):\n     return deserialize(identifier)\n-  elif callable(identifier):\n+  if callable(identifier):\n     return identifier\n-  else:\n   raise ValueError(\n-        'Could not interpret loss function identifier: {}'.format(identifier))\n+      f'Could not interpret loss function identifier: {identifier}')\n \n \n LABEL_DTYPES_FOR_LOSSES = {\n\n@@ -1777,7 +1777,7 @@ class HuberLossTest(tf.test.TestCase):\n class BinaryTruePositivesViaControlFlow(losses.Loss):\n \n   def __init__(self, reduction=losses_utils.ReductionV2.AUTO):\n-    super(BinaryTruePositivesViaControlFlow, self).__init__(reduction=reduction)\n+    super().__init__(reduction=reduction)\n \n   def call(self, y_true, y_pred):\n     y_true = tf.cast(y_true, tf.bool)\n\n@@ -22,7 +22,6 @@ from keras.mixed_precision import loss_scale as keras_loss_scale_module\n from keras.optimizer_v2 import optimizer_v2\n from keras.optimizer_v2 import utils as optimizer_utils\n from tensorflow.python.platform import tf_logging\n-from tensorflow.python.training.experimental import mixed_precision\n from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -1176,8 +1175,7 @@ class FakeOptimizerForRestoration(tf.__internal__.tracking.Trackable):\n         slot_variable_position, slot_name, variable)\n \n \n-# pylint: disable=protected-access\n-mixed_precision._register_wrapper_optimizer_cls(optimizer_v2.OptimizerV2,\n+tf.__internal__.mixed_precision.register_loss_scale_wrapper(optimizer_v2.OptimizerV2,\n                                             LossScaleOptimizerV1)\n \n \n\n@@ -28,7 +28,6 @@ from keras.utils import mode_keys\n from keras.utils.generic_utils import LazyLoader\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.saved_model import builder as saved_model_builder\n-from tensorflow.python.training.tracking import graph_view\n from tensorflow.python.util.tf_export import keras_export\n \n # To avoid circular dependencies between keras/engine and keras/saving,\n@@ -214,7 +213,7 @@ def _save_v1_format(model, path, custom_objects, as_text, input_signature):\n \n def _get_var_list(model):\n   \"\"\"Returns list of all checkpointed saveable objects in the model.\"\"\"\n-  var_list, _, _ = graph_view.ObjectGraphView(model).serialize_object_graph()\n+  var_list, _, _ = tf.__internal__.tracking.ObjectGraphView(model).serialize_object_graph()\n   return var_list\n \n \n\n@@ -25,7 +25,6 @@ from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n from keras.layers import normalization\n-from tensorflow.python.training.tracking import base\n from tensorflow.python.training.tracking import data_structures\n from tensorflow.python.training.tracking import util\n \n@@ -544,7 +543,7 @@ class InterfaceTests(keras_parameterized.TestCase):\n \n     class NoDependencyModel(training.Model):\n \n-      @base.no_automatic_dependency_tracking\n+      @tf.__internal__.tracking.no_automatic_dependency_tracking\n       def __init__(self):\n         super(NoDependencyModel, self).__init__()\n         self.a = []\n\n@@ -29,7 +29,6 @@ from keras.engine import training\n from keras.layers import core\n from keras.optimizer_v2 import adam\n from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training.tracking import graph_view\n from tensorflow.python.training.tracking import util as trackable_utils\n \n \n@@ -111,7 +110,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n     self.evaluate(trackable_utils.gather_initializers(\n         root_trackable))\n     self.evaluate(train_op)\n-    named_variables, serialized_graph, _ = graph_view.ObjectGraphView(\n+    named_variables, serialized_graph, _ = tf.__internal__.tracking.ObjectGraphView(\n         root_trackable).serialize_object_graph()\n     expected_slot_keys = (\n         \"model/_second/kernel/.OPTIMIZER_SLOT/optimizer/m\",\n@@ -806,7 +805,7 @@ class CheckpointCompatibilityTests(keras_parameterized.TestCase):\n         with self.assertRaises(AssertionError):\n           self._check_sentinels(root)\n         object_saver = trackable_utils.TrackableSaver(\n-            graph_view.ObjectGraphView(root))\n+            tf.__internal__.tracking.ObjectGraphView(root))\n         self._set_sentinels(root)\n         status = object_saver.restore(save_path)\n         if tf.executing_eagerly():\n\n@@ -25,7 +25,6 @@ from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import training\n from keras.layers import core\n-from tensorflow.python.training.tracking import graph_view\n from tensorflow.python.training.tracking import util as trackable_utils\n \n \n@@ -82,7 +81,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n       self.evaluate(trackable_utils.gather_initializers(\n           root_trackable))\n       self.evaluate(train_op)\n-    named_variables, serialized_graph, _ = graph_view.ObjectGraphView(\n+    named_variables, serialized_graph, _ = tf.__internal__.tracking.ObjectGraphView(\n         root_trackable).serialize_object_graph()\n     expected_checkpoint_names = (\n         # Created in the root node, so no prefix.\n@@ -617,7 +616,7 @@ class CheckpointCompatibilityTests(keras_parameterized.TestCase):\n         with self.assertRaises(AssertionError):\n           self._check_sentinels(root)\n         object_saver = trackable_utils.TrackableSaver(\n-            graph_view.ObjectGraphView(root))\n+            tf.__internal__.tracking.ObjectGraphView(root))\n         self._set_sentinels(root)\n         status = object_saver.restore(save_path)\n         if tf.executing_eagerly():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
