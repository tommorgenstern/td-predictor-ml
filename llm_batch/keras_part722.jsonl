{"custom_id": "keras#a49f7b27b292d3760b6619832d3bded6eaeff103", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 804 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -142,6 +142,7 @@ class LayerNormalization(Layer):\n         self.gamma_constraint = constraints.get(gamma_constraint)\n \n         self.supports_masking = True\n+        self.autocast = False\n \n     def build(self, input_shape):\n         if isinstance(self.axis, list):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ac8df7e9812931fd2037e2f363b34f672a13e7c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1791 | Contributors (this commit): 8 | Commits (past 90d): 25 | Contributors (cumulative): 8 | DMM Complexity: 0.5\n\nDIFF:\n@@ -18,6 +18,7 @@ class Trainer:\n         self._run_eagerly = False\n         self._jit_compile = None\n         self.compiled = False\n+        self.loss = None\n         self.steps_per_execution = 1\n \n     @traceback_utils.filter_traceback\n@@ -42,6 +43,7 @@ class Trainer:\n             self._compile_loss = CompileLoss(\n                 loss, loss_weights, output_names=output_names\n             )\n+            self.loss = loss\n         else:\n             self._compile_loss = None\n         if metrics is not None or weighted_metrics is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10f41439edfb308ca57c4b48b740fcf828627657", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 866 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -143,7 +143,9 @@ def _get_subclassed_model(compile=True):\n \n \n def _get_custom_sequential_model(compile=True):\n-    sequential_model = keras_core.Sequential([MyDense(1), MyDense(1)])\n+    sequential_model = keras_core.Sequential(\n+        [MyDense(1), MyDense(1)], name=\"sequential\"\n+    )\n     if compile:\n         sequential_model.compile(\n             optimizer=\"adam\",\n@@ -158,7 +160,8 @@ def _get_basic_sequential_model(compile=True):\n         [\n             keras_core.layers.Dense(1, name=\"dense_1\"),\n             keras_core.layers.Dense(1, name=\"dense_2\"),\n-        ]\n+        ],\n+        name=\"sequential\",\n     )\n     if compile:\n         sequential_model.compile(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9661a11942d947db4fb3d3a0d6cf4fc846c28ef0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 668 | Contributors (this commit): 12 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -253,9 +253,9 @@ class TestCase(unittest.TestCase):\n                 elif isinstance(expected_output_shape, list):\n                     self.assertTrue(isinstance(output, list))\n                     self.assertEqual(\n-                        len(output.keys()),\n+                        len(output),\n                         len(\n-                            expected_output_shape.keys(),\n+                            expected_output_shape,\n                             msg=\"Unexpected number of outputs\",\n                         ),\n                     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6cf28ba09bd3fe60a7cfd5981231138ca3be2135", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 167 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 167 | Churn Cumulative: 167 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,167 @@\n+import torch\n+import torch.nn as nn\n+\n+import keras_core\n+from keras_core.layers import Layer\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export([\"keras_core.utils.torch_utils.TorchModuleWrapper\"])\n+class TorchModuleWrapper(Layer):\n+    \"\"\"Torch module wrapper layer.\n+\n+    `TorchModuleWrapper` is an abstraction that can be wrapped around a\n+    `torch.nn.Module` to make its parameters trackable as a\n+    `keras_core.layers.Layer`. It works with both vanilla and lazy PyTorch\n+    modules.\n+\n+    Args:\n+        module: torch.nn.Module, A vanilla or lazy PyTorch neural network\n+            module.\n+        name: The name of the layer (string).\n+\n+    References:\n+    - [PyTorch docs for `torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) # noqa: E501\n+    - [PyTorch docs for `LazyModuleMixin`](https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html) # noqa: E501\n+\n+    Examples:\n+\n+    Here's an example of how the `TorchModuleWrapper` can be used with vanilla\n+    PyTorch modules.\n+\n+    ```python\n+    import torch.nn as nn\n+    import torch.nn.functional as F\n+\n+    import keras_core\n+    from keras_core.backend.torch import TorchModuleWrapper\n+\n+\n+    class Classifier(keras_core.Model):\n+\n+        def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            # Wrap all `torch.nn.Module`s with `TorchModuleWrapper`\n+            self.conv1 = TorchModuleWrapper(\n+                nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3))\n+            )\n+            self.conv2 = TorchModuleWrapper(\n+                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3))\n+            )\n+            self.pool = TorchModuleWrapper(\n+                nn.MaxPool2d(kernel_size=(2, 2))\n+            )\n+            self.flatten = TorchModuleWrapper(nn.Flatten())\n+            self.dropout = TorchModuleWrapper(nn.Dropout(p=0.5))\n+            self.fc = TorchModuleWrapper(nn.Linear(1600, 10))\n+\n+        def call(self, inputs):\n+            x = F.relu(self.conv1(inputs))\n+            x = self.pool(x)\n+            x = F.relu(self.conv2(x))\n+            x = self.pool(x)\n+            x = self.flatten(x)\n+            x = self.dropout(x)\n+            x = self.fc(x)\n+            return F.softmax(x, dim=1)\n+\n+\n+    model = Classifier()\n+    model.build((1, 28, 28))\n+    print(\"Output shape:\", model(torch.ones(1, 1, 28, 28).to(\"cuda\")).shape)\n+\n+    model.compile(\n+        loss=\"sparse_categorical_crossentropy\",\n+        optimizer=\"adam\",\n+        metrics=[\"accuracy\"]\n+    )\n+    model.fit(train_loader, epochs=5)\n+    ```\n+\n+    Here's an example of how the `TorchModuleWrapper` can be used with PyTorch\n+    Lazy modules.\n+\n+    ```python\n+    import torch.nn as nn\n+    import torch.nn.functional as F\n+\n+    import keras_core\n+    from keras_core.backend.torch import TorchModuleWrapper\n+\n+\n+    class LazyClassifier(keras.Model):\n+\n+        def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            # You can wrap all `torch.nn.Module`s with `TorchModuleWrapper`\n+            # irrespective of whether they are lazy or not.\n+            self.conv1 = TorchModuleWrapper(\n+                nn.LazyConv2d(out_channels=32, kernel_size=(3, 3))\n+            )\n+            self.conv2 = TorchModuleWrapper(\n+                nn.LazyConv2d(out_channels=64, kernel_size=(3, 3))\n+            )\n+            self.pool = TorchModuleWrapper(nn.MaxPool2d(kernel_size=(2, 2)))\n+            self.flatten = TorchModuleWrapper(nn.Flatten())\n+            self.dropout = TorchModuleWrapper(nn.Dropout(p=0.5))\n+            self.fc = TorchModuleWrapper(nn.LazyLinear(10))\n+\n+        def call(self, inputs):\n+            x = F.relu(self.conv1(inputs))\n+            x = self.pool(x)\n+            x = F.relu(self.conv2(x))\n+            x = self.pool(x)\n+            x = self.flatten(x)\n+            x = self.dropout(x)\n+            x = self.fc(x)\n+            return F.softmax(x, dim=1)\n+\n+\n+    model = Classifier()\n+    model.build((1, 28, 28))\n+    print(\"Output shape:\", model(torch.ones(1, 1, 28, 28).to(\"cuda\")).shape)\n+\n+    model.compile(\n+        loss=\"sparse_categorical_crossentropy\",\n+        optimizer=\"adam\",\n+        metrics=[\"accuracy\"]\n+    )\n+    model.fit(train_loader, epochs=5)\n+    ```\n+\n+    \"\"\"\n+\n+    def __init__(self, module, name=None):\n+        super().__init__(name=name)\n+        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+        self.module = module.to(self.device)\n+        self.lazy = isinstance(self.module, nn.modules.lazy.LazyModuleMixin)\n+        if not self.lazy:\n+            self.track_module_parameters()\n+\n+    def parameters(self, recurse=True):\n+        return self.module.parameters(recurse=not self.lazy)\n+\n+    def track_module_parameters(self):\n+        from keras_core.backend.torch import Variable\n+\n+        for param in self.module.parameters():\n+            variable = Variable(\n+                initializer=param, trainable=param.requires_grad\n+            )\n+            variable._value = param\n+            self._track_variable(variable)\n+        self.built = True\n+\n+    def build(self, *args, **kwargs):\n+        if not self.lazy:\n+            self._build_by_run_for_single_pos_arg(args)\n+            self._build_by_run_for_kwargs(kwargs)\n+        else:\n+            # sample_input = torch.ones(*input_shape).to(\"cuda\")\n+            # _ = self.module(sample_input)\n+            _ = keras_core.backend.torch.core.compute_output_spec(self.__call__)\n+        self.track_module_parameters()\n+\n+    def call(self, inputs, **kwargs):\n+        return self.module.forward(inputs, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3cccbc53972339eb993a4f5c39c77e6d7a977819", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 48 | Churn Cumulative: 2917 | Contributors (this commit): 17 | Commits (past 90d): 51 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1168,6 +1168,17 @@ class Layer(BackendLayer, Operation):\n                 )\n             )\n \n+    def _build_by_run(self, *args, **kwargs):\n+        call_spec = CallSpec(self._call_signature, args, kwargs)\n+        shapes_dict = get_shapes_dict(call_spec)\n+        if len(shapes_dict) == 1:\n+            success = self._build_by_run_for_single_pos_arg(\n+                tuple(shapes_dict.values())[0]\n+            )\n+        else:\n+            success = self._build_by_run_for_kwargs(shapes_dict)\n+        return success\n+\n     def _build_by_run_for_single_pos_arg(self, input_shape):\n         # Case: all inputs are in the first arg (possibly nested).\n         input_tensors = map_shape_structure(\n\n@@ -1,12 +1,9 @@\n-import torch\n-import torch.nn as nn\n-\n import keras_core\n-from keras_core.layers import Layer\n from keras_core.api_export import keras_core_export\n+from keras_core.layers import Layer\n \n \n-@keras_core_export([\"keras_core.utils.torch_utils.TorchModuleWrapper\"])\n+@keras_core_export([\"keras_core.utils.TorchModuleWrapper\"])\n class TorchModuleWrapper(Layer):\n     \"\"\"Torch module wrapper layer.\n \n@@ -128,21 +125,26 @@ class TorchModuleWrapper(Layer):\n     )\n     model.fit(train_loader, epochs=5)\n     ```\n-\n     \"\"\"\n \n     def __init__(self, module, name=None):\n         super().__init__(name=name)\n-        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-        self.module = module.to(self.device)\n-        self.lazy = isinstance(self.module, nn.modules.lazy.LazyModuleMixin)\n-        if not self.lazy:\n-            self.track_module_parameters()\n+        self.module = module\n+        import torch\n+        import torch.nn as nn\n+\n+        # self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+        # self.module = module.to(self.device)\n+        self._is_lazy_module = isinstance(\n+            self.module, nn.modules.lazy.LazyModuleMixin\n+        )\n+        if not self._is_lazy_module:\n+            self._track_module_parameters()\n \n     def parameters(self, recurse=True):\n-        return self.module.parameters(recurse=not self.lazy)\n+        return self.module.parameters(recurse=recurse)\n \n-    def track_module_parameters(self):\n+    def _track_module_parameters(self):\n         from keras_core.backend.torch import Variable\n \n         for param in self.module.parameters():\n@@ -154,13 +156,8 @@ class TorchModuleWrapper(Layer):\n         self.built = True\n \n     def build(self, *args, **kwargs):\n-        if not self.lazy:\n-            self._build_by_run_for_single_pos_arg(args)\n-            self._build_by_run_for_kwargs(kwargs)\n-        else:\n-            # sample_input = torch.ones(*input_shape).to(\"cuda\")\n-            # _ = self.module(sample_input)\n-            _ = keras_core.backend.torch.core.compute_output_spec(self.__call__)\n+        if self._is_lazy_module:\n+            self._build_by_run(*args, **kwargs)\n         self.track_module_parameters()\n \n     def call(self, inputs, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6486eb5b0ceddf0a1bff502cbd6cd5460bb0bd6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 207 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,6 @@\n-import keras_core\n-from keras_core.api_export import keras_core_export\n from keras_core.layers import Layer\n \n \n-@keras_core_export([\"keras_core.utils.TorchModuleWrapper\"])\n class TorchModuleWrapper(Layer):\n     \"\"\"Torch module wrapper layer.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d0b6cdc0a7313f9c4ae285b88b9b0622a25e002c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 208 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -127,7 +127,6 @@ class TorchModuleWrapper(Layer):\n     def __init__(self, module, name=None):\n         super().__init__(name=name)\n         self.module = module\n-        import torch\n         import torch.nn as nn\n \n         # self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c0c7376a06e886479bb1fc68ebf0f67de94d027", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 24 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 80 | Churn Cumulative: 3381 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,8 @@ import tensorflow as tf\n \n from keras_core import layers\n from keras_core import losses\n-from keras_core import models\n from keras_core import metrics\n+from keras_core import models\n from keras_core import optimizers\n from keras_core.utils import rng_utils\n \n@@ -43,8 +43,8 @@ def test_model_fit():\n             optimizer=optimizers.SGD(learning_rate=0.001, momentum=0.01),\n             loss=losses.MeanSquaredError(),\n             metrics=[metrics.MeanSquaredError()],\n-            # TODO(scottzhu): Find out where is the variable that is not created eagerly\n-            # and break the usage of XLA.\n+            # TODO(scottzhu): Find out where is the variable\n+            #  that is not created eagerly and break the usage of XLA.\n             jit_compile=False,\n         )\n         history = model.fit(\n\n@@ -27,7 +27,7 @@ def plot_sequential_model():\n     )\n     plot_model(\n         model,\n-        \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",\n+        \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -35,7 +35,7 @@ def plot_sequential_model():\n     )\n     plot_model(\n         model,\n-        \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",\n+        \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -44,7 +44,7 @@ def plot_sequential_model():\n     )\n     plot_model(\n         model,\n-        \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",\n+        \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -94,7 +94,7 @@ def plot_functional_model():\n     )\n     plot_model(\n         model,\n-        \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",\n+        \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -102,7 +102,7 @@ def plot_functional_model():\n     )\n     plot_model(\n         model,\n-        \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",\n+        \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -111,7 +111,7 @@ def plot_functional_model():\n     )\n     plot_model(\n         model,\n-        \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",\n+        \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -164,7 +164,7 @@ def plot_subclassed_model():\n     )\n     plot_model(\n         model,\n-        \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",\n+        \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -172,7 +172,7 @@ def plot_subclassed_model():\n     )\n     plot_model(\n         model,\n-        \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",\n+        \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -181,7 +181,7 @@ def plot_subclassed_model():\n     )\n     plot_model(\n         model,\n-        \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",\n+        \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -249,7 +249,7 @@ def plot_nested_functional_model():\n     )\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",\n+        \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -258,7 +258,7 @@ def plot_nested_functional_model():\n     )\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",\n+        \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -268,7 +268,7 @@ def plot_nested_functional_model():\n     )\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",\n+        \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",  # noqa: E501\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -286,7 +286,7 @@ def plot_nested_functional_model():\n     )\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_layer_activations-show_trainable.png\",\n+        \"nested-functional-show_shapes-show_layer_activations-show_trainable.png\",  # noqa: E501\n         show_shapes=True,\n         show_layer_activations=True,\n         show_trainable=True,\n\n@@ -502,24 +502,32 @@ class PrecisionTest(testing.TestCase):\n         with self.assertRaisesRegex(\n             ValueError,\n             r\"When class_id is provided, y_pred must be a 2D array \"\n-            r\"with shape \\(num_samples, num_classes\\), found shape:.*\"\n+            r\"with shape \\(num_samples, num_classes\\), found shape:.*\",\n         ):\n             p_obj(y_true, y_pred)\n \n     def test_unweighted_class_id_multiclass(self):\n         p_obj = metrics.Precision(class_id=1)\n \n-        y_pred = np.array([[0.1, 0.2, 0.7],\n+        y_pred = np.array(\n+            [\n+                [0.1, 0.2, 0.7],\n                 [0.5, 0.3, 0.2],\n                 [0.2, 0.6, 0.2],\n                 [0.7, 0.2, 0.1],\n-                           [0.1, 0.1, 0.8]])\n+                [0.1, 0.1, 0.8],\n+            ]\n+        )\n \n-        y_true = np.array([[0., 0., 1.],\n-                           [1., 0., 0.],\n-                           [0., 1., 0.],\n-                           [1., 0., 0.],\n-                           [0., 0., 1.]])\n+        y_true = np.array(\n+            [\n+                [0.0, 0.0, 1.0],\n+                [1.0, 0.0, 0.0],\n+                [0.0, 1.0, 0.0],\n+                [1.0, 0.0, 0.0],\n+                [0.0, 0.0, 1.0],\n+            ]\n+        )\n \n         result = p_obj(y_true, y_pred)\n         self.assertAlmostEqual(1.0, result)\n@@ -655,24 +663,32 @@ class RecallTest(testing.TestCase):\n         with self.assertRaisesRegex(\n             ValueError,\n             r\"When class_id is provided, y_pred must be a 2D array \"\n-            r\"with shape \\(num_samples, num_classes\\), found shape:.*\"\n+            r\"with shape \\(num_samples, num_classes\\), found shape:.*\",\n         ):\n             r_obj(y_true, y_pred)\n \n     def test_unweighted_class_id_multiclass(self):\n         r_obj = metrics.Recall(class_id=1)\n \n-        y_pred = np.array([[0.1, 0.2, 0.7],\n+        y_pred = np.array(\n+            [\n+                [0.1, 0.2, 0.7],\n                 [0.5, 0.3, 0.2],\n                 [0.2, 0.6, 0.2],\n                 [0.7, 0.2, 0.1],\n-                           [0.1, 0.1, 0.8]])\n+                [0.1, 0.1, 0.8],\n+            ]\n+        )\n \n-        y_true = np.array([[0., 0., 1.],\n-                           [1., 0., 0.],\n-                           [0., 1., 0.],\n-                           [1., 0., 0.],\n-                           [0., 0., 1.]])\n+        y_true = np.array(\n+            [\n+                [0.0, 0.0, 1.0],\n+                [1.0, 0.0, 0.0],\n+                [0.0, 1.0, 0.0],\n+                [1.0, 0.0, 0.0],\n+                [0.0, 0.0, 1.0],\n+            ]\n+        )\n \n         result = r_obj(y_true, y_pred)\n         self.assertAlmostEqual(1.0, result)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#584352bcc04595dc66706023b666d2573aa12936", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 56 | Churn Cumulative: 1270 | Contributors (this commit): 12 | Commits (past 90d): 29 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,11 +24,16 @@ class Variable(KerasVariable):\n         # due to circular dependency.\n         distribution = global_state.get_global_attribute(\"distribution\")\n         if distribution is not None:\n-            layout = distribution.get_variable_layout(self)\n-            value = distribution_lib.distribute_value(value, layout)\n-        self._value = value\n+            self._layout = distribution_lib.to_jax_layout(\n+                distribution.get_variable_layout(self)\n+            )\n+        else:\n+            self._layout = None\n+        self._direct_assign(value)\n \n     def _direct_assign(self, value):\n+        if getattr(self, \"_layout\", None) is not None:\n+            value = distribution_lib.distribute_value(value, self._layout)\n         self._value = value\n \n     def _convert_to_tensor(self, value, dtype=None):\n\n@@ -61,9 +61,12 @@ def distribute_value(value, tensor_layout):\n \n     Args:\n         value: `jax.Array` that need to be distributed.\n-        tensor_layout: `TensorLayout` for the distribution information.\n+        tensor_layout: `TensorLayout` for the distribution information, or a\n+            `jax.sharding.Sharding` instance.\n \n     Returns:\n         Distributed value.\n     \"\"\"\n-    return jax.device_put(value, to_jax_layout(tensor_layout))\n+    if not isinstance(tensor_layout, jax.sharding.Sharding):\n+        tensor_layout = to_jax_layout(tensor_layout)\n+    return jax.device_put(value, tensor_layout)\n\n@@ -392,6 +392,44 @@ class JaxDistributionLibTest(testing.TestCase):\n         ):\n             backend_dlib.to_jax_layout(layout)\n \n+    def test_variable_assignment_reuse_layout(self):\n+        shape = (4, 2)\n+        axis_names = [\"batch\", \"model\"]\n+        device_mesh = distribution_lib.DeviceMesh(\n+            shape, axis_names, backend_dlib.list_devices()\n+        )\n+        layout_map = distribution_lib.LayoutMap(device_mesh)\n+        layout_map[\".*dense.*kernel\"] = distribution_lib.TensorLayout(\n+            [None, \"model\"]\n+        )\n+        layout_map[\".*dense.*bias\"] = distribution_lib.TensorLayout([\"model\"])\n+\n+        distribution = distribution_lib.ModelParallel(\n+            device_mesh, layout_map, batch_dim_name=\"batch\"\n+        )\n+\n+        with distribution.scope():\n+            dense_layer = layers.Dense(8)\n+            dense_layer.build((16, 16))\n+\n+        self.assertEqual(\n+            dense_layer.kernel._value.sharding.spec, (None, \"model\")\n+        )\n+        self.assertEqual(dense_layer.bias._value.sharding.spec, (\"model\",))\n+\n+        # Assign a numpy value to dense layer to mimic the model weight loading\n+        new_kernel = np.random.normal(size=(16, 8))\n+        new_bias = np.random.normal(size=(8))\n+        dense_layer.kernel.assign(new_kernel)\n+        dense_layer.bias.assign(new_bias)\n+\n+        # Make sure the loaded value still use the layout when it is\n+        # initialized, even outside of the distribution scope.\n+        self.assertEqual(\n+            dense_layer.kernel._value.sharding.spec, (None, \"model\")\n+        )\n+        self.assertEqual(dense_layer.bias._value.sharding.spec, (\"model\",))\n+\n     def test_e2e_data_parallel_model(self):\n         distribution = distribution_lib.DataParallel(\n             devices=backend_dlib.list_devices()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#48bf48b25d574d4fd0581ad11793f8643496dfa2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 53 | Files Changed: 6 | Hunks: 25 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 105 | Churn Cumulative: 5536 | Contributors (this commit): 18 | Commits (past 90d): 109 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,2 @@\n class JaxLayer:\n-    def _post_build(self):\n-        \"\"\"Can be overriden to perform post-build actions.\"\"\"\n     pass\n\n@@ -1,3 +1,2 @@\n class NumpyLayer:\n-    def _post_build(self):\n     pass\n\n@@ -9,10 +9,6 @@ class TFLayer(tf.__internal__.tracking.AutoTrackable):\n         self._saved_model_inputs_spec = None\n         self._saved_model_arg_spec = None\n \n-    def _post_build(self):\n-        \"\"\"Can be overriden to perform post-build actions.\"\"\"\n-        pass\n-\n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def _set_save_spec(self, inputs, args=None, kwargs=None):\n         \"\"\"Defines the save spec so that serialization can trace layer calls.\n\n@@ -218,7 +218,13 @@ class Layer(BackendLayer, Operation):\n         def build_wrapper(*args, **kwargs):\n             with backend.name_scope(obj.name, caller=obj):\n                 original_build_method(*args, **kwargs)\n+            # Record build config.\n+            signature = inspect.signature(original_build_method)\n+            obj._build_shapes_dict = signature.bind(*args, **kwargs).arguments\n+            # Set built, post build actions, and lock state.\n             obj.built = True\n+            obj._post_build()\n+            obj._lock_state()\n \n         obj.build = build_wrapper\n         return obj\n@@ -355,6 +361,23 @@ class Layer(BackendLayer, Operation):\n             )\n         self.built = True\n \n+    def _post_build(self):\n+        \"\"\"Can be overridden for per backend post build actions.\"\"\"\n+        pass\n+\n+    def _lock_state(self):\n+        \"\"\"Prevent further state updates, called automatically in `build()`.\"\"\"\n+        if not self._tracker.locked:\n+            self._tracker.lock(\n+                msg=(\n+                    \"You cannot add new elements of state \"\n+                    \"(variables or sub-layers) \"\n+                    \"to a layer that is already built. All state \"\n+                    \"must be created in the `__init__()` method or \"\n+                    \"in the `build()` method.\"\n+                )\n+            )\n+\n     def get_build_config(self):\n         \"\"\"Returns a dictionary with the layer's input shape.\n \n@@ -393,10 +416,8 @@ class Layer(BackendLayer, Operation):\n         if config:\n             if \"input_shape\" in config:\n                 self.build(config[\"input_shape\"])\n-                self._build_shapes_dict = config\n             elif \"shapes_dict\" in config:\n                 self.build(**config[\"shapes_dict\"])\n-                self._build_shapes_dict = config[\"shapes_dict\"]\n             self.built = True\n \n     def add_variable(\n@@ -699,7 +720,7 @@ class Layer(BackendLayer, Operation):\n         self._assert_input_compatibility(call_spec.first_arg)\n \n         ################\n-        # 4. Call build.\n+        # 4. Call build\n         with backend.name_scope(self.name, caller=self):\n             self._maybe_build(call_spec)\n \n@@ -1107,10 +1128,13 @@ class Layer(BackendLayer, Operation):\n         return summary_utils.count_params(self.weights)\n \n     def _maybe_build(self, call_spec):\n-        if not self.built:\n-            shapes_dict = get_shapes_dict(call_spec)\n-            self._build_shapes_dict = shapes_dict\n+        if self.built:\n+            return\n \n+        shapes_dict = get_shapes_dict(call_spec)\n+        first_shape = next(iter(shapes_dict.values()), None)\n+\n+        # If the layer has a build method, call it with our input shapes.\n         if not utils.is_default(self.build):\n             shapes_dict = update_shapes_dict_for_target_fn(\n                 self.build,\n@@ -1119,12 +1143,15 @@ class Layer(BackendLayer, Operation):\n                 class_name=self.__class__.__name__,\n             )\n             self.build(**shapes_dict)\n-            elif might_have_unbuilt_state(self):\n+            # Check input spec again (after build, since self.input_spec\n+            # may have been updated\n+            self._assert_input_compatibility(call_spec.first_arg)\n+            return\n+\n+        # Otherwise, attempt to build the layer by calling it on symbolic input.\n+        if might_have_unbuilt_state(self):\n             if len(shapes_dict) == 1:\n-                    # Single arg: pass it positionally\n-                    success = self._build_by_run_for_single_pos_arg(\n-                        tuple(shapes_dict.values())[0]\n-                    )\n+                success = self._build_by_run_for_single_pos_arg(first_shape)\n             else:\n                 success = self._build_by_run_for_kwargs(shapes_dict)\n             if not success:\n@@ -1132,41 +1159,21 @@ class Layer(BackendLayer, Operation):\n                     # Will let the actual eager call do state-building\n                     return\n                 raise ValueError(\n-                        f\"Layer '{self.name}' looks like it has \"\n-                        \"unbuilt state, but Keras is not able to \"\n-                        \"trace the layer `call()` in order to \"\n+                    f\"Layer '{self.name}' looks like it has unbuilt state, but \"\n+                    \"Keras is not able to trace the layer `call()` in order to \"\n                     \"build it automatically. Possible causes:\\n\"\n-                        \"1. The `call()` method of your layer may be \"\n-                        \"crashing. Try to `__call__()` the layer \"\n-                        \"eagerly on some test input \"\n+                    \"1. The `call()` method of your layer may be crashing. Try \"\n+                    \"to `__call__()` the layer eagerly on some test input \"\n                     \"first to see if it works. \"\n-                        \"E.g. `x = np.random.random((3, 4)); \"\n-                        \"y = layer(x)`\\n\"\n-                        \"2. If the `call()` method is correct, \"\n-                        \"then you may need to implement \"\n-                        \"the `def build(self, input_shape)` method on your \"\n-                        \"layer. It should create all variables used by the \"\n+                    \"E.g. `x = np.random.random((3, 4)); y = layer(x)`\\n\"\n+                    \"2. If the `call()` method is correct, then you may need \"\n+                    \"to implement the `def build(self, input_shape)` method on \"\n+                    \"your layer. It should create all variables used by the \"\n                     \"layer (e.g. by calling `layer.build()` on all its \"\n                     \"children layers).\"\n                 )\n-            self.built = True\n \n-            # Check input spec again (after build, since self.input_spec\n-            # may have been updated\n-            self._assert_input_compatibility(call_spec.first_arg)\n-            # Hook used to do post-build actions\n-            self._post_build()\n-        if not self._tracker.locked:\n-            # No state updates past this point.\n-            self._tracker.lock(\n-                msg=(\n-                    \"You cannot add new elements of state \"\n-                    \"(variables or sub-layers) \"\n-                    \"to a layer that is already built. All state \"\n-                    \"must be created in the `__init__()` method or \"\n-                    \"in the`build()` method.\"\n-                )\n-            )\n+        self.build(first_shape)\n \n     def _build_by_run(self, *args, **kwargs):\n         call_spec = CallSpec(self._call_signature, args, kwargs)\n\n@@ -154,13 +154,12 @@ class Functional(Function, Model):\n             self.trainable = trainable\n \n         self._layers = self.layers\n-        self.built = True\n+        self.build(None)\n         # We will convert directly (to the correct dtype per input).\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n         output_layers = [x._keras_history[0] for x in self.outputs]\n         self.output_names = [x.name for x in output_layers]\n-        self._post_build()\n \n     @property\n     def layers(self):\n\n@@ -116,14 +116,12 @@ class Sequential(Model):\n             self._maybe_rebuild()\n         else:\n             self.built = False\n-            self._tracker.locked = False\n             self._functional = None\n \n     def pop(self, rebuild=True):\n         \"\"\"Removes the last layer in the model.\"\"\"\n         layer = self._layers.pop()\n         self.built = False\n-        self._tracker.locked = False\n         self._functional = None\n         if rebuild:\n             self._maybe_rebuild()\n@@ -132,11 +130,14 @@ class Sequential(Model):\n     def _maybe_rebuild(self):\n         self.built = False\n         self._functional = None\n-        self._tracker.locked = False\n         if isinstance(self._layers[0], InputLayer) and len(self._layers) > 1:\n             input_shape = self._layers[0].batch_shape\n             self.build(input_shape)\n \n+    def _lock_state(self):\n+        # Unlike other layers, Sequential is mutable after build.\n+        pass\n+\n     def build(self, input_shape=None):\n         if not isinstance(input_shape, (tuple, list)):\n             # Do not attempt to build if the model does not have a single\n@@ -180,7 +181,6 @@ class Sequential(Model):\n         outputs = x\n         self._functional = Functional(inputs=inputs, outputs=outputs)\n         self.built = True\n-        self._post_build()\n \n     def call(self, inputs, training=None, mask=None):\n         if self._functional:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1b2250568b3b1c75a3dc43205135280276e2701", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 82 | Files Changed: 1 | Hunks: 14 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 110 | Churn Cumulative: 318 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,23 +1,22 @@\n+from keras_core.api_export import keras_core_export\n from keras_core.layers import Layer\n \n \n+@keras_core_export(\"keras_core.utils.TorchModuleWrapper\")\n class TorchModuleWrapper(Layer):\n     \"\"\"Torch module wrapper layer.\n \n-    `TorchModuleWrapper` is an abstraction that can be wrapped around a\n-    `torch.nn.Module` to make its parameters trackable as a\n-    `keras_core.layers.Layer`. It works with both vanilla and lazy PyTorch\n-    modules.\n+    `TorchModuleWrapper` is a wrapper class that can turn any\n+    `torch.nn.Module` into a Keras layer, in particular by making its\n+    parameters trackable by Keras.\n \n     Args:\n-        module: torch.nn.Module, A vanilla or lazy PyTorch neural network\n-            module.\n+        module: `torch.nn.Module` instance. If it's a `LazyModule`\n+            instance, then its parameters must be initialized before\n+            passing the instance to `TorchModuleWrapper` (e.g. by calling\n+            it once).\n         name: The name of the layer (string).\n \n-    References:\n-    - [PyTorch docs for `torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) # noqa: E501\n-    - [PyTorch docs for `LazyModuleMixin`](https://pytorch.org/docs/stable/generated/torch.nn.modules.lazy.LazyModuleMixin.html) # noqa: E501\n-\n     Examples:\n \n     Here's an example of how the `TorchModuleWrapper` can be used with vanilla\n@@ -30,23 +29,20 @@ class TorchModuleWrapper(Layer):\n     import keras_core\n     from keras_core.backend.torch import TorchModuleWrapper\n \n-\n     class Classifier(keras_core.Model):\n-\n         def __init__(self, *args, **kwargs):\n             super().__init__(*args, **kwargs)\n-            # Wrap all `torch.nn.Module`s with `TorchModuleWrapper`\n+            # Wrap `torch.nn.Module`s with `TorchModuleWrapper`\n+            # if they contain parameters\n             self.conv1 = TorchModuleWrapper(\n                 nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3))\n             )\n             self.conv2 = TorchModuleWrapper(\n                 nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3))\n             )\n-            self.pool = TorchModuleWrapper(\n-                nn.MaxPool2d(kernel_size=(2, 2))\n-            )\n-            self.flatten = TorchModuleWrapper(nn.Flatten())\n-            self.dropout = TorchModuleWrapper(nn.Dropout(p=0.5))\n+            self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n+            self.flatten = nn.Flatten()\n+            self.dropout = nn.Dropout(p=0.5)\n             self.fc = TorchModuleWrapper(nn.Linear(1600, 10))\n \n         def call(self, inputs):\n@@ -60,57 +56,6 @@ class TorchModuleWrapper(Layer):\n             return F.softmax(x, dim=1)\n \n \n-    model = Classifier()\n-    model.build((1, 28, 28))\n-    print(\"Output shape:\", model(torch.ones(1, 1, 28, 28).to(\"cuda\")).shape)\n-\n-    model.compile(\n-        loss=\"sparse_categorical_crossentropy\",\n-        optimizer=\"adam\",\n-        metrics=[\"accuracy\"]\n-    )\n-    model.fit(train_loader, epochs=5)\n-    ```\n-\n-    Here's an example of how the `TorchModuleWrapper` can be used with PyTorch\n-    Lazy modules.\n-\n-    ```python\n-    import torch.nn as nn\n-    import torch.nn.functional as F\n-\n-    import keras_core\n-    from keras_core.backend.torch import TorchModuleWrapper\n-\n-\n-    class LazyClassifier(keras.Model):\n-\n-        def __init__(self, *args, **kwargs):\n-            super().__init__(*args, **kwargs)\n-            # You can wrap all `torch.nn.Module`s with `TorchModuleWrapper`\n-            # irrespective of whether they are lazy or not.\n-            self.conv1 = TorchModuleWrapper(\n-                nn.LazyConv2d(out_channels=32, kernel_size=(3, 3))\n-            )\n-            self.conv2 = TorchModuleWrapper(\n-                nn.LazyConv2d(out_channels=64, kernel_size=(3, 3))\n-            )\n-            self.pool = TorchModuleWrapper(nn.MaxPool2d(kernel_size=(2, 2)))\n-            self.flatten = TorchModuleWrapper(nn.Flatten())\n-            self.dropout = TorchModuleWrapper(nn.Dropout(p=0.5))\n-            self.fc = TorchModuleWrapper(nn.LazyLinear(10))\n-\n-        def call(self, inputs):\n-            x = F.relu(self.conv1(inputs))\n-            x = self.pool(x)\n-            x = F.relu(self.conv2(x))\n-            x = self.pool(x)\n-            x = self.flatten(x)\n-            x = self.dropout(x)\n-            x = self.fc(x)\n-            return F.softmax(x, dim=1)\n-\n-\n     model = Classifier()\n     model.build((1, 28, 28))\n     print(\"Output shape:\", model(torch.ones(1, 1, 28, 28).to(\"cuda\")).shape)\n@@ -126,15 +71,21 @@ class TorchModuleWrapper(Layer):\n \n     def __init__(self, module, name=None):\n         super().__init__(name=name)\n-        self.module = module\n         import torch.nn as nn\n \n-        # self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-        # self.module = module.to(self.device)\n-        self._is_lazy_module = isinstance(\n-            self.module, nn.modules.lazy.LazyModuleMixin\n+        if (\n+            isinstance(module, nn.modules.lazy.LazyModuleMixin)\n+            and module.has_uninitialized_params()\n+        ):\n+            raise ValueError(\n+                \"LazyModules are not supported unless they \"\n+                \"are already initialized. \"\n+                f\"Received uninitialized LazyModule: module={module}\"\n             )\n-        if not self._is_lazy_module:\n+\n+        from keras_core.backend.torch.core import get_device\n+\n+        self.module = module.to(get_device())\n         self._track_module_parameters()\n \n     def parameters(self, recurse=True):\n@@ -151,10 +102,5 @@ class TorchModuleWrapper(Layer):\n             self._track_variable(variable)\n         self.built = True\n \n-    def build(self, *args, **kwargs):\n-        if self._is_lazy_module:\n-            self._build_by_run(*args, **kwargs)\n-        self.track_module_parameters()\n-\n-    def call(self, inputs, **kwargs):\n-        return self.module.forward(inputs, **kwargs)\n+    def call(self, *args, **kwargs):\n+        return self.module.forward(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f115643142b5426a119f985a8514c37e396d5d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 25 | Churn Cumulative: 25 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,25 @@\n+import numpy as np\n+import torch\n+\n+from keras_core import models\n+from keras_core import testing\n+from keras_core.utils.torch_utils import TorchModuleWrapper\n+\n+\n+class Classifier(models.Model):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.fc = TorchModuleWrapper(torch.nn.Linear(2, 4))\n+\n+    def call(self, x):\n+        return self.fc(x)\n+\n+\n+class TorchUtilsTest(testing.TestCase):\n+    def test_basic_usage(self):\n+        model = Classifier()\n+        self.assertEqual(len(model.layers), 1)\n+        self.assertEqual(len(model.trainable_weights), 2)\n+        model(np.random.random((3, 2)))\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(np.random.random((3, 2)), np.random.random((3, 4)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc6ecf2d6f413613701539d132639ce273aadb88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 19 | Files Changed: 5 | Hunks: 17 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 1865 | Contributors (this commit): 9 | Commits (past 90d): 37 | Contributors (cumulative): 25 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -123,17 +123,18 @@ class RandomBrightness(TFDataLayer):\n             return inputs\n \n     def _randomly_adjust_brightness(self, images):\n-        rank = len(images.shape)\n+        images_shape = self.backend.shape(images)\n+        rank = len(images_shape)\n         if rank == 3:\n             rgb_delta_shape = (1, 1, 1)\n         elif rank == 4:\n             # Keep only the batch dim. This will ensure to have same adjustment\n             # with in one image, but different across the images.\n-            rgb_delta_shape = [self.backend.shape(images)[0], 1, 1, 1]\n+            rgb_delta_shape = [images_shape[0], 1, 1, 1]\n         else:\n             raise ValueError(\n                 \"Expected the input image to be rank 3 or 4. Received \"\n-                f\"inputs.shape={images.shape}\"\n+                f\"inputs.shape={images_shape}\"\n             )\n \n         seed_generator = self._get_seed_generator(self.backend._backend)\n\n@@ -48,10 +48,13 @@ class RandomFlip(TFDataLayer):\n         self._allow_non_tensor_positional_args = True\n \n     def _randomly_flip_inputs(self, inputs):\n-        unbatched = len(inputs.shape) == 3\n+        inputs_shape = self.backend.shape(inputs)\n+        unbatched = len(inputs_shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n-        batch_size = self.backend.shape(inputs)[0]\n+            inputs_shape = self.backend.shape(inputs)\n+\n+        batch_size = inputs_shape[0]\n         flipped_outputs = inputs\n         seed_generator = self._get_seed_generator(self.backend._backend)\n         if self.mode == HORIZONTAL or self.mode == HORIZONTAL_AND_VERTICAL:\n\n@@ -178,6 +178,8 @@ class RandomRotation(TFDataLayer):\n             else:\n                 image_height = shape[1]\n                 image_width = shape[2]\n+        image_height = float(image_height)\n+        image_width = float(image_width)\n \n         lower = self._factor[0] * 2.0 * self.backend.convert_to_tensor(np.pi)\n         upper = self._factor[1] * 2.0 * self.backend.convert_to_tensor(np.pi)\n\n@@ -164,17 +164,19 @@ class RandomTranslation(TFDataLayer):\n             return inputs\n \n     def _randomly_translate_inputs(self, inputs):\n-        unbatched = len(inputs.shape) == 3\n+        inputs_shape = self.backend.shape(inputs)\n+        unbatched = len(inputs_shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n+            inputs_shape = self.backend.shape(inputs)\n \n-        batch_size = self.backend.shape(inputs)[0]\n+        batch_size = inputs_shape[0]\n         if self.data_format == \"channels_first\":\n-            height = inputs.shape[-2]\n-            width = inputs.shape[-1]\n+            height = inputs_shape[-2]\n+            width = inputs_shape[-1]\n         else:\n-            height = inputs.shape[-3]\n-            width = inputs.shape[-2]\n+            height = inputs_shape[-3]\n+            width = inputs_shape[-2]\n \n         seed_generator = self._get_seed_generator(self.backend._backend)\n         height_translate = self.backend.random.uniform(\n\n@@ -171,17 +171,19 @@ class RandomZoom(TFDataLayer):\n             return inputs\n \n     def _randomly_zoom_inputs(self, inputs):\n-        unbatched = len(inputs.shape) == 3\n+        inputs_shape = self.backend.shape(inputs)\n+        unbatched = len(inputs_shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n+            inputs_shape = self.backend.shape(inputs)\n \n-        batch_size = self.backend.shape(inputs)[0]\n+        batch_size = inputs_shape[0]\n         if self.data_format == \"channels_first\":\n-            height = inputs.shape[-2]\n-            width = inputs.shape[-1]\n+            height = inputs_shape[-2]\n+            width = inputs_shape[-1]\n         else:\n-            height = inputs.shape[-3]\n-            width = inputs.shape[-2]\n+            height = inputs_shape[-3]\n+            width = inputs_shape[-2]\n \n         seed_generator = self._get_seed_generator(self.backend._backend)\n         height_zoom = self.backend.random.uniform(\n@@ -225,8 +227,8 @@ class RandomZoom(TFDataLayer):\n         #      [0 0 1]]\n         # where the last entry is implicit.\n         # zoom matrices are always float32.\n-        x_offset = ((image_width - 1.0) / 2.0) * (1.0 - zooms[:, 0:1])\n-        y_offset = ((image_height - 1.0) / 2.0) * (1.0 - zooms[:, 1:])\n+        x_offset = ((float(image_width) - 1.0) / 2.0) * (1.0 - zooms[:, 0:1])\n+        y_offset = ((float(image_height) - 1.0) / 2.0) * (1.0 - zooms[:, 1:])\n         return self.backend.numpy.concatenate(\n             [\n                 zooms[:, 0:1],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
