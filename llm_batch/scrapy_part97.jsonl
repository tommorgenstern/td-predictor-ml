{"custom_id": "scrapy#3bbad369984020f7900e729191e0a04a8582270f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 22 | Churn Cumulative: 301 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -65,9 +65,15 @@ class DictItem(DictMixin, BaseItem):\n \n     def __getattr__(self, name):\n         if name in self.fields:\n-            raise AttributeError(\"Use [%r] to access item field value\" % name)\n+            raise AttributeError(\"Use item[%r] to get field value\" % name)\n         raise AttributeError(name)\n \n+    def __setattr__(self, name, value):\n+        if not name.startswith('_'):\n+            raise AttributeError(\"Use item[%r] = %r to set field value\" % \\\n+                (name, value))\n+        super(DictItem, self).__setattr__(name, value)\n+\n     def keys(self):\n         return self._values.keys()\n \n\n@@ -71,6 +71,20 @@ class ItemTest(unittest.TestCase):\n         i._private = 'test'\n         self.assertEqual(i._private, 'test')\n \n+    def test_raise_getattr(self):\n+        class TestItem(Item):\n+            name = Field()\n+\n+        i = TestItem()\n+        self.assertRaises(AttributeError, getattr, i, 'name')\n+\n+    def test_raise_setattr(self):\n+        class TestItem(Item):\n+            name = Field()\n+\n+        i = TestItem()\n+        self.assertRaises(AttributeError, setattr, i, 'name', 'john')\n+\n     def test_custom_methods(self):\n         class TestItem(Item):\n             name = Field()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9148321765e8ddaaab23a36c8adc6af5340c3238", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 328 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -112,6 +112,16 @@ class ResponseUtilsTest(unittest.TestCase):\n         response = TextResponse(url='http://example.org')\n         self.assertEqual(get_meta_refresh(response), (None, None))\n \n+        # html commented meta refresh header must not directed\n+        body = \"\"\"<!--<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/\">-->\"\"\"\n+        response = TextResponse(url='http://example.com', body=body)\n+        self.assertEqual(get_meta_refresh(response), (None, None))\n+\n+        # html comments must not interfere with uncommented meta refresh header\n+        body = \"\"\"<!-- commented --><meta http-equiv=\"refresh\" content=\"3; url=http://example.com/\">-->\"\"\"\n+        response = TextResponse(url='http://example.com', body=body)\n+        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/'))\n+\n     def test_response_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n         self.assertEqual(response_httprepr(r1), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n\n@@ -12,7 +12,7 @@ import tempfile\n from twisted.web import http\n from twisted.web.http import RESPONSES\n \n-from scrapy.utils.markup import remove_entities\n+from scrapy.utils.markup import remove_entities, remove_comments\n from scrapy.utils.url import safe_url_string, urljoin_rfc\n from scrapy.xlib.BeautifulSoup import BeautifulSoup\n from scrapy.http import Response, HtmlResponse\n@@ -48,7 +48,7 @@ def get_meta_refresh(response):\n     If no meta redirect is found, (None, None) is returned.\n     \"\"\"\n     if response not in _metaref_cache:\n-        body_chunk = remove_entities(response.body_as_unicode()[0:4096])\n+        body_chunk = remove_comments(remove_entities(response.body_as_unicode()[0:4096]))\n         match = META_REFRESH_RE.search(body_chunk)\n         if match:\n             interval = int(match.group('int'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22555df56a90615d64c886d4655ed1be95e04408", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 333 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -129,6 +129,9 @@ class ResponseUtilsTest(unittest.TestCase):\n         r1 = Response(\"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n         self.assertEqual(response_httprepr(r1), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n+        r1 = Response(\"http://www.example.com\", status=6666, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n+        self.assertEqual(response_httprepr(r1), 'HTTP/1.1 6666 \\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+\n     def test_get_cached_beautifulsoup(self):\n         r1 = Response('http://www.example.com', body='')\n \n\n@@ -85,7 +85,7 @@ def response_httprepr(response):\n     that was received (that's not exposed by Twisted).\n     \"\"\"\n \n-    s  = \"HTTP/1.1 %d %s\\r\\n\" % (response.status, RESPONSES[response.status])\n+    s = \"HTTP/1.1 %d %s\\r\\n\" % (response.status, RESPONSES.get(response.status, ''))\n     if response.headers:\n         s += response.headers.to_string() + \"\\r\\n\"\n     s += \"\\r\\n\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#861c04fb7ecfed5dcf78999ca7754d631f679d83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 180 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -96,12 +96,12 @@ class ResponseUtilsTest(unittest.TestCase):\n         response = TextResponse(url='http://example.com', body=body, encoding='utf-16')\n         self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/redirect'))\n \n-        # non-ascii chars in the url (default encoding - utf8)\n+        # non-ascii chars in the url (utf8)\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/to\\xc2\\xa3\">\"\"\"\n-        response = TextResponse(url='http://example.com', body=body)\n+        response = TextResponse(url='http://example.com', body=body, encoding='utf-8')\n         self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/to%C2%A3'))\n \n-        # non-ascii chars in the url (custom encoding - latin1)\n+        # non-ascii chars in the url (latin1)\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/to\\xa3\">\"\"\"\n         response = TextResponse(url='http://example.com', body=body, encoding='latin1')\n         self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/to%C2%A3'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6525d3fe8c0db067841118830577ac21ee637699", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 65 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 9, 0, 'rc1')\n-__version__ = \"0.9-rc1\"\n+version_info = (0, 9, 0, '')\n+__version__ = \"0.9\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b258fc3305ddceaa5d0df6138db4f99a28861a92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 352 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,6 +122,15 @@ class ResponseUtilsTest(unittest.TestCase):\n         response = TextResponse(url='http://example.com', body=body)\n         self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/'))\n \n+        # float refresh intervals\n+        body = \"\"\"<meta http-equiv=\"refresh\" content=\".1;URL=index.html\" />\"\"\"\n+        response = TextResponse(url='http://example.com', body=body)\n+        self.assertEqual(get_meta_refresh(response), (0.1, 'http://example.com/index.html'))\n+\n+        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3.1;URL=index.html\" />\"\"\"\n+        response = TextResponse(url='http://example.com', body=body)\n+        self.assertEqual(get_meta_refresh(response), (3.1, 'http://example.com/index.html'))\n+\n     def test_response_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n         self.assertEqual(response_httprepr(r1), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n\n@@ -36,7 +36,7 @@ def get_base_url(response):\n         _baseurl_cache[response] = urljoin_rfc(response.url, match.group(1)) if match else response.url\n     return _baseurl_cache[response]\n \n-META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\\s*=\\s*(?P<quote>[\"\\'])(?P<int>\\d+)\\s*;\\s*url=(?P<url>.*?)(?P=quote)', \\\n+META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\\s*=\\s*(?P<quote>[\"\\'])(?P<int>(\\d*\\.)?\\d+)\\s*;\\s*url=(?P<url>.*?)(?P=quote)', \\\n     re.DOTALL | re.IGNORECASE)\n _metaref_cache = weakref.WeakKeyDictionary()\n def get_meta_refresh(response):\n@@ -51,7 +51,7 @@ def get_meta_refresh(response):\n         body_chunk = remove_comments(remove_entities(response.body_as_unicode()[0:4096]))\n         match = META_REFRESH_RE.search(body_chunk)\n         if match:\n-            interval = int(match.group('int'))\n+            interval = float(match.group('int'))\n             url = safe_url_string(match.group('url').strip(' \"\\''))\n             url = urljoin_rfc(response.url, url)\n             _metaref_cache[response] = (interval, url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ddbbc815202734d1c98b474d7d9c58bc896654e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 312 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 0.9333333333333333\n\nDIFF:\n@@ -62,4 +62,6 @@ class FormRequest(Request):\n         else:\n             url, body, headers = form.click_request_data(**(clickdata or {}))\n \n-        return cls(url, method=form.method, body=body, headers=headers, **kwargs)\n+        kwargs.setdefault('headers', {}).update(headers)\n+\n+        return cls(url, method=form.method, body=body, **kwargs)\n\n@@ -231,6 +231,21 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs['test2'].value, 'xxx')\n         self.assertEqual(fs['six'].value, 'seven')\n \n+    def test_from_response_extra_headers(self):\n+        respbody = \"\"\"\n+<form action=\"post.php\" method=\"POST\">\n+<input type=\"hidden\" name=\"test\" value=\"val1\">\n+<input type=\"hidden\" name=\"test\" value=\"val2\">\n+<input type=\"hidden\" name=\"test2\" value=\"xxx\">\n+</form>\n+        \"\"\"\n+        headers = {\"Accept-Encoding\": \"gzip,deflate\"}\n+        response = Response(\"http://www.example.com/this/list.html\", body=respbody)\n+        r1 = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'}, headers=headers, callback=lambda x: x)\n+        self.assertEqual(r1.method, 'POST')\n+        self.assertEqual(r1.headers['Content-type'], 'application/x-www-form-urlencoded')\n+        self.assertEqual(r1.headers['Accept-Encoding'], 'gzip,deflate')\n+\n     def test_from_response_get(self):\n         respbody = \"\"\"\n <form action=\"get.php\" method=\"GET\">\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b3a65d3313724ab2dc9f3dca0c0b0a025ed68fe5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 26 | Churn Cumulative: 894 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -123,6 +123,7 @@ HTTPCACHE_DIR = ''\n HTTPCACHE_IGNORE_MISSING = False\n HTTPCACHE_STORAGE = 'scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage'\n HTTPCACHE_EXPIRATION_SECS = 0\n+HTTPCACHE_IGNORE_HTTP_CODES = []\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n\n@@ -22,6 +22,7 @@ class HttpCacheMiddleware(object):\n     def __init__(self, settings=conf.settings):\n         self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n+        self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n@@ -35,17 +36,20 @@ class HttpCacheMiddleware(object):\n         if not self.is_cacheable(request):\n             return\n         response = self.storage.retrieve_response(spider, request)\n-        if response:\n+        if response and self.is_cacheable_response(response):\n             response.flags.append('cached')\n             return response\n         elif self.ignore_missing:\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n     def process_response(self, request, response, spider):\n-        if self.is_cacheable(request):\n+        if self.is_cacheable(request) and self.is_cacheable_response(response):\n             self.storage.store_response(spider, request, response)\n         return response\n \n+    def is_cacheable_response(self, response):\n+        return response.status not in self.ignore_http_codes\n+\n     def is_cacheable(self, request):\n         return urlparse_cached(request).scheme in ['http', 'https']\n \n\n@@ -24,6 +24,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         settings = {\n             'HTTPCACHE_DIR': self.tmpdir,\n             'HTTPCACHE_EXPIRATION_SECS': 1,\n+            'HTTPCACHE_IGNORE_HTTP_CODES': [],\n         }\n         settings.update(new_settings)\n         return Settings(settings)\n@@ -76,6 +77,22 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         self.assertEqualResponse(self.response, response)\n         assert 'cached' in response.flags\n \n+    def test_middleware_ignore_http_codes(self):\n+        # test response is not cached\n+        mw = self._get_middleware(HTTPCACHE_IGNORE_HTTP_CODES=[202])\n+        assert mw.process_request(self.request, self.spider) is None\n+        mw.process_response(self.request, self.response, self.spider)\n+        assert mw.storage.retrieve_response(self.spider, self.request) is None\n+        assert mw.process_request(self.request, self.spider) is None\n+\n+        # test response is cached\n+        mw = self._get_middleware(HTTPCACHE_IGNORE_HTTP_CODES=[203])\n+        mw.process_response(self.request, self.response, self.spider)\n+        response = mw.process_request(self.request, self.spider)\n+        assert isinstance(response, HtmlResponse)\n+        self.assertEqualResponse(self.response, response)\n+        assert 'cached' in response.flags\n+\n     def assertEqualResponse(self, response1, response2):\n         self.assertEqual(response1.url, response2.url)\n         self.assertEqual(response1.status, response2.status)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc32f6ec66fed89f5ee6b13d51589de9a7dee267", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 13 | Churn Cumulative: 15 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,8 @@\n+import unittest\n+\n+from scrapy.xlib import ClientForm\n+\n+class ClientFormPatchTests(unittest.TestCase):\n+\n+    def test_patched_unescape_charref(self):\n+        self.assertEqual(ClientForm.unescape_charref('c', 'utf-8'), 'c')\n\n@@ -241,7 +241,12 @@ def unescape_charref(data, encoding):\n     name, base = data, 10\n     if name.startswith(\"x\"):\n         name, base= name[1:], 16\n+    try:\n         uc = unichr(int(name, base))\n+    except ValueError:\n+        # invalid literal for int()\n+        # or integer not in unichr()'s range\n+        uc = name\n     if encoding is None:\n         return uc\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec850b9fd131d2a1f2555c70649ca90001757f4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 133 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/topics/extensions.rst\n \n import os\n import socket\n+from pprint import pformat\n \n from twisted.internet import task\n \n@@ -97,6 +98,6 @@ class MemoryUsage(object):\n \n         s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n         s += \"\\r\\n\"\n-        s += get_engine_status()\n+        s += pformat(get_engine_status())\n         s += \"\\r\\n\"\n         self.mail.send(rcpts, subject, s)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b8aa74ee9ec6546ff615d7f0759bb2bcff50e1d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 76 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,10 +63,13 @@ class UtilsRequestTest(unittest.TestCase):\n \n     def test_request_httprepr(self):\n         r1 = Request(\"http://www.example.com\")\n-        self.assertEqual(request_httprepr(r1), 'GET http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n+        self.assertEqual(request_httprepr(r1), 'GET / HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n+\n+        r1 = Request(\"http://www.example.com/some/page.html?arg=1\")\n+        self.assertEqual(request_httprepr(r1), 'GET /some/page.html?arg=1 HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n \n         r1 = Request(\"http://www.example.com\", method='POST', headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n-        self.assertEqual(request_httprepr(r1), 'POST http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+        self.assertEqual(request_httprepr(r1), 'POST / HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -6,6 +6,7 @@ scrapy.http.Request objects\n import hashlib\n import weakref\n from base64 import urlsafe_b64encode\n+from urlparse import urlunparse\n \n from scrapy.utils.url import canonicalize_url\n from scrapy.utils.httpobj import urlparse_cached\n@@ -76,9 +77,10 @@ def request_httprepr(request):\n     bytes that will be send when performing the request (that's controlled\n     by Twisted).\n     \"\"\"\n-    hostname = urlparse_cached(request).hostname\n-    s  = \"%s %s HTTP/1.1\\r\\n\" % (request.method, request.url)\n-    s += \"Host: %s\\r\\n\" % hostname\n+    parsed = urlparse_cached(request)\n+    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n+    s  = \"%s %s HTTP/1.1\\r\\n\" % (request.method, path)\n+    s += \"Host: %s\\r\\n\" % parsed.hostname\n     if request.headers:\n         s += request.headers.to_string() + \"\\r\\n\"\n     s += \"\\r\\n\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b799e5ee373a26c06abd7ee59271722df746a3e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 38 | Churn Cumulative: 89 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,15 +3,27 @@ DefaultHeaders downloader middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-\n from scrapy.conf import settings\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.core import signals\n+\n \n class DefaultHeadersMiddleware(object):\n \n     def __init__(self):\n-        self.default_headers = settings.get('DEFAULT_REQUEST_HEADERS')\n+        self.global_default_headers = settings.get('DEFAULT_REQUEST_HEADERS')\n+        self._default_headers = {}\n+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n     def process_request(self, request, spider):\n-        for k, v in self.default_headers.iteritems():\n+        for k, v in self._default_headers[spider].iteritems():\n             if v:\n                 request.headers.setdefault(k, v)\n+\n+    def spider_opened(self, spider):\n+        self._default_headers[spider] = dict(self.global_default_headers,\n+                **getattr(spider, 'default_headers', {}))\n+\n+    def spider_closed(self, spider):\n+        self._default_headers.pop(spider)\n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.conf import settings\n from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware\n-from scrapy.http import Response, Request\n+from scrapy.http import Request\n from scrapy.spider import BaseSpider\n \n \n@@ -16,15 +16,33 @@ class TestDefaultHeadersMiddleware(TestCase):\n \n     def test_process_request(self):\n         req = Request('http://www.scrapytest.org')\n+        self.mw.spider_opened(self.spider)\n         self.mw.process_request(req, self.spider)\n+        self.mw.spider_closed(self.spider)\n         self.assertEquals(req.headers, self.default_headers)\n \n+    def test_spider_default_headers(self):\n+        spider_headers = {'Unexistant-Header': ['value']}\n+        # override one of the global default headers by spider\n+        if self.default_headers:\n+            k = set(self.default_headers).pop()\n+            spider_headers[k] = ['__newvalue__']\n+        self.spider.default_headers = spider_headers\n+\n+        req = Request('http://www.scrapytest.org')\n+        self.mw.spider_opened(self.spider)\n+        self.mw.process_request(req, self.spider)\n+        self.mw.spider_closed(self.spider)\n+        self.assertEquals(req.headers, dict(self.default_headers, **spider_headers))\n+\n     def test_update_headers(self):\n         headers = {'Accept-Language': ['es'], 'Test-Header': ['test']}\n         req = Request('http://www.scrapytest.org', headers=headers)\n         self.assertEquals(req.headers, headers)\n \n+        self.mw.spider_opened(self.spider)\n         self.mw.process_request(req, self.spider)\n+        self.mw.spider_closed(self.spider)\n         self.default_headers.update(headers)\n         self.assertEquals(req.headers, self.default_headers)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e013f564b95ea9022ce8cff39c2210bcf76e61e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 109 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ class DefaultHeadersMiddleware(object):\n \n     def spider_opened(self, spider):\n         self._default_headers[spider] = dict(self.global_default_headers,\n-                **getattr(spider, 'default_headers', {}))\n+                **getattr(spider, 'default_request_headers', {}))\n \n     def spider_closed(self, spider):\n         self._default_headers.pop(spider)\n\n@@ -11,7 +11,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n     def setUp(self):\n         self.spider = BaseSpider('foo')\n         self.mw = DefaultHeadersMiddleware()\n-        self.default_headers = dict([(k, [v]) for k, v in \\\n+        self.default_request_headers = dict([(k, [v]) for k, v in \\\n             settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])\n \n     def test_process_request(self):\n@@ -19,21 +19,21 @@ class TestDefaultHeadersMiddleware(TestCase):\n         self.mw.spider_opened(self.spider)\n         self.mw.process_request(req, self.spider)\n         self.mw.spider_closed(self.spider)\n-        self.assertEquals(req.headers, self.default_headers)\n+        self.assertEquals(req.headers, self.default_request_headers)\n \n-    def test_spider_default_headers(self):\n+    def test_spider_default_request_headers(self):\n         spider_headers = {'Unexistant-Header': ['value']}\n         # override one of the global default headers by spider\n-        if self.default_headers:\n-            k = set(self.default_headers).pop()\n+        if self.default_request_headers:\n+            k = set(self.default_request_headers).pop()\n             spider_headers[k] = ['__newvalue__']\n-        self.spider.default_headers = spider_headers\n+        self.spider.default_request_headers = spider_headers\n \n         req = Request('http://www.scrapytest.org')\n         self.mw.spider_opened(self.spider)\n         self.mw.process_request(req, self.spider)\n         self.mw.spider_closed(self.spider)\n-        self.assertEquals(req.headers, dict(self.default_headers, **spider_headers))\n+        self.assertEquals(req.headers, dict(self.default_request_headers, **spider_headers))\n \n     def test_update_headers(self):\n         headers = {'Accept-Language': ['es'], 'Test-Header': ['test']}\n@@ -43,6 +43,6 @@ class TestDefaultHeadersMiddleware(TestCase):\n         self.mw.spider_opened(self.spider)\n         self.mw.process_request(req, self.spider)\n         self.mw.spider_closed(self.spider)\n-        self.default_headers.update(headers)\n-        self.assertEquals(req.headers, self.default_headers)\n+        self.default_request_headers.update(headers)\n+        self.assertEquals(req.headers, self.default_request_headers)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68c7ef7d9811a0dcc216da60b304cc7e69a253c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 517 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,23 +79,24 @@ class Scraper(object):\n         site = self.sites[spider]\n         site.closing = defer.Deferred()\n         self.itemproc.close_spider(spider)\n+        self._check_if_closing(spider, site)\n+        return site.closing\n \n     def is_idle(self):\n         \"\"\"Return True if there isn't any more spiders to process\"\"\"\n         return not self.sites\n \n-    def enqueue_scrape(self, response, request, spider):\n-        site = self.sites[spider]\n-        dfd = site.add_response_request(response, request)\n-        # FIXME: this can't be called here because the stats spider may be\n-        # already closed\n-        #stats.max_value('scraper/max_active_size', site.active_size, \\\n-        #    spider=spider)\n-        def finish_scraping(_):\n-            site.finish_response(response)\n+    def _check_if_closing(self, spider, site):\n         if site.closing and site.is_idle():\n             del self.sites[spider]\n             site.closing.callback(None)\n+\n+    def enqueue_scrape(self, response, request, spider):\n+        site = self.sites[spider]\n+        dfd = site.add_response_request(response, request)\n+        def finish_scraping(_):\n+            site.finish_response(response)\n+            self._check_if_closing(spider, site)\n             self._scrape_next(spider, site)\n             return _\n         dfd.addBoth(finish_scraping)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2290a5359ee80c75d8bcf8de8b6894e61fad83a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 57 | Files Changed: 1 | Hunks: 15 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 64 | Churn Cumulative: 102 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,84 +13,40 @@ from scrapy.utils.spider import iterate_spider_output\n from scrapy.contrib.spiders.init import InitSpider\n from scrapy.conf import settings\n \n+def identity(x):\n+    return x\n+\n class Rule(object):\n-    \"\"\"\n-    A rule for crawling, which receives the following constructor arguments:\n \n-    link_extractor (required)\n-       A LinkExtractor which defines the policy for extracting links\n-    callback (optional)\n-       A function to use to process the page once it has been downloaded. If\n-       callback is omitted the page is not procesed, just crawled. If callback\n-       is a string (instead of callable) a method of the spider class with that\n-       name is used as the callback function\n-    cb_kwargs (optional)\n-       A dict specifying keyword arguments to pass to the callback function\n-    follow (optional)\n-       If True, links will be followed from the pages crawled by this rule.\n-       It defaults to True when no callback is specified or False when a\n-       callback is specified\n-    process_links (optional)\n-       Can be either a callable, or a string with the name of a method defined\n-       in the spider's class.\n-       This method will be called with the list of extracted links matching\n-       this rule (if any) and must return another list of links.\n-    \"\"\"\n-\n-    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None):\n+    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=identity):\n         self.link_extractor = link_extractor\n         self.callback = callback\n         self.cb_kwargs = cb_kwargs or {}\n         self.process_links = process_links\n+        self.process_request = process_request\n         if follow is None:\n             self.follow = False if callback else True\n         else:\n             self.follow = follow\n \n class CrawlSpider(InitSpider):\n-    \"\"\"\n-    Class for spiders that crawl over web pages and extract/parse their links\n-    given some crawling rules.\n \n-    These crawling rules are established by setting the 'rules' class attribute,\n-    which is a tuple of Rule objects.\n-    When the spider is running, it iterates over these rules with each response\n-    and do what it has to (extract links if follow=True, and return items/requests if\n-    there's a parsing method defined in the rule).\n-    \"\"\"\n     rules = ()\n \n     def __init__(self, *a, **kw):\n-        \"\"\"Constructor takes care of compiling rules\"\"\"\n         super(CrawlSpider, self).__init__(*a, **kw)\n         self._compile_rules()\n \n     def parse(self, response):\n-        \"\"\"This function is called by the framework core for all the\n-        start_urls. Do not override this function, override parse_start_url\n-        instead.\"\"\"\n         return self._response_downloaded(response, self.parse_start_url, cb_kwargs={}, follow=True)\n \n     def parse_start_url(self, response):\n-        \"\"\"Overrideable callback function for processing start_urls. It must\n-        return a list of BaseItem and/or Requests\"\"\"\n         return []\n \n     def process_results(self, response, results):\n-        \"\"\"This overridable method is called for each result (item or request)\n-        returned by the spider, and it's intended to perform any last time\n-        processing required before returning the results to the framework core,\n-        for example setting the item GUIDs. It receives a list of results and\n-        the response which originated that results. It must return a list\n-        of results (Items or Requests).\"\"\"\n         return results\n \n     def _requests_to_follow(self, response):\n-        \"\"\"\n-        This method iterates over each of the spider's rules, extracts the links\n-        matching each case, filters them (if needed), and returns a list of unique\n-        requests per response.\n-        \"\"\"\n         seen = set()\n         for rule in self._rules:\n             links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]\n@@ -102,14 +58,9 @@ class CrawlSpider(InitSpider):\n                     cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n                 r = Request(url=link.url, callback=callback)\n                 r.meta['link_text'] = link.text\n-                yield r\n+                yield rule.process_request(r)\n \n     def _response_downloaded(self, response, callback, cb_kwargs, follow):\n-        \"\"\"\n-        This is were any response arrives, and were it's decided whether\n-        to extract links or not from it, and if it will be parsed or not.\n-        It returns a list of requests/items.\n-        \"\"\"\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n             cb_res = self.process_results(response, cb_res)\n@@ -122,8 +73,6 @@ class CrawlSpider(InitSpider):\n                 \n \n     def _compile_rules(self):\n-        \"\"\"Compile the crawling rules\"\"\"\n-\n         def get_method(method):\n             if callable(method):\n                 return method\n@@ -134,3 +83,4 @@ class CrawlSpider(InitSpider):\n         for rule in self._rules:\n             rule.callback = get_method(rule.callback)\n             rule.process_links = get_method(rule.process_links)\n+            rule.process_request = get_method(rule.process_request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2349d241e06e9032c60a1551a03dac029f7730be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 109 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -75,14 +75,7 @@ if len(sys.argv) > 1 and sys.argv[1] == 'bdist_wininst':\n         file_info[0] = '\\\\PURELIB\\\\%s' % file_info[0]\n \n # Dynamically calculate the version based on scrapy.__version__\n-version = __import__('scrapy').__version__\n-if 'dev' in version:\n-    try:\n-        from subprocess import Popen, PIPE\n-        hgrev = Popen(['hg', 'tip', '--template={rev}'], stdout=PIPE).communicate()[0]\n-        version = version.replace('-dev', '-r%s' % hgrev)\n-    except:\n-        pass\n+version = \".\".join(map(str, __import__('scrapy').version_info[:2]))\n \n setup_args = {\n     'name': 'Scrapy',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8781ef3914d1d6bf28db078ada1c7962e2e3cb86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 339 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -117,7 +117,7 @@ class TemplatePageParser(InstanceLearningParser):\n     def _handle_unpaired_tag(self, html_tag):\n         if self._read_bool_template_attribute(html_tag, \"ignore\") and html_tag.tag == \"img\":\n             self.ignored_regions.append((self.next_tag_index, self.next_tag_index + 1))\n-        elif self._read_bool_template_attribute(html_tag, \"ignore-beneath\") and html_tag.tag == \"img\":\n+        elif self._read_bool_template_attribute(html_tag, \"ignore-beneath\"):\n             self.ignored_regions.append((self.next_tag_index, None))\n         jannotation = self._read_template_annotation(html_tag)\n         if jannotation:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6312826220a5c3d662fa668626de2ceee66a8ad6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 433 | Files Changed: 9 | Hunks: 8 | Methods Changed: 29 | Complexity Δ (Sum/Max): -72/0 | Churn Δ: 433 | Churn Cumulative: 1239 | Contributors (this commit): 6 | Commits (past 90d): 23 | Contributors (cumulative): 25 | DMM Complexity: 0.46236559139784944\n\nDIFF:\n@@ -234,10 +234,6 @@ USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = 6023  # if None, uses a dynamic port\n \n-WEBCONSOLE_ENABLED = True\n-WEBCONSOLE_PORT = 6080\n-WEBCONSOLE_LOGFILE = None\n-\n WEBSERVICE_ENABLED = True\n WEBSERVICE_LOGFILE = None\n WEBSERVICE_PORT = 6080\n\n@@ -1,3 +0,0 @@\n-import warnings\n-warnings.warn(\"Web console is deprecated. Consider using web service instead.\", \\\n-    DeprecationWarning, stacklevel=2)\n\n@@ -1,27 +0,0 @@\n-\"\"\"\n-Scheduler information module for Scrapy webconsole\n-\"\"\"\n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.utils.engine import get_engine_status\n-from scrapy.management.web import banner\n-\n-class EngineStatus(object):\n-    webconsole_id = 'enginestatus'\n-    webconsole_name = 'Engine status'\n-\n-    def __init__(self):\n-        from scrapy.management.web import webconsole_discover_module\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        s = banner(self)\n-        s += \"<pre><code>\\n\"\n-        s += get_engine_status()\n-        s += \"</pre></code>\\n\"\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return s\n-\n-    def webconsole_discover_module(self):\n-        return self\n\n@@ -1,82 +0,0 @@\n-\"\"\"\n-Live statistics extension\n-\"\"\"\n-from datetime import datetime\n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core import signals\n-from scrapy.core.manager import scrapymanager\n-from scrapy.management.web import banner, webconsole_discover_module\n-\n-class SpiderStats(object):\n-    def __init__(self):\n-        self.scraped = 0\n-        self.crawled = 0\n-        self.started = None\n-\n-class LiveStats(object):\n-    webconsole_id = 'livestats'\n-    webconsole_name = 'Spider live statistics of current run'\n-\n-    def __init__(self):\n-        self.domains = {}\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n-        dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n-        dispatcher.connect(self.response_downloaded, signal=signals.response_downloaded)\n-\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def spider_opened(self, spider):\n-        pstats = SpiderStats()\n-        self.domains[spider] = pstats\n-        pstats.started = datetime.now().replace(microsecond=0)\n-\n-    def spider_closed(self, spider):\n-        del self.domains[spider]\n-\n-    def item_scraped(self, item, spider):\n-        self.domains[spider].scraped += 1\n-\n-    def response_downloaded(self, response, spider):\n-        # sometimes we download responses without opening/closing domains,\n-        # for example from scrapy shell\n-        if self.domains.get(spider):\n-            self.domains[spider].crawled += 1\n-            \n-    def webconsole_render(self, wc_request):\n-        sch = scrapymanager.engine.scheduler\n-        dwl = scrapymanager.engine.downloader\n-\n-        totdomains = totscraped = totcrawled = totscheduled = totactive = totdqueued = tottransf = 0\n-        s = banner(self)\n-        s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>Spider</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Run time</th></tr>\\n\"\n-        for spider in sorted(self.domains.keys()):\n-            scheduled = len(sch.pending_requests[spider]) if spider in sch.pending_requests else 0\n-            active = len(dwl.sites[spider].active) if spider in dwl.sites else 0\n-            dqueued = len(dwl.sites[spider].queue) if spider in dwl.sites else 0\n-            transf = len(dwl.sites[spider].transferring) if spider in dwl.sites else 0\n-            stats = self.domains[spider]\n-            runtime = datetime.now() - stats.started\n-\n-            s += '<tr><td>%s</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td>%s</td><td>%s</td></tr>\\n' % \\\n-                 (spider.name, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(runtime))\n-\n-            totdomains += 1\n-            totscraped += stats.scraped\n-            totcrawled += stats.crawled\n-            totscheduled += scheduled\n-            totactive += active\n-            totdqueued += dqueued\n-            tottransf += transf\n-        s += '<tr><td><b>%d domains</b></td><td align=\"right\"><b>%d</b></td><td align=\"right\"><b>%d</b></td><td align=\"right\"><b>%d</b></td><td align=\"right\"><b>%d</b></td><td align=\"right\"><b>%d</b></td><td align=\"right\"><b>%d</b></td><td/><td/></tr>\\n' % \\\n-             (totdomains, totscraped, totcrawled, totscheduled, totdqueued, totactive, tottransf)\n-        s += \"</table>\\n\"\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return s\n-\n-    def webconsole_discover_module(self):\n-        return self\n\n@@ -1,37 +0,0 @@\n-\"\"\"\n-Scheduler queue web console module \n-\n-See documentation in docs/topics/extensions.rst\n-\"\"\"\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core.manager import scrapymanager\n-from scrapy.management.web import banner, webconsole_discover_module\n-\n-class SchedulerQueue(object):\n-    webconsole_id = 'scheduler'\n-    webconsole_name = 'Scheduler queue'\n-\n-    def __init__(self):\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_discover_module(self):\n-        return self\n-\n-    def webconsole_render(self, wc_request):\n-        s = banner(self)\n-        s += \"<ul>\\n\"\n-        for domain, request_queue in scrapymanager.engine.scheduler.pending_requests.iteritems():\n-            s += \"<li>\\n\"\n-            s += \"%s (<b>%s</b> requests)\\n\" % (domain, len(request_queue))\n-            s += \"<ul>\\n\"\n-            for ((req, _), prio) in request_queue:\n-                s += \"<li><a href='%s'>%s</a> (priority: %d)</li>\\n\" % (req.url, req.url, prio)\n-            s += \"</ul>\\n\"\n-            s += \"</li>\\n\" \n-        s += \"</ul>\\n\"\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return s\n\n@@ -1,152 +0,0 @@\n-\"\"\"\n-Extensions for allowing spider control from web console\n-\"\"\"\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy.core import signals\n-from scrapy.core.manager import scrapymanager\n-from scrapy.spider import spiders\n-from scrapy.management.web import banner, webconsole_discover_module\n-from scrapy.conf import settings\n-\n-class Spiderctl(object):\n-    webconsole_id = 'spiderctl'\n-    webconsole_name = 'Spider control panel'\n-\n-    def __init__(self):\n-        self.running = {}\n-        self.finished = set()\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def spider_opened(self, spider):\n-        self.running[spider.name] = spider\n-\n-    def spider_closed(self, spider):\n-        del self.running[spider.name]\n-        self.finished.add(spider.name)\n-\n-    def webconsole_render(self, wc_request):\n-        if wc_request.args:\n-            changes = self.webconsole_control(wc_request)\n-\n-        self.scheduled = [s[0].name for s in scrapymanager.queue.spider_requests]\n-        self.idle = [d for d in self.enabled_spiders if d not in self.scheduled\n-                                                        and d not in self.running\n-                                                        and d not in self.finished]\n-\n-        s = banner(self)\n-        s += '<table border=1\">\\n'\n-        s += \"<tr><th>Idle (%d)</th><th>Scheduled (%d)</th><th>Running (%d/%d)</th><th>Finished (%d)</th></tr>\\n\" % \\\n-                (len(self.idle),\n-                 len(self.scheduled),\n-                 len(self.running),\n-                 settings['CONCURRENT_SPIDERS'],\n-                 len(self.finished))\n-        s += \"<tr>\\n\"\n-\n-        # idle\n-        s += \"<td valign='top'>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"add_pending_spiders\" multiple=\"multiple\">\\n'\n-        for name in sorted(self.idle):\n-            s += \"<option>%s</option>\\n\" % name\n-        s += '</select><br>\\n'\n-        s += '<br />'\n-        s += '<input type=\"submit\" value=\"Schedule selected\">\\n'\n-        s += '</form>\\n'\n-        s += \"</td>\\n\"\n-\n-        # scheduled\n-        s += \"<td valign='top'>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"remove_pending_spiders\" multiple=\"multiple\">\\n'\n-        for name in self.scheduled:\n-            s += \"<option>%s</option>\\n\" % name\n-        s += '</select><br>\\n'\n-        s += '<br />'\n-        s += '<input type=\"submit\" value=\"Remove selected\">\\n'\n-        s += '</form>\\n'\n-\n-        s += \"</td>\\n\"\n-\n-        # running\n-        s += \"<td valign='top'>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"stop_running_spiders\" multiple=\"multiple\">\\n'\n-        for name in sorted(self.running):\n-            s += \"<option>%s</option>\\n\" % name \n-        s += '</select><br>\\n'\n-        s += '<br />'\n-        s += '<input type=\"submit\" value=\"Stop selected\">\\n'\n-        s += '</form>\\n'\n-        s += \"</td>\\n\"\n-\n-        # finished\n-        s += \"<td valign='top'>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"rerun_finished_spiders\" multiple=\"multiple\">\\n'\n-        for name in sorted(self.finished):\n-            s += \"<option>%s</option>\\n\" % name\n-        s += '</select><br>\\n'\n-        s += '<br />'\n-        s += '<input type=\"submit\" value=\"Re-schedule selected\">\\n'\n-        s += '</form>\\n'\n-        s += \"</td>\\n\"\n-\n-        s += \"</tr>\\n\"\n-        s += \"</table>\\n\"\n-\n-        if wc_request.args:\n-            s += changes\n-\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return s\n-\n-    def webconsole_control(self, wc_request):\n-        args = wc_request.args\n-        s = \"<hr />\\n\"\n-\n-        if \"stop_running_spiders\" in args:\n-            s += \"<p>\"\n-            stopped_spiders = []\n-            for name in args[\"stop_running_spiders\"]:\n-                if name in self.running:\n-                    scrapymanager.engine.close_spider(self.running[name])\n-                    stopped_spiders.append(name)\n-            s += \"Stopped spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(stopped_spiders)\n-            s += \"</p>\"\n-        if \"remove_pending_spiders\" in args:\n-            removed = []\n-            for name in args[\"remove_pending_spiders\"]:\n-                q = scrapymanager.queue\n-                q.spider_requests = [x for x in q.spider_requests if x[0].name != name]\n-            if removed:\n-                s += \"<p>\"\n-                s += \"Removed scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"remove_pending_spiders\"])\n-                s += \"</p>\"\n-        if \"add_pending_spiders\" in args:\n-            for name in args[\"add_pending_spiders\"]:\n-                if name not in scrapymanager.engine.scheduler.pending_requests:\n-                    scrapymanager.queue.append_spider_name(name)\n-            s += \"<p>\"\n-            s += \"Scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"add_pending_spiders\"])\n-            s += \"</p>\"\n-        if \"rerun_finished_spiders\" in args:\n-            for name in args[\"rerun_finished_spiders\"]:\n-                if name not in scrapymanager.engine.scheduler.pending_requests:\n-                    scrapymanager.queue.append_spider_name(name)\n-                self.finished.remove(name)\n-            s += \"<p>\"\n-            s += \"Re-scheduled finished spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"rerun_finished_spiders\"])\n-            s += \"</p>\"\n-\n-        return s\n-        \n-    def webconsole_discover_module(self):\n-        self.enabled_spiders = spiders.list()\n-        return self\n\n@@ -1,35 +0,0 @@\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy.stats import stats\n-from scrapy.management.web import banner, webconsole_discover_module\n-\n-def stats_html_table(statsdict):\n-    s = \"\"\n-    s += \"<table border='1'>\\n\"\n-    for kv in statsdict.iteritems():\n-        s += \"<tr><th align='left'>%s</th><td>%s</td></tr>\\n\" % kv\n-    s += \"</table>\\n\"\n-    return s\n-\n-class StatsDump(object):\n-    webconsole_id = 'stats'\n-    webconsole_name = 'StatsCollector dump'\n-\n-    def __init__(self):\n-        dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n-\n-    def webconsole_render(self, wc_request):\n-        s = banner(self)\n-        s += \"<h3>Global stats</h3>\\n\"\n-        s += stats_html_table(stats.get_stats())\n-        for spider, spider_stats in stats.iter_spider_stats():\n-            s += \"<h3>%s</h3>\\n\" % spider.name\n-            s += stats_html_table(spider_stats)\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-\n-        return str(s)\n-\n-    def webconsole_discover_module(self):\n-        return self\n-\n\n\n@@ -1,93 +0,0 @@\n-import warnings\n-warnings.warn(\"Scrapy web console is deprecated. Consider using web service instead.\", \\\n-    DeprecationWarning, stacklevel=2)\n-\n-import re\n-import socket\n-from time import time\n-\n-from twisted.internet import reactor\n-from twisted.web import server, resource\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.core.manager import scrapymanager\n-from scrapy.conf import settings\n-\n-# web management signals\n-webconsole_discover_module = object()\n-\n-urlpath_re = re.compile(r\"^/(\\w+)/\")\n-\n-def error404(module):\n-    return \"\"\"\n-<html>\n-  <head><title>404 Not Found</title></head>\n-  <body>\n-    <h1>Not found</h1>\n-    <p>Web console module not found: <b>%s</b></p>\n-    <p><a href=\"/\">Back to main menu</a></p>\n-  </body>\n-</html>\n-\"\"\" % module\n-\n-def banner(module=None):\n-    s  = \"<html>\\n\"\n-    s += \"<head><title>Scrapy</title></head>\\n\"\n-    s += \"<body>\\n\"\n-    s += \"<h1><a href='/'>Scrapy web console</a></h1>\\n\"\n-    uptime = time() - scrapymanager.engine.start_time\n-    s += \"<p>Bot: <b>%s</b> | Host: <b>%s</b> | Uptime: <b>%ds</b></p>\\n\" % \\\n-        (settings['BOT_NAME'], socket.gethostname(), uptime)\n-    if module:\n-        s += \"<h2><a href='/%s/'>%s</a></h2>\\n\" % (module.webconsole_id, \\\n-            module.webconsole_name)\n-    return s\n-\n-class WebConsoleResource(resource.Resource):\n-    isLeaf = True\n-\n-    @property\n-    def modules(self):\n-        if not hasattr(self, '_modules'):\n-            self._modules = {}\n-            for _, obj in dispatcher.send(signal=webconsole_discover_module, \\\n-                    sender=self.__class__):\n-                self._modules[obj.webconsole_id] = obj\n-        return self._modules\n-\n-    def render_GET(self, request):\n-        m = urlpath_re.search(request.path)\n-        if m:\n-            module = m.group(1)\n-            if module in self.modules:\n-                return self.modules[m.group(1)].webconsole_render(request)\n-            else:\n-                request.setResponseCode(404)\n-                return error404(module)\n-        else:\n-            return self.module_list()\n-\n-    render_POST = render_GET\n-\n-    def module_list(self):\n-        s  = banner()\n-        s += \"<p>Available modules:</p>\\n\"\n-        s += \"<ul>\\n\"\n-        for name, obj in self.modules.iteritems():\n-            s += \"<li><a href='/%s/'>%s</a></li>\\n\" % (name, obj.webconsole_name)\n-        s += \"</ul>\\n\"\n-        s += \"</body>\\n\"\n-        s += \"</html>\\n\"\n-        return s\n-\n-class WebConsole(server.Site):\n-\n-    def __init__(self):\n-        if not settings.getbool('WEBCONSOLE_ENABLED'):\n-            raise NotConfigured\n-        logfile = settings['WEBCONSOLE_LOGFILE']\n-        server.Site.__init__(self, WebConsoleResource(), logPath=logfile)\n-        self.noisy = False\n-        port = settings.getint('WEBCONSOLE_PORT')\n-        reactor.callWhenRunning(reactor.listenTCP, port, self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
