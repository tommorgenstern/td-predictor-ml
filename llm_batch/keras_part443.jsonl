{"custom_id": "keras#60d220fe0e5a25ca6015dd217e3fe27d07a6612e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 16 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 75 | Churn Cumulative: 1142 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: 0.21052631578947367\n\nDIFF:\n@@ -80,18 +80,40 @@ def split_dataset(dataset,\n                                                       right_size,\n                                                       total_length)\n \n+  \n+  \n+  \n   left_split = dataset_as_list[:left_size]\n   right_split = dataset_as_list[-right_size:]\n \n+  \n+  \n+  try:\n     left_split = tf.data.Dataset.from_tensor_slices(left_split)\n+  except Exception as e:\n+    raise ValueError(f' with `left_size`={left_size} and '\n+                     f' `right_size`={right_size}'\n+                     f' unable to create the dataset from '\n+                     f' left_split of shape {np.array(left_split).shape}. '\n+                     f' Received error: {e}')\n+  \n+  try:\n     right_split = tf.data.Dataset.from_tensor_slices(right_split)\n+  except Exception as e:\n+    raise ValueError(f' with `left_size`={left_size} and '\n+                     f' `right_size`={right_size}'\n+                     f' unable to create the dataset from '\n+                     f' right_split of shape {np.array(right_split).shape}. '\n+                     f' Received error: {e}')\n     \n   left_split = left_split.prefetch(tf.data.AUTOTUNE)\n   right_split = right_split.prefetch(tf.data.AUTOTUNE)\n   \n   return left_split, right_split\n \n-def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n+def _convert_dataset_to_list(dataset,\n+                             data_size_warning_flag= True,\n+                             ensure_shape_similarity = True):\n   \"\"\"Helper function to convert a tf.data.Dataset  object or a list/tuple of numpy.ndarrays to a list\n   \"\"\"\n   # TODO (prakashsellathurai): add support for Batched  and unbatched dict tf datasets\n@@ -100,31 +122,39 @@ def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n       raise ValueError('`dataset` must be a non-empty list/tuple of'\n                        ' numpy.ndarrays or tf.data.Dataset objects.')\n \n-  \n     if isinstance(dataset[0],np.ndarray):\n-      if not all(element.shape == dataset[0].shape  for element in dataset):\n-        raise ValueError('all elements of `dataset` must have the same shape.')\n+      expected_shape = dataset[0].shape\n+      for i,element in enumerate(dataset):\n+        if not np.array(element).shape[0] == expected_shape[0]:\n+          raise ValueError(f' Expected all numpy arrays of {type(dataset)} '\n+                           f'in `dataset` to have the same length. '\n+                           f'\\n Received: dataset[{i}] with length = '\n+                           f'{np.array(element).shape},'\n+                           f' while dataset[0] has length {dataset[0].shape}') \n+    else:\n+      raise ValueError('Expected a list/tuple of numpy.ndarrays,'\n+                       'Received: {}'.format(type(dataset[0])))\n       \n     dataset_iterator = iter(zip(*dataset))\n-    else:\n-      dataset_iterator = iter(dataset)\n+      \n   elif isinstance(dataset,tf.data.Dataset):\n     if is_batched(dataset):\n       dataset = dataset.unbatch()\n     dataset_iterator = iter(dataset)\n   elif isinstance(dataset,np.ndarray):\n     dataset_iterator = iter(dataset)\n-  else:\n-    raise TypeError('`dataset` must be either a tf.data.Dataset object'\n-                   f' or a list/tuple of arrays. Received : {type(dataset)}')\n \n   dataset_as_list = []\n   \n   try:\n     dataset_iterator = iter(dataset_iterator)\n     first_datum = next(dataset_iterator)\n+    if isinstance(first_datum,(tf.Tensor,np.ndarray,tuple)):\n+      first_datum_shape = np.array(first_datum).shape\n+    else:\n+      ensure_shape_similarity  = False\n     dataset_as_list.append(first_datum)\n-  except ValueError:\n+  except StopIteration:\n     raise ValueError(' Received  an empty Dataset i.e dataset with no elements.'\n                      ' `dataset` must be a non-empty list/tuple of'\n                      ' numpy.ndarrays or tf.data.Dataset objects.')\n@@ -133,9 +163,17 @@ def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n     raise TypeError('`dataset` must be either a tf.data.Dataset object'\n                     ' or a list/tuple of arrays. '\n                     ' Received : tf.data.Dataset with dict elements')\n-  else:\n+  \n   start_time = time.time()\n   for i,datum in enumerate(dataset_iterator):\n+    \n+    if ensure_shape_similarity:\n+      if first_datum_shape != np.array(datum).shape:\n+        raise ValueError(' All elements of `dataset` must have the same shape,'\n+                        f' Expected shape: {np.array(first_datum).shape}'\n+                        f' Received shape: {np.array(datum).shape} at'\n+                        f' index {i}')\n+\n     if data_size_warning_flag:\n       if i % 10 == 0:\n         cur_time = time.time()\n@@ -143,7 +181,8 @@ def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n         if int(cur_time - start_time) > 10 and data_size_warning_flag:\n           warnings.warn(' Takes too long time to process the `dataset`,'\n                         ' this function is only  for small datasets '\n-                          '(e.g. < 10,000 samples).')\n+                        ' (e.g. < 10,000 samples).',category=ResourceWarning,\n+                        source='split_dataset',stacklevel=2)\n           data_size_warning_flag = False\n     \n     dataset_as_list.append(datum)\n\n@@ -43,7 +43,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertAllEqual(list(zip(*dataset)),\n                         list(left_split)+list(right_split))\n     \n-  def test_illegal_shaped_numpy_array(self):\n+  def test_dataset_with_irregular_shape(self):\n     with self.assertRaises(ValueError):\n       dataset=[np.ones(shape=(200, 32)), np.zeros(shape=(100, 32))]\n       dataset_utils.split_dataset(dataset, left_size=4)\n@@ -52,6 +52,16 @@ class SplitDatasetTest(tf.test.TestCase):\n       dataset=(np.ones(shape=(200, 32)), np.zeros(shape=(201, 32)))\n       dataset_utils.split_dataset(dataset, left_size=4)\n       \n+    with self.assertRaises(ValueError):\n+      dataset=tf.data.Dataset.from_tensor_slices(\n+        np.ones(shape=(200, 32,32,1)), np.zeros(shape=(32)))\n+      dataset_utils.split_dataset(dataset, left_size=4)\n+      \n+    with self.assertRaises(ValueError):\n+      dataset=tf.data.Dataset.from_tensor_slices(\n+        (np.ones(shape=(200, 32,32,1)), np.zeros(shape=(32))))\n+      dataset_utils.split_dataset(dataset, left_size=4)\n+            \n   def test_tuple_of_numpy_arrays(self):\n     dataset=(np.ones(shape=(200, 32)), np.zeros(shape=(200, 32)))\n     left_split,right_split = dataset_utils.split_dataset(dataset, \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6ebc31c3a440459766a8822d1ed225212e972bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 29 | Churn Cumulative: 395 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,16 +27,20 @@ from keras.utils import audio_dataset\n \n @test_utils.run_v2_only\n class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n-    def _get_audio_samples(self, count=16):\n+    def _get_audio_samples(self, count=16, ragged=False):\n         sequence_length = 30\n         num_channels = 1\n         audio_samples = []\n         for _ in range(count):\n+            if ragged:\n+                random_sequence_length = np.random.randint(10, sequence_length+1)\n+                audio = np.random.random((random_sequence_length, num_channels))\n+            else:\n                 audio = np.random.random((sequence_length, num_channels))\n             audio_samples.append(audio)\n         return audio_samples\n \n-    def _prepare_directory(self, num_classes=2, nested_dirs=False, count=16):\n+    def _prepare_directory(self, num_classes=2, nested_dirs=False, count=16, ragged=False):\n         # Get a unique temp directory\n         temp_dir = os.path.join(self.get_temp_dir(), str(np.random.randint(1e6)))\n         os.mkdir(temp_dir)\n@@ -61,7 +65,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n \n         # Save audio samples to the paths\n         i = 0\n-        for audio in self._get_audio_samples(count=count):\n+        for audio in self._get_audio_samples(count=count, ragged=ragged):\n             path = paths[i % len(paths)]\n             ext = \".wav\"\n             filename = os.path.join(path, \"audio_%s.%s\" % (i, ext))\n@@ -245,6 +249,24 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         with self.assertRaisesRegex(ValueError, \"No audio found.\"):\n             _ = audio_dataset.audio_dataset_from_directory(directory)\n \n+\n+    def test_audio_dataset_from_directory_ragged(self):\n+        directory = self._prepare_directory(num_classes=2, count=16, ragged=True)\n+        dataset = audio_dataset.audio_dataset_from_directory(directory, ragged=True, batch_size=8)\n+        batch = next(iter(dataset))\n+        self.assertEqual(batch[0].shape, (32, None, None))\n+    \n+\n+    def test_audio_dataset_from_directory_no_output_sequence_length(self):\n+        directory = self._prepare_directory(num_classes=2, count=16, ragged=True)\n+        # The tensor shapes are different and output_sequence_length is None\n+        # should work fine and pad each sequence to the length of the longest sequence\n+        # in it's batch\n+        dataset = audio_dataset.audio_dataset_from_directory(directory, batch_size=2)\n+        sequence_lengths = set([batch[0].shape[1] for batch in dataset])\n+        assert len(sequence_lengths) > 1\n+\n+\n     def test_audio_dataset_from_directory_errors(self):\n         directory = self._prepare_directory(num_classes=3, count=5)\n \n@@ -342,3 +364,4 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n \n if __name__ == \"__main__\":\n     tf.test.main()\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3500d8e78dfb1dc1c2eeb534f9ce0eaecb40b5c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 43 | Churn Cumulative: 438 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,12 +27,12 @@ from keras.utils import audio_dataset\n \n @test_utils.run_v2_only\n class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n-    def _get_audio_samples(self, count=16, ragged=False):\n+    def _get_audio_samples(self, count=16, different_sequence_lengths=False):\n         sequence_length = 30\n         num_channels = 1\n         audio_samples = []\n         for _ in range(count):\n-            if ragged:\n+            if different_sequence_lengths:\n                 random_sequence_length = np.random.randint(10, sequence_length + 1)\n                 audio = np.random.random((random_sequence_length, num_channels))\n             else:\n@@ -40,7 +40,13 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n             audio_samples.append(audio)\n         return audio_samples\n \n-    def _prepare_directory(self, num_classes=2, nested_dirs=False, count=16, ragged=False):\n+    def _prepare_directory(\n+        self,\n+        num_classes=2,\n+        nested_dirs=False,\n+        count=16,\n+        different_sequence_lengths=False,\n+    ):\n         # Get a unique temp directory\n         temp_dir = os.path.join(self.get_temp_dir(), str(np.random.randint(1e6)))\n         os.mkdir(temp_dir)\n@@ -65,7 +71,9 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n \n         # Save audio samples to the paths\n         i = 0\n-        for audio in self._get_audio_samples(count=count, ragged=ragged):\n+        for audio in self._get_audio_samples(\n+            count=count, different_sequence_lengths=different_sequence_lengths\n+        ):\n             path = paths[i % len(paths)]\n             ext = \".wav\"\n             filename = os.path.join(path, \"audio_%s.%s\" % (i, ext))\n@@ -249,16 +257,20 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         with self.assertRaisesRegex(ValueError, \"No audio found.\"):\n             _ = audio_dataset.audio_dataset_from_directory(directory)\n \n-\n     def test_audio_dataset_from_directory_ragged(self):\n-        directory = self._prepare_directory(num_classes=2, count=16, ragged=True)\n-        dataset = audio_dataset.audio_dataset_from_directory(directory, ragged=True, batch_size=8)\n+        directory = self._prepare_directory(\n+            num_classes=2, count=16, different_sequence_lengths=True\n+        )\n+        dataset = audio_dataset.audio_dataset_from_directory(\n+            directory, ragged=True, batch_size=8\n+        )\n         batch = next(iter(dataset))\n         self.assertEqual(batch[0].shape, (32, None, None))\n \n-\n-    def test_audio_dataset_from_directory_no_output_sequence_length(self):\n-        directory = self._prepare_directory(num_classes=2, count=16, ragged=True)\n+    def test_audio_dataset_from_directory_no_output_length_and_different_lengths(self,):\n+        directory = self._prepare_directory(\n+            num_classes=2, count=16, different_sequence_lengths=True\n+        )\n         # The tensor shapes are different and output_sequence_length is None\n         # should work fine and pad each sequence to the length of the longest sequence\n         # in it's batch\n@@ -266,6 +278,16 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         sequence_lengths = set([batch[0].shape[1] for batch in dataset])\n         assert len(sequence_lengths) > 1\n \n+    def test_audio_dataset_from_directory_no_output_length_and_same_lengths(self):\n+        directory = self._prepare_directory(\n+            num_classes=2, count=16, different_sequence_lengths=False\n+        )\n+        # The tensor shapes are different and output_sequence_length is None\n+        # should work fine and pad each sequence to the length of the longest sequence\n+        # in it's batch\n+        dataset = audio_dataset.audio_dataset_from_directory(directory, batch_size=2)\n+        sequence_lengths = set([batch[0].shape[1] for batch in dataset])\n+        assert len(sequence_lengths) == 1\n \n     def test_audio_dataset_from_directory_errors(self):\n         directory = self._prepare_directory(num_classes=3, count=5)\n@@ -364,4 +386,3 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n \n if __name__ == \"__main__\":\n     tf.test.main()\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8c57031c3d692b1d6c3a42c76156388a2394df8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 450 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,7 +37,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n                 audio = np.random.random((random_sequence_length, num_channels))\n             else:\n                 audio = np.random.random((sequence_length, num_channels))\n-            audio_samples.append(audio)\n+            audio_samples.append(tf.audio.encode_wav(audio, 1000))\n         return audio_samples\n \n     def _prepare_directory(\n@@ -77,7 +77,8 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n             path = paths[i % len(paths)]\n             ext = \".wav\"\n             filename = os.path.join(path, \"audio_%s.%s\" % (i, ext))\n-            audio.save(os.path.join(temp_dir, filename))\n+            with open(os.path.join(temp_dir, filename), 'wb') as f:\n+                f.write(audio.numpy())\n             i += 1\n         return temp_dir\n \n@@ -86,9 +87,10 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n \n         # Save a few extra audio in the parent directory.\n         directory = self._prepare_directory(count=7, num_classes=2)\n-        for i, img in enumerate(self._get_audio_samples(3)):\n+        for i, audio in enumerate(self._get_audio_samples(3)):\n             filename = \"audio_%s.wav\" % (i,)\n-            img.save(os.path.join(directory, filename))\n+            with open(os.path.join(directory, filename), 'wb') as f:\n+                f.write(audio.numpy())\n \n         dataset = audio_dataset.audio_dataset_from_directory(\n             directory, batch_size=5, output_sequence_length=30, labels=None\n@@ -267,7 +269,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         batch = next(iter(dataset))\n         self.assertEqual(batch[0].shape, (32, None, None))\n \n-    def test_audio_dataset_from_directory_no_output_length_and_different_lengths(self,):\n+    def test_audio_dataset_from_directory_no_output_length_and_different_lengths(self):\n         directory = self._prepare_directory(\n             num_classes=2, count=16, different_sequence_lengths=True\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d49ca2299d314bd7376cf760f534286dde3502c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 1974 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -161,6 +161,10 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with\n   values in the [0-255] range.\n \n+  When calling the `summary()` method after instantiating a ConvNeXt model, prefer\n+  setting the `expand_nested` argument `summary()` to `True` to better investigate\n+  the instantiated model.\n+\n   Args:\n     include_top: Whether to include the fully-connected\n         layer at the top of the network. Defaults to True.\n@@ -311,7 +315,7 @@ def PreStem(name=None):\n       mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n       variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n       name=name + \"_prestem_normalization\"\n-    )\n+    )(x)\n     return x\n \n   return apply\n@@ -347,7 +351,7 @@ def ConvNeXt(depths,\n            model_name=\"convnext\",\n            include_preprocessing=True,\n            include_top=True,\n-           weights=\"imagenet\",\n+           weights=None,\n            input_tensor=None,\n            input_shape=None,\n            pooling=None,\n@@ -467,7 +471,7 @@ def ConvNeXt(depths,\n       [\n         *[\n             Block(\n-              dim=projection_dims[i],\n+              projection_dim=projection_dims[i],\n               drop_path=dp_rates[cur + j],\n               layer_scale_init_value=layer_scale_init_value,\n               name=model_name + f\"stage_{i}_block_{j}\",\n@@ -480,6 +484,11 @@ def ConvNeXt(depths,\n     stages.append(stage)\n     cur += depths[i]\n \n+  # Apply the stages.\n+  for i in range(len(stages)):\n+    x = downsample_layers[i](x)\n+    x = stages[i](x)\n+\n   if include_top:\n     x = Head(num_classes=classes)(x)\n     imagenet_utils.validate_activation(classifier_activation, weights)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8292b37254c68787be525a44f5a3318b4c75d60b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 14/13 | Churn Δ: 15 | Churn Cumulative: 2718 | Contributors (this commit): 12 | Commits (past 90d): 16 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1873,6 +1873,11 @@ class ResetStatesTest(test_combinations.TestCase):\n     self.assertEqual(self.evaluate(p_obj.true_positives), 50.)\n     self.assertEqual(self.evaluate(p_obj.false_positives), 50.)\n \n+  def test_precision_update_state_with_logits(self):\n+    p_obj = metrics.Precision()\n+    # Update state with logits (not in range (0, 1)) should not an raise error.\n+    p_obj.update_state([-0.5, 0.5], [-2., 2.])\n+\n   def test_reset_state_recall(self):\n     r_obj = metrics.Recall()\n     model = _get_model([r_obj])\n\n@@ -598,16 +598,6 @@ def update_confusion_matrix_variables(variables_to_update,\n         f'Invalid keys: \"{invalid_keys}\". '\n         f'Valid variable key options are: \"{list(ConfusionMatrix)}\"')\n \n-  with tf.control_dependencies([\n-      tf.debugging.assert_greater_equal(\n-          y_pred,\n-          tf.cast(0.0, dtype=y_pred.dtype),\n-          message='predictions must be >= 0'),\n-      tf.debugging.assert_less_equal(\n-          y_pred,\n-          tf.cast(1.0, dtype=y_pred.dtype),\n-          message='predictions must be <= 1')\n-  ]):\n   if sample_weight is None:\n     y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n         y_pred, y_true)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c52c11968b096580577c75b169f51c5b39002106", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 19 | Files Changed: 5 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -79/0 | Churn Δ: 27 | Churn Cumulative: 883 | Contributors (this commit): 5 | Commits (past 90d): 27 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -230,7 +230,7 @@ def check_validation_split_arg(validation_split, subset, shuffle, seed):\n   Args:\n     validation_split: float between 0 and 1, fraction of data to reserve for\n       validation.\n-    subset: One of \"training\" or \"validation\". Only used if `validation_split`\n+    subset: One of \"training\", \"validation\" or \"both\". Only used if `validation_split`\n       is set.\n     shuffle: Whether to shuffle the data. Either True or False.\n     seed: random seed for shuffling and transformations.\n@@ -242,9 +242,9 @@ def check_validation_split_arg(validation_split, subset, shuffle, seed):\n   if (validation_split or subset) and not (validation_split and subset):\n     raise ValueError(\n         'If `subset` is set, `validation_split` must be set, and inversely.')\n-  if subset not in ('training', 'validation', None):\n-    raise ValueError('`subset` must be either \"training\" '\n-                     'or \"validation\", received: %s' % (subset,))\n+  if subset not in ('training', 'validation', 'both', None):\n+    raise ValueError('`subset` must be either \"training\", '\n+                     '\"validation\" or \"both\", received: %s' % (subset,))\n   if validation_split and shuffle and seed is None:\n     raise ValueError(\n         'If using `validation_split` and shuffling the data, you must provide '\n\n@@ -203,12 +203,6 @@ def image_dataset_from_directory(directory,\n         f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n         f'class_names. Received: class_names={class_names}')\n \n-  image_paths, labels = dataset_utils.get_training_or_validation_split(\n-      image_paths, labels, validation_split, subset)\n-  if not image_paths:\n-    raise ValueError(f'No images found in directory {directory}. '\n-                     f'Allowed formats: {ALLOWLIST_FORMATS}')\n-\n   if subset == \"both\":\n     image_paths_train, labels_train = dataset_utils.get_training_or_validation_split(\n         image_paths, labels, validation_split, \"training\")\n@@ -249,7 +243,6 @@ def image_dataset_from_directory(directory,\n     else:\n       if shuffle:\n         train_dataset = train_dataset.shuffle(buffer_size=1024, seed=seed)\n-        val_dataset = val_dataset.shuffle(buffer_size=1024, seed=seed)\n \n     # Users may need to reference `class_names`.\n     train_dataset.class_names = class_names\n\n@@ -346,7 +346,8 @@ class ImageDatasetFromDirectoryTest(test_combinations.TestCase):\n           directory, validation_split=2)\n \n     with self.assertRaisesRegex(ValueError,\n-                                '`subset` must be either \"training\" or'):\n+                                '`subset` must be either \"training\", '\n+                     '\"validation\" or \"both\"'):\n       _ = image_dataset.image_dataset_from_directory(\n           directory, validation_split=0.2, subset='other')\n \n\n@@ -154,12 +154,6 @@ def text_dataset_from_directory(directory,\n         f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n         f'class_names. Received: class_names={class_names}')\n \n-  file_paths, labels = dataset_utils.get_training_or_validation_split(\n-      file_paths, labels, validation_split, subset)\n-  if not file_paths:\n-    raise ValueError(f'No text files found in directory {directory}. '\n-                     f'Allowed format: .txt')\n-\n   if subset == \"both\":\n     file_paths_train, labels_train = dataset_utils.get_training_or_validation_split(\n         file_paths, labels, validation_split, \"training\")\n\n@@ -248,7 +248,8 @@ class TextDatasetFromDirectoryTest(test_combinations.TestCase):\n           directory, validation_split=2)\n \n     with self.assertRaisesRegex(ValueError,\n-                                '`subset` must be either \"training\" or'):\n+                                '`subset` must be either \"training\", '\n+                     '\"validation\" or \"both\"'):\n       _ = text_dataset.text_dataset_from_directory(\n           directory, validation_split=0.2, subset='other')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bfd1af8ad691e910e9f6458e880869407fe59e33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 192 | Lines Deleted: 103 | Files Changed: 1 | Hunks: 20 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 295 | Churn Cumulative: 2269 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,77 +34,21 @@ from tensorflow.python.util.tf_export import keras_export\n BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/convnext-tf/keras-applications/convnext/\"\n \n WEIGHTS_HASHES = {\n-    \"x002\":\n-        (\"49fb46e56cde07fdaf57bffd851461a86548f6a3a4baef234dd37290b826c0b8\",\n-         \"5445b66cd50445eb7ecab094c1e78d4d3d29375439d1a7798861c4af15ffff21\"),\n-    \"x004\":\n-        (\"3523c7f5ac0dbbcc2fd6d83b3570e7540f7449d3301cc22c29547302114e4088\",\n-         \"de139bf07a66c9256f2277bf5c1b6dd2d5a3a891a5f8a925a10c8a0a113fd6f3\"),\n-    \"x006\":\n-        (\"340216ef334a7bae30daac9f414e693c136fac9ab868704bbfcc9ce6a5ec74bb\",\n-         \"a43ec97ad62f86b2a96a783bfdc63a5a54de02eef54f26379ea05e1bf90a9505\"),\n-    \"x008\":\n-        (\"8f145d6a5fae6da62677bb8d26eb92d0b9dfe143ec1ebf68b24a57ae50a2763d\",\n-         \"3c7e4b0917359304dc18e644475c5c1f5e88d795542b676439c4a3acd63b7207\"),\n-    \"x016\":\n-        (\"31c386f4c7bfef4c021a583099aa79c1b3928057ba1b7d182f174674c5ef3510\",\n-         \"1b8e3d545d190271204a7b2165936a227d26b79bb7922bac5ee4d303091bf17a\"),\n-    \"x032\":\n-        (\"6c025df1409e5ea846375bc9dfa240956cca87ef57384d93fef7d6fa90ca8c7f\",\n-         \"9cd4522806c0fcca01b37874188b2bd394d7c419956d77472a4e072b01d99041\"),\n-    \"x040\":\n-        (\"ba128046c588a26dbd3b3a011b26cb7fa3cf8f269c184c132372cb20b6eb54c1\",\n-         \"b4ed0ca0b9a98e789e05000e830403a7ade4d8afa01c73491c44610195198afe\"),\n-    \"x064\":\n-        (\"0f4489c3cd3ad979bd6b0324213998bcb36dc861d178f977997ebfe53c3ba564\",\n-         \"3e706fa416a18dfda14c713423eba8041ae2509db3e0a611d5f599b5268a46c4\"),\n-    \"x080\":\n-        (\"76320e43272719df648db37271a247c22eb6e810fe469c37a5db7e2cb696d162\",\n-         \"7b1ce8e29ceefec10a6569640ee329dba7fbc98b5d0f6346aabade058b66cf29\"),\n-    \"x120\":\n-        (\"5cafc461b78897d5e4f24e68cb406d18e75f31105ef620e7682b611bb355eb3a\",\n-         \"36174ddd0299db04a42631d028abcb1cc7afec2b705e42bd28fcd325e5d596bf\"),\n-    \"x160\":\n-        (\"8093f57a5824b181fb734ea21ae34b1f7ee42c5298e63cf6d587c290973195d2\",\n-         \"9d1485050bdf19531ffa1ed7827c75850e0f2972118a996b91aa9264b088fd43\"),\n-    \"x320\":\n-        (\"91fb3e6f4e9e44b3687e80977f7f4412ee9937c0c704232664fc83e4322ea01e\",\n-         \"9db7eacc37b85c98184070e1a172e6104c00846f44bcd4e727da9e50d9692398\"),\n-    \"y002\":\n-        (\"1e8091c674532b1a61c04f6393a9c570113e0197f22bd1b98cc4c4fe800c6465\",\n-         \"f63221f63d625b8e201221499682587bfe29d33f50a4c4f4d53be00f66c0f12c\"),\n-    \"y004\":\n-        (\"752fdbad21c78911bf1dcb8c513e5a0e14697b068e5d9e73525dbaa416d18d8e\",\n-         \"45e6ba8309a17a77e67afc05228454b2e0ee6be0dae65edc0f31f1da10cc066b\"),\n-    \"y006\":\n-        (\"98942e07b273da500ff9699a1f88aca78dfad4375faabb0bab784bb0dace80a9\",\n-         \"b70261cba4e60013c99d130cc098d2fce629ff978a445663b6fa4f8fc099a2be\"),\n-    \"y008\":\n-        (\"1b099377cc9a4fb183159a6f9b24bc998e5659d25a449f40c90cbffcbcfdcae4\",\n-         \"b11f5432a216ee640fe9be6e32939defa8d08b8d136349bf3690715a98752ca1\"),\n-    \"y016\":\n-        (\"b7ce1f5e223f0941c960602de922bcf846288ce7a4c33b2a4f2e4ac4b480045b\",\n-         \"d7404f50205e82d793e219afb9eb2bfeb781b6b2d316a6128c6d7d7dacab7f57\"),\n-    \"y032\":\n-        (\"6a6a545cf3549973554c9b94f0cd40e25f229fffb1e7f7ac779a59dcbee612bd\",\n-         \"eb3ac1c45ec60f4f031c3f5180573422b1cf7bebc26c004637517372f68f8937\"),\n-    \"y040\":\n-        (\"98d00118b335162bbffe8f1329e54e5c8e75ee09b2a5414f97b0ddfc56e796f6\",\n-         \"b5be2a5e5f072ecdd9c0b8a437cd896df0efa1f6a1f77e41caa8719b7dfcb05d\"),\n-    \"y064\":\n-        (\"65c948c7a18aaecaad2d1bd4fd978987425604ba6669ef55a1faa0069a2804b7\",\n-         \"885c4b7ed7ea339daca7dafa1a62cb7d41b1068897ef90a5a3d71b4a2e2db31a\"),\n-    \"y080\":\n-        (\"7a2c62da2982e369a4984d3c7c3b32d6f8d3748a71cb37a31156c436c37f3e95\",\n-         \"3d119577e1e3bf8d153b895e8ea9e4ec150ff2d92abdca711b6e949c3fd7115d\"),\n-    \"y120\":\n-        (\"a96ab0d27d3ae35a422ee7df0d789069b3e3217a99334e0ce861a96595bc5986\",\n-         \"4a6fa387108380b730b71feea2ad80b5224b5ea9dc21dc156c93fe3c6186485c\"),\n-    \"y160\":\n-        (\"45067240ffbc7ca2591313fee2f80dbdda6d66ec1a7451446f9a6d00d8f7ac6e\",\n-         \"ead1e6b568be8f34447ec8941299a9df4368736ba9a8205de5427fa20a1fb316\"),\n-    \"y320\": (\"b05e173e4ae635cfa22d06392ee3741284d17dadfee68f2aa6fd8cb2b7561112\",\n-             \"cad78f74a586e24c61d38be17f3ae53bb9674380174d2585da1a526b8c20e1fd\")\n+  \"tiny\":\n+    (\"dec324e40ebe943afc7b75b72484646eeb092c04bb079df35911d7080364f9a8\",\n+      \"4d4f0e079db2cc0e627b55f7d0d76c367145d14f2c90674415373457cd822346\"),\n+  \"small\":\n+    (\"f964ea5cd5618a1e64902a74ca5ccff3797a4fa5dba11a14f2c4d1a562b72f08\",\n+      \"fd8f0ac74faa4e364d7cb5b2d32af9ae35b54ce5e80525b5beb7b7571320065a\"),\n+  \"base\":\n+    (\"d30e0c509f4e1abe2784d33765d4391ce8fbff259b0bd79f4a63684b20db87d2\",\n+      \"736f7a96cd933ee568611e29f334737fb9aebaaea021ea7adfe4d2f5cbb4a9aa\"),\n+  \"large\":\n+    (\"8a304c66deb782b0d59837bc13127068901adaaa280cfac604d3341aaf44b2cf\",\n+      \"b02b623b3c28586423e6be4aa214e2f5619280b97b4ef6b35ffb686e83235f01\"),\n+  \"xlarge\":\n+    (\"da65d1294d386c71aebd81bc2520b8d42f7f60eee4414806c60730cd63eb15cb\",\n+      \"2bfbf5f0c2b3f004f1c32e9a76661e11a9ac49014ed2a68a49ecd0cd6c88d377\"),\n }\n \n \n@@ -150,10 +94,11 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   [guide to transfer learning & fine-tuning](\n     https://keras.io/guides/transfer_learning/).\n \n-  `base`, `large`, and `xlarge` models were first pre-trained on the ImageNet-21k\n-  dataset and were then fine-tuned on the ImageNet-1k dataset. The pre-trained \n-  parameters of the models were assembled from the [official repository](https://github.com/facebookresearch/ConvNeXt).\n-  To get a sense of how these parameters were converted to Keras compatible parameters,\n+  The `base`, `large`, and `xlarge` models were first pre-trained on the\n+  ImageNet-21k dataset and then fine-tuned on the ImageNet-1k dataset. The\n+  pre-trained parameters of the models were assembled from the\n+  [official repository](https://github.com/facebookresearch/ConvNeXt). To get a\n+  sense of how these parameters were converted to Keras compatible parameters,\n   please refer to [this repository](https://github.com/sayakpaul/ConvNeXt-TF).\n \n   Note: Each Keras Application expects a specific kind of input preprocessing.\n@@ -169,8 +114,8 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     include_top: Whether to include the fully-connected\n       layer at the top of the network. Defaults to True.\n     weights: One of `None` (random initialization),\n-          `\"imagenet\"` (pre-training on ImageNet), or the path to the weights\n-          file to be loaded. Defaults to `\"imagenet\"`.\n+      `\"imagenet\"` (pre-training on ImageNet-1k), or the path to the weights file\n+      to be loaded. Defaults to `\"imagenet\"`.\n     input_tensor: Optional Keras tensor\n       (i.e. output of `layers.Input()`)\n       to use as image input for the model.\n@@ -180,8 +125,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     pooling: Optional pooling mode for feature extraction\n       when `include_top` is `False`. Defaults to None.\n       - `None` means that the output of the model will be\n-            the 4D tensor output of the\n-            last convolutional layer.\n+        the 4D tensor output of the last convolutional layer.\n       - `avg` means that global average pooling\n         will be applied to the output of the\n         last convolutional layer, and thus\n@@ -262,25 +206,25 @@ class Block(tf.keras.Model):\n     def __init__(self, projection_dim, drop_path=0.0, layer_scale_init_value=1e-6, **kwargs):\n       super().__init__(**kwargs)\n       self.projection_dim = projection_dim\n-      self.name = kwargs[\"name\"]\n+      name = kwargs[\"name\"]\n       \n       if layer_scale_init_value > 0.0:\n-        self.gamma = tf.Variable(layer_scale_init_value * tf.ones((projection_dim,)), name=self.name + \"_layer_scale_gamma\")\n+        self.gamma = tf.Variable(layer_scale_init_value * tf.ones((projection_dim,)), name=name + \"_layer_scale_gamma\")\n       else:\n         self.gamma = None\n       \n       self.dw_conv_1 = layers.Conv2D(\n         filters=projection_dim, kernel_size=7, padding=\"same\", groups=projection_dim,\n-        name=self.name + \"_depthwise_conv\"\n+        name=name + \"_depthwise_conv\"\n       )\n-      self.layer_norm = layers.LayerNormalization(epsilon=1e-6, name=self.name + \"_layernorm\")\n-      self.pw_conv_1 = layers.Dense(4 * projection_dim, name=self.name + \"_pointwise_conv_1\")\n-      self.act_fn = layers.Activation(\"gelu\", name=self.name + \"_gelu\")\n-      self.pw_conv_2 = layers.Dense(projection_dim, name=self.name + \"_pointwise_conv_2\")\n+      self.layer_norm = layers.LayerNormalization(epsilon=1e-6, name=name + \"_layernorm\")\n+      self.pw_conv_1 = layers.Dense(4 * projection_dim, name=name + \"_pointwise_conv_1\")\n+      self.act_fn = layers.Activation(\"gelu\", name=name + \"_gelu\")\n+      self.pw_conv_2 = layers.Dense(projection_dim, name=name + \"_pointwise_conv_2\")\n       self.drop_path = (\n-        StochasticDepth(drop_path, name=self.name + \"_stochastic_depth\")\n+        StochasticDepth(drop_path, name=name + \"_stochastic_depth\")\n         if drop_path > 0.0\n-        else layers.Activation(\"linear\", name=self.name + \"_identity\")\n+        else layers.Activation(\"linear\", name=name + \"_identity\")\n       )\n \n     def call(self, inputs):\n@@ -345,9 +289,9 @@ def Head(num_classes=1000, name=None):\n \n def ConvNeXt(depths,\n   projection_dims,\n-           drop_path_rate,\n-           layer_scale_init_value,\n-           default_size,\n+  drop_path_rate=0.0,\n+  layer_scale_init_value=1e-6,\n+  default_size=224,\n   model_name=\"convnext\",\n   include_preprocessing=True,\n   include_top=True,\n@@ -373,8 +317,8 @@ def ConvNeXt(depths,\n       the model.\n     include_top: Boolean denoting whether to include classification head to the\n       model.\n-    weights: one of `None` (random initialization), \"imagenet\" (pre-training on\n-      ImageNet), or the path to the weights file to be loaded.\n+    weights: one of `None` (random initialization), `\"imagenet\"` (pre-training on\n+      ImageNet-1k), or the path to the weights file to be loaded.\n     input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use\n       as image input for the model.\n     input_shape: optional shape tuple, only to be specified if `include_top` is\n@@ -398,9 +342,10 @@ def ConvNeXt(depths,\n   Raises:\n       ValueError: in case of invalid argument for `weights`,\n         or invalid input shape.\n-      ValueError: if `classifier_activation` is not `softmax` or `None` when\n-        using a pretrained top layer.\n-      ValueError: if `include_top` is True but `num_classes` is not 1000.\n+      ValueError: if `classifier_activation` is not `softmax`, or `None` \n+        when using a pretrained top layer.\n+      ValueError: if `include_top` is True but `num_classes` is not 1000 \n+        when using ImageNet.\n   \"\"\"\n   if not (weights in {\"imagenet\", None} or tf.io.gfile.exists(weights)):\n     raise ValueError(\"The `weights` argument should be either \"\n@@ -453,10 +398,10 @@ def ConvNeXt(depths,\n   for i in range(3):\n     downsample_layer = tf.keras.Sequential(\n       [\n-          layers.LayerNormalization(epsilon=1e-6, name=model_name + \"_downsampling_layernorm_\" + str(i+1)),\n-          layers.Conv2D(projection_dims[i + 1], kernel_size=2, strides=2, name=model_name + \"_downsampling_conv_\" + str(i+1)),\n+        layers.LayerNormalization(epsilon=1e-6, name=model_name + \"_downsampling_layernorm_\" + str(i)),\n+        layers.Conv2D(projection_dims[i + 1], kernel_size=2, strides=2, name=model_name + \"_downsampling_conv_\" + str(i)),\n       ],\n-        name=model_name + \"_downsampling_block_\" + str(i+1),\n+      name=model_name + \"_downsampling_block_\" + str(i),\n     )\n     downsample_layers.append(downsample_layer)\n   \n@@ -474,7 +419,7 @@ def ConvNeXt(depths,\n               projection_dim=projection_dims[i],\n               drop_path=dp_rates[cur + j],\n               layer_scale_init_value=layer_scale_init_value,\n-              name=model_name + f\"stage_{i}_block_{j}\",\n+              name=model_name + f\"_stage_{i}_block_{j}\",\n             )\n             for j in range(depths[i])\n         ]\n@@ -506,10 +451,10 @@ def ConvNeXt(depths,\n   if weights == \"imagenet\":\n     if include_top:\n       file_suffix = \".h5\"\n-      file_hash = WEIGHTS_HASHES[model_name[-4:]][0]\n+      file_hash = WEIGHTS_HASHES[model_name][0]\n     else:\n       file_suffix = \"_notop.h5\"\n-      file_hash = WEIGHTS_HASHES[model_name[-4:]][1]\n+      file_hash = WEIGHTS_HASHES[model_name][1]\n     file_name = model_name + file_suffix\n     weights_path = data_utils.get_file(\n       file_name,\n@@ -525,8 +470,152 @@ def ConvNeXt(depths,\n \n ## Instantiating variants ##\n \n+@keras_export(\"keras.applications.convnext.ConvNeXtTiny\",\n+              \"keras.applications.ConvNeXtTiny\")\n+def ConvNeXtTiny(model_name=\"convnext_tiny\",\n+  include_top=True,\n+  include_preprocessing=True,\n+  weights=\"imagenet\",\n+  input_tensor=None,\n+  input_shape=None,\n+  pooling=None,\n+  classes=1000,\n+  classifier_activation=\"softmax\"):\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"tiny\"][\"depths\"],\n+    projection_dims=[\"tiny\"][\"projection_dims\"],\n+    drop_path_rate=0.0,\n+    layer_scale_init_value=1e-6,\n+    default_size=[\"tiny\"][\"default_size\"],\n+    model_name=model_name,\n+    include_top=include_top,\n+    include_preprocessing=include_preprocessing,\n+    weights=weights,\n+    input_tensor=input_tensor,\n+    input_shape=input_shape,\n+    pooling=pooling,\n+    classes=classes,\n+    classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.convnext.ConvNeXtSmall\",\n+              \"keras.applications.ConvNeXtSmall\")\n+def ConvNeXtSmall(model_name=\"convnext_small\",\n+  include_top=True,\n+  include_preprocessing=True,\n+  weights=\"imagenet\",\n+  input_tensor=None,\n+  input_shape=None,\n+  pooling=None,\n+  classes=1000,\n+  classifier_activation=\"softmax\"):\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"small\"][\"depths\"],\n+    projection_dims=[\"small\"][\"projection_dims\"],\n+    drop_path_rate=0.0,\n+    layer_scale_init_value=1e-6,\n+    default_size=[\"small\"][\"default_size\"],\n+    model_name=model_name,\n+    include_top=include_top,\n+    include_preprocessing=include_preprocessing,\n+    weights=weights,\n+    input_tensor=input_tensor,\n+    input_shape=input_shape,\n+    pooling=pooling,\n+    classes=classes,\n+    classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.convnext.ConvNeXtBase\",\n+              \"keras.applications.ConvNeXtBase\")\n+def ConvNeXtBase(model_name=\"convnext_base\",\n+  include_top=True,\n+  include_preprocessing=True,\n+  weights=\"imagenet\",\n+  input_tensor=None,\n+  input_shape=None,\n+  pooling=None,\n+  classes=1000,\n+  classifier_activation=\"softmax\"):\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"base\"][\"depths\"],\n+    projection_dims=[\"base\"][\"projection_dims\"],\n+    drop_path_rate=0.0,\n+    layer_scale_init_value=1e-6,\n+    default_size=[\"base\"][\"default_size\"],\n+    model_name=model_name,\n+    include_top=include_top,\n+    include_preprocessing=include_preprocessing,\n+    weights=weights,\n+    input_tensor=input_tensor,\n+    input_shape=input_shape,\n+    pooling=pooling,\n+    classes=classes,\n+    classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.convnext.ConvNeXtLarge\",\n+              \"keras.applications.ConvNeXtLarge\")\n+def ConvNeXtLarge(model_name=\"convnext_large\",\n+  include_top=True,\n+  include_preprocessing=True,\n+  weights=\"imagenet\",\n+  input_tensor=None,\n+  input_shape=None,\n+  pooling=None,\n+  classes=1000,\n+  classifier_activation=\"softmax\"):\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"large\"][\"depths\"],\n+    projection_dims=[\"large\"][\"projection_dims\"],\n+    drop_path_rate=0.0,\n+    layer_scale_init_value=1e-6,\n+    default_size=[\"large\"][\"default_size\"],\n+    model_name=model_name,\n+    include_top=include_top,\n+    include_preprocessing=include_preprocessing,\n+    weights=weights,\n+    input_tensor=input_tensor,\n+    input_shape=input_shape,\n+    pooling=pooling,\n+    classes=classes,\n+    classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.convnext.ConvNeXtXLarge\",\n+              \"keras.applications.ConvNeXtXLarge\")\n+def ConvNeXtXLarge(model_name=\"convnext_xlarge\",\n+  include_top=True,\n+  include_preprocessing=True,\n+  weights=\"imagenet\",\n+  input_tensor=None,\n+  input_shape=None,\n+  pooling=None,\n+  classes=1000,\n+  classifier_activation=\"softmax\"):\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"xlarge\"][\"depths\"],\n+    projection_dims=[\"xlarge\"][\"projection_dims\"],\n+    drop_path_rate=0.0,\n+    layer_scale_init_value=1e-6,\n+    default_size=[\"xlarge\"][\"default_size\"],\n+    model_name=model_name,\n+    include_top=include_top,\n+    include_preprocessing=include_preprocessing,\n+    weights=weights,\n+    input_tensor=input_tensor,\n+    input_shape=input_shape,\n+    pooling=pooling,\n+    classes=classes,\n+    classifier_activation=classifier_activation)\n+\n+\n+ConvNeXtTiny.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtTiny\")\n+ConvNeXtSmall.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtSmall\")\n+ConvNeXtBase.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtBase\")\n+ConvNeXtLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtLarge\")\n+ConvNeXtXLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtXLarge\")\n \n-## TODO\n \n @keras_export(\"keras.applications.convnext.preprocess_input\")\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9844e278611dbb2b39cf16d02e7bfee6aa5c1a07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 18 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 71 | Churn Cumulative: 2340 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.6\n\nDIFF:\n@@ -24,11 +24,14 @@ References:\n   (CVPR 2022)\n \"\"\"\n \n-import tensorflow.compat.v2 as tf\n-from keras import backend, layers\n+from keras import backend\n+from keras import layers\n+from keras import utils\n+from keras import Model\n from keras.applications import imagenet_utils\n from keras.engine import training\n-from keras.utils import data_utils, layer_utils\n+\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/convnext-tf/keras-applications/convnext/\"\n@@ -162,7 +165,6 @@ class StochasticDepth(layers.Layer):\n     Tensor either with the residual path dropped or kept.\n \n   \"\"\"\n-\n   def __init__(self, drop_path, **kwargs):\n     super().__init__(**kwargs)\n     self.drop_path = drop_path\n@@ -182,7 +184,7 @@ class StochasticDepth(layers.Layer):\n     return config\n \n \n-class Block(tf.keras.Model):\n+class Block(Model):\n   \"\"\"ConvNeXt block.\n   \n   References:\n@@ -191,36 +193,42 @@ class Block(tf.keras.Model):\n \n   Notes:\n     In the original ConvNeXt implementation (linked above), the authors use\n-      `Dense` layers for pointwise convolutions for increased efficiency. Following\n-      that, this implementation also uses the same.\n+    `Dense` layers for pointwise convolutions for increased efficiency. \n+    Following that, this implementation also uses the same.\n \n   Args:\n-      projection_dim (int): Number of filters for convolution layers. In the ConvNeXt paper, this is\n-        referred to as projection dimension.\n+    projection_dim (int): Number of filters for convolution layers. In the\n+    ConvNeXt paper, this is referred to as projection dimension.\n     drop_path (float): Probability of dropping paths. Should be within [0, 1].\n-      layer_scale_init_value (float): Layer scale value. Should be a small float number.\n+    layer_scale_init_value (float): Layer scale value. Should be a small float\n+      number.\n \n   Returns:\n     A keras.Model instance.\n   \"\"\"\n-    def __init__(self, projection_dim, drop_path=0.0, layer_scale_init_value=1e-6, **kwargs):\n+  def __init__(self, projection_dim, drop_path=0.0, \n+    layer_scale_init_value=1e-6, **kwargs):\n     super().__init__(**kwargs)\n     self.projection_dim = projection_dim\n     name = kwargs[\"name\"]\n     \n     if layer_scale_init_value > 0.0:\n-        self.gamma = tf.Variable(layer_scale_init_value * tf.ones((projection_dim,)), name=name + \"_layer_scale_gamma\")\n+      self.gamma = tf.Variable(\n+        layer_scale_init_value * tf.ones((projection_dim,)),\n+        name=name + \"_layer_scale_gamma\")\n     else:\n       self.gamma = None\n     \n     self.dw_conv_1 = layers.Conv2D(\n-        filters=projection_dim, kernel_size=7, padding=\"same\", groups=projection_dim,\n-        name=name + \"_depthwise_conv\"\n-      )\n-      self.layer_norm = layers.LayerNormalization(epsilon=1e-6, name=name + \"_layernorm\")\n-      self.pw_conv_1 = layers.Dense(4 * projection_dim, name=name + \"_pointwise_conv_1\")\n+      filters=projection_dim, kernel_size=7, padding=\"same\",\n+      groups=projection_dim, name=name + \"_depthwise_conv\")\n+    self.layer_norm = layers.LayerNormalization(epsilon=1e-6, \n+      name=name + \"_layernorm\")\n+    self.pw_conv_1 = layers.Dense(4 * projection_dim,\n+      name=name + \"_pointwise_conv_1\")\n     self.act_fn = layers.Activation(\"gelu\", name=name + \"_gelu\")\n-      self.pw_conv_2 = layers.Dense(projection_dim, name=name + \"_pointwise_conv_2\")\n+    self.pw_conv_2 = layers.Dense(projection_dim, \n+      name=name + \"_pointwise_conv_2\")\n     self.drop_path = (\n       StochasticDepth(drop_path, name=name + \"_stochastic_depth\")\n       if drop_path > 0.0\n@@ -280,7 +288,8 @@ def Head(num_classes=1000, name=None):\n \n   def apply(x):\n     x = layers.GlobalAveragePooling2D(name=name + \"_head_gap\")(x)\n-    x = layers.LayerNormalization(epsilon=1e-6, name=name + \"_head_layernorm\")(x)\n+    x = layers.LayerNormalization(\n+      epsilon=1e-6, name=name + \"_head_layernorm\")(x)\n     x = layers.Dense(num_classes, name=name + \"head_dense\")(x)\n     return x\n \n@@ -305,8 +314,8 @@ def ConvNeXt(depths,\n \n   Args:\n     depths: An iterable containing depths for each individual stages.\n-    projection_dims: An iterable containing output number of channels of each individual\n-      stages.\n+    projection_dims: An iterable containing output number of channels of\n+    each individual stages.\n     drop_path_rate: Stochastic depth probability. If 0.0, then stochastic depth\n       won't be used.\n     layer_scale_init_value: Layer scale coefficient. If 0.0, layer scaling won't\n@@ -317,8 +326,8 @@ def ConvNeXt(depths,\n       the model.\n     include_top: Boolean denoting whether to include classification head to the\n       model.\n-    weights: one of `None` (random initialization), `\"imagenet\"` (pre-training on\n-      ImageNet-1k), or the path to the weights file to be loaded.\n+    weights: one of `None` (random initialization), `\"imagenet\"` (pre-training \n+      on ImageNet-1k), or the path to the weights file to be loaded.\n     input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use\n       as image input for the model.\n     input_shape: optional shape tuple, only to be specified if `include_top` is\n@@ -375,7 +384,7 @@ def ConvNeXt(depths,\n       img_input = input_tensor\n \n   if input_tensor is not None:\n-    inputs = layer_utils.get_source_inputs(input_tensor)\n+    inputs = utils.layer_utils.get_source_inputs(input_tensor)\n   else:\n     inputs = img_input\n \n@@ -386,8 +395,10 @@ def ConvNeXt(depths,\n   # Stem block.\n   stem = tf.keras.Sequential(\n     [\n-      layers.Conv2D(projection_dims[0], kernel_size=4, strides=4, name=model_name + \"_stem_conv\"),\n-      layers.LayerNormalization(epsilon=1e-6, name=model_name + \"_stem_layernorm\"),\n+      layers.Conv2D(projection_dims[0], kernel_size=4, strides=4,\n+        name=model_name + \"_stem_conv\"),\n+      layers.LayerNormalization(epsilon=1e-6, \n+        name=model_name + \"_stem_layernorm\"),\n     ],\n     name=model_name + \"_stem\",\n   )\n@@ -398,8 +409,10 @@ def ConvNeXt(depths,\n   for i in range(3):\n     downsample_layer = tf.keras.Sequential(\n       [\n-        layers.LayerNormalization(epsilon=1e-6, name=model_name + \"_downsampling_layernorm_\" + str(i)),\n-        layers.Conv2D(projection_dims[i + 1], kernel_size=2, strides=2, name=model_name + \"_downsampling_conv_\" + str(i)),\n+        layers.LayerNormalization(epsilon=1e-6, \n+          name=model_name + \"_downsampling_layernorm_\" + str(i)),\n+        layers.Conv2D(projection_dims[i + 1], kernel_size=2, strides=2,\n+          name=model_name + \"_downsampling_conv_\" + str(i)),\n       ],\n       name=model_name + \"_downsampling_block_\" + str(i),\n     )\n@@ -456,7 +469,7 @@ def ConvNeXt(depths,\n       file_suffix = \"_notop.h5\"\n       file_hash = WEIGHTS_HASHES[model_name][1]\n     file_name = model_name + file_suffix\n-    weights_path = data_utils.get_file(\n+    weights_path = utils.data_utils.get_file(\n       file_name,\n       BASE_WEIGHTS_PATH + file_name,\n       cache_subdir=\"models\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
