{"custom_id": "keras#77742304aca43fffe8d25d4b5033528aa54300e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 8 | Methods Changed: 5 | Complexity Î” (Sum/Max): 6/6 | Churn Î”: 38 | Churn Cumulative: 2311 | Contributors (this commit): 7 | Commits (past 90d): 51 | Contributors (cumulative): 14 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -101,7 +101,7 @@ class KerasVariable:\n \n     def assign(self, value):\n         value = self._convert_to_tensor(value, dtype=self.dtype)\n-        if value.shape != self.value.shape:\n+        if not shape_equal(value, self.value):\n             raise ValueError(\n                 \"The shape of the target variable and \"\n                 \"the shape of the target value in \"\n@@ -443,6 +443,16 @@ def standardize_shape(\n     return shape\n \n \n+def shape_equal(a, b):\n+    \"\"\"Return whether a.shape == b.shape (allows None entries).\"\"\"\n+    if len(a.shape) != len(b.shape):\n+        return False\n+    for e1, e2 in zip(a.shape, b.shape):\n+        if e1 is not None and e2 is not None and e1 != e2:\n+            return False\n+    return True\n+\n+\n def is_float_dtype(dtype):\n     if hasattr(dtype, \"name\"):\n         dtype = dtype.name\n\n@@ -1,6 +1,8 @@\n import tensorflow as tf\n from tensorflow.experimental import numpy as tfnp\n \n+from keras_core.backend.tensorflow.core import convert_to_tensor\n+\n \n def add(x1, x2):\n     return tfnp.add(x1, x2)\n@@ -503,7 +505,22 @@ def round(x, decimals=0):\n \n \n def tile(x, repeats):\n-    return tfnp.tile(x, repeats)\n+    # The TFNP implementation is buggy, we roll our own.\n+    x = convert_to_tensor(x)\n+    repeats = tf.reshape(convert_to_tensor(repeats, dtype=\"int32\"), [-1])\n+    repeats_size = tf.size(repeats)\n+    repeats = tf.pad(\n+        repeats,\n+        [[tf.maximum(x.shape.rank - repeats_size, 0), 0]],\n+        constant_values=1,\n+    )\n+    x_shape = tf.pad(\n+        tf.shape(x),\n+        [[tf.maximum(repeats_size - x.shape.rank, 0), 0]],\n+        constant_values=1,\n+    )\n+    x = tf.reshape(x, x_shape)\n+    return tf.tile(x, repeats)\n \n \n def trace(x, offset=0, axis1=0, axis2=1):\n\n@@ -1,5 +1,6 @@\n from keras_core import backend\n from keras_core import initializers\n+from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.utils.naming import auto_name\n from keras_core.utils.tracking import Tracker\n@@ -102,7 +103,7 @@ class Metric:\n         when a metric is evaluated during training.\n         \"\"\"\n         for v in self.variables:\n-            v.assign(0)\n+            v.assign(ops.zeros(v.shape, dtype=v.dtype))\n \n     def update_state(self, *args, **kwargs):\n         \"\"\"Accumulate statistics for the metric.\"\"\"\n\n@@ -436,7 +436,7 @@ def update_confusion_matrix_variables(\n     if multi_label:\n         one_thresh = ops.equal(\n             ops.cast(1, dtype=\"int32\"),\n-            thresholds.ndim,\n+            len(thresholds.shape),\n         )\n     else:\n         one_thresh = ops.cast(True, dtype=\"bool\")\n@@ -478,7 +478,7 @@ def update_confusion_matrix_variables(\n \n     pred_shape = ops.shape(y_pred)\n     num_predictions = pred_shape[0]\n-    if y_pred.ndim == 1:\n+    if len(y_pred.shape) == 1:\n         num_labels = 1\n     else:\n         num_labels = ops.cast(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b740259675a3c2cf3ab1f194a9513fe3bbccc54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 153 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 153 | Churn Cumulative: 153 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,153 @@\n+\"\"\"\n+Title: Imbalanced classification: credit card fraud detection\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2019/05/28\n+Last modified: 2020/04/17\n+Description: Demonstration of how to handle highly imbalanced classification problems.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example looks at the\n+[Kaggle Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud/)\n+dataset to demonstrate how\n+to train a classification model on data with highly imbalanced classes.\n+\"\"\"\n+\n+\"\"\"\n+## First, vectorize the CSV data\n+\"\"\"\n+\n+import numpy as np\n+import keras_core as keras\n+\n+# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/\n+fname = \"/Users/fchollet/Downloads/creditcard.csv\"\n+\n+all_features = []\n+all_targets = []\n+with open(fname) as f:\n+    for i, line in enumerate(f):\n+        if i == 0:\n+            print(\"HEADER:\", line.strip())\n+            continue  # Skip header\n+        fields = line.strip().split(\",\")\n+        all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])\n+        all_targets.append([int(fields[-1].replace('\"', \"\"))])\n+        if i == 1:\n+            print(\"EXAMPLE FEATURES:\", all_features[-1])\n+\n+features = np.array(all_features, dtype=\"float32\")\n+targets = np.array(all_targets, dtype=\"uint8\")\n+print(\"features.shape:\", features.shape)\n+print(\"targets.shape:\", targets.shape)\n+\n+\"\"\"\n+## Prepare a validation set\n+\"\"\"\n+\n+num_val_samples = int(len(features) * 0.2)\n+train_features = features[:-num_val_samples]\n+train_targets = targets[:-num_val_samples]\n+val_features = features[-num_val_samples:]\n+val_targets = targets[-num_val_samples:]\n+\n+print(\"Number of training samples:\", len(train_features))\n+print(\"Number of validation samples:\", len(val_features))\n+\n+\"\"\"\n+## Analyze class imbalance in the targets\n+\"\"\"\n+\n+counts = np.bincount(train_targets[:, 0])\n+print(\n+    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n+        counts[1], 100 * float(counts[1]) / len(train_targets)\n+    )\n+)\n+\n+weight_for_0 = 1.0 / counts[0]\n+weight_for_1 = 1.0 / counts[1]\n+\n+\"\"\"\n+## Normalize the data using training set statistics\n+\"\"\"\n+\n+mean = np.mean(train_features, axis=0)\n+train_features -= mean\n+val_features -= mean\n+std = np.std(train_features, axis=0)\n+train_features /= std\n+val_features /= std\n+\n+\"\"\"\n+## Build a binary classification model\n+\"\"\"\n+\n+model = keras.Sequential(\n+    [\n+        keras.layers.Dense(\n+            256, activation=\"relu\", input_shape=(train_features.shape[-1],)\n+        ),\n+        keras.layers.Dense(256, activation=\"relu\"),\n+        keras.layers.Dropout(0.3),\n+        keras.layers.Dense(256, activation=\"relu\"),\n+        keras.layers.Dropout(0.3),\n+        keras.layers.Dense(1, activation=\"sigmoid\"),\n+    ]\n+)\n+model.summary()\n+\n+\"\"\"\n+## Train the model with `class_weight` argument\n+\"\"\"\n+\n+metrics = [\n+    keras.metrics.FalseNegatives(name=\"fn\"),\n+    keras.metrics.FalsePositives(name=\"fp\"),\n+    keras.metrics.TrueNegatives(name=\"tn\"),\n+    keras.metrics.TruePositives(name=\"tp\"),\n+    keras.metrics.Precision(name=\"precision\"),\n+    keras.metrics.Recall(name=\"recall\"),\n+]\n+\n+model.compile(\n+    optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics\n+)\n+\n+callbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.keras\")]\n+class_weight = {0: weight_for_0, 1: weight_for_1}\n+\n+model.fit(\n+    train_features,\n+    train_targets,\n+    batch_size=2048,\n+    epochs=30,\n+    verbose=2,\n+    callbacks=callbacks,\n+    validation_data=(val_features, val_targets),\n+    class_weight=class_weight,\n+)\n+\n+\"\"\"\n+## Conclusions\n+\n+At the end of training, out of 56,961 validation transactions, we are:\n+\n+- Correctly identifying 66 of them as fraudulent\n+- Missing 9 fraudulent transactions\n+- At the cost of incorrectly flagging 441 legitimate transactions\n+\n+In the real world, one would put an even higher weight on class 1,\n+so as to reflect that False Negatives are more costly than False Positives.\n+\n+Next time your credit card gets  declined in an online purchase -- this is why.\n+\n+Example available on HuggingFace.\n+\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Model-Imbalanced%20Classification-black.svg)](https://huggingface.co/keras-io/imbalanced_classification) | [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Spaces-Imbalanced%20Classification-black.svg)](https://huggingface.co/spaces/keras-io/Credit_Card_Fraud_Detection) |\n+\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e99f1b4b6a450a0bba88da280a99bdf8885ae2ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 389 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 1 | Methods Changed: 5 | Complexity Î” (Sum/Max): 9/9 | Churn Î”: 389 | Churn Cumulative: 389 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n\n\n@@ -0,0 +1,389 @@\n+\"\"\"\n+Title: Timeseries forecasting for weather prediction\n+Authors: [Prabhanshu Attri](https://prabhanshu.com/github), [Yashika Sharma](https://github.com/yashika51), [Kristi Takach](https://github.com/ktakattack), [Falak Shah](https://github.com/falaktheoptimist)\n+Date created: 2020/06/23\n+Last modified: 2020/07/20\n+Description: This notebook demonstrates how to do timeseries forecasting using a LSTM model.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+This example requires TensorFlow 2.3 or higher.\n+\"\"\"\n+\n+import pandas as pd\n+import matplotlib.pyplot as plt\n+import keras_core as keras\n+\n+\"\"\"\n+## Climate Data Time-Series\n+\n+We will be using Jena Climate dataset recorded by the\n+[Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/).\n+The dataset consists of 14 features such as temperature, pressure, humidity etc, recorded once per\n+10 minutes.\n+\n+**Location**: Weather Station, Max Planck Institute for Biogeochemistry\n+in Jena, Germany\n+\n+**Time-frame Considered**: Jan 10, 2009 - December 31, 2016\n+\n+\n+The table below shows the column names, their value formats, and their description.\n+\n+Index| Features      |Format             |Description\n+-----|---------------|-------------------|-----------------------\n+1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n+2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n+3    |T (degC)       |-8.02              |Temperature in Celsius\n+4    |Tpot (K)       |265.4              |Temperature in Kelvin\n+5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n+6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n+7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n+8    |VPact (mbar)   |3.11               |Vapor pressure\n+9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n+10   |sh (g/kg)      |1.94               |Specific humidity\n+11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n+12   |rho (g/m ** 3) |1307.75            |Airtight\n+13   |wv (m/s)       |1.03               |Wind speed\n+14   |max. wv (m/s)  |1.75               |Maximum wind speed\n+15   |wd (deg)       |152.3              |Wind direction in degrees\n+\"\"\"\n+\n+from zipfile import ZipFile\n+\n+uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n+zip_path = keras.utils.get_file(origin=uri, fname=\"jena_climate_2009_2016.csv.zip\")\n+zip_file = ZipFile(zip_path)\n+zip_file.extractall()\n+csv_path = \"jena_climate_2009_2016.csv\"\n+\n+df = pd.read_csv(csv_path)\n+\n+\"\"\"\n+## Raw Data Visualization\n+\n+To give us a sense of the data we are working with, each feature has been plotted below.\n+This shows the distinct pattern of each feature over the time period from 2009 to 2016.\n+It also shows where anomalies are present, which will be addressed during normalization.\n+\"\"\"\n+\n+titles = [\n+    \"Pressure\",\n+    \"Temperature\",\n+    \"Temperature in Kelvin\",\n+    \"Temperature (dew point)\",\n+    \"Relative Humidity\",\n+    \"Saturation vapor pressure\",\n+    \"Vapor pressure\",\n+    \"Vapor pressure deficit\",\n+    \"Specific humidity\",\n+    \"Water vapor concentration\",\n+    \"Airtight\",\n+    \"Wind speed\",\n+    \"Maximum wind speed\",\n+    \"Wind direction in degrees\",\n+]\n+\n+feature_keys = [\n+    \"p (mbar)\",\n+    \"T (degC)\",\n+    \"Tpot (K)\",\n+    \"Tdew (degC)\",\n+    \"rh (%)\",\n+    \"VPmax (mbar)\",\n+    \"VPact (mbar)\",\n+    \"VPdef (mbar)\",\n+    \"sh (g/kg)\",\n+    \"H2OC (mmol/mol)\",\n+    \"rho (g/m**3)\",\n+    \"wv (m/s)\",\n+    \"max. wv (m/s)\",\n+    \"wd (deg)\",\n+]\n+\n+colors = [\n+    \"blue\",\n+    \"orange\",\n+    \"green\",\n+    \"red\",\n+    \"purple\",\n+    \"brown\",\n+    \"pink\",\n+    \"gray\",\n+    \"olive\",\n+    \"cyan\",\n+]\n+\n+date_time_key = \"Date Time\"\n+\n+\n+def show_raw_visualization(data):\n+    time_data = data[date_time_key]\n+    fig, axes = plt.subplots(\n+        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n+    )\n+    for i in range(len(feature_keys)):\n+        key = feature_keys[i]\n+        c = colors[i % (len(colors))]\n+        t_data = data[key]\n+        t_data.index = time_data\n+        t_data.head()\n+        ax = t_data.plot(\n+            ax=axes[i // 2, i % 2],\n+            color=c,\n+            title=\"{} - {}\".format(titles[i], key),\n+            rot=25,\n+        )\n+        ax.legend([titles[i]])\n+    plt.tight_layout()\n+\n+\n+show_raw_visualization(df)\n+\n+\"\"\"\n+This heat map shows the correlation between different features.\n+\"\"\"\n+\n+\n+def show_heatmap(data):\n+    plt.matshow(data.corr())\n+    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n+    plt.gca().xaxis.tick_bottom()\n+    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n+\n+    cb = plt.colorbar()\n+    cb.ax.tick_params(labelsize=14)\n+    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n+    plt.show()\n+\n+\n+show_heatmap(df)\n+\n+\n+\"\"\"\n+## Data Preprocessing\n+\n+Here we are picking ~300,000 data points for training. Observation is recorded every\n+10 mins, that means 6 times per hour. We will resample one point per hour since no\n+drastic change is expected within 60 minutes. We do this via the `sampling_rate`\n+argument in `timeseries_dataset_from_array` utility.\n+\n+We are tracking data from past 720 timestamps (720/6=120 hours). This data will be\n+used to predict the temperature after 72 timestamps (72/6=12 hours).\n+\n+Since every feature has values with\n+varying ranges, we do normalization to confine feature values to a range of `[0, 1]` before\n+training a neural network.\n+We do this by subtracting the mean and dividing by the standard deviation of each feature.\n+\n+71.5 % of the data will be used to train the model, i.e. 300,693 rows. `split_fraction` can\n+be changed to alter this percentage.\n+\n+The model is shown data for first 5 days i.e. 720 observations, that are sampled every\n+hour. The temperature after 72 (12 hours * 6 observation per hour) observation will be\n+used as a label.\n+\"\"\"\n+\n+split_fraction = 0.715\n+train_split = int(split_fraction * int(df.shape[0]))\n+step = 6\n+\n+past = 720\n+future = 72\n+learning_rate = 0.001\n+batch_size = 256\n+epochs = 10\n+\n+\n+def normalize(data, train_split):\n+    data_mean = data[:train_split].mean(axis=0)\n+    data_std = data[:train_split].std(axis=0)\n+    return (data - data_mean) / data_std\n+\n+\n+\"\"\"\n+We can see from the correlation heatmap, few parameters like Relative Humidity and\n+Specific Humidity are redundant. Hence we will be using select features, not all.\n+\"\"\"\n+\n+print(\n+    \"The selected parameters are:\",\n+    \", \".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]),\n+)\n+selected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]]\n+features = df[selected_features]\n+features.index = df[date_time_key]\n+features.head()\n+\n+features = normalize(features.values, train_split)\n+features = pd.DataFrame(features)\n+features.head()\n+\n+train_data = features.loc[0 : train_split - 1]\n+val_data = features.loc[train_split:]\n+\n+\"\"\"\n+# Training dataset\n+\n+The training dataset labels starts from the 792nd observation (720 + 72).\n+\"\"\"\n+\n+start = past + future\n+end = start + train_split\n+\n+x_train = train_data[[i for i in range(7)]].values\n+y_train = features.iloc[start:end][[1]]\n+\n+sequence_length = int(past / step)\n+\n+\"\"\"\n+The `timeseries_dataset_from_array` function takes in a sequence of data-points gathered at\n+equal intervals, along with time series parameters such as length of the\n+sequences/windows, spacing between two sequence/windows, etc., to produce batches of\n+sub-timeseries inputs and targets sampled from the main timeseries.\n+\"\"\"\n+\n+dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n+    x_train,\n+    y_train,\n+    sequence_length=sequence_length,\n+    sampling_rate=step,\n+    batch_size=batch_size,\n+)\n+\n+\"\"\"\n+## Validation dataset\n+\n+The validation dataset must not contain the last 792 rows as we won't have label data for\n+those records, hence 792 must be subtracted from the end of the data.\n+\n+The validation label dataset must start from 792 after train_split, hence we must add\n+past + future (792) to label_start.\n+\"\"\"\n+\n+x_end = len(val_data) - past - future\n+\n+label_start = train_split + past + future\n+\n+x_val = val_data.iloc[:x_end][[i for i in range(7)]].values\n+y_val = features.iloc[label_start:][[1]]\n+\n+dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n+    x_val,\n+    y_val,\n+    sequence_length=sequence_length,\n+    sampling_rate=step,\n+    batch_size=batch_size,\n+)\n+\n+\n+for batch in dataset_train.take(1):\n+    inputs, targets = batch\n+\n+print(\"Input shape:\", inputs.numpy().shape)\n+print(\"Target shape:\", targets.numpy().shape)\n+\n+\"\"\"\n+## Training\n+\"\"\"\n+\n+inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n+lstm_out = keras.layers.LSTM(32)(inputs)\n+outputs = keras.layers.Dense(1)(lstm_out)\n+\n+model = keras.Model(inputs=inputs, outputs=outputs)\n+model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n+model.summary()\n+\n+\"\"\"\n+We'll use the `ModelCheckpoint` callback to regularly save checkpoints, and\n+the `EarlyStopping` callback to interrupt training when the validation loss\n+is not longer improving.\n+\"\"\"\n+\n+path_checkpoint = \"model_checkpoint.weights.h5\"\n+es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n+\n+modelckpt_callback = keras.callbacks.ModelCheckpoint(\n+    monitor=\"val_loss\",\n+    filepath=path_checkpoint,\n+    verbose=1,\n+    save_weights_only=True,\n+    save_best_only=True,\n+)\n+\n+history = model.fit(\n+    dataset_train,\n+    epochs=epochs,\n+    validation_data=dataset_val,\n+    callbacks=[es_callback, modelckpt_callback],\n+)\n+\n+\"\"\"\n+We can visualize the loss with the function below. After one point, the loss stops\n+decreasing.\n+\"\"\"\n+\n+\n+def visualize_loss(history, title):\n+    loss = history.history[\"loss\"]\n+    val_loss = history.history[\"val_loss\"]\n+    epochs = range(len(loss))\n+    plt.figure()\n+    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n+    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n+    plt.title(title)\n+    plt.xlabel(\"Epochs\")\n+    plt.ylabel(\"Loss\")\n+    plt.legend()\n+    plt.show()\n+\n+\n+visualize_loss(history, \"Training and Validation Loss\")\n+\n+\"\"\"\n+## Prediction\n+\n+The trained model above is now able to make predictions for 5 sets of values from\n+validation set.\n+\"\"\"\n+\n+\n+def show_plot(plot_data, delta, title):\n+    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n+    marker = [\".-\", \"rx\", \"go\"]\n+    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n+    if delta:\n+        future = delta\n+    else:\n+        future = 0\n+\n+    plt.title(title)\n+    for i, val in enumerate(plot_data):\n+        if i:\n+            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n+        else:\n+            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n+    plt.legend()\n+    plt.xlim([time_steps[0], (future + 5) * 2])\n+    plt.xlabel(\"Time-Step\")\n+    plt.show()\n+    return\n+\n+\n+for x, y in dataset_val.take(5):\n+    show_plot(\n+        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n+        12,\n+        \"Single Step Prediction\",\n+    )\n+\n+\n+\"\"\"\n+**Example available on HuggingFace**\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Time%20Series-black.svg)](https://huggingface.co/keras-io/timeseries_forecasting_for_weather) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Time%20Series-black.svg)](https://huggingface.co/spaces/keras-io/timeseries_forecasting_for_weather) |\n+\"\"\"\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2ccc9c7178de01b76c06826f55bf1367d6f6ac5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 433 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 10 | Methods Changed: 11 | Complexity Î” (Sum/Max): 13/8 | Churn Î”: 441 | Churn Cumulative: 1004 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,198 @@\n+\"\"\"\n+Title: Variational AutoEncoder\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2020/05/03\n+Last modified: 2020/05/03\n+Description: Convolutional Variational AutoEncoder (VAE) trained on MNIST digits.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+\n+\"\"\"\n+## Create a sampling layer\n+\"\"\"\n+\n+\n+class Sampling(layers.Layer):\n+    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n+\n+    def call(self, inputs):\n+        z_mean, z_log_var = inputs\n+        batch = tf.shape(z_mean)[0]\n+        dim = tf.shape(z_mean)[1]\n+        epsilon = tf.random.normal(shape=(batch, dim))\n+        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n+\n+\n+\"\"\"\n+## Build the encoder\n+\"\"\"\n+\n+latent_dim = 2\n+\n+encoder_inputs = keras.Input(shape=(28, 28, 1))\n+x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n+    encoder_inputs\n+)\n+x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n+x = layers.Flatten()(x)\n+x = layers.Dense(16, activation=\"relu\")(x)\n+z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n+z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n+z = Sampling()([z_mean, z_log_var])\n+encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n+encoder.summary()\n+\n+\"\"\"\n+## Build the decoder\n+\"\"\"\n+\n+latent_inputs = keras.Input(shape=(latent_dim,))\n+x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n+x = layers.Reshape((7, 7, 64))(x)\n+x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(\n+    x\n+)\n+x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n+    x\n+)\n+decoder_outputs = layers.Conv2DTranspose(\n+    1, 3, activation=\"sigmoid\", padding=\"same\"\n+)(x)\n+decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n+decoder.summary()\n+\n+\"\"\"\n+## Define the VAE as a `Model` with a custom `train_step`\n+\"\"\"\n+\n+\n+class VAE(keras.Model):\n+    def __init__(self, encoder, decoder, **kwargs):\n+        super().__init__(**kwargs)\n+        self.encoder = encoder\n+        self.decoder = decoder\n+        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n+        self.reconstruction_loss_tracker = keras.metrics.Mean(\n+            name=\"reconstruction_loss\"\n+        )\n+        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n+\n+    @property\n+    def metrics(self):\n+        return [\n+            self.total_loss_tracker,\n+            self.reconstruction_loss_tracker,\n+            self.kl_loss_tracker,\n+        ]\n+\n+    def train_step(self, data):\n+        with tf.GradientTape() as tape:\n+            z_mean, z_log_var, z = self.encoder(data)\n+            reconstruction = self.decoder(z)\n+            reconstruction_loss = tf.reduce_mean(\n+                tf.reduce_sum(\n+                    keras.losses.binary_crossentropy(data, reconstruction),\n+                    axis=(1, 2),\n+                )\n+            )\n+            kl_loss = -0.5 * (\n+                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n+            )\n+            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n+            total_loss = reconstruction_loss + kl_loss\n+        grads = tape.gradient(total_loss, self.trainable_weights)\n+        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n+        self.total_loss_tracker.update_state(total_loss)\n+        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n+        self.kl_loss_tracker.update_state(kl_loss)\n+        return {\n+            \"loss\": self.total_loss_tracker.result(),\n+            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n+            \"kl_loss\": self.kl_loss_tracker.result(),\n+        }\n+\n+\n+\"\"\"\n+## Train the VAE\n+\"\"\"\n+\n+(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n+mnist_digits = np.concatenate([x_train, x_test], axis=0)\n+mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n+\n+vae = VAE(encoder, decoder)\n+vae.compile(optimizer=keras.optimizers.Adam())\n+vae.fit(mnist_digits, epochs=30, batch_size=128)\n+\n+\"\"\"\n+## Display a grid of sampled digits\n+\"\"\"\n+\n+import matplotlib.pyplot as plt\n+\n+\n+def plot_latent_space(vae, n=30, figsize=15):\n+    # display a n*n 2D manifold of digits\n+    digit_size = 28\n+    scale = 1.0\n+    figure = np.zeros((digit_size * n, digit_size * n))\n+    # linearly spaced coordinates corresponding to the 2D plot\n+    # of digit classes in the latent space\n+    grid_x = np.linspace(-scale, scale, n)\n+    grid_y = np.linspace(-scale, scale, n)[::-1]\n+\n+    for i, yi in enumerate(grid_y):\n+        for j, xi in enumerate(grid_x):\n+            z_sample = np.array([[xi, yi]])\n+            x_decoded = vae.decoder.predict(z_sample)\n+            digit = x_decoded[0].reshape(digit_size, digit_size)\n+            figure[\n+                i * digit_size : (i + 1) * digit_size,\n+                j * digit_size : (j + 1) * digit_size,\n+            ] = digit\n+\n+    plt.figure(figsize=(figsize, figsize))\n+    start_range = digit_size // 2\n+    end_range = n * digit_size + start_range\n+    pixel_range = np.arange(start_range, end_range, digit_size)\n+    sample_range_x = np.round(grid_x, 1)\n+    sample_range_y = np.round(grid_y, 1)\n+    plt.xticks(pixel_range, sample_range_x)\n+    plt.yticks(pixel_range, sample_range_y)\n+    plt.xlabel(\"z[0]\")\n+    plt.ylabel(\"z[1]\")\n+    plt.imshow(figure, cmap=\"Greys_r\")\n+    plt.show()\n+\n+\n+plot_latent_space(vae)\n+\n+\"\"\"\n+## Display how the latent space clusters different digit classes\n+\"\"\"\n+\n+\n+def plot_label_clusters(vae, data, labels):\n+    # display a 2D plot of the digit classes in the latent space\n+    z_mean, _, _ = vae.encoder.predict(data, verbose=0)\n+    plt.figure(figsize=(12, 10))\n+    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n+    plt.colorbar()\n+    plt.xlabel(\"z[0]\")\n+    plt.ylabel(\"z[1]\")\n+    plt.show()\n+\n+\n+(x_train, y_train), _ = keras.datasets.mnist.load_data()\n+x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n+\n+plot_label_clusters(vae, x_train, y_train)\n\n@@ -113,10 +113,14 @@ metrics = [\n ]\n \n model.compile(\n-    optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics\n+    optimizer=keras.optimizers.Adam(1e-2),\n+    loss=\"binary_crossentropy\",\n+    metrics=metrics,\n )\n \n-callbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.keras\")]\n+callbacks = [\n+    keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.keras\")\n+]\n class_weight = {0: weight_for_0, 1: weight_for_1}\n \n model.fit(\n\n@@ -54,7 +54,9 @@ Index| Features      |Format             |Description\n from zipfile import ZipFile\n \n uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n-zip_path = keras.utils.get_file(origin=uri, fname=\"jena_climate_2009_2016.csv.zip\")\n+zip_path = keras.utils.get_file(\n+    origin=uri, fname=\"jena_climate_2009_2016.csv.zip\"\n+)\n zip_file = ZipFile(zip_path)\n zip_file.extractall()\n csv_path = \"jena_climate_2009_2016.csv\"\n@@ -294,7 +296,9 @@ lstm_out = keras.layers.LSTM(32)(inputs)\n outputs = keras.layers.Dense(1)(lstm_out)\n \n model = keras.Model(inputs=inputs, outputs=outputs)\n-model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n+model.compile(\n+    optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\"\n+)\n model.summary()\n \n \"\"\"\n@@ -304,7 +308,9 @@ is not longer improving.\n \"\"\"\n \n path_checkpoint = \"model_checkpoint.weights.h5\"\n-es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n+es_callback = keras.callbacks.EarlyStopping(\n+    monitor=\"val_loss\", min_delta=0, patience=5\n+)\n \n modelckpt_callback = keras.callbacks.ModelCheckpoint(\n     monitor=\"val_loss\",\n@@ -363,9 +369,13 @@ def show_plot(plot_data, delta, title):\n     plt.title(title)\n     for i, val in enumerate(plot_data):\n         if i:\n-            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n+            plt.plot(\n+                future, plot_data[i], marker[i], markersize=10, label=labels[i]\n+            )\n         else:\n-            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n+            plt.plot(\n+                time_steps, plot_data[i].flatten(), marker[i], label=labels[i]\n+            )\n     plt.legend()\n     plt.xlim([time_steps[0], (future + 5) * 2])\n     plt.xlabel(\"Time-Step\")\n\n@@ -0,0 +1,211 @@\n+\"\"\"\n+Title: Grad-CAM class activation visualization\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2020/04/26\n+Last modified: 2021/03/07\n+Description: How to obtain a class activation heatmap for an image classification model.\n+Accelerator: NONE\n+\"\"\"\n+\"\"\"\n+Adapted from Deep Learning with Python (2017).\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+import keras_core as keras\n+\n+# Display\n+from IPython.display import Image, display\n+import matplotlib.pyplot as plt\n+import matplotlib.cm as cm\n+\n+keras.config.disable_traceback_filtering()\n+\n+\n+\"\"\"\n+## Configurable parameters\n+\n+You can change these to another model.\n+\n+To get the values for `last_conv_layer_name` use `model.summary()`\n+to see the names of all layers in the model.\n+\n+\"\"\"\n+\n+model_builder = keras.applications.xception.Xception\n+img_size = (299, 299)\n+preprocess_input = keras.applications.xception.preprocess_input\n+decode_predictions = keras.applications.xception.decode_predictions\n+\n+last_conv_layer_name = \"block14_sepconv2_act\"\n+\n+# The local path to our target image\n+img_path = keras.utils.get_file(\n+    fname=\"african_elephant.jpg\", origin=\"https://i.imgur.com/Bvro0YD.png\"\n+)\n+\n+display(Image(img_path))\n+\n+\n+\"\"\"\n+## The Grad-CAM algorithm\n+\"\"\"\n+\n+\n+def get_img_array(img_path, size):\n+    # `img` is a PIL image of size 299x299\n+    img = keras.utils.load_img(img_path, target_size=size)\n+    # `array` is a float32 Numpy array of shape (299, 299, 3)\n+    array = keras.utils.img_to_array(img)\n+    # We add a dimension to transform our array into a \"batch\"\n+    # of size (1, 299, 299, 3)\n+    array = np.expand_dims(array, axis=0)\n+    return array\n+\n+\n+def make_gradcam_heatmap(\n+    img_array, model, last_conv_layer_name, pred_index=None\n+):\n+    # First, we create a model that maps the input image to the activations\n+    # of the last conv layer as well as the output predictions\n+    grad_model = keras.models.Model(\n+        model.inputs,\n+        [model.get_layer(last_conv_layer_name).output, model.output],\n+    )\n+\n+    # Then, we compute the gradient of the top predicted class for our input image\n+    # with respect to the activations of the last conv layer\n+    with tf.GradientTape() as tape:\n+        last_conv_layer_output, preds = grad_model(img_array)\n+        if pred_index is None:\n+            pred_index = tf.argmax(preds[0])\n+        class_channel = preds[:, pred_index]\n+\n+    # This is the gradient of the output neuron (top predicted or chosen)\n+    # with regard to the output feature map of the last conv layer\n+    grads = tape.gradient(class_channel, last_conv_layer_output)\n+\n+    # This is a vector where each entry is the mean intensity of the gradient\n+    # over a specific feature map channel\n+    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n+\n+    # We multiply each channel in the feature map array\n+    # by \"how important this channel is\" with regard to the top predicted class\n+    # then sum all the channels to obtain the heatmap class activation\n+    last_conv_layer_output = last_conv_layer_output[0]\n+    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n+    heatmap = tf.squeeze(heatmap)\n+\n+    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n+    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n+    return heatmap.numpy()\n+\n+\n+\"\"\"\n+## Let's test-drive it\n+\n+\"\"\"\n+\n+# Prepare image\n+img_array = preprocess_input(get_img_array(img_path, size=img_size))\n+\n+# Make model\n+model = model_builder(weights=\"imagenet\")\n+\n+# Remove last layer's softmax\n+model.layers[-1].activation = None\n+\n+# Print what the top predicted class is\n+preds = model.predict(img_array)\n+print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n+\n+# Generate class activation heatmap\n+heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n+\n+# Display heatmap\n+plt.matshow(heatmap)\n+plt.show()\n+\n+\n+\"\"\"\n+## Create a superimposed visualization\n+\n+\"\"\"\n+\n+\n+def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n+    # Load the original image\n+    img = keras.utils.load_img(img_path)\n+    img = keras.utils.img_to_array(img)\n+\n+    # Rescale heatmap to a range 0-255\n+    heatmap = np.uint8(255 * heatmap)\n+\n+    # Use jet colormap to colorize heatmap\n+    jet = cm.get_cmap(\"jet\")\n+\n+    # Use RGB values of the colormap\n+    jet_colors = jet(np.arange(256))[:, :3]\n+    jet_heatmap = jet_colors[heatmap]\n+\n+    # Create an image with RGB colorized heatmap\n+    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n+    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n+    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n+\n+    # Superimpose the heatmap on original image\n+    superimposed_img = jet_heatmap * alpha + img\n+    superimposed_img = keras.utils.array_to_img(superimposed_img)\n+\n+    # Save the superimposed image\n+    superimposed_img.save(cam_path)\n+\n+    # Display Grad CAM\n+    display(Image(cam_path))\n+\n+\n+save_and_display_gradcam(img_path, heatmap)\n+\n+\"\"\"\n+## Let's try another image\n+\n+We will see how the grad cam explains the model's outputs for a multi-label image. Let's\n+try an image with a cat and a dog together, and see how the grad cam behaves.\n+\"\"\"\n+\n+img_path = keras.utils.get_file(\n+    fname=\"cat_and_dog.jpg\",\n+    origin=\"https://storage.googleapis.com/petbacker/images/blog/2017/dog-and-cat-cover.jpg\",\n+)\n+\n+display(Image(img_path))\n+\n+# Prepare image\n+img_array = preprocess_input(get_img_array(img_path, size=img_size))\n+\n+# Print what the two top predicted classes are\n+preds = model.predict(img_array)\n+print(\"Predicted:\", decode_predictions(preds, top=2)[0])\n+\n+\"\"\"\n+We generate class activation heatmap for \"chow,\" the class index is 260\n+\n+\"\"\"\n+\n+heatmap = make_gradcam_heatmap(\n+    img_array, model, last_conv_layer_name, pred_index=260\n+)\n+\n+save_and_display_gradcam(img_path, heatmap)\n+\n+\"\"\"\n+We generate class activation heatmap for \"egyptian cat,\" the class index is 285\n+\n+\"\"\"\n+\n+heatmap = make_gradcam_heatmap(\n+    img_array, model, last_conv_layer_name, pred_index=285\n+)\n+\n+save_and_display_gradcam(img_path, heatmap)\n\n@@ -103,7 +103,9 @@ class Dense(Layer):\n         x = ops.matmul(inputs, self.kernel)\n         if self.use_bias:\n             x = x + self.bias\n-        return self.activation(x)\n+        if self.activation:\n+            x = self.activation(x)\n+        return x\n \n     def compute_output_shape(self, input_shape):\n         output_shape = list(input_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c4e2eb569b2fb2d6c1e90d0935a08594f8502f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 205 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Î” (Sum/Max): 9/9 | Churn Î”: 205 | Churn Cumulative: 205 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,205 @@\n+\"\"\"\n+Title: Deep Dream\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2016/01/13\n+Last modified: 2020/05/02\n+Description: Generating Deep Dreams with Keras.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+\"Deep dream\" is an image-filtering technique which consists of taking an image\n+classification model, and running gradient ascent over an input image to\n+try to maximize the activations of specific layers (and sometimes, specific units in\n+specific layers) for this input. It produces hallucination-like visuals.\n+\n+It was first introduced by Alexander Mordvintsev from Google in July 2015.\n+\n+Process:\n+\n+- Load the original image.\n+- Define a number of processing scales (\"octaves\"),\n+from smallest to largest.\n+- Resize the original image to the smallest scale.\n+- For every scale, starting with the smallest (i.e. current one):\n+    - Run gradient ascent\n+    - Upscale image to the next scale\n+    - Reinject the detail that was lost at upscaling time\n+- Stop when we are back to the original size.\n+To obtain the detail lost during upscaling, we simply\n+take the original image, shrink it down, upscale it,\n+and compare the result to the (resized) original image.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core.applications import inception_v3\n+\n+base_image_path = keras.utils.get_file(\"sky.jpg\", \"https://i.imgur.com/aGBdQyK.jpg\")\n+result_prefix = \"sky_dream\"\n+\n+# These are the names of the layers\n+# for which we try to maximize activation,\n+# as well as their weight in the final loss\n+# we try to maximize.\n+# You can tweak these setting to obtain new visual effects.\n+layer_settings = {\n+    \"mixed4\": 1.0,\n+    \"mixed5\": 1.5,\n+    \"mixed6\": 2.0,\n+    \"mixed7\": 2.5,\n+}\n+\n+# Playing with these hyperparameters will also allow you to achieve new effects\n+step = 0.01  # Gradient ascent step size\n+num_octave = 3  # Number of scales at which to run gradient ascent\n+octave_scale = 1.4  # Size ratio between scales\n+iterations = 20  # Number of ascent steps per scale\n+max_loss = 15.0\n+\n+\"\"\"\n+This is our base image:\n+\"\"\"\n+\n+from IPython.display import Image, display\n+\n+display(Image(base_image_path))\n+\n+\"\"\"\n+Let's set up some image preprocessing/deprocessing utilities:\n+\"\"\"\n+\n+\n+def preprocess_image(image_path):\n+    # Util function to open, resize and format pictures\n+    # into appropriate arrays.\n+    img = keras.utils.load_img(image_path)\n+    img = keras.utils.img_to_array(img)\n+    img = np.expand_dims(img, axis=0)\n+    img = inception_v3.preprocess_input(img)\n+    return img\n+\n+\n+def deprocess_image(x):\n+    # Util function to convert a NumPy array into a valid image.\n+    x = x.reshape((x.shape[1], x.shape[2], 3))\n+    # Undo inception v3 preprocessing\n+    x /= 2.0\n+    x += 0.5\n+    x *= 255.0\n+    # Convert to uint8 and clip to the valid range [0, 255]\n+    x = np.clip(x, 0, 255).astype(\"uint8\")\n+    return x\n+\n+\n+\"\"\"\n+## Compute the Deep Dream loss\n+\n+First, build a feature extraction model to retrieve the activations of our target layers\n+given an input image.\n+\"\"\"\n+\n+# Build an InceptionV3 model loaded with pre-trained ImageNet weights\n+model = inception_v3.InceptionV3(weights=\"imagenet\", include_top=False)\n+\n+# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n+outputs_dict = dict(\n+    [\n+        (layer.name, layer.output)\n+        for layer in [model.get_layer(name) for name in layer_settings.keys()]\n+    ]\n+)\n+\n+# Set up a model that returns the activation values for every target layer\n+# (as a dict)\n+feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n+\n+\"\"\"\n+The actual loss computation is very simple:\n+\"\"\"\n+\n+\n+def compute_loss(input_image):\n+    features = feature_extractor(input_image)\n+    # Initialize the loss\n+    loss = tf.zeros(shape=())\n+    for name in features.keys():\n+        coeff = layer_settings[name]\n+        activation = features[name]\n+        # We avoid border artifacts by only involving non-border pixels in the loss.\n+        scaling = tf.reduce_prod(tf.cast(tf.shape(activation), \"float32\"))\n+        loss += coeff * tf.reduce_sum(tf.square(activation[:, 2:-2, 2:-2, :])) / scaling\n+    return loss\n+\n+\n+\"\"\"\n+## Set up the gradient ascent loop for one octave\n+\"\"\"\n+\n+\n+@tf.function\n+def gradient_ascent_step(img, learning_rate):\n+    with tf.GradientTape() as tape:\n+        tape.watch(img)\n+        loss = compute_loss(img)\n+    # Compute gradients.\n+    grads = tape.gradient(loss, img)\n+    # Normalize gradients.\n+    grads /= tf.maximum(tf.reduce_mean(tf.abs(grads)), 1e-6)\n+    img += learning_rate * grads\n+    return loss, img\n+\n+\n+def gradient_ascent_loop(img, iterations, learning_rate, max_loss=None):\n+    for i in range(iterations):\n+        loss, img = gradient_ascent_step(img, learning_rate)\n+        if max_loss is not None and loss > max_loss:\n+            break\n+        print(\"... Loss value at step %d: %.2f\" % (i, loss))\n+    return img\n+\n+\n+\"\"\"\n+## Run the training loop, iterating over different octaves\n+\"\"\"\n+\n+original_img = preprocess_image(base_image_path)\n+original_shape = original_img.shape[1:3]\n+\n+successive_shapes = [original_shape]\n+for i in range(1, num_octave):\n+    shape = tuple([int(dim / (octave_scale**i)) for dim in original_shape])\n+    successive_shapes.append(shape)\n+successive_shapes = successive_shapes[::-1]\n+shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])\n+\n+img = tf.identity(original_img)  # Make a copy\n+for i, shape in enumerate(successive_shapes):\n+    print(\"Processing octave %d with shape %s\" % (i, shape))\n+    img = tf.image.resize(img, shape)\n+    img = gradient_ascent_loop(\n+        img, iterations=iterations, learning_rate=step, max_loss=max_loss\n+    )\n+    upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)\n+    same_size_original = tf.image.resize(original_img, shape)\n+    lost_detail = same_size_original - upscaled_shrunk_original_img\n+\n+    img += lost_detail\n+    shrunk_original_img = tf.image.resize(original_img, shape)\n+\n+keras.utils.save_img(result_prefix + \".png\", deprocess_image(img.numpy()))\n+\n+\"\"\"\n+Display the result.\n+\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/deep-dream)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/deep-dream).\n+\"\"\"\n+\n+display(Image(result_prefix + \".png\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5f1869c20bcedc781edb047eeebfa7c42f1c690", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 6 | Files Changed: 16 | Hunks: 22 | Methods Changed: 18 | Complexity Î” (Sum/Max): 5/3 | Churn Î”: 44 | Churn Cumulative: 5927 | Contributors (this commit): 10 | Commits (past 90d): 162 | Contributors (cumulative): 36 | DMM Complexity: 0.0\n\nDIFF:\n@@ -41,7 +41,9 @@ import tensorflow as tf\n import keras_core as keras\n from keras_core.applications import inception_v3\n \n-base_image_path = keras.utils.get_file(\"sky.jpg\", \"https://i.imgur.com/aGBdQyK.jpg\")\n+base_image_path = keras.utils.get_file(\n+    \"sky.jpg\", \"https://i.imgur.com/aGBdQyK.jpg\"\n+)\n result_prefix = \"sky_dream\"\n \n # These are the names of the layers\n@@ -134,7 +136,11 @@ def compute_loss(input_image):\n         activation = features[name]\n         # We avoid border artifacts by only involving non-border pixels in the loss.\n         scaling = tf.reduce_prod(tf.cast(tf.shape(activation), \"float32\"))\n-        loss += coeff * tf.reduce_sum(tf.square(activation[:, 2:-2, 2:-2, :])) / scaling\n+        loss += (\n+            coeff\n+            * tf.reduce_sum(tf.square(activation[:, 2:-2, 2:-2, :]))\n+            / scaling\n+        )\n     return loss\n \n \n\n@@ -230,6 +230,7 @@ class Layer(Operation):\n         self.dtype_policy = mixed_precision.resolve_policy(dtype)\n         self.autocast = autocast\n         self.input_spec = None\n+        self.supports_jit = True\n \n         self._trainable = trainable\n         self._layers = []\n\n@@ -122,6 +122,7 @@ class Discretization(Layer):\n         self.epsilon = epsilon\n         self.output_mode = output_mode\n         self.sparse = sparse\n+        self.supports_jit = False\n \n     def build(self, input_shape):\n         self.layer.build(input_shape)\n\n@@ -93,6 +93,7 @@ class HashedCrossing(Layer):\n         self.sparse = sparse\n         self._allow_non_tensor_positional_args = True\n         self._convert_input_args = False\n+        self.supports_jit = False\n \n     def compute_output_shape(self, input_shape):\n         return tuple(self.layer.compute_output_shape(input_shape))\n\n@@ -169,6 +169,7 @@ class Hashing(Layer):\n                 )\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n+        self.supports_jit = False\n \n     def call(self, inputs):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n\n@@ -328,6 +328,7 @@ class IntegerLookup(Layer):\n             **kwargs,\n         )\n         self._allow_non_tensor_positional_args = True\n+        self.supports_jit = False\n \n     def adapt(self, data, batch_size=None, steps=None):\n         \"\"\"Computes a vocabulary of interger terms from tokens in a dataset.\n\n@@ -5,9 +5,11 @@ import tensorflow as tf\n \n from keras_core import backend\n from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n \n \n+@keras_core_export(\"keras_core.layers.Normalization\")\n class Normalization(Layer):\n     \"\"\"A preprocessing layer that normalizes continuous features.\n \n@@ -290,6 +292,7 @@ class Normalization(Layer):\n         self.variance = ops.cast(self.variance, self.compute_dtype)\n \n     def call(self, inputs):\n+        inputs = backend.convert_to_tensor(inputs, dtype=self.compute_dtype)\n         if self.invert:\n             return self.mean + (\n                 inputs * ops.maximum(ops.sqrt(self.variance), backend.epsilon())\n\n@@ -58,6 +58,7 @@ class RandomCrop(Layer):\n             name=name,\n         )\n         self.supports_masking = False\n+        self.supports_jit = False\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n\n@@ -50,6 +50,7 @@ class RandomFlip(Layer):\n             seed=self.seed,\n             **kwargs,\n         )\n+        self.supports_jit = False\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n\n@@ -93,6 +93,7 @@ class RandomRotation(Layer):\n             name=name,\n             **kwargs,\n         )\n+        self.supports_jit = False\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n\n@@ -83,6 +83,7 @@ class RandomTranslation(Layer):\n             name=name,\n             **kwargs,\n         )\n+        self.supports_jit = False\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n\n@@ -106,6 +106,7 @@ class RandomZoom(Layer):\n             fill_value=fill_value,\n             **kwargs,\n         )\n+        self.supports_jit = False\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n\n@@ -330,6 +330,7 @@ class StringLookup(Layer):\n         )\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n+        self.supports_jit = False\n \n     def adapt(self, data, batch_size=None, steps=None):\n         \"\"\"Computes a vocabulary of interger terms from tokens in a dataset.\n\n@@ -238,6 +238,7 @@ class TextVectorization(Layer):\n         )\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n+        self.supports_jit = False\n \n     def compute_output_shape(self, input_shape):\n         return tuple(self.layer.compute_output_shape(input_shape))\n\n@@ -163,11 +163,17 @@ def convert_to_arrays(arrays, dtype=None):\n         if isinstance(x, (tf.Tensor, tf.Variable)):\n             x = x.numpy()\n         if not isinstance(x, np.ndarray):\n+            if hasattr(x, \"__array__\"):\n+                x = np.array(x, dtype=dtype)\n+            else:\n                 raise ValueError(\n-                \"Expected a NumPy array, tf.Tensor, Pandas Dataframe or Pandas \"\n-                f\"Series. Received invalid input: {x} (of type {type(x)})\"\n+                    \"Expected a NumPy array, tf.Tensor, \"\n+                    \"Pandas Dataframe, or Pandas Series. \"\n+                    f\"Received invalid input: {x} (of type {type(x)})\"\n                 )\n-        if not str(x.dtype) == str(dtype):\n+        if x.dtype == object:\n+            return x\n+        if not x.dtype == dtype:\n             x = x.astype(dtype)\n         return x\n \n\n@@ -28,7 +28,7 @@ class Trainer:\n         weighted_metrics=None,\n         run_eagerly=False,\n         steps_per_execution=1,\n-        jit_compile=True,\n+        jit_compile=\"auto\",\n     ):\n         self.optimizer = optimizers.get(optimizer)\n         if loss is not None:\n@@ -39,6 +39,11 @@ class Trainer:\n             self._compile_metrics = CompileMetrics(metrics, weighted_metrics)\n         else:\n             self._compile_metrics = None\n+        if jit_compile == \"auto\":\n+            if all(x.supports_jit for x in self._flatten_layers()):\n+                jit_compile = True\n+            else:\n+                jit_compile = False\n         if jit_compile and run_eagerly:\n             jit_compile = False\n             warnings.warn(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
