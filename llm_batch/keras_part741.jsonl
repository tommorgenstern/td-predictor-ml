{"custom_id": "keras#148ea83a868543dee4fbe268a769f6b27cc13e2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 241 | Files Changed: 46 | Hunks: 79 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 326 | Churn Cumulative: 21962 | Contributors (this commit): 54 | Commits (past 90d): 92 | Contributors (cumulative): 243 | DMM Complexity: 0.42857142857142855\n\nDIFF:\n@@ -207,9 +207,7 @@ class LayerBenchmark:\n             callbacks=[tf_keras_callback],\n         )\n \n-        keras_throughput = (\n-            callback._callback.state[\"throughput\"] * batch_size\n-        )\n+        keras_throughput = callback._callback.state[\"throughput\"] * batch_size\n         tf_keras_throughput = (\n             tf_keras_callback._callback.state[\"throughput\"] * batch_size\n         )\n@@ -240,16 +238,12 @@ class LayerBenchmark:\n             if self.flat_call_inputs:\n                 # Scale by a small factor to avoid zero gradients.\n                 label = (\n-                    keras.backend.convert_to_numpy(\n-                        self._keras_layer(*data)\n-                    )\n+                    keras.backend.convert_to_numpy(self._keras_layer(*data))\n                     * 1.001\n                 )\n             else:\n                 label = (\n-                    keras.backend.convert_to_numpy(\n-                        self._keras_layer(data)\n-                    )\n+                    keras.backend.convert_to_numpy(self._keras_layer(data))\n                     * 1.001\n                 )\n \n@@ -272,9 +266,7 @@ class LayerBenchmark:\n             callbacks=[tf_keras_callback],\n         )\n \n-        keras_throughput = (\n-            callback._callback.state[\"throughput\"] * batch_size\n-        )\n+        keras_throughput = callback._callback.state[\"throughput\"] * batch_size\n         tf_keras_throughput = (\n             tf_keras_callback._callback.state[\"throughput\"] * batch_size\n         )\n\n@@ -194,9 +194,7 @@ def benchmark_bidirectional(\n ):\n     layer_name = \"Bidirectional\"\n     init_args = {}\n-    keras_layer = keras.layers.Bidirectional(\n-        keras.layers.LSTM(32)\n-    )\n+    keras_layer = keras.layers.Bidirectional(keras.layers.LSTM(32))\n     tf_keras_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))\n     benchmark = LayerBenchmark(\n         layer_name,\n@@ -225,9 +223,7 @@ def benchmark_time_distributed(\n ):\n     layer_name = \"TimeDistributed\"\n     init_args = {}\n-    keras_layer = keras.layers.TimeDistributed(\n-        keras.layers.Conv2D(16, (3, 3))\n-    )\n+    keras_layer = keras.layers.TimeDistributed(keras.layers.Conv2D(16, (3, 3)))\n     tf_keras_layer = tf.keras.layers.TimeDistributed(\n         tf.keras.layers.Conv2D(16, (3, 3))\n     )\n\n@@ -1,7 +1,7 @@\n import keras  # isort: skip, keep it on top for torch test\n \n import numpy as np\n-from tensorflow import keras\n+from tensorflow import keras as tf_keras\n \n NUM_CLASSES = 10\n \n@@ -78,27 +78,27 @@ def eval_model(model, x, y):\n def numerical_test():\n     x_train, y_train, x_test, y_test = build_mnist_data(NUM_CLASSES)\n     keras_model = build_keras_model(keras, NUM_CLASSES)\n-    keras_model = build_keras_model(keras, NUM_CLASSES)\n+    tf_keras_model = build_keras_model(tf_keras, NUM_CLASSES)\n \n     # Make sure both model have same weights before training\n     weights = [weight.numpy() for weight in keras_model.weights]\n-    keras_model.set_weights(weights)\n+    tf_keras_model.set_weights(weights)\n \n-    for kw, kcw in zip(keras_model.weights, keras_model.weights):\n+    for kw, kcw in zip(keras_model.weights, tf_keras_model.weights):\n         np.testing.assert_allclose(kw.numpy(), kcw.numpy())\n \n     keras_history = train_model(keras_model, x_train, y_train)\n-    keras_history = train_model(keras_model, x_train, y_train)\n+    tf_keras_history = train_model(tf_keras_model, x_train, y_train)\n \n     for key in keras_history.history.keys():\n         np.testing.assert_allclose(\n             keras_history.history[key],\n-            keras_history.history[key],\n+            tf_keras_history.history[key],\n             atol=1e-3,\n         )\n \n \n if __name__ == \"__main__\":\n     keras.utils.set_random_seed(1337)\n-    keras.utils.set_random_seed(1337)\n+    tf_keras.utils.set_random_seed(1337)\n     numerical_test()\n\n@@ -257,9 +257,7 @@ def softsign(x):\n     return ops.softsign(x)\n \n \n-@keras_export(\n-    [\"keras.activations.silu\", \"keras.activations.swish\"]\n-)\n+@keras_export([\"keras.activations.silu\", \"keras.activations.swish\"])\n def silu(x):\n     \"\"\"Swish (or Silu) activation function.\n \n\n@@ -374,18 +374,14 @@ def inception_resnet_block(x, scale, block_type, block_idx, activation=\"relu\"):\n     return x\n \n \n-@keras_export(\n-    \"keras.applications.inception_resnet_v2.preprocess_input\"\n-)\n+@keras_export(\"keras.applications.inception_resnet_v2.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"tf\"\n     )\n \n \n-@keras_export(\n-    \"keras.applications.inception_resnet_v2.decode_predictions\"\n-)\n+@keras_export(\"keras.applications.inception_resnet_v2.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -17,9 +17,7 @@ WEIGHTS_PATH_NO_TOP = (\n )\n \n \n-@keras_export(\n-    [\"keras.applications.vgg16.VGG16\", \"keras.applications.VGG16\"]\n-)\n+@keras_export([\"keras.applications.vgg16.VGG16\", \"keras.applications.VGG16\"])\n def VGG16(\n     include_top=True,\n     weights=\"imagenet\",\n\n@@ -17,9 +17,7 @@ WEIGHTS_PATH_NO_TOP = (\n )\n \n \n-@keras_export(\n-    [\"keras.applications.vgg19.VGG19\", \"keras.applications.VGG19\"]\n-)\n+@keras_export([\"keras.applications.vgg19.VGG19\", \"keras.applications.VGG19\"])\n def VGG19(\n     include_top=True,\n     weights=\"imagenet\",\n\n@@ -20,9 +20,7 @@ def get_global_attribute(name, default=None, set_to_default=False):\n     return attr\n \n \n-@keras_export(\n-    [\"keras.utils.clear_session\", \"keras.backend.clear_session\"]\n-)\n+@keras_export([\"keras.utils.clear_session\", \"keras.backend.clear_session\"])\n def clear_session():\n     \"\"\"Resets all state generated by Keras.\n \n\n@@ -300,9 +300,7 @@ def any_symbolic_tensors(args=None, kwargs=None):\n     return False\n \n \n-@keras_export(\n-    [\"keras.utils.is_keras_tensor\", \"keras.backend.is_keras_tensor\"]\n-)\n+@keras_export([\"keras.utils.is_keras_tensor\", \"keras.backend.is_keras_tensor\"])\n def is_keras_tensor(x):\n     \"\"\"Returns whether `x` is a Keras tensor.\n \n\n@@ -59,9 +59,7 @@ class StatelessScope:\n             # We're back in eager scope;\n             # if any variables were created within the stateless\n             # scope, we initialize them here.\n-            from keras.backend.common.variables import (\n-                initialize_all_variables,\n-            )\n+            from keras.backend.common.variables import initialize_all_variables\n \n             initialize_all_variables()\n \n\n@@ -34,9 +34,7 @@ def floatx():\n     return _FLOATX\n \n \n-@keras_export(\n-    [\"keras.config.set_floatx\", \"keras.backend.set_floatx\"]\n-)\n+@keras_export([\"keras.config.set_floatx\", \"keras.backend.set_floatx\"])\n def set_floatx(value):\n     \"\"\"Set the default float dtype.\n \n@@ -89,9 +87,7 @@ def epsilon():\n     return _EPSILON\n \n \n-@keras_export(\n-    [\"keras.config.set_epsilon\", \"keras.backend.set_epsilon\"]\n-)\n+@keras_export([\"keras.config.set_epsilon\", \"keras.backend.set_epsilon\"])\n def set_epsilon(value):\n     \"\"\"Set the value of the fuzz factor used in numeric expressions.\n \n\n@@ -77,9 +77,7 @@ class Constraint:\n         return cls(**config)\n \n \n-@keras_export(\n-    [\"keras.constraints.MaxNorm\", \"keras.constraints.max_norm\"]\n-)\n+@keras_export([\"keras.constraints.MaxNorm\", \"keras.constraints.max_norm\"])\n class MaxNorm(Constraint):\n     \"\"\"MaxNorm weight constraint.\n \n@@ -118,9 +116,7 @@ class MaxNorm(Constraint):\n         return {\"max_value\": self.max_value, \"axis\": self.axis}\n \n \n-@keras_export(\n-    [\"keras.constraints.NonNeg\", \"keras.constraints.non_neg\"]\n-)\n+@keras_export([\"keras.constraints.NonNeg\", \"keras.constraints.non_neg\"])\n class NonNeg(Constraint):\n     \"\"\"Constrains the weights to be non-negative.\"\"\"\n \n@@ -129,9 +125,7 @@ class NonNeg(Constraint):\n         return w * ops.cast(ops.greater_equal(w, 0.0), dtype=w.dtype)\n \n \n-@keras_export(\n-    [\"keras.constraints.UnitNorm\", \"keras.constraints.unit_norm\"]\n-)\n+@keras_export([\"keras.constraints.UnitNorm\", \"keras.constraints.unit_norm\"])\n class UnitNorm(Constraint):\n     \"\"\"Constrains the weights incident to each hidden unit to have unit norm.\n \n\n@@ -4,9 +4,7 @@ from keras.backend import standardize_dtype\n from keras.initializers.initializer import Initializer\n \n \n-@keras_export(\n-    [\"keras.initializers.Constant\", \"keras.initializers.constant\"]\n-)\n+@keras_export([\"keras.initializers.Constant\", \"keras.initializers.constant\"])\n class Constant(Initializer):\n     \"\"\"Initializer that generates tensors with constant values.\n \n@@ -41,9 +39,7 @@ class Constant(Initializer):\n         return {\"value\": self.value}\n \n \n-@keras_export(\n-    [\"keras.initializers.Zeros\", \"keras.initializers.zeros\"]\n-)\n+@keras_export([\"keras.initializers.Zeros\", \"keras.initializers.zeros\"])\n class Zeros(Initializer):\n     \"\"\"Initializer that generates tensors initialized to 0.\n \n@@ -72,9 +68,7 @@ class Zeros(Initializer):\n         return ops.zeros(shape, dtype=dtype)\n \n \n-@keras_export(\n-    [\"keras.initializers.Ones\", \"keras.initializers.ones\"]\n-)\n+@keras_export([\"keras.initializers.Ones\", \"keras.initializers.ones\"])\n class Ones(Initializer):\n     \"\"\"Initializer that generates tensors initialized to 1.\n \n\n@@ -1,9 +1,7 @@\n from keras.api_export import keras_export\n \n \n-@keras_export(\n-    [\"keras.Initializer\", \"keras.initializers.Initializer\"]\n-)\n+@keras_export([\"keras.Initializer\", \"keras.initializers.Initializer\"])\n class Initializer:\n     \"\"\"Initializer base class: all Keras initializers inherit from this class.\n \n\n@@ -509,9 +509,7 @@ class LecunUniform(VarianceScaling):\n         }\n \n \n-@keras_export(\n-    [\"keras.initializers.HeNormal\", \"keras.initializers.he_normal\"]\n-)\n+@keras_export([\"keras.initializers.HeNormal\", \"keras.initializers.he_normal\"])\n class HeNormal(VarianceScaling):\n     \"\"\"He normal initializer.\n \n@@ -555,9 +553,7 @@ class HeNormal(VarianceScaling):\n         }\n \n \n-@keras_export(\n-    [\"keras.initializers.HeUniform\", \"keras.initializers.he_uniform\"]\n-)\n+@keras_export([\"keras.initializers.HeUniform\", \"keras.initializers.he_uniform\"])\n class HeUniform(VarianceScaling):\n     \"\"\"He uniform variance scaling initializer.\n \n\n@@ -44,15 +44,9 @@ from keras.layers.merging.multiply import Multiply\n from keras.layers.merging.multiply import multiply\n from keras.layers.merging.subtract import Subtract\n from keras.layers.merging.subtract import subtract\n-from keras.layers.normalization.batch_normalization import (\n-    BatchNormalization,\n-)\n-from keras.layers.normalization.group_normalization import (\n-    GroupNormalization,\n-)\n-from keras.layers.normalization.layer_normalization import (\n-    LayerNormalization,\n-)\n+from keras.layers.normalization.batch_normalization import BatchNormalization\n+from keras.layers.normalization.group_normalization import GroupNormalization\n+from keras.layers.normalization.layer_normalization import LayerNormalization\n from keras.layers.normalization.spectral_normalization import (\n     SpectralNormalization,\n )\n@@ -60,15 +54,9 @@ from keras.layers.normalization.unit_normalization import UnitNormalization\n from keras.layers.pooling.average_pooling1d import AveragePooling1D\n from keras.layers.pooling.average_pooling2d import AveragePooling2D\n from keras.layers.pooling.average_pooling3d import AveragePooling3D\n-from keras.layers.pooling.global_average_pooling1d import (\n-    GlobalAveragePooling1D,\n-)\n-from keras.layers.pooling.global_average_pooling2d import (\n-    GlobalAveragePooling2D,\n-)\n-from keras.layers.pooling.global_average_pooling3d import (\n-    GlobalAveragePooling3D,\n-)\n+from keras.layers.pooling.global_average_pooling1d import GlobalAveragePooling1D\n+from keras.layers.pooling.global_average_pooling2d import GlobalAveragePooling2D\n+from keras.layers.pooling.global_average_pooling3d import GlobalAveragePooling3D\n from keras.layers.pooling.global_max_pooling1d import GlobalMaxPooling1D\n from keras.layers.pooling.global_max_pooling2d import GlobalMaxPooling2D\n from keras.layers.pooling.global_max_pooling3d import GlobalMaxPooling3D\n\n@@ -3,9 +3,7 @@ from keras.api_export import keras_export\n from keras.layers.convolutional.base_conv import BaseConv\n \n \n-@keras_export(\n-    [\"keras.layers.Conv1D\", \"keras.layers.Convolution1D\"]\n-)\n+@keras_export([\"keras.layers.Conv1D\", \"keras.layers.Convolution1D\"])\n class Conv1D(BaseConv):\n     \"\"\"1D convolution layer (e.g. temporal convolution).\n \n\n@@ -1,7 +1,5 @@\n from keras.api_export import keras_export\n-from keras.layers.convolutional.base_conv_transpose import (\n-    BaseConvTranspose,\n-)\n+from keras.layers.convolutional.base_conv_transpose import BaseConvTranspose\n \n \n @keras_export(\n\n@@ -2,9 +2,7 @@ from keras.api_export import keras_export\n from keras.layers.convolutional.base_conv import BaseConv\n \n \n-@keras_export(\n-    [\"keras.layers.Conv2D\", \"keras.layers.Convolution2D\"]\n-)\n+@keras_export([\"keras.layers.Conv2D\", \"keras.layers.Convolution2D\"])\n class Conv2D(BaseConv):\n     \"\"\"2D convolution layer.\n \n\n@@ -1,7 +1,5 @@\n from keras.api_export import keras_export\n-from keras.layers.convolutional.base_conv_transpose import (\n-    BaseConvTranspose,\n-)\n+from keras.layers.convolutional.base_conv_transpose import BaseConvTranspose\n \n \n @keras_export(\n\n@@ -2,9 +2,7 @@ from keras.api_export import keras_export\n from keras.layers.convolutional.base_conv import BaseConv\n \n \n-@keras_export(\n-    [\"keras.layers.Conv3D\", \"keras.layers.Convolution3D\"]\n-)\n+@keras_export([\"keras.layers.Conv3D\", \"keras.layers.Convolution3D\"])\n class Conv3D(BaseConv):\n     \"\"\"3D convolution layer.\n \n\n@@ -1,7 +1,5 @@\n from keras.api_export import keras_export\n-from keras.layers.convolutional.base_conv_transpose import (\n-    BaseConvTranspose,\n-)\n+from keras.layers.convolutional.base_conv_transpose import BaseConvTranspose\n \n \n @keras_export(\n\n@@ -1,7 +1,5 @@\n from keras.api_export import keras_export\n-from keras.layers.convolutional.base_depthwise_conv import (\n-    BaseDepthwiseConv,\n-)\n+from keras.layers.convolutional.base_depthwise_conv import BaseDepthwiseConv\n \n \n @keras_export(\"keras.layers.DepthwiseConv1D\")\n\n@@ -1,7 +1,5 @@\n from keras.api_export import keras_export\n-from keras.layers.convolutional.base_depthwise_conv import (\n-    BaseDepthwiseConv,\n-)\n+from keras.layers.convolutional.base_depthwise_conv import BaseDepthwiseConv\n \n \n @keras_export(\"keras.layers.DepthwiseConv2D\")\n\n@@ -1,7 +1,5 @@\n from keras.api_export import keras_export\n-from keras.layers.convolutional.base_separable_conv import (\n-    BaseSeparableConv,\n-)\n+from keras.layers.convolutional.base_separable_conv import BaseSeparableConv\n \n \n @keras_export(\n\n@@ -1,7 +1,5 @@\n from keras.api_export import keras_export\n-from keras.layers.convolutional.base_separable_conv import (\n-    BaseSeparableConv,\n-)\n+from keras.layers.convolutional.base_separable_conv import BaseSeparableConv\n \n \n @keras_export(\n\n@@ -6,12 +6,8 @@ from keras import layers\n from keras import testing\n from keras.layers.convolutional.conv_test import np_conv1d\n from keras.layers.convolutional.conv_test import np_conv2d\n-from keras.layers.convolutional.depthwise_conv_test import (\n-    np_depthwise_conv1d,\n-)\n-from keras.layers.convolutional.depthwise_conv_test import (\n-    np_depthwise_conv2d,\n-)\n+from keras.layers.convolutional.depthwise_conv_test import np_depthwise_conv1d\n+from keras.layers.convolutional.depthwise_conv_test import np_depthwise_conv2d\n \n \n class SeparableConvBasicTest(testing.TestCase, parameterized.TestCase):\n\n@@ -2,9 +2,7 @@ from keras.api_export import keras_export\n from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_export(\n-    [\"keras.layers.AveragePooling1D\", \"keras.layers.AvgPool1D\"]\n-)\n+@keras_export([\"keras.layers.AveragePooling1D\", \"keras.layers.AvgPool1D\"])\n class AveragePooling1D(BasePooling):\n     \"\"\"Average pooling for temporal data.\n \n\n@@ -2,9 +2,7 @@ from keras.api_export import keras_export\n from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_export(\n-    [\"keras.layers.AveragePooling2D\", \"keras.layers.AvgPool2D\"]\n-)\n+@keras_export([\"keras.layers.AveragePooling2D\", \"keras.layers.AvgPool2D\"])\n class AveragePooling2D(BasePooling):\n     \"\"\"Average pooling operation for 2D spatial data.\n \n\n@@ -2,9 +2,7 @@ from keras.api_export import keras_export\n from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_export(\n-    [\"keras.layers.AveragePooling3D\", \"keras.layers.AvgPool3D\"]\n-)\n+@keras_export([\"keras.layers.AveragePooling3D\", \"keras.layers.AvgPool3D\"])\n class AveragePooling3D(BasePooling):\n     \"\"\"Average pooling operation for 3D data (spatial or spatio-temporal).\n \n\n@@ -2,9 +2,7 @@ from keras.api_export import keras_export\n from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_export(\n-    [\"keras.layers.MaxPooling1D\", \"keras.layers.MaxPool1D\"]\n-)\n+@keras_export([\"keras.layers.MaxPooling1D\", \"keras.layers.MaxPool1D\"])\n class MaxPooling1D(BasePooling):\n     \"\"\"Max pooling operation for 1D temporal data.\n \n\n@@ -2,9 +2,7 @@ from keras.api_export import keras_export\n from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_export(\n-    [\"keras.layers.MaxPooling2D\", \"keras.layers.MaxPool2D\"]\n-)\n+@keras_export([\"keras.layers.MaxPooling2D\", \"keras.layers.MaxPool2D\"])\n class MaxPooling2D(BasePooling):\n     \"\"\"Max pooling operation for 2D spatial data.\n \n\n@@ -2,9 +2,7 @@ from keras.api_export import keras_export\n from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_export(\n-    [\"keras.layers.MaxPooling3D\", \"keras.layers.MaxPool3D\"]\n-)\n+@keras_export([\"keras.layers.MaxPooling3D\", \"keras.layers.MaxPool3D\"])\n class MaxPooling3D(BasePooling):\n     \"\"\"Max pooling operation for 3D data (spatial or spatio-temporal).\n \n\n@@ -1688,9 +1688,7 @@ def apply_channel_shift(x, intensity, channel_axis=0):\n     return x\n \n \n-@keras_export(\n-    \"keras._legacy.preprocessing.image.random_channel_shift\"\n-)\n+@keras_export(\"keras._legacy.preprocessing.image.random_channel_shift\")\n def random_channel_shift(x, intensity_range, channel_axis=0):\n     \"\"\"Performs a random channel shift.\n \n@@ -1708,9 +1706,7 @@ def random_channel_shift(x, intensity_range, channel_axis=0):\n     return apply_channel_shift(x, intensity, channel_axis=channel_axis)\n \n \n-@keras_export(\n-    \"keras._legacy.preprocessing.image.apply_brightness_shift\"\n-)\n+@keras_export(\"keras._legacy.preprocessing.image.apply_brightness_shift\")\n def apply_brightness_shift(x, brightness, scale=True):\n     \"\"\"Performs a brightness shift.\n \n@@ -1778,9 +1774,7 @@ def transform_matrix_offset_center(matrix, x, y):\n     return transform_matrix\n \n \n-@keras_export(\n-    \"keras._legacy.preprocessing.image.apply_affine_transform\"\n-)\n+@keras_export(\"keras._legacy.preprocessing.image.apply_affine_transform\")\n def apply_affine_transform(\n     x,\n     theta=0,\n\n@@ -9,9 +9,7 @@ from keras.api_export import keras_export\n from keras.trainers.data_adapters.py_dataset_adapter import PyDataset\n \n \n-@keras_export(\n-    \"keras._legacy.preprocessing.sequence.TimeseriesGenerator\"\n-)\n+@keras_export(\"keras._legacy.preprocessing.sequence.TimeseriesGenerator\")\n class TimeseriesGenerator(PyDataset):\n     \"\"\"Utility class for generating batches of temporal data.\n \n@@ -178,9 +176,7 @@ class TimeseriesGenerator(PyDataset):\n         return json.dumps(timeseries_generator_config, **kwargs)\n \n \n-@keras_export(\n-    \"keras._legacy.preprocessing.sequence.make_sampling_table\"\n-)\n+@keras_export(\"keras._legacy.preprocessing.sequence.make_sampling_table\")\n def make_sampling_table(size, sampling_factor=1e-5):\n     \"\"\"Generates a word rank-based probabilistic sampling table.\n \n\n@@ -10,9 +10,7 @@ import numpy as np\n from keras.api_export import keras_export\n \n \n-@keras_export(\n-    \"keras._legacy.preprocessing.text.text_to_word_sequence\"\n-)\n+@keras_export(\"keras._legacy.preprocessing.text.text_to_word_sequence\")\n def text_to_word_sequence(\n     input_text,\n     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n\n@@ -31,9 +31,7 @@ from keras.metrics.probabilistic_metrics import BinaryCrossentropy\n from keras.metrics.probabilistic_metrics import CategoricalCrossentropy\n from keras.metrics.probabilistic_metrics import KLDivergence\n from keras.metrics.probabilistic_metrics import Poisson\n-from keras.metrics.probabilistic_metrics import (\n-    SparseCategoricalCrossentropy,\n-)\n+from keras.metrics.probabilistic_metrics import SparseCategoricalCrossentropy\n from keras.metrics.reduction_metrics import Mean\n from keras.metrics.reduction_metrics import MeanMetricWrapper\n from keras.metrics.reduction_metrics import Sum\n\n@@ -17,9 +17,7 @@ from keras.utils import summary_utils\n from keras.utils import traceback_utils\n \n if backend.backend() == \"tensorflow\":\n-    from keras.backend.tensorflow.trainer import (\n-        TensorFlowTrainer as Trainer,\n-    )\n+    from keras.backend.tensorflow.trainer import TensorFlowTrainer as Trainer\n elif backend.backend() == \"jax\":\n     from keras.backend.jax.trainer import JAXTrainer as Trainer\n elif backend.backend() == \"torch\":\n\n@@ -263,9 +263,7 @@ class LeakyRelu(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_export(\n-    [\"keras.ops.leaky_relu\", \"keras.ops.nn.leaky_relu\"]\n-)\n+@keras_export([\"keras.ops.leaky_relu\", \"keras.ops.nn.leaky_relu\"])\n def leaky_relu(x, negative_slope=0.2):\n     \"\"\"Leaky version of a Rectified Linear Unit activation function.\n \n\n@@ -1440,9 +1440,7 @@ class Conjugate(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_export(\n-    [\"keras.ops.conjugate\", \"keras.ops.numpy.conjugate\"]\n-)\n+@keras_export([\"keras.ops.conjugate\", \"keras.ops.numpy.conjugate\"])\n def conjugate(x):\n     \"\"\"Returns the complex conjugate, element-wise.\n \n@@ -2532,9 +2530,7 @@ class FullLike(Operation):\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n-@keras_export(\n-    [\"keras.ops.full_like\", \"keras.ops.numpy.full_like\"]\n-)\n+@keras_export([\"keras.ops.full_like\", \"keras.ops.numpy.full_like\"])\n def full_like(x, fill_value, dtype=None):\n     \"\"\"Return a full tensor with the same shape and type as the given tensor.\n \n@@ -3137,9 +3133,7 @@ class Logaddexp(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_export(\n-    [\"keras.ops.logaddexp\", \"keras.ops.numpy.logaddexp\"]\n-)\n+@keras_export([\"keras.ops.logaddexp\", \"keras.ops.numpy.logaddexp\"])\n def logaddexp(x1, x2):\n     \"\"\"Logarithm of the sum of exponentiations of the inputs.\n \n@@ -3786,9 +3780,7 @@ class NotEqual(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_export(\n-    [\"keras.ops.not_equal\", \"keras.ops.numpy.not_equal\"]\n-)\n+@keras_export([\"keras.ops.not_equal\", \"keras.ops.numpy.not_equal\"])\n def not_equal(x1, x2):\n     \"\"\"Return `(x1 != x2)` element-wise.\n \n@@ -3814,9 +3806,7 @@ class OnesLike(Operation):\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n-@keras_export(\n-    [\"keras.ops.ones_like\", \"keras.ops.numpy.ones_like\"]\n-)\n+@keras_export([\"keras.ops.ones_like\", \"keras.ops.numpy.ones_like\"])\n def ones_like(x, dtype=None):\n     \"\"\"Return a tensor of ones with the same shape and type of `x`.\n \n@@ -4762,9 +4752,7 @@ class Tensordot(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_export(\n-    [\"keras.ops.tensordot\", \"keras.ops.numpy.tensordot\"]\n-)\n+@keras_export([\"keras.ops.tensordot\", \"keras.ops.numpy.tensordot\"])\n def tensordot(x1, x2, axes=2):\n     \"\"\"Compute the tensor dot product along specified axes.\n \n@@ -5364,9 +5352,7 @@ class Transpose(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype, sparse=x.sparse)\n \n \n-@keras_export(\n-    [\"keras.ops.transpose\", \"keras.ops.numpy.transpose\"]\n-)\n+@keras_export([\"keras.ops.transpose\", \"keras.ops.numpy.transpose\"])\n def transpose(x, axes=None):\n     \"\"\"Returns a tensor with `axes` transposed.\n \n@@ -5577,9 +5563,7 @@ class FloorDivide(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_export(\n-    [\"keras.ops.floor_divide\", \"keras.ops.numpy.floor_divide\"]\n-)\n+@keras_export([\"keras.ops.floor_divide\", \"keras.ops.numpy.floor_divide\"])\n def floor_divide(x1, x2):\n     \"\"\"Returns the largest integer smaller or equal to the division of inputs.\n \n@@ -5606,9 +5590,7 @@ class LogicalXor(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_export(\n-    [\"keras.ops.logical_xor\", \"keras.ops.numpy.logical_xor\"]\n-)\n+@keras_export([\"keras.ops.logical_xor\", \"keras.ops.numpy.logical_xor\"])\n def logical_xor(x1, x2):\n     \"\"\"Compute the truth value of `x1 XOR x2`, element-wise.\n \n\n@@ -2,15 +2,9 @@ from keras.optimizers.schedules.learning_rate_schedule import CosineDecay\n from keras.optimizers.schedules.learning_rate_schedule import (\n     CosineDecayRestarts,\n )\n-from keras.optimizers.schedules.learning_rate_schedule import (\n-    ExponentialDecay,\n-)\n-from keras.optimizers.schedules.learning_rate_schedule import (\n-    InverseTimeDecay,\n-)\n+from keras.optimizers.schedules.learning_rate_schedule import ExponentialDecay\n+from keras.optimizers.schedules.learning_rate_schedule import InverseTimeDecay\n from keras.optimizers.schedules.learning_rate_schedule import (\n     PiecewiseConstantDecay,\n )\n-from keras.optimizers.schedules.learning_rate_schedule import (\n-    PolynomialDecay,\n-)\n+from keras.optimizers.schedules.learning_rate_schedule import PolynomialDecay\n\n@@ -5,9 +5,7 @@ from keras.api_export import keras_export\n from keras.utils.numerical_utils import normalize\n \n \n-@keras_export(\n-    [\"keras.Regularizer\", \"keras.regularizers.Regularizer\"]\n-)\n+@keras_export([\"keras.Regularizer\", \"keras.regularizers.Regularizer\"])\n class Regularizer:\n     \"\"\"Regularizer base class.\n \n@@ -167,9 +165,7 @@ class Regularizer:\n         raise NotImplementedError(f\"{self} does not implement get_config()\")\n \n \n-@keras_export(\n-    [\"keras.regularizers.L1L2\", \"keras.regularizers.l1_l2\"]\n-)\n+@keras_export([\"keras.regularizers.L1L2\", \"keras.regularizers.l1_l2\"])\n class L1L2(Regularizer):\n     \"\"\"A regularizer that applies both L1 and L2 regularization penalties.\n \n\n@@ -15,9 +15,7 @@ except ImportError:\n     h5py = None\n \n \n-@keras_export(\n-    [\"keras.saving.save_model\", \"keras.models.save_model\"]\n-)\n+@keras_export([\"keras.saving.save_model\", \"keras.models.save_model\"])\n def save_model(model, filepath, overwrite=True, **kwargs):\n     \"\"\"Saves a model as a `.keras` file.\n \n@@ -112,9 +110,7 @@ def save_model(model, filepath, overwrite=True, **kwargs):\n         )\n \n \n-@keras_export(\n-    [\"keras.saving.load_model\", \"keras.models.load_model\"]\n-)\n+@keras_export([\"keras.saving.load_model\", \"keras.models.load_model\"])\n def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     \"\"\"Loads a model saved via `model.save()`.\n \n\n@@ -19,9 +19,7 @@ if backend.backend() == \"jax\":\n elif backend.backend() == \"torch\":\n     from keras.backend.torch.trainer import TorchTrainer as Trainer\n elif backend.backend() == \"tensorflow\":\n-    from keras.backend.tensorflow.trainer import (\n-        TensorFlowTrainer as Trainer,\n-    )\n+    from keras.backend.tensorflow.trainer import TensorFlowTrainer as Trainer\n elif backend.backend() == \"numpy\":\n     from keras.backend.numpy.trainer import NumpyTrainer as Trainer\n else:\n\n@@ -21,6 +21,4 @@ from keras.utils.python_utils import removesuffix\n from keras.utils.rng_utils import set_random_seed\n from keras.utils.sequence_utils import pad_sequences\n from keras.utils.text_dataset_utils import text_dataset_from_directory\n-from keras.utils.timeseries_dataset_utils import (\n-    timeseries_dataset_from_array,\n-)\n+from keras.utils.timeseries_dataset_utils import timeseries_dataset_from_array\n\n@@ -159,9 +159,7 @@ def img_to_array(img, data_format=None, dtype=None):\n     return x\n \n \n-@keras_export(\n-    [\"keras.utils.save_img\", \"keras.preprocessing.image.save_img\"]\n-)\n+@keras_export([\"keras.utils.save_img\", \"keras.preprocessing.image.save_img\"])\n def save_img(path, x, data_format=None, file_format=None, scale=True, **kwargs):\n     \"\"\"Saves an image stored as a NumPy array to a path or file object.\n \n@@ -186,9 +184,7 @@ def save_img(path, x, data_format=None, file_format=None, scale=True, **kwargs):\n     img.save(path, format=file_format, **kwargs)\n \n \n-@keras_export(\n-    [\"keras.utils.load_img\", \"keras.preprocessing.image.load_img\"]\n-)\n+@keras_export([\"keras.utils.load_img\", \"keras.preprocessing.image.load_img\"])\n def load_img(\n     path,\n     color_mode=\"rgb\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a953503b566681739e15bafb366de1bab77b69dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +1,2 @@\n # Unique source of truth for the version number.\n-__version__ = \"0.1.7\"\n+__version__ = \"3.0.0\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f7ea7f18fdf5d7be7c3a3dd57f132d1ad30b6e88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 143 | Lines Deleted: 16 | Files Changed: 4 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 14/7 | Churn Δ: 159 | Churn Cumulative: 942 | Contributors (this commit): 17 | Commits (past 90d): 7 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,8 +32,6 @@ from keras.utils.io_utils import print_msg\n if backend() == \"tensorflow\":\n     print_msg(\"Using TensorFlow backend\")\n     from keras.backend.tensorflow import *  # noqa: F403\n-\n-    distribution_lib = None\n elif backend() == \"jax\":\n     print_msg(\"Using JAX backend.\")\n     from keras.backend.jax import *  # noqa: F403\n\n@@ -1,4 +1,5 @@\n from keras.backend.tensorflow import core\n+from keras.backend.tensorflow import distribution_lib\n from keras.backend.tensorflow import image\n from keras.backend.tensorflow import math\n from keras.backend.tensorflow import nn\n\n@@ -0,0 +1,70 @@\n+\"\"\"!!!DO NOT USE!!!\n+\n+Distribution related class for Tensorflow backend.\n+\n+This is just a prototype and we might want to unify it\n+with other backends in the future.\n+\"\"\"\n+import tensorflow as tf\n+from tensorflow.experimental import dtensor\n+\n+\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note that this should return the global devices in a distributed setting.\n+\n+    Args:\n+        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Default to `gpu` or\n+        `tpu` if available when device_type is not provided. Otherwise will\n+        return the `cpu` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    device_type = (\n+        device_type.upper() if device_type else dtensor.preferred_device_type()\n+    )\n+\n+    # DTensor doesn't support getting global devices, even when knowing the\n+    # Mesh. Use TF API instead to get global devices. Coordinator service is\n+    # enabled by default with DTensor, so that list_logical_devices() returns\n+    # a list of global devices. More context can be found in b/254911601.\n+    return tf.config.list_logical_devices(device_type=device_type)\n+\n+\n+def to_dtensor_mesh(device_mesh):\n+    \"\"\"Convert the DeviceMesh to Tensorflow backend specific Mesh.\n+\n+    Args:\n+        device_mesh: DeviceMesh instance to convert.\n+\n+    Returns:\n+        A `tf.dtensor.Mesh` instance.\n+    \"\"\"\n+    mesh_dims = list(zip(device_mesh.axis_names, device_mesh.shape))\n+    return dtensor.create_distributed_mesh(\n+        mesh_dims=mesh_dims, local_devices=device_mesh.devices.flatten()\n+    )\n+\n+\n+def to_dtensor_layout(tensor_layout):\n+    \"\"\"Convert the TensorLayout to Tensorflow backend specific Sharding.\n+\n+    Args:\n+        tensor_layout: TensorLayout instance to convert.\n+\n+    Returns:\n+        A `tf.dtensor.Layout` instance.\n+    \"\"\"\n+    if tensor_layout.device_mesh is None:\n+        raise ValueError(\n+            \"Cannot create sharding when device mesh is not set for \"\n+            \"TensorLayout.\"\n+        )\n+\n+    sharding_specs = [\n+        axis if axis else dtensor.UNSHARDED for axis in tensor_layout.axes\n+    ]\n+    dtensor_mesh = to_dtensor_mesh(tensor_layout.device_mesh)\n+    return dtensor.Layout(sharding_specs=sharding_specs, mesh=dtensor_mesh)\n\\ No newline at end of file\n\n@@ -6,13 +6,16 @@ from unittest import mock\n import jax\n import numpy as np\n import pytest\n+import tensorflow as tf\n+from tensorflow.experimental import dtensor\n+from tensorflow.python.eager import context\n \n-from keras import backend\n-from keras import layers\n-from keras import models\n-from keras import testing\n-from keras.backend import distribution_lib as backend_dlib\n-from keras.distribution import distribution_lib\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import models\n+from keras_core import testing\n+from keras_core.backend import distribution_lib as backend_dlib\n+from keras_core.distribution import distribution_lib\n \n if backend.backend() == \"jax\":\n     # Due to https://github.com/google/jax/issues/17188, we can't\n@@ -28,7 +31,7 @@ if backend.backend() == \"jax\":\n \n class DeviceMeshTest(testing.TestCase):\n     def test_mesh_creation(self):\n-        devices = [\"CPU:{i}\" for i in range(8)]\n+        devices = [f\"CPU:{i}\" for i in range(8)]\n         shape = (4, 2)\n         axis_names = [\"batch\", \"model\"]\n \n@@ -38,7 +41,7 @@ class DeviceMeshTest(testing.TestCase):\n         self.assertEqual(mesh.devices.shape, shape)\n \n     def test_input_validation(self):\n-        devices = [\"CPU:{i}\" for i in range(4)]\n+        devices = [f\"CPU:{i}\" for i in range(4)]\n         with self.assertRaisesRegex(\n             ValueError, \"Shape and axis_names cannot be empty\"\n         ):\n@@ -103,7 +106,7 @@ class TensorLayoutTest(testing.TestCase):\n class DistributionTest(testing.TestCase):\n     def setUp(self):\n         super().setUp()\n-        devices = [\"CPU:{i}\" for i in range(8)]\n+        devices = [f\"CPU:{i}\" for i in range(8)]\n         shape = (4, 2)\n         axis_names = [\"batch\", \"model\"]\n \n@@ -133,7 +136,7 @@ class DistributionTest(testing.TestCase):\n class DataParallelDistributionTest(testing.TestCase):\n     def setUp(self):\n         super().setUp()\n-        self.devices = [\"CPU:{i}\" for i in range(8)]\n+        self.devices = [f\"CPU:{i}\" for i in range(8)]\n         shape = (8,)\n         axis_names = [\"data\"]\n \n@@ -161,7 +164,7 @@ class DataParallelDistributionTest(testing.TestCase):\n     @mock.patch.object(\n         distribution_lib,\n         \"list_devices\",\n-        return_value=[\"CPU:{i}\" for i in range(8)],\n+        return_value=[f\"CPU:{i}\" for i in range(8)],\n     )\n     def test_create_with_list_devices(self, mock_list_devices):\n         distribution = distribution_lib.DataParallel()\n@@ -196,7 +199,7 @@ class DataParallelDistributionTest(testing.TestCase):\n class ModelParallelDistributionTest(testing.TestCase):\n     def setUp(self):\n         super().setUp()\n-        self.devices = [\"CPU:{i}\" for i in range(8)]\n+        self.devices = [f\"CPU:{i}\" for i in range(8)]\n         shape = (2, 4)\n         axis_names = [\"data\", \"model\"]\n \n@@ -243,7 +246,7 @@ class ModelParallelDistributionTest(testing.TestCase):\n class LayoutMapTest(testing.TestCase):\n     def setUp(self):\n         super().setUp()\n-        self.devices = [\"CPU:{i}\" for i in range(8)]\n+        self.devices = [f\"CPU:{i}\" for i in range(8)]\n         shape = (4, 2)\n         axis_names = [\"data\", \"model\"]\n \n@@ -357,7 +360,7 @@ class JaxDistributionLibTest(testing.TestCase):\n         self.assertEqual(len(distribution_lib.list_devices(\"CPU\")), 8)\n \n     def test_to_jax_mesh(self):\n-        devices = [\"CPU:{i}\" for i in range(8)]\n+        devices = [f\"CPU:{i}\" for i in range(8)]\n         shape = (4, 2)\n         axis_names = [\"batch\", \"model\"]\n \n@@ -492,3 +495,58 @@ class JaxDistributionLibTest(testing.TestCase):\n         with distribution.scope():\n             model.compile(loss=\"mse\")\n             model.fit(inputs, labels)\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() != \"tensorflow\",\n+    reason=\"Backend specific test\",\n+)\n+class TensorflowDistributionLibTest(testing.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        # Config virtual devices for testing.\n+        cpus = tf.config.list_physical_devices(\"CPU\")\n+        context._reset_context()\n+        tf.config.set_logical_device_configuration(\n+            cpus[0], [tf.config.LogicalDeviceConfiguration()] * 8\n+        )\n+        dtensor.dtensor.initialize_accelerator_system(\"CPU\")\n+\n+    def test_list_devices(self):\n+        self.assertEqual(len(distribution_lib.list_devices()), 8)\n+        self.assertEqual(len(distribution_lib.list_devices(\"cpu\")), 8)\n+        self.assertEqual(len(distribution_lib.list_devices(\"CPU\")), 8)\n+\n+    def test_to_dtensor_mesh(self):\n+        devices = [f\"CPU:{i}\" for i in range(8)]\n+        shape = (4, 2)\n+        axis_names = [\"batch\", \"model\"]\n+\n+        mesh = distribution_lib.DeviceMesh(shape, axis_names, devices)\n+        dtensor_mesh = backend_dlib.to_dtensor_mesh(mesh)\n+\n+        self.assertIsInstance(dtensor_mesh, dtensor.Mesh)\n+        self.assertEqual(dtensor_mesh.shape(), list(shape))\n+        self.assertEqual(dtensor_mesh.dim_names, axis_names)\n+\n+    def test_to_dtensor_layout(self):\n+        axes = [\"data\", None]\n+        mesh = distribution_lib.DeviceMesh(\n+            (4, 2), [\"data\", \"model\"], [f\"CPU:{i}\" for i in range(8)]\n+        )\n+        layout = distribution_lib.TensorLayout(axes, mesh)\n+        dtensor_layout = backend_dlib.to_dtensor_layout(layout)\n+        dtensor_mesh = backend_dlib.to_dtensor_mesh(mesh)\n+        self.assertEqual(\n+            dtensor_layout,\n+            dtensor.Layout([\"data\", dtensor.UNSHARDED], dtensor_mesh),\n+        )\n+\n+    def test_validation_for_device_mesh(self):\n+        axes = [\"data\", None]\n+        layout = distribution_lib.TensorLayout(axes, device_mesh=None)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Cannot create sharding when device mesh is not set\"\n+        ):\n+            backend_dlib.to_dtensor_layout(layout)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a006803fd61eda152327ef9da0c782fea5fb3ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 117 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,12 +10,12 @@ import tensorflow as tf\n from tensorflow.experimental import dtensor\n from tensorflow.python.eager import context\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.backend import distribution_lib as backend_dlib\n-from keras_core.distribution import distribution_lib\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras.backend import distribution_lib as backend_dlib\n+from keras.distribution import distribution_lib\n \n if backend.backend() == \"jax\":\n     # Due to https://github.com/google/jax/issues/17188, we can't\n@@ -510,7 +510,12 @@ class TensorflowDistributionLibTest(testing.TestCase):\n         tf.config.set_logical_device_configuration(\n             cpus[0], [tf.config.LogicalDeviceConfiguration()] * 8\n         )\n-        dtensor.dtensor.initialize_accelerator_system(\"CPU\")\n+\n+        dtensor.initialize_accelerator_system(\"CPU\")\n+\n+    def tearDown(self) -> None:\n+        super().tearDown()\n+        dtensor.shutdown_accelerator_system()\n \n     def test_list_devices(self):\n         self.assertEqual(len(distribution_lib.list_devices()), 8)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#153066653b8e4658b67e417112fe0ce6bcff4c07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 52 | Files Changed: 1 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 101 | Churn Cumulative: 266 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,9 +6,6 @@ from unittest import mock\n import jax\n import numpy as np\n import pytest\n-import tensorflow as tf\n-from tensorflow.experimental import dtensor\n-from tensorflow.python.eager import context\n \n from keras import backend\n from keras import layers\n@@ -497,61 +494,61 @@ class JaxDistributionLibTest(testing.TestCase):\n             model.fit(inputs, labels)\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\",\n-    reason=\"Backend specific test\",\n-)\n-class TensorflowDistributionLibTest(testing.TestCase):\n-    def setUp(self):\n-        super().setUp()\n-        # Config virtual devices for testing.\n-        cpus = tf.config.list_physical_devices(\"cpu\")\n-        context._reset_context()\n-        tf.config.set_logical_device_configuration(\n-            cpus[0], [tf.config.LogicalDeviceConfiguration()] * 8\n-        )\n+# @pytest.mark.skipif(\n+#     backend.backend() != \"tensorflow\",\n+#     reason=\"Backend specific test\",\n+# )\n+# class TensorflowDistributionLibTest(testing.TestCase):\n+#     def setUp(self):\n+#         super().setUp()\n+#         # Config virtual devices for testing.\n+#         cpus = tf.config.list_physical_devices(\"cpu\")\n+#         context._reset_context()\n+#         tf.config.set_logical_device_configuration(\n+#             cpus[0], [tf.config.LogicalDeviceConfiguration()] * 8\n+#         )\n \n-        dtensor.initialize_accelerator_system(\"cpu\")\n+#         dtensor.initialize_accelerator_system(\"cpu\")\n \n-    def tearDown(self) -> None:\n-        super().tearDown()\n-        dtensor.shutdown_accelerator_system()\n+#     def tearDown(self) -> None:\n+#         super().tearDown()\n+#         dtensor.shutdown_accelerator_system()\n \n-    def test_list_devices(self):\n-        self.assertEqual(len(distribution_lib.list_devices()), 8)\n-        self.assertEqual(len(distribution_lib.list_devices(\"cpu\")), 8)\n-        self.assertEqual(len(distribution_lib.list_devices(\"cpu\")), 8)\n+#     def test_list_devices(self):\n+#         self.assertEqual(len(distribution_lib.list_devices()), 8)\n+#         self.assertEqual(len(distribution_lib.list_devices(\"cpu\")), 8)\n+#         self.assertEqual(len(distribution_lib.list_devices(\"cpu\")), 8)\n \n-    def test_to_dtensor_mesh(self):\n-        devices = [f\"cpu:{i}\" for i in range(8)]\n-        shape = (4, 2)\n-        axis_names = [\"batch\", \"model\"]\n+#     def test_to_dtensor_mesh(self):\n+#         devices = [f\"cpu:{i}\" for i in range(8)]\n+#         shape = (4, 2)\n+#         axis_names = [\"batch\", \"model\"]\n \n-        mesh = distribution_lib.DeviceMesh(shape, axis_names, devices)\n-        dtensor_mesh = backend_dlib._to_dtensor_mesh(mesh)\n+#         mesh = distribution_lib.DeviceMesh(shape, axis_names, devices)\n+#         dtensor_mesh = backend_dlib._to_dtensor_mesh(mesh)\n \n-        self.assertIsInstance(dtensor_mesh, dtensor.Mesh)\n-        self.assertEqual(dtensor_mesh.shape(), list(shape))\n-        self.assertEqual(dtensor_mesh.dim_names, axis_names)\n+#         self.assertIsInstance(dtensor_mesh, dtensor.Mesh)\n+#         self.assertEqual(dtensor_mesh.shape(), list(shape))\n+#         self.assertEqual(dtensor_mesh.dim_names, axis_names)\n \n-    def test_to_dtensor_layout(self):\n-        axes = [\"data\", None]\n-        mesh = distribution_lib.DeviceMesh(\n-            (4, 2), [\"data\", \"model\"], [f\"cpu:{i}\" for i in range(8)]\n-        )\n-        layout = distribution_lib.TensorLayout(axes, mesh)\n-        dtensor_layout = backend_dlib._to_dtensor_layout(layout)\n-        dtensor_mesh = backend_dlib._to_dtensor_mesh(mesh)\n-        self.assertEqual(\n-            dtensor_layout,\n-            dtensor.Layout([\"data\", dtensor.UNSHARDED], dtensor_mesh),\n-        )\n+#     def test_to_dtensor_layout(self):\n+#         axes = [\"data\", None]\n+#         mesh = distribution_lib.DeviceMesh(\n+#             (4, 2), [\"data\", \"model\"], [f\"cpu:{i}\" for i in range(8)]\n+#         )\n+#         layout = distribution_lib.TensorLayout(axes, mesh)\n+#         dtensor_layout = backend_dlib._to_dtensor_layout(layout)\n+#         dtensor_mesh = backend_dlib._to_dtensor_mesh(mesh)\n+#         self.assertEqual(\n+#             dtensor_layout,\n+#             dtensor.Layout([\"data\", dtensor.UNSHARDED], dtensor_mesh),\n+#         )\n \n-    def test_validation_for_device_mesh(self):\n-        axes = [\"data\", None]\n-        layout = distribution_lib.TensorLayout(axes, device_mesh=None)\n+#     def test_validation_for_device_mesh(self):\n+#         axes = [\"data\", None]\n+#         layout = distribution_lib.TensorLayout(axes, device_mesh=None)\n \n-        with self.assertRaisesRegex(\n-            ValueError, \"Cannot create sharding when device mesh is not set\"\n-        ):\n-            backend_dlib._to_dtensor_layout(layout)\n+#         with self.assertRaisesRegex(\n+#             ValueError, \"Cannot create sharding when device mesh is not set\"\n+#         ):\n+#             backend_dlib._to_dtensor_layout(layout)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b06ff278c9749e32eb05aabe2643b97841c284f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 6 | Methods Changed: 10 | Complexity Δ (Sum/Max): 11/4 | Churn Δ: 91 | Churn Cumulative: 213 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -248,3 +248,7 @@ def istft(\n \n def rsqrt(x):\n     return jax.lax.rsqrt(x)\n+\n+\n+def erf(x):\n+    return jax.lax.erf(x)\n\n@@ -302,3 +302,7 @@ def istft(\n \n def rsqrt(x):\n     return 1.0 / np.sqrt(x)\n+\n+\n+def erf(x):\n+    return np.array(scipy.special.erf(x))\n\n@@ -239,3 +239,7 @@ def istft(\n \n def rsqrt(x):\n     return tf.math.rsqrt(x)\n+\n+\n+def erf(x):\n+    return tf.math.erf(x)\n\n@@ -408,3 +408,8 @@ def istft(\n def rsqrt(x):\n     x = convert_to_tensor(x)\n     return torch.rsqrt(x)\n+\n+\n+def erf(x):\n+    x = convert_to_tensor(x)\n+    return torch.erf(x)\n\n@@ -929,3 +929,39 @@ def rsqrt(x):\n         return Rsqrt().symbolic_call(x)\n     x = backend.convert_to_tensor(x)\n     return backend.math.rsqrt(x)\n+\n+\n+class Erf(Operation):\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape, dtype=x.dtype)\n+\n+    def call(self, x):\n+        return backend.math.erf(x)\n+\n+\n+@keras_core_export(\"keras_core.ops.erf\")\n+def erf(x):\n+    \"\"\"Computes the error function of x element-wise.\n+\n+    Args:\n+        x: input tensor\n+\n+    Returns:\n+        A tensor with the same type as `x`.\n+\n+    Examples:\n+\n+    Basic usage\n+    >>> x = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n+    >>> y = Erf()(x)\n+\n+    Using `float32` data type\n+    >>> x_float32 = np.array([-3.0, -2.0], dtype=np.float32)\n+    >>> y_float32 = Erf()(x_float32)\n+\n+    Using large values\n+    >>> x_large = np.array([1e10, -1e10])\n+    >>> y_large = Erf()(x_large)\n+    \"\"\"\n+\n+    return Erf()(x)\n\n@@ -835,3 +835,41 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         x = np.array([[1, 4, 9], [16, 25, 36]], dtype=\"float32\")\n         self.assertAllClose(kmath.rsqrt(x), 1 / np.sqrt(x))\n         self.assertAllClose(kmath.Rsqrt()(x), 1 / np.sqrt(x))\n+\n+    def test_erf_operation_basic(self):\n+        # Sample values for testing\n+        sample_values = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n+\n+        # Expected output using numpy's approximation of the error function\n+        expected_output = (2 / np.sqrt(np.pi)) * np.vectorize(math.erf)(\n+            sample_values\n+        )\n+\n+        # Output from the erf operation in keras_core\n+        output_from_erf_op = kmath.erf(sample_values)\n+\n+        # Assert that the outputs are close\n+        self.assertAllClose(expected_output, output_from_erf_op, atol=1e-4)\n+\n+    def test_erf_operation_dtype(self):\n+        # Test for float32 and float64 data types\n+        for dtype in (\"float32\", \"float64\"):\n+            sample_values = np.array(\n+                [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0], dtype=dtype\n+            )\n+            expected_output = (2 / np.sqrt(np.pi)) * np.vectorize(math.erf)(\n+                sample_values\n+            )\n+            output_from_erf_op = kmath.erf(sample_values)\n+            self.assertAllClose(expected_output, output_from_erf_op, atol=1e-4)\n+\n+    def test_erf_operation_edge_cases(self):\n+        # Test for edge cases\n+        edge_values = np.array([1e5, -1e5, 1e-5, -1e-5], dtype=np.float64)\n+        expected_edge_output = (2 / np.sqrt(np.pi)) * np.vectorize(math.erf)(\n+            edge_values\n+        )\n+        output_from_edge_erf_op = kmath.erf(edge_values)\n+        self.assertAllClose(\n+            expected_edge_output, output_from_edge_erf_op, atol=1e-4\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
