{"custom_id": "keras#8b42fff90e14503d5612106d8d17aebc58a9c061", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 48 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -54,8 +54,8 @@ def test_adagrad():\n \n \n def test_adadelta():\n-    _test_optimizer(Adadelta())\n-    _test_optimizer(Adadelta(decay=1e-3))\n+    _test_optimizer(Adadelta(), target=0.83)\n+    _test_optimizer(Adadelta(decay=1e-3), target=0.83)\n \n \n def test_adam():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56f3c85b87c2fbe75d9d791b7eb924bd455776d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 737 | Contributors (this commit): 16 | Commits (past 90d): 8 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -140,7 +140,7 @@ class BatchNormalization(Layer):\n                 self.updates = [K.moving_average_update(self.running_mean, mean, self.momentum),\n                                 K.moving_average_update(self.running_std, std, self.momentum)]\n \n-                if sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n+                if K.backend() == 'tensorflow' and sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n                     x_normed_running = K.batch_normalization(\n                         x, self.running_mean, self.running_std,\n                         self.beta, self.gamma,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4fb3f1b3f384c3a05306b37ea9a736144ed6394a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 23 | Files Changed: 3 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 99 | Churn Cumulative: 4362 | Contributors (this commit): 37 | Commits (past 90d): 73 | Contributors (cumulative): 54 | DMM Complexity: 0.3\n\nDIFF:\n@@ -1168,15 +1168,24 @@ def rnn(step_function, inputs, initial_states,\n         states = initial_states\n         nb_states = len(states)\n         if nb_states == 0:\n-            raise Exception('No initial states provided.')\n-        elif nb_states == 1:\n+            # use dummy state, otherwise _dynamic_rnn_loop breaks\n+            state = inputs[:, 0, :]\n+            state_size = state.get_shape()[-1]\n+        else:\n+            state_size = int(states[0].get_shape()[-1])\n+            if nb_states == 1:\n                 state = states[0]\n             else:\n                 state = tf.concat(1, states)\n \n-        state_size = int(states[0].get_shape()[-1])\n-\n         if mask is not None:\n+            if len(initial_states) == 0:\n+                raise ValueError('No initial states provided! '\n+                                 'When using masking in an RNN, you should '\n+                                 'provide initial states '\n+                                 '(and your step function should return '\n+                                 'as its first state at time `t` '\n+                                 'the output at time `t-1`).')\n             if go_backwards:\n                 mask = tf.reverse(mask, [True] + [False] * (ndim - 2))\n \n@@ -1213,14 +1222,19 @@ def rnn(step_function, inputs, initial_states,\n                     states = []\n                     for i in range(nb_states):\n                         states.append(state[:, i * state_size: (i + 1) * state_size])\n-                else:\n+                elif nb_states == 1:\n                     states = [state]\n+                else:\n+                    states = []\n                 output, new_states = step_function(input, states + constants)\n \n-                if len(new_states) == 1:\n+                if len(new_states) > 1:\n+                    new_state = tf.concat(1, new_states)\n+                elif len(new_states) == 1:\n                     new_state = new_states[0]\n                 else:\n-                    new_state = tf.concat(1, new_states)\n+                    # return dummy state, otherwise _dynamic_rnn_loop breaks\n+                    new_state = output\n                 return output, new_state\n \n         _step.state_size = state_size * nb_states\n@@ -1238,8 +1252,10 @@ def rnn(step_function, inputs, initial_states,\n             new_states = []\n             for i in range(nb_states):\n                 new_states.append(final_state[:, i * state_size: (i + 1) * state_size])\n-        else:\n+        elif nb_states == 1:\n             new_states = [final_state]\n+        else:\n+            new_states = []\n \n         # all this circus is to recover the last vector in the sequence.\n         begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))\n\n@@ -93,17 +93,6 @@ class TimeDistributed(Wrapper):\n     def build(self, input_shape):\n         assert len(input_shape) >= 3\n         self.input_spec = [InputSpec(shape=input_shape)]\n-        if K._BACKEND == 'tensorflow':\n-            if not input_shape[1]:\n-                raise Exception('When using TensorFlow, you should define '\n-                                'explicitly the number of timesteps of '\n-                                'your sequences.\\n'\n-                                'If your first layer is an Embedding, '\n-                                'make sure to pass it an \"input_length\" '\n-                                'argument. Otherwise, make sure '\n-                                'the first layer has '\n-                                'an \"input_shape\" or \"batch_input_shape\" '\n-                                'argument, including the time axis.')\n         child_input_shape = (input_shape[0],) + input_shape[2:]\n         if not self.layer.built:\n             self.layer.build(child_input_shape)\n@@ -125,8 +114,7 @@ class TimeDistributed(Wrapper):\n                 return output, []\n \n             last_output, outputs, states = K.rnn(step, X,\n-                                                 initial_states=[],\n-                                                 unroll=True)\n+                                                 initial_states=[])\n             y = outputs\n         else:\n             # no batch size specified, therefore the layer will be able\n@@ -144,13 +132,18 @@ class TimeDistributed(Wrapper):\n \n \n class Bidirectional(Wrapper):\n-    ''' Bidirectional wrapper for RNNs\n+    ''' Bidirectional wrapper for RNNs.\n \n     # Arguments:\n         layer: `Recurrent` instance.\n-        merge_mode: Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list.\n+        merge_mode: Mode by which outputs of the\n+            forward and backward RNNs will be combined.\n+            One of {'sum', 'mul', 'concat', 'ave', None}.\n+            If None, the outputs will not be combined,\n+            they will be returned as a list.\n \n     # Examples:\n+\n     ```python\n         model = Sequential()\n         model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))\n\n@@ -430,6 +430,50 @@ class TestBackend(object):\n         assert_allclose(unrolled_masked_th_outputs, masked_th_outputs, atol=1e-04)\n         assert_allclose(unrolled_masked_th_state, masked_th_state, atol=1e-04)\n \n+    def test_rnn_no_states(self):\n+        # implement a simple RNN without states\n+        input_dim = 8\n+        output_dim = 4\n+        timesteps = 5\n+\n+        input_val = np.random.random((32, timesteps, input_dim))\n+        W_i_val = np.random.random((input_dim, output_dim))\n+\n+        def rnn_step_fn(input_dim, output_dim, K):\n+            W_i = K.variable(W_i_val)\n+\n+            def step_function(x, states):\n+                assert len(states) == 0\n+                output = K.dot(x, W_i)\n+                return output, []\n+            return step_function\n+\n+        # test default setup\n+        th_rnn_step_fn = rnn_step_fn(input_dim, output_dim, KTH)\n+        th_inputs = KTH.variable(input_val)\n+        th_initial_states = []\n+        last_output, outputs, new_states = KTH.rnn(th_rnn_step_fn, th_inputs,\n+                                                   th_initial_states,\n+                                                   go_backwards=False,\n+                                                   mask=None)\n+        th_last_output = KTH.eval(last_output)\n+        th_outputs = KTH.eval(outputs)\n+        assert len(new_states) == 0\n+\n+        tf_rnn_step_fn = rnn_step_fn(input_dim, output_dim, KTF)\n+        tf_inputs = KTF.variable(input_val)\n+        tf_initial_states = []\n+        last_output, outputs, new_states = KTF.rnn(tf_rnn_step_fn, tf_inputs,\n+                                                   tf_initial_states,\n+                                                   go_backwards=False,\n+                                                   mask=None)\n+        tf_last_output = KTF.eval(last_output)\n+        tf_outputs = KTF.eval(outputs)\n+        assert len(new_states) == 0\n+\n+        assert_allclose(tf_last_output, th_last_output, atol=1e-04)\n+        assert_allclose(tf_outputs, th_outputs, atol=1e-04)\n+\n     def test_switch(self):\n         val = np.random.random()\n         xth = KTH.variable(val)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a319c72552381a98c7a11b339c623cb999164b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 10 | Churn Cumulative: 603 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,9 +4,13 @@ from . import backend as K\n \n class Regularizer(object):\n     def set_param(self, p):\n+        if hasattr(self, 'p'):\n+            raise Exception('Regularizers cannot be reused')\n         self.p = p\n \n     def set_layer(self, layer):\n+        if hasattr(self, 'layer'):\n+            raise Exception('Regularizers cannot be reused')\n         self.layer = layer\n \n     def __call__(self, loss):\n@@ -29,6 +33,8 @@ class EigenvalueRegularizer(Regularizer):\n         self.uses_learning_phase = True\n \n     def set_param(self, p):\n+        if hasattr(self, 'p'):\n+            raise Exception('Regularizers cannot be reused')\n         self.p = p\n \n     def __call__(self, loss):\n@@ -63,6 +69,8 @@ class WeightRegularizer(Regularizer):\n         self.uses_learning_phase = True\n \n     def set_param(self, p):\n+        if hasattr(self, 'p'):\n+            raise Exception('Regularizers cannot be reused')\n         self.p = p\n \n     def __call__(self, loss):\n@@ -95,6 +103,8 @@ class ActivityRegularizer(Regularizer):\n         self.uses_learning_phase = True\n \n     def set_layer(self, layer):\n+        if hasattr(self, 'layer'):\n+            raise Exception('Regularizers cannot be reused')\n         self.layer = layer\n \n     def __call__(self, loss):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f2aa89f44380044d9d6ed94e9e2f471e68198654", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2114 | Contributors (this commit): 22 | Commits (past 90d): 17 | Contributors (cumulative): 22 | DMM Complexity: 0.5\n\nDIFF:\n@@ -680,6 +680,8 @@ class Model(Container):\n         self.test_function = None\n         self.predict_function = None\n \n+        self._collected_trainable_weights = collect_trainable_weights(self)\n+\n     def _make_train_function(self):\n         if not hasattr(self, 'train_function'):\n             raise Exception('You must compile your model before using it.')\n@@ -689,9 +691,9 @@ class Model(Container):\n             else:\n                 inputs = self.inputs + self.targets + self.sample_weights\n \n-            # get trainable weights\n-            trainable_weights = collect_trainable_weights(self)\n-            training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)\n+            training_updates = self.optimizer.get_updates(self._collected_trainable_weights,\n+                                                          self.constraints,\n+                                                          self.total_loss)\n             updates = self.updates + training_updates\n \n             # returns loss and metrics. Updates weights at each call.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#90d0eb9b88c5ef6f756574f60d314c0aa7916f2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 15 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 31 | Churn Cumulative: 634 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,16 +1,14 @@\n from __future__ import absolute_import\n from . import backend as K\n+from .utils.generic_utils import get_from_module\n \n \n class Regularizer(object):\n+\n     def set_param(self, p):\n-        if hasattr(self, 'p'):\n-            raise Exception('Regularizers cannot be reused')\n         self.p = p\n \n     def set_layer(self, layer):\n-        if hasattr(self, 'layer'):\n-            raise Exception('Regularizers cannot be reused')\n         self.layer = layer\n \n     def __call__(self, loss):\n@@ -34,7 +32,8 @@ class EigenvalueRegularizer(Regularizer):\n \n     def set_param(self, p):\n         if hasattr(self, 'p'):\n-            raise Exception('Regularizers cannot be reused')\n+            raise Exception('Regularizers cannot be reused. '\n+                            'Instantiate one regularizer per layer.')\n         self.p = p\n \n     def __call__(self, loss):\n@@ -56,25 +55,30 @@ class EigenvalueRegularizer(Regularizer):\n         WWd = K.dot(WW, main_eigenvect)\n \n         # the corresponding dominant eigenvalue:\n-        main_eigenval = K.dot(K.transpose(WWd), main_eigenvect) / K.dot(K.transpose(main_eigenvect), main_eigenvect)\n-        regularized_loss = loss + (main_eigenval ** 0.5) * self.k  # multiplied by the given regularization gain\n+        main_eigenval = (K.dot(K.transpose(WWd), main_eigenvect) /\n+                         K.dot(K.transpose(main_eigenvect), main_eigenvect))\n+        # multiplied by the given regularization gain\n+        regularized_loss = loss + (main_eigenval ** 0.5) * self.k\n \n         return K.in_train_phase(regularized_loss[0, 0], loss)\n \n \n class WeightRegularizer(Regularizer):\n+\n     def __init__(self, l1=0., l2=0.):\n         self.l1 = K.cast_to_floatx(l1)\n         self.l2 = K.cast_to_floatx(l2)\n         self.uses_learning_phase = True\n+        self.p = None\n \n     def set_param(self, p):\n-        if hasattr(self, 'p'):\n-            raise Exception('Regularizers cannot be reused')\n+        if self.p is not None:\n+            raise Exception('Regularizers cannot be reused. '\n+                            'Instantiate one regularizer per layer.')\n         self.p = p\n \n     def __call__(self, loss):\n-        if not hasattr(self, 'p'):\n+        if self.p is None:\n             raise Exception('Need to call `set_param` on '\n                             'WeightRegularizer instance '\n                             'before calling the instance. '\n@@ -97,18 +101,20 @@ class WeightRegularizer(Regularizer):\n \n \n class ActivityRegularizer(Regularizer):\n+\n     def __init__(self, l1=0., l2=0.):\n         self.l1 = K.cast_to_floatx(l1)\n         self.l2 = K.cast_to_floatx(l2)\n         self.uses_learning_phase = True\n+        self.layer = None\n \n     def set_layer(self, layer):\n-        if hasattr(self, 'layer'):\n+        if self.layer is not None:\n             raise Exception('Regularizers cannot be reused')\n         self.layer = layer\n \n     def __call__(self, loss):\n-        if not hasattr(self, 'layer'):\n+        if self.layer is None:\n             raise Exception('Need to call `set_layer` on '\n                             'ActivityRegularizer instance '\n                             'before calling the instance.')\n@@ -151,7 +157,6 @@ def activity_l1l2(l1=0.01, l2=0.01):\n     return ActivityRegularizer(l1=l1, l2=l2)\n \n \n-from .utils.generic_utils import get_from_module\n def get(identifier, kwargs=None):\n     return get_from_module(identifier, globals(), 'regularizer',\n                            instantiate=True, kwargs=kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b04ac3117e44e9fcc0222b479a34048171462a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 3180 | Contributors (this commit): 35 | Commits (past 90d): 55 | Contributors (cumulative): 39 | DMM Complexity: 0.6363636363636364\n\nDIFF:\n@@ -1238,7 +1238,12 @@ def rnn(step_function, inputs, initial_states,\n                 return output, new_state\n \n         _step.state_size = state_size * nb_states\n-        _step.output_size = int(_step(tf.unpack(inputs)[0], state)[0].get_shape()[-1])\n+        # recover output size by calling _step on the first input\n+        slice_begin = tf.pack([0] * ndim)\n+        slice_size = tf.pack([1] + [-1] * (ndim - 1))\n+        first_input = tf.slice(inputs, slice_begin, slice_size)\n+        first_input = tf.squeeze(first_input, [0])\n+        _step.output_size = int(_step(first_input, state)[0].get_shape()[-1])\n \n         (outputs, final_state) = _dynamic_rnn_loop(\n             _step,\n@@ -1258,9 +1263,9 @@ def rnn(step_function, inputs, initial_states,\n             new_states = []\n \n         # all this circus is to recover the last vector in the sequence.\n-        begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))\n-        size = tf.pack([1] + [-1] * (ndim - 1))\n-        last_output = tf.slice(outputs, begin, size)\n+        slice_begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))\n+        slice_size = tf.pack([1] + [-1] * (ndim - 1))\n+        last_output = tf.slice(outputs, slice_begin, slice_size)\n         last_output = tf.squeeze(last_output, [0])\n \n     axes = [1, 0] + list(range(2, len(outputs.get_shape())))\n\n@@ -26,6 +26,15 @@ def _runner(layer_class):\n                        'return_sequences': True},\n                input_shape=(nb_samples, timesteps, embedding_dim))\n \n+    # check dynamic behavior\n+    layer = layer_class(output_dim, input_dim=embedding_dim)\n+    model = Sequential()\n+    model.add(layer)\n+    model.compile('sgd', 'mse')\n+    x = np.random.random((nb_samples, timesteps, embedding_dim))\n+    y = np.random.random((nb_samples, output_dim))\n+    model.train_on_batch(x, y)\n+\n     # check dropout\n     layer_test(layer_class,\n                kwargs={'output_dim': output_dim,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2e3780e8cf864508d29df554d1cd26aa44824cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 234 | Contributors (this commit): 9 | Commits (past 90d): 11 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -15,4 +15,4 @@ from . import objectives\n from . import optimizers\r\n from . import regularizers\r\n \r\n-__version__ = '1.0.8'\r\n+__version__ = '1.1.0'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='1.0.8',\n+      version='1.1.0',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/1.0.8',\n+      download_url='https://github.com/fchollet/keras/tarball/1.1.0',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#54fc6465377da71da506827df086374b3f95cc42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 22 | Files Changed: 1 | Hunks: 9 | Methods Changed: 12 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 60 | Churn Cumulative: 404 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import pytest\n+import sys\n import numpy as np\n from numpy.testing import assert_allclose\n \n@@ -15,18 +16,40 @@ nb_samples, timesteps, embedding_dim, output_dim = 2, 5, 4, 3\n embedding_num = 12\n \n \n-def _runner(layer_class):\n+def rnn_test(f):\n     \"\"\"\n     All the recurrent layers share the same interface,\n     so we can run through them with a single function.\n     \"\"\"\n-    # check return_sequences\n+    kf = keras_test(f)\n+\n+    def wrapped(layer_class):\n+        return kf(layer_class)\n+\n+    # functools doesnt propagate arguments info for pytest correctly in 2.7\n+    # and wrapped doesnt work with pytest in 3.4\n+    if sys.version_info >= (3, 0):\n+        f = kf\n+    else:\n+        f = wrapped\n+\n+    return pytest.mark.parametrize(\"layer_class\", [\n+        recurrent.SimpleRNN,\n+        recurrent.GRU,\n+        recurrent.LSTM\n+    ])(f)\n+\n+\n+@rnn_test\n+def test_return_sequences(layer_class):\n     layer_test(layer_class,\n                kwargs={'output_dim': output_dim,\n                        'return_sequences': True},\n                input_shape=(nb_samples, timesteps, embedding_dim))\n \n-    # check dynamic behavior\n+\n+@rnn_test\n+def test_dynamic_behavior(layer_class):\n     layer = layer_class(output_dim, input_dim=embedding_dim)\n     model = Sequential()\n     model.add(layer)\n@@ -35,21 +58,27 @@ def _runner(layer_class):\n     y = np.random.random((nb_samples, output_dim))\n     model.train_on_batch(x, y)\n \n-    # check dropout\n+\n+@rnn_test\n+def test_dropout(layer_class):\n     layer_test(layer_class,\n                kwargs={'output_dim': output_dim,\n                        'dropout_U': 0.1,\n                        'dropout_W': 0.1},\n                input_shape=(nb_samples, timesteps, embedding_dim))\n \n-    # check implementation modes\n+\n+@rnn_test\n+def test_implementation_mode(layer_class):\n     for mode in ['cpu', 'mem', 'gpu']:\n         layer_test(layer_class,\n                    kwargs={'output_dim': output_dim,\n                            'consume_less': mode},\n                    input_shape=(nb_samples, timesteps, embedding_dim))\n \n-    # check statefulness\n+\n+@rnn_test\n+def test_statefulness(layer_class):\n     model = Sequential()\n     model.add(embeddings.Embedding(embedding_num, embedding_dim,\n                                    mask_zero=True,\n@@ -103,7 +132,9 @@ def _runner(layer_class):\n \n     assert_allclose(out7, out6, atol=1e-5)\n \n-    # check regularizers\n+\n+@rnn_test\n+def test_regularizer(layer_class):\n     layer = layer_class(output_dim, return_sequences=False, weights=None,\n                         batch_input_shape=(nb_samples, timesteps, embedding_dim),\n                         W_regularizer=regularizers.WeightRegularizer(l1=0.01),\n@@ -115,21 +146,6 @@ def _runner(layer_class):\n     K.eval(layer.output)\n \n \n-@keras_test\n-def test_SimpleRNN():\n-    _runner(recurrent.SimpleRNN)\n-\n-\n-@keras_test\n-def test_GRU():\n-    _runner(recurrent.GRU)\n-\n-\n-@keras_test\n-def test_LSTM():\n-    _runner(recurrent.LSTM)\n-\n-\n @keras_test\n def test_masking_layer():\n     ''' This test based on a previously failing issue here:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06f18fa1b9aaf9796445aa14a809a59281186a19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 167 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,8 +83,8 @@ def matthews_correlation(y_true, y_pred):\n     tp = K.sum(y_pos * y_pred_pos)\n     tn = K.sum(y_neg * y_pred_neg)\n \n-    fp = K.sum(1 - y_neg * y_pred_pos)\n-    fn = K.sum(1 - y_pos * y_pred_neg)\n+    fp = K.sum(y_neg * y_pred_pos)\n+    fn = K.sum(y_pos * y_pred_neg)\n \n     numerator = (tp * tn - fp * fn)\n     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n@@ -34,6 +34,18 @@ def test_metrics():\n         assert K.eval(output).shape == ()\n \n \n+def test_matthews_correlation():\n+    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n+    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n+\n+    # Calculated using sklearn.metrics.matthews_corrcoef\n+    actual = -0.14907119849998601\n+\n+    calc = K.eval(metrics.matthews_correlation(y_true, y_pred))\n+    epsilon = 1e-05\n+    assert actual - epsilon <= calc <= actual + epsilon\n+\n+\n def test_sparse_metrics():\n     for metric in all_sparse_metrics:\n         y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b8363632ee6787f05dc68864d29807bd60e0e24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2840 | Contributors (this commit): 33 | Commits (past 90d): 53 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,9 @@\n import tensorflow as tf\n from tensorflow.python.training import moving_averages\n try:\n-    import tensorflow.contrib.ctc as ctc\n-except ImportError:\n     from tensorflow.python.ops import ctc_ops as ctc\n+except ImportError:\n+    import tensorflow.contrib.ctc as ctc\n import numpy as np\n import os\n import copy\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#875c5214137401f2e29ed7995940d8cb6613a350", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 75 | Files Changed: 1 | Hunks: 22 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 124 | Churn Cumulative: 419 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,17 +15,16 @@ If running on CPU, prefer the TensorFlow backend (much faster).\n Example results: http://i.imgur.com/FX6ROg9.jpg\n '''\n from __future__ import print_function\n-from scipy.misc import imread, imresize, imsave\n+from keras.preprocessing.image import load_img, img_to_array\n import numpy as np\n+from scipy.misc import imsave\n from scipy.optimize import fmin_l_bfgs_b\n import time\n import argparse\n-import h5py\n-import os\n \n-from keras.models import Sequential\n-from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n+from keras.applications import vgg16\n from keras import backend as K\n+from keras.layers import Input\n \n parser = argparse.ArgumentParser(description='Deep Dreams with Keras.')\n parser.add_argument('base_image_path', metavar='base', type=str,\n@@ -46,14 +45,14 @@ weights_path = 'vgg16_weights.h5'\n \n # some settings we found interesting\n saved_settings = {\n-    'bad_trip': {'features': {'conv4_1': 0.05,\n-                              'conv4_2': 0.01,\n-                              'conv4_3': 0.01},\n+    'bad_trip': {'features': {'block4_conv1': 0.05,\n+                              'block4_conv2': 0.01,\n+                              'block4_conv3': 0.01},\n                  'continuity': 0.1,\n                  'dream_l2': 0.8,\n                  'jitter': 5},\n-    'dreamy': {'features': {'conv5_1': 0.05,\n-                            'conv5_2': 0.02},\n+    'dreamy': {'features': {'block5_conv1': 0.05,\n+                            'block5_conv2': 0.02},\n                'continuity': 0.1,\n                'dream_l2': 0.02,\n                'jitter': 0},\n@@ -63,73 +62,37 @@ settings = saved_settings['dreamy']\n \n # util function to open, resize and format pictures into appropriate tensors\n def preprocess_image(image_path):\n-    img = imresize(imread(image_path), (img_width, img_height))\n-    img = img.transpose((2, 0, 1)).astype('float64')\n+    img = load_img(image_path, target_size=(img_width, img_height))\n+    img = img_to_array(img)\n     img = np.expand_dims(img, axis=0)\n+    img = vgg16.preprocess_input(img)\n     return img\n \n # util function to convert a tensor into a valid image\n def deprocess_image(x):\n+    if K.image_dim_ordering() == 'th':\n+        x = x.reshape((3, img_width, img_height))\n         x = x.transpose((1, 2, 0))\n+    else:\n+        x = x.reshape((img_width, img_height, 3))\n+    x = x[:, :, ::-1]\n+    x[:, :, 0] += 103.939\n+    x[:, :, 1] += 116.779\n+    x[:, :, 2] += 123.68\n     x = np.clip(x, 0, 255).astype('uint8')\n     return x\n \n-# build the VGG16 network\n-model = Sequential()\n-model.add(ZeroPadding2D((1, 1), batch_input_shape=(1, 3, img_width, img_height)))\n-first_layer = model.layers[-1]\n-# this is a placeholder tensor that will contain our generated images\n-dream = first_layer.input\n+if K.image_dim_ordering() == 'th':\n+    img_size = (3, img_width, img_height)\n+else:\n+    img_size = (img_width, img_height, 3)\n+# this will contain our generated image\n+dream = Input(batch_shape=(1,) + img_size)\n \n-model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n-model.add(ZeroPadding2D((1, 1)))\n-model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n-model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n-\n-# load the weights of the VGG16 networks\n-# (trained on ImageNet, won the ILSVRC competition in 2014)\n-# note: when there is a complete match between your model definition\n-# and your weight savefile, you can simply call model.load_weights(filename)\n-assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n-f = h5py.File(weights_path)\n-for k in range(f.attrs['nb_layers']):\n-    if k >= len(model.layers):\n-        # we don't look at the last (fully-connected) layers in the savefile\n-        break\n-    g = f['layer_{}'.format(k)]\n-    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n-    model.layers[k].set_weights(weights)\n-f.close()\n+# build the VGG16 network with our placeholder\n+# the model will be loaded with pre-trained ImageNet weights\n+model = vgg16.VGG16(input_tensor=dream,\n+                    weights='imagenet', include_top=False)\n print('Model loaded.')\n \n # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n@@ -138,8 +101,16 @@ layer_dict = dict([(layer.name, layer) for layer in model.layers])\n # continuity loss util function\n def continuity_loss(x):\n     assert K.ndim(x) == 4\n-    a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])\n-    b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])\n+    if K.image_dim_ordering() == 'th':\n+        a = K.square(x[:, :, :img_width - 1, :img_height - 1] -\n+                     x[:, :, 1:, :img_height - 1])\n+        b = K.square(x[:, :, :img_width - 1, :img_height - 1] -\n+                     x[:, :, :img_width - 1, 1:])\n+    else:\n+        a = K.square(x[:, :img_width - 1, :img_height-1, :] -\n+                     x[:, 1:, :img_height - 1, :])\n+        b = K.square(x[:, :img_width - 1, :img_height-1, :] -\n+                     x[:, :img_width - 1, 1:, :])\n     return K.sum(K.pow(a + b, 1.25))\n \n # define the loss\n@@ -151,12 +122,15 @@ for layer_name in settings['features']:\n     x = layer_dict[layer_name].output\n     shape = layer_dict[layer_name].output_shape\n     # we avoid border artifacts by only involving non-border pixels in the loss\n+    if K.image_dim_ordering() == 'th':\n         loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2] - 2, 2: shape[3] - 2])) / np.prod(shape[1:])\n+    else:\n+        loss -= coeff * K.sum(K.square(x[:, 2: shape[1] - 2, 2: shape[2] - 2, :])) / np.prod(shape[1:])\n \n # add continuity loss (gives image local coherence, can result in an artful blur)\n-loss += settings['continuity'] * continuity_loss(dream) / (3 * img_width * img_height)\n+loss += settings['continuity'] * continuity_loss(dream) / np.prod(img_size)\n # add image L2 norm to loss (prevents pixels from taking very high values, makes image darker)\n-loss += settings['dream_l2'] * K.sum(K.square(dream)) / (3 * img_width * img_height)\n+loss += settings['dream_l2'] * K.sum(K.square(dream)) / np.prod(img_size)\n \n # feel free to further modify the loss as you see fit, to achieve new effects...\n \n@@ -171,7 +145,7 @@ else:\n \n f_outputs = K.function([dream], outputs)\n def eval_loss_and_grads(x):\n-    x = x.reshape((1, 3, img_width, img_height))\n+    x = x.reshape((1,) + img_size)\n     outs = f_outputs([x])\n     loss_value = outs[0]\n     if len(outs[1:]) == 1:\n@@ -215,7 +189,7 @@ for i in range(5):\n     start_time = time.time()\n \n     # add a random jitter to the initial image. This will be reverted at decoding time\n-    random_jitter = (settings['jitter'] * 2) * (np.random.random((3, img_width, img_height)) - 0.5)\n+    random_jitter = (settings['jitter'] * 2) * (np.random.random(img_size) - 0.5)\n     x += random_jitter\n \n     # run L-BFGS for 7 steps\n@@ -223,9 +197,9 @@ for i in range(5):\n                                      fprime=evaluator.grads, maxfun=7)\n     print('Current loss value:', min_val)\n     # decode the dream and save it\n-    x = x.reshape((3, img_width, img_height))\n+    x = x.reshape(img_size)\n     x -= random_jitter\n-    img = deprocess_image(x)\n+    img = deprocess_image(np.copy(x))\n     fname = result_prefix + '_at_iteration_%d.png' % i\n     imsave(fname, img)\n     end_time = time.time()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#82a22b20fcbc4db596713fc9b0766e6bca167417", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 19 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 16136 | Contributors (this commit): 70 | Commits (past 90d): 35 | Contributors (cumulative): 82 | DMM Complexity: None\n\nDIFF:\n@@ -348,7 +348,7 @@ class Convolution2D(Layer):\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n         bias: whether to include a bias\n             (i.e. make the layer affine rather than linear).\n \n@@ -564,7 +564,7 @@ class Deconvolution2D(Convolution2D):\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n         bias: whether to include a bias (i.e. make the layer affine rather than linear).\n \n     # Input shape\n@@ -704,7 +704,7 @@ class AtrousConvolution2D(Convolution2D):\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n         bias: whether to include a bias (i.e. make the layer affine rather than linear).\n \n     # Input shape\n@@ -853,7 +853,7 @@ class SeparableConvolution2D(Layer):\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n         bias: whether to include a bias\n             (i.e. make the layer affine rather than linear).\n \n@@ -1068,7 +1068,7 @@ class Convolution3D(Layer):\n             (the depth) is at index 1, in 'tf' mode is it at index 4.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n         bias: whether to include a bias (i.e. make the layer affine rather than linear).\n \n     # Input shape\n@@ -1271,7 +1271,7 @@ class UpSampling2D(Layer):\n             is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         4D tensor with shape:\n@@ -1334,7 +1334,7 @@ class UpSampling3D(Layer):\n             is at index 1, in 'tf' mode is it at index 4.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         5D tensor with shape:\n@@ -1437,7 +1437,7 @@ class ZeroPadding2D(Layer):\n             is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         4D tensor with shape:\n@@ -1497,7 +1497,7 @@ class ZeroPadding3D(Layer):\n             is at index 1, in 'tf' mode is it at index 4.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         5D tensor with shape:\n@@ -1601,7 +1601,7 @@ class Cropping2D(Layer):\n             is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         4D tensor with shape:\n@@ -1685,7 +1685,7 @@ class Cropping3D(Layer):\n             is at index 1, in 'tf' mode is it at index 4.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         5D tensor with shape:\n\n@@ -111,7 +111,7 @@ class SpatialDropout2D(Dropout):\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         4D tensor with shape:\n@@ -159,7 +159,7 @@ class SpatialDropout3D(Dropout):\n             is at index 1, in 'tf' mode is it at index 4.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         5D tensor with shape:\n\n@@ -186,7 +186,7 @@ class MaxPooling2D(_Pooling2D):\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         4D tensor with shape:\n@@ -228,7 +228,7 @@ class AveragePooling2D(_Pooling2D):\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         4D tensor with shape:\n@@ -333,7 +333,7 @@ class MaxPooling3D(_Pooling3D):\n             (the depth) is at index 1, in 'tf' mode is it at index 4.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         5D tensor with shape:\n@@ -373,7 +373,7 @@ class AveragePooling3D(_Pooling3D):\n             (the depth) is at index 1, in 'tf' mode is it at index 4.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         5D tensor with shape:\n@@ -474,7 +474,7 @@ class GlobalAveragePooling2D(_GlobalPooling2D):\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         4D tensor with shape:\n@@ -502,7 +502,7 @@ class GlobalMaxPooling2D(_GlobalPooling2D):\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"th\".\n+            If you never set it, then it will be \"tf\".\n \n     # Input shape\n         4D tensor with shape:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#99bd066f38ac9603a5c00b2eab57f6d15412ddc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 18 | Churn Cumulative: 481 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -112,9 +112,23 @@ class TimeDistributed(Wrapper):\n             def step(x, states):\n                 output = self.layer.call(x)\n                 return output, []\n-\n+            input_length = input_shape[1]\n+            if K.backend() == 'tensorflow' and len(input_shape) > 3:\n+                if input_length is None:\n+                    raise Exception('When using TensorFlow, you should define '\n+                                    'explicitly the number of timesteps of '\n+                                    'your sequences.\\n'\n+                                    'If your first layer is an Embedding, '\n+                                    'make sure to pass it an \"input_length\" '\n+                                    'argument. Otherwise, make sure '\n+                                    'the first layer has '\n+                                    'an \"input_shape\" or \"batch_input_shape\" '\n+                                    'argument, including the time axis.')\n+                unroll = True\n+            else:\n+                unroll = False\n             last_output, outputs, states = K.rnn(step, X,\n-                                                 initial_states=[])\n+                                                 initial_states=[], input_length=input_length, unroll=unroll)\n             y = outputs\n         else:\n             # no batch size specified, therefore the layer will be able\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#414d5f09781161dcc367226acb71e90d41b01013", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 1231 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -390,6 +390,9 @@ class ImageDataGenerator(object):\n                 how many augmentation passes to do over the data\n             seed: random seed.\n         '''\n+        if seed is not None:\n+            np.random.seed(seed)\n+\n         X = np.copy(X)\n         if augment:\n             aX = np.zeros(tuple([rounds * X.shape[0]] + list(X.shape)[1:]))\n@@ -431,11 +434,11 @@ class Iterator(object):\n         # ensure self.batch_index is 0\n         self.reset()\n         while 1:\n+            if seed is not None:\n+                np.random.seed(seed + self.total_batches_seen)\n             if self.batch_index == 0:\n                 index_array = np.arange(N)\n                 if shuffle:\n-                    if seed is not None:\n-                        np.random.seed(seed + self.total_batches_seen)\n                     index_array = np.random.permutation(N)\n \n             current_index = (self.batch_index * batch_size) % N\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
