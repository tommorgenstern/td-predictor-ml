{"custom_id": "keras#ac938b1a6380921e32767f92d7debaad1a0bbfd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 278 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 278 | Churn Cumulative: 439 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.6388888888888888\n\nDIFF:\n@@ -0,0 +1,272 @@\n+\"\"\"\n+Title: Image segmentation with a U-Net-like architecture\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2019/03/20\n+Last modified: 2020/04/20\n+Description: Image segmentation model trained from scratch on the Oxford Pets dataset.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Download the data\n+\"\"\"\n+\n+\"\"\"shell\n+!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n+!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n+\n+curl -O https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\n+curl -O https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\n+\n+tar -xf images.tar.gz\n+tar -xf annotations.tar.gz\n+\"\"\"\n+\n+\"\"\"\n+## Prepare paths of input images and target segmentation masks\n+\"\"\"\n+\n+import os\n+\n+input_dir = \"images/\"\n+target_dir = \"annotations/trimaps/\"\n+img_size = (160, 160)\n+num_classes = 3\n+batch_size = 32\n+\n+input_img_paths = sorted(\n+    [\n+        os.path.join(input_dir, fname)\n+        for fname in os.listdir(input_dir)\n+        if fname.endswith(\".jpg\")\n+    ]\n+)\n+target_img_paths = sorted(\n+    [\n+        os.path.join(target_dir, fname)\n+        for fname in os.listdir(target_dir)\n+        if fname.endswith(\".png\") and not fname.startswith(\".\")\n+    ]\n+)\n+\n+print(\"Number of samples:\", len(input_img_paths))\n+\n+for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n+    print(input_path, \"|\", target_path)\n+\n+\"\"\"\n+## What does one input image and corresponding segmentation mask look like?\n+\"\"\"\n+\n+from IPython.display import Image, display\n+from keras_core.utils import load_img\n+from PIL import ImageOps\n+\n+# Display input image #7\n+display(Image(filename=input_img_paths[9]))\n+\n+# Display auto-contrast version of corresponding target (per-pixel categories)\n+img = ImageOps.autocontrast(load_img(target_img_paths[9]))\n+display(img)\n+\n+\"\"\"\n+## Prepare dataset to load & vectorize batches of data\n+\"\"\"\n+\n+import keras_core as keras\n+import numpy as np\n+from tensorflow import data as tf_data\n+from tensorflow import image as tf_image\n+from tensorflow import io as tf_io\n+\n+\n+def get_dataset(\n+    batch_size,\n+    img_size,\n+    input_img_paths,\n+    target_img_paths,\n+    max_dataset_len=None,\n+):\n+    \"\"\"Returns a TF Dataset.\"\"\"\n+\n+    def load_img_masks(input_img_path, target_img_path):\n+        input_img = tf_io.read_file(input_img_path)\n+        input_img = tf_io.decode_png(input_img, channels=3)\n+        input_img = tf_image.resize(input_img, img_size)\n+        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n+\n+        target_img = tf_io.read_file(target_img_path)\n+        target_img = tf_io.decode_png(target_img, channels=1)\n+        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n+        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n+\n+        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n+        target_img -= 1\n+        return input_img, target_img\n+\n+    # For faster debugging, limit the size of data\n+    if max_dataset_len:\n+        input_img_paths = input_img_paths[:max_dataset_len]\n+        target_img_paths = target_img_paths[:max_dataset_len]\n+    dataset = tf_data.Dataset.from_tensor_slices(\n+        (input_img_paths, target_img_paths)\n+    )\n+    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n+    return dataset.batch(batch_size)\n+\n+\n+\"\"\"\n+## Prepare U-Net Xception-style model\n+\"\"\"\n+\n+from keras_core import layers\n+\n+\n+def get_model(img_size, num_classes):\n+    inputs = keras.Input(shape=img_size + (3,))\n+\n+    ### [First half of the network: downsampling inputs] ###\n+\n+    # Entry block\n+    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n+    x = layers.BatchNormalization()(x)\n+    x = layers.Activation(\"relu\")(x)\n+\n+    previous_block_activation = x  # Set aside residual\n+\n+    # Blocks 1, 2, 3 are identical apart from the feature depth.\n+    for filters in [64, 128, 256]:\n+        x = layers.Activation(\"relu\")(x)\n+        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n+        x = layers.BatchNormalization()(x)\n+\n+        x = layers.Activation(\"relu\")(x)\n+        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n+        x = layers.BatchNormalization()(x)\n+\n+        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n+\n+        # Project residual\n+        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n+            previous_block_activation\n+        )\n+        x = layers.add([x, residual])  # Add back residual\n+        previous_block_activation = x  # Set aside next residual\n+\n+    ### [Second half of the network: upsampling inputs] ###\n+\n+    for filters in [256, 128, 64, 32]:\n+        x = layers.Activation(\"relu\")(x)\n+        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n+        x = layers.BatchNormalization()(x)\n+\n+        x = layers.Activation(\"relu\")(x)\n+        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n+        x = layers.BatchNormalization()(x)\n+\n+        x = layers.UpSampling2D(2)(x)\n+\n+        # Project residual\n+        residual = layers.UpSampling2D(2)(previous_block_activation)\n+        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n+        x = layers.add([x, residual])  # Add back residual\n+        previous_block_activation = x  # Set aside next residual\n+\n+    # Add a per-pixel classification layer\n+    outputs = layers.Conv2D(\n+        num_classes, 3, activation=\"softmax\", padding=\"same\"\n+    )(x)\n+\n+    # Define the model\n+    model = keras.Model(inputs, outputs)\n+    return model\n+\n+\n+# Build model\n+model = get_model(img_size, num_classes)\n+model.summary()\n+\n+\"\"\"\n+## Set aside a validation split\n+\"\"\"\n+\n+import random\n+\n+# Split our img paths into a training and a validation set\n+val_samples = 1000\n+random.Random(1337).shuffle(input_img_paths)\n+random.Random(1337).shuffle(target_img_paths)\n+train_input_img_paths = input_img_paths[:-val_samples]\n+train_target_img_paths = target_img_paths[:-val_samples]\n+val_input_img_paths = input_img_paths[-val_samples:]\n+val_target_img_paths = target_img_paths[-val_samples:]\n+\n+# Instantiate dataset for each split\n+# Limit input files in `max_dataset_len` for faster epoch training time.\n+# Remove the `max_dataset_len` arg when running with full dataset.\n+train_dataset = get_dataset(\n+    batch_size,\n+    img_size,\n+    train_input_img_paths,\n+    train_target_img_paths,\n+    max_dataset_len=1000,\n+)\n+valid_dataset = get_dataset(\n+    batch_size, img_size, val_input_img_paths, val_target_img_paths\n+)\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+# Configure the model for training.\n+# We use the \"sparse\" version of categorical_crossentropy\n+# because our target data is integers.\n+model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n+\n+callbacks = [\n+    keras.callbacks.ModelCheckpoint(\n+        \"oxford_segmentation.keras\", save_best_only=True\n+    )\n+]\n+\n+# Train the model, doing validation at the end of each epoch.\n+epochs = 15\n+model.fit(\n+    train_dataset,\n+    epochs=epochs,\n+    validation_data=valid_dataset,\n+    callbacks=callbacks,\n+)\n+\n+\"\"\"\n+## Visualize predictions\n+\"\"\"\n+\n+# Generate predictions for all images in the validation set\n+\n+val_dataset = get_dataset(\n+    batch_size, img_size, val_input_img_paths, val_target_img_paths\n+)\n+val_preds = model.predict(val_dataset)\n+\n+\n+def display_mask(i):\n+    \"\"\"Quick utility to display a model's prediction.\"\"\"\n+    mask = np.argmax(val_preds[i], axis=-1)\n+    mask = np.expand_dims(mask, axis=-1)\n+    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n+    display(img)\n+\n+\n+# Display results for validation image #10\n+i = 10\n+\n+# Display input image\n+display(Image(filename=val_input_img_paths[i]))\n+\n+# Display ground-truth target mask\n+img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n+display(img)\n+\n+# Display mask predicted by our model\n+display_mask(i)  # Note that the model only sees inputs at 150x150.\n\n@@ -154,6 +154,12 @@ class UpSampling2D(Layer):\n \n         if data_format == \"channels_first\":\n             x = ops.transpose(x, [0, 2, 3, 1])\n+        # https://github.com/keras-team/keras-core/issues/294\n+        # Use `ops.repeat` for `nearest` interpolation\n+        if interpolation == \"nearest\":\n+            x = ops.repeat(x, height_factor, axis=1)\n+            x = ops.repeat(x, width_factor, axis=2)\n+        else:\n             x = ops.image.resize(x, new_shape, method=interpolation)\n         if data_format == \"channels_first\":\n             x = ops.transpose(x, [0, 3, 1, 2])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2978ae66f0552e6f9fcee946b7cce42939c98d74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 693 | Files Changed: 5 | Hunks: 5 | Methods Changed: 12 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 700 | Churn Cumulative: 2261 | Contributors (this commit): 6 | Commits (past 90d): 46 | Contributors (cumulative): 13 | DMM Complexity: 0.1984732824427481\n\nDIFF:\n@@ -1,236 +0,0 @@\n-\"\"\"\n-Title: Collaborative Filtering for Movie Recommendations\n-Author: [Siddhartha Banerjee](https://twitter.com/sidd2006)\n-Date created: 2020/05/24\n-Last modified: 2020/05/24\n-Description: Recommending movies using a model trained on Movielens dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates\n-[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)\n-using the [Movielens dataset](https://www.kaggle.com/c/movielens-100k)\n-to recommend movies to users.\n-The MovieLens ratings dataset lists the ratings given by a set of users to a set of movies.\n-Our goal is to be able to predict ratings for movies a user has not yet watched.\n-The movies with the highest predicted ratings can then be recommended to the user.\n-\n-The steps in the model are as follows:\n-\n-1. Map user ID to a \"user vector\" via an embedding matrix\n-2. Map movie ID to a \"movie vector\" via an embedding matrix\n-3. Compute the dot product between the user vector and movie vector, to obtain\n-the a match score between the user and the movie (predicted rating).\n-4. Train the embeddings via gradient descent using all known user-movie pairs.\n-\n-**References:**\n-\n-- [Collaborative Filtering](https://dl.acm.org/doi/pdf/10.1145/371920.372071)\n-- [Neural Collaborative Filtering](https://dl.acm.org/doi/pdf/10.1145/3038912.3052569)\n-\"\"\"\n-\n-import pandas as pd\n-from pathlib import Path\n-import matplotlib.pyplot as plt\n-import numpy as np\n-from zipfile import ZipFile\n-\n-import keras_core as keras\n-from keras_core import layers\n-import keras_core.operations as ops\n-\"\"\"\n-## First, load the data and apply preprocessing\n-\"\"\"\n-\n-# Download the actual data from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n-# Use the ratings.csv file\n-movielens_data_file_url = (\n-    \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n-)\n-movielens_zipped_file = keras.utils.get_file(\n-    \"ml-latest-small.zip\", movielens_data_file_url, extract=False\n-)\n-keras_datasets_path = Path(movielens_zipped_file).parents[0]\n-movielens_dir = keras_datasets_path / \"ml-latest-small\"\n-\n-# Only extract the data the first time the script is run.\n-if not movielens_dir.exists():\n-    with ZipFile(movielens_zipped_file, \"r\") as zip:\n-        # Extract files\n-        print(\"Extracting all the files now...\")\n-        zip.extractall(path=keras_datasets_path)\n-        print(\"Done!\")\n-\n-ratings_file = movielens_dir / \"ratings.csv\"\n-df = pd.read_csv(ratings_file)\n-\n-\"\"\"\n-First, need to perform some preprocessing to encode users and movies as integer indices.\n-\"\"\"\n-user_ids = df[\"userId\"].unique().tolist()\n-user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n-userencoded2user = {i: x for i, x in enumerate(user_ids)}\n-movie_ids = df[\"movieId\"].unique().tolist()\n-movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n-movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n-df[\"user\"] = df[\"userId\"].map(user2user_encoded)\n-df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n-\n-num_users = len(user2user_encoded)\n-num_movies = len(movie_encoded2movie)\n-df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n-# min and max ratings will be used to normalize the ratings later\n-min_rating = min(df[\"rating\"])\n-max_rating = max(df[\"rating\"])\n-\n-print(\n-    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n-        num_users, num_movies, min_rating, max_rating\n-    )\n-)\n-\n-\"\"\"\n-## Prepare training and validation data\n-\"\"\"\n-df = df.sample(frac=1, random_state=42)\n-x = df[[\"user\", \"movie\"]].values\n-# Normalize the targets between 0 and 1. Makes it easy to train.\n-y = df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n-# Assuming training on 90% of the data and validating on 10%.\n-train_indices = int(0.9 * df.shape[0])\n-x_train, x_val, y_train, y_val = (\n-    x[:train_indices],\n-    x[train_indices:],\n-    y[:train_indices],\n-    y[train_indices:],\n-)\n-\n-\"\"\"\n-## Create the model\n-\n-We embed both users and movies in to 50-dimensional vectors.\n-\n-The model computes a match score between user and movie embeddings via a dot product,\n-and adds a per-movie and per-user bias. The match score is scaled to the `[0, 1]`\n-interval via a sigmoid (since our ratings are normalized to this range).\n-\"\"\"\n-EMBEDDING_SIZE = 50\n-\n-\n-class RecommenderNet(keras.Model):\n-    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n-        super().__init__(**kwargs)\n-        self.num_users = num_users\n-        self.num_movies = num_movies\n-        self.embedding_size = embedding_size\n-        self.user_embedding = layers.Embedding(\n-            num_users,\n-            embedding_size,\n-            embeddings_initializer=\"he_normal\",\n-            embeddings_regularizer=keras.regularizers.l2(1e-6),\n-        )\n-        self.user_bias = layers.Embedding(num_users, 1)\n-        self.movie_embedding = layers.Embedding(\n-            num_movies,\n-            embedding_size,\n-            embeddings_initializer=\"he_normal\",\n-            embeddings_regularizer=keras.regularizers.l2(1e-6),\n-        )\n-        self.movie_bias = layers.Embedding(num_movies, 1)\n-\n-    def call(self, inputs):\n-        user_vector = self.user_embedding(inputs[:, 0])\n-        user_bias = self.user_bias(inputs[:, 0])\n-        movie_vector = self.movie_embedding(inputs[:, 1])\n-        movie_bias = self.movie_bias(inputs[:, 1])\n-        dot_user_movie = ops.tensordot(user_vector, movie_vector, 2)\n-        # Add all the components (including bias)\n-        x = dot_user_movie + user_bias + movie_bias\n-        # The sigmoid activation forces the rating to between 0 and 1\n-        return ops.nn.sigmoid(x)\n-\n-\n-model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n-model.compile(\n-    loss=keras.losses.BinaryCrossentropy(),\n-    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n-)\n-\n-\"\"\"\n-## Train the model based on the data split\n-\"\"\"\n-history = model.fit(\n-    x=x_train,\n-    y=y_train,\n-    batch_size=64,\n-    epochs=5,\n-    verbose=1,\n-    validation_data=(x_val, y_val),\n-)\n-\n-\"\"\"\n-## Plot training and validation loss\n-\"\"\"\n-plt.plot(history.history[\"loss\"])\n-plt.plot(history.history[\"val_loss\"])\n-plt.title(\"model loss\")\n-plt.ylabel(\"loss\")\n-plt.xlabel(\"epoch\")\n-plt.legend([\"train\", \"test\"], loc=\"upper left\")\n-plt.show()\n-\n-\"\"\"\n-## Show top 10 movie recommendations to a user\n-\"\"\"\n-\n-movie_df = pd.read_csv(movielens_dir / \"movies.csv\")\n-\n-# Let us get a user and see the top recommendations.\n-user_id = df.userId.sample(1).iloc[0]\n-movies_watched_by_user = df[df.userId == user_id]\n-movies_not_watched = movie_df[\n-    ~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)\n-][\"movieId\"]\n-movies_not_watched = list(\n-    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n-)\n-movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n-user_encoder = user2user_encoded.get(user_id)\n-user_movie_array = np.hstack(\n-    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n-)\n-ratings = model.predict(user_movie_array).flatten()\n-top_ratings_indices = ratings.argsort()[-10:][::-1]\n-recommended_movie_ids = [\n-    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices\n-]\n-\n-print(\"Showing recommendations for user: {}\".format(user_id))\n-print(\"====\" * 9)\n-print(\"Movies with high ratings from user\")\n-print(\"----\" * 8)\n-top_movies_user = (\n-    movies_watched_by_user.sort_values(by=\"rating\", ascending=False)\n-    .head(5)\n-    .movieId.values\n-)\n-movie_df_rows = movie_df[movie_df[\"movieId\"].isin(top_movies_user)]\n-for row in movie_df_rows.itertuples():\n-    print(row.title, \":\", row.genres)\n-\n-print(\"----\" * 8)\n-print(\"Top 10 movie recommendations\")\n-print(\"----\" * 8)\n-recommended_movies = movie_df[movie_df[\"movieId\"].isin(recommended_movie_ids)]\n-for row in recommended_movies.itertuples():\n-    print(row.title, \":\", row.genres)\n-\n-\"\"\"\n-**Example available on HuggingFace**\n-\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Collaborative%20Filtering-black.svg)](https://huggingface.co/keras-io/collaborative-filtering-movielens) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Collaborative%20Filtering-black.svg)](https://huggingface.co/spaces/keras-io/collaborative-filtering-movielens) |\n-\"\"\"\n\n@@ -1,176 +0,0 @@\n-\"\"\"\n-Title: Timeseries classification with a Transformer model\n-Author: [Theodoros Ntakouris](https://github.com/ntakouris)\n-Date created: 2021/06/25\n-Last modified: 2021/08/05\n-Description: This notebook demonstrates how to do timeseries classification using a Transformer model.\n-Accelerator: GPU\n-\"\"\"\n-\n-\n-\"\"\"\n-## Introduction\n-\n-This is the Transformer architecture from\n-[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n-applied to timeseries instead of natural language.\n-\n-This example requires TensorFlow 2.4 or higher.\n-\n-## Load the dataset\n-\n-We are going to use the same dataset and preprocessing as the\n-[TimeSeries Classification from Scratch](https://keras.io/examples/timeseries/timeseries_classification_from_scratch)\n-example.\n-\"\"\"\n-\n-import numpy as np\n-\n-\n-def readucr(filename):\n-    data = np.loadtxt(filename, delimiter=\"\\t\")\n-    y = data[:, 0]\n-    x = data[:, 1:]\n-    return x, y.astype(int)\n-\n-\n-root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n-\n-x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n-x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n-\n-x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n-x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n-\n-n_classes = len(np.unique(y_train))\n-\n-idx = np.random.permutation(len(x_train))\n-x_train = x_train[idx]\n-y_train = y_train[idx]\n-\n-y_train[y_train == -1] = 0\n-y_test[y_test == -1] = 0\n-\n-\"\"\"\n-## Build the model\n-\n-Our model processes a tensor of shape `(batch size, sequence length, features)`,\n-where `sequence length` is the number of time steps and `features` is each input\n-timeseries.\n-\n-You can replace your classification RNN layers with this one: the\n-inputs are fully compatible!\n-\"\"\"\n-\n-import keras_core as keras\n-from keras_core import layers\n-\n-\"\"\"\n-We include residual connections, layer normalization, and dropout.\n-The resulting layer can be stacked multiple times.\n-\n-The projection layers are implemented through `keras.layers.Conv1D`.\n-\"\"\"\n-\n-\n-def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n-    # Attention and Normalization\n-    x = layers.MultiHeadAttention(\n-        key_dim=head_size, num_heads=num_heads, dropout=dropout\n-    )(inputs, inputs)\n-    x = layers.Dropout(dropout)(x)\n-    x = layers.LayerNormalization(epsilon=1e-6)(x)\n-    res = x + inputs\n-\n-    # Feed Forward Part\n-    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n-    x = layers.Dropout(dropout)(x)\n-    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n-    x = layers.LayerNormalization(epsilon=1e-6)(x)\n-    return x + res\n-\n-\n-\"\"\"\n-The main part of our model is now complete. We can stack multiple of those\n-`transformer_encoder` blocks and we can also proceed to add the final\n-Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n-layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n-our model down to a vector of features for each data point in the current\n-batch. A common way to achieve this is to use a pooling layer. For\n-this example, a `GlobalAveragePooling1D` layer is sufficient.\n-\"\"\"\n-\n-\n-def build_model(\n-    input_shape,\n-    head_size,\n-    num_heads,\n-    ff_dim,\n-    num_transformer_blocks,\n-    mlp_units,\n-    dropout=0,\n-    mlp_dropout=0,\n-):\n-    inputs = keras.Input(shape=input_shape)\n-    x = inputs\n-    for _ in range(num_transformer_blocks):\n-        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n-\n-    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n-    for dim in mlp_units:\n-        x = layers.Dense(dim, activation=\"relu\")(x)\n-        x = layers.Dropout(mlp_dropout)(x)\n-    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n-    return keras.Model(inputs, outputs)\n-\n-\n-\"\"\"\n-## Train and evaluate\n-\"\"\"\n-\n-input_shape = x_train.shape[1:]\n-\n-model = build_model(\n-    input_shape,\n-    head_size=256,\n-    num_heads=4,\n-    ff_dim=4,\n-    num_transformer_blocks=4,\n-    mlp_units=[128],\n-    mlp_dropout=0.4,\n-    dropout=0.25,\n-)\n-\n-model.compile(\n-    loss=\"sparse_categorical_crossentropy\",\n-    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n-    metrics=[\"sparse_categorical_accuracy\"],\n-)\n-model.summary()\n-\n-callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n-\n-model.fit(\n-    x_train,\n-    y_train,\n-    validation_split=0.2,\n-    epochs=2,\n-    batch_size=64,\n-    callbacks=callbacks,\n-)\n-\n-model.evaluate(x_test, y_test, verbose=1)\n-\n-\"\"\"\n-## Conclusions\n-\n-In about 110-120 epochs (25s each on Colab), the model reaches a training\n-accuracy of ~0.95, validation accuracy of ~84 and a testing\n-accuracy of ~85, without hyperparameter tuning. And that is for a model\n-with less than 100k parameters. Of course, parameter count and accuracy could be\n-improved by a hyperparameter search and a more sophisticated learning rate\n-schedule, or a different optimizer.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/timeseries_transformer_classification)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification).\n-\"\"\"\n\n@@ -1,272 +0,0 @@\n-\"\"\"\n-Title: Image segmentation with a U-Net-like architecture\n-Author: [fchollet](https://twitter.com/fchollet)\n-Date created: 2019/03/20\n-Last modified: 2020/04/20\n-Description: Image segmentation model trained from scratch on the Oxford Pets dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Download the data\n-\"\"\"\n-\n-\"\"\"shell\n-!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n-!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n-\n-curl -O https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\n-curl -O https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\n-\n-tar -xf images.tar.gz\n-tar -xf annotations.tar.gz\n-\"\"\"\n-\n-\"\"\"\n-## Prepare paths of input images and target segmentation masks\n-\"\"\"\n-\n-import os\n-\n-input_dir = \"images/\"\n-target_dir = \"annotations/trimaps/\"\n-img_size = (160, 160)\n-num_classes = 3\n-batch_size = 32\n-\n-input_img_paths = sorted(\n-    [\n-        os.path.join(input_dir, fname)\n-        for fname in os.listdir(input_dir)\n-        if fname.endswith(\".jpg\")\n-    ]\n-)\n-target_img_paths = sorted(\n-    [\n-        os.path.join(target_dir, fname)\n-        for fname in os.listdir(target_dir)\n-        if fname.endswith(\".png\") and not fname.startswith(\".\")\n-    ]\n-)\n-\n-print(\"Number of samples:\", len(input_img_paths))\n-\n-for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n-    print(input_path, \"|\", target_path)\n-\n-\"\"\"\n-## What does one input image and corresponding segmentation mask look like?\n-\"\"\"\n-\n-from IPython.display import Image, display\n-from keras_core.utils import load_img\n-from PIL import ImageOps\n-\n-# Display input image #7\n-display(Image(filename=input_img_paths[9]))\n-\n-# Display auto-contrast version of corresponding target (per-pixel categories)\n-img = ImageOps.autocontrast(load_img(target_img_paths[9]))\n-display(img)\n-\n-\"\"\"\n-## Prepare dataset to load & vectorize batches of data\n-\"\"\"\n-\n-import keras_core as keras\n-import numpy as np\n-from tensorflow import data as tf_data\n-from tensorflow import image as tf_image\n-from tensorflow import io as tf_io\n-\n-\n-def get_dataset(\n-    batch_size,\n-    img_size,\n-    input_img_paths,\n-    target_img_paths,\n-    max_dataset_len=None,\n-):\n-    \"\"\"Returns a TF Dataset.\"\"\"\n-\n-    def load_img_masks(input_img_path, target_img_path):\n-        input_img = tf_io.read_file(input_img_path)\n-        input_img = tf_io.decode_png(input_img, channels=3)\n-        input_img = tf_image.resize(input_img, img_size)\n-        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n-\n-        target_img = tf_io.read_file(target_img_path)\n-        target_img = tf_io.decode_png(target_img, channels=1)\n-        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n-        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n-\n-        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n-        target_img -= 1\n-        return input_img, target_img\n-\n-    # For faster debugging, limit the size of data\n-    if max_dataset_len:\n-        input_img_paths = input_img_paths[:max_dataset_len]\n-        target_img_paths = target_img_paths[:max_dataset_len]\n-    dataset = tf_data.Dataset.from_tensor_slices(\n-        (input_img_paths, target_img_paths)\n-    )\n-    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n-    return dataset.batch(batch_size)\n-\n-\n-\"\"\"\n-## Prepare U-Net Xception-style model\n-\"\"\"\n-\n-from keras_core import layers\n-\n-\n-def get_model(img_size, num_classes):\n-    inputs = keras.Input(shape=img_size + (3,))\n-\n-    ### [First half of the network: downsampling inputs] ###\n-\n-    # Entry block\n-    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n-    x = layers.BatchNormalization()(x)\n-    x = layers.Activation(\"relu\")(x)\n-\n-    previous_block_activation = x  # Set aside residual\n-\n-    # Blocks 1, 2, 3 are identical apart from the feature depth.\n-    for filters in [64, 128, 256]:\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n-\n-        # Project residual\n-        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n-            previous_block_activation\n-        )\n-        x = layers.add([x, residual])  # Add back residual\n-        previous_block_activation = x  # Set aside next residual\n-\n-    ### [Second half of the network: upsampling inputs] ###\n-\n-    for filters in [256, 128, 64, 32]:\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.Activation(\"relu\")(x)\n-        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n-        x = layers.BatchNormalization()(x)\n-\n-        x = layers.UpSampling2D(2)(x)\n-\n-        # Project residual\n-        residual = layers.UpSampling2D(2)(previous_block_activation)\n-        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n-        x = layers.add([x, residual])  # Add back residual\n-        previous_block_activation = x  # Set aside next residual\n-\n-    # Add a per-pixel classification layer\n-    outputs = layers.Conv2D(\n-        num_classes, 3, activation=\"softmax\", padding=\"same\"\n-    )(x)\n-\n-    # Define the model\n-    model = keras.Model(inputs, outputs)\n-    return model\n-\n-\n-# Build model\n-model = get_model(img_size, num_classes)\n-model.summary()\n-\n-\"\"\"\n-## Set aside a validation split\n-\"\"\"\n-\n-import random\n-\n-# Split our img paths into a training and a validation set\n-val_samples = 1000\n-random.Random(1337).shuffle(input_img_paths)\n-random.Random(1337).shuffle(target_img_paths)\n-train_input_img_paths = input_img_paths[:-val_samples]\n-train_target_img_paths = target_img_paths[:-val_samples]\n-val_input_img_paths = input_img_paths[-val_samples:]\n-val_target_img_paths = target_img_paths[-val_samples:]\n-\n-# Instantiate dataset for each split\n-# Limit input files in `max_dataset_len` for faster epoch training time.\n-# Remove the `max_dataset_len` arg when running with full dataset.\n-train_dataset = get_dataset(\n-    batch_size,\n-    img_size,\n-    train_input_img_paths,\n-    train_target_img_paths,\n-    max_dataset_len=1000,\n-)\n-valid_dataset = get_dataset(\n-    batch_size, img_size, val_input_img_paths, val_target_img_paths\n-)\n-\n-\"\"\"\n-## Train the model\n-\"\"\"\n-\n-# Configure the model for training.\n-# We use the \"sparse\" version of categorical_crossentropy\n-# because our target data is integers.\n-model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n-\n-callbacks = [\n-    keras.callbacks.ModelCheckpoint(\n-        \"oxford_segmentation.keras\", save_best_only=True\n-    )\n-]\n-\n-# Train the model, doing validation at the end of each epoch.\n-epochs = 15\n-model.fit(\n-    train_dataset,\n-    epochs=epochs,\n-    validation_data=valid_dataset,\n-    callbacks=callbacks,\n-)\n-\n-\"\"\"\n-## Visualize predictions\n-\"\"\"\n-\n-# Generate predictions for all images in the validation set\n-\n-val_dataset = get_dataset(\n-    batch_size, img_size, val_input_img_paths, val_target_img_paths\n-)\n-val_preds = model.predict(val_dataset)\n-\n-\n-def display_mask(i):\n-    \"\"\"Quick utility to display a model's prediction.\"\"\"\n-    mask = np.argmax(val_preds[i], axis=-1)\n-    mask = np.expand_dims(mask, axis=-1)\n-    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n-    display(img)\n-\n-\n-# Display results for validation image #10\n-i = 10\n-\n-# Display input image\n-display(Image(filename=val_input_img_paths[i]))\n-\n-# Display ground-truth target mask\n-img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n-display(img)\n-\n-# Display mask predicted by our model\n-display_mask(i)  # Note that the model only sees inputs at 150x150.\n\n@@ -154,12 +154,6 @@ class UpSampling2D(Layer):\n \n         if data_format == \"channels_first\":\n             x = ops.transpose(x, [0, 2, 3, 1])\n-        # https://github.com/keras-team/keras-core/issues/294\n-        # Use `ops.repeat` for `nearest` interpolation\n-        if interpolation == \"nearest\":\n-            x = ops.repeat(x, height_factor, axis=1)\n-            x = ops.repeat(x, width_factor, axis=2)\n-        else:\n         x = ops.image.resize(x, new_shape, method=interpolation)\n         if data_format == \"channels_first\":\n             x = ops.transpose(x, [0, 3, 1, 2])\n\n@@ -371,9 +371,13 @@ class Model(Trainer, Layer):\n         model_config = serialization_lib.serialize_keras_object(self)\n         return json.dumps(model_config, **kwargs)\n \n-    @traceback_utils.filter_traceback\n-    def export(self, filepath):\n-        raise NotImplementedError\n+    def export(self, filepath, format=\"tf_saved_model\"):\n+        raise NotImplementedError(\n+            \"The export() method is not yet supported. It will \"\n+            \"be added in the next version. For the time being, you \"\n+            \"can use `tf.saved_model.save(model)` to save a \"\n+            \"TensorFlow SavedModel for your Keras Core model.\"\n+        )\n \n \n @keras_core_export(\"keras_core.models.model_from_json\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35825ec2067ed18ec5e61b1f0d5b2c127901d0d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 22 | Churn Cumulative: 3040 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,6 +17,7 @@\n import collections\n import contextlib\n import copy\n+import platform\n import random\n import threading\n \n@@ -707,6 +708,13 @@ def _astuple(attrs):\n \n def can_jit_compile(warn=False):\n     \"\"\"Returns True if TensorFlow XLA is available for the platform.\"\"\"\n+    if platform.system() == \"Darwin\" and \"arm\" in platform.processor().lower():\n+        if warn:\n+            logging.warning(\n+                \"XLA (`jit_compile`) is not yet supported on Apple M1/M2 ARM \"\n+                \"processors. Falling back to `jit_compile=False`.\"\n+            )\n+        return False\n     if pywrap_tfe.TF_ListPluggablePhysicalDevices():\n         if warn:\n             logging.warning(\n\n@@ -14,6 +14,9 @@\n # ==============================================================================\n \"\"\"Tests for Keras TF utils.\"\"\"\n \n+from unittest.mock import MagicMock\n+from unittest.mock import patch\n+\n import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n@@ -470,5 +473,16 @@ class TestSyncToNumpyOrPythonType(parameterized.TestCase):\n         self.assertEqual(tf_utils.sync_to_numpy_or_python_type(tensor), value)\n \n \n+class TestCanJitCompile(tf.test.TestCase):\n+    def test_darwin_arm_xla(self):\n+        with patch(\"platform.processor\", MagicMock(return_value=\"arm\")):\n+            with patch(\"platform.system\", MagicMock(return_value=\"Darwin\")):\n+                self.assertFalse(tf_utils.can_jit_compile())\n+\n+    def test_linux_xla(self):\n+        with patch(\"platform.system\", MagicMock(return_value=\"Linux\")):\n+            self.assertTrue(tf_utils.can_jit_compile())\n+\n+\n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0dc10252643b9be6a1e0a1ef1a485b15273e40aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 690 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 10 | Complexity Δ (Sum/Max): 15/7 | Churn Δ: 690 | Churn Cumulative: 2231 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 9 | DMM Complexity: 0.8088235294117647\n\nDIFF:\n@@ -0,0 +1,236 @@\n+\"\"\"\n+Title: Collaborative Filtering for Movie Recommendations\n+Author: [Siddhartha Banerjee](https://twitter.com/sidd2006)\n+Date created: 2020/05/24\n+Last modified: 2020/05/24\n+Description: Recommending movies using a model trained on Movielens dataset.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example demonstrates\n+[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)\n+using the [Movielens dataset](https://www.kaggle.com/c/movielens-100k)\n+to recommend movies to users.\n+The MovieLens ratings dataset lists the ratings given by a set of users to a set of movies.\n+Our goal is to be able to predict ratings for movies a user has not yet watched.\n+The movies with the highest predicted ratings can then be recommended to the user.\n+\n+The steps in the model are as follows:\n+\n+1. Map user ID to a \"user vector\" via an embedding matrix\n+2. Map movie ID to a \"movie vector\" via an embedding matrix\n+3. Compute the dot product between the user vector and movie vector, to obtain\n+the a match score between the user and the movie (predicted rating).\n+4. Train the embeddings via gradient descent using all known user-movie pairs.\n+\n+**References:**\n+\n+- [Collaborative Filtering](https://dl.acm.org/doi/pdf/10.1145/371920.372071)\n+- [Neural Collaborative Filtering](https://dl.acm.org/doi/pdf/10.1145/3038912.3052569)\n+\"\"\"\n+\n+import pandas as pd\n+from pathlib import Path\n+import matplotlib.pyplot as plt\n+import numpy as np\n+from zipfile import ZipFile\n+\n+import keras_core as keras\n+from keras_core import layers\n+import keras_core.operations as ops\n+\"\"\"\n+## First, load the data and apply preprocessing\n+\"\"\"\n+\n+# Download the actual data from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n+# Use the ratings.csv file\n+movielens_data_file_url = (\n+    \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n+)\n+movielens_zipped_file = keras.utils.get_file(\n+    \"ml-latest-small.zip\", movielens_data_file_url, extract=False\n+)\n+keras_datasets_path = Path(movielens_zipped_file).parents[0]\n+movielens_dir = keras_datasets_path / \"ml-latest-small\"\n+\n+# Only extract the data the first time the script is run.\n+if not movielens_dir.exists():\n+    with ZipFile(movielens_zipped_file, \"r\") as zip:\n+        # Extract files\n+        print(\"Extracting all the files now...\")\n+        zip.extractall(path=keras_datasets_path)\n+        print(\"Done!\")\n+\n+ratings_file = movielens_dir / \"ratings.csv\"\n+df = pd.read_csv(ratings_file)\n+\n+\"\"\"\n+First, need to perform some preprocessing to encode users and movies as integer indices.\n+\"\"\"\n+user_ids = df[\"userId\"].unique().tolist()\n+user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n+userencoded2user = {i: x for i, x in enumerate(user_ids)}\n+movie_ids = df[\"movieId\"].unique().tolist()\n+movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n+movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n+df[\"user\"] = df[\"userId\"].map(user2user_encoded)\n+df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n+\n+num_users = len(user2user_encoded)\n+num_movies = len(movie_encoded2movie)\n+df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n+# min and max ratings will be used to normalize the ratings later\n+min_rating = min(df[\"rating\"])\n+max_rating = max(df[\"rating\"])\n+\n+print(\n+    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n+        num_users, num_movies, min_rating, max_rating\n+    )\n+)\n+\n+\"\"\"\n+## Prepare training and validation data\n+\"\"\"\n+df = df.sample(frac=1, random_state=42)\n+x = df[[\"user\", \"movie\"]].values\n+# Normalize the targets between 0 and 1. Makes it easy to train.\n+y = df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n+# Assuming training on 90% of the data and validating on 10%.\n+train_indices = int(0.9 * df.shape[0])\n+x_train, x_val, y_train, y_val = (\n+    x[:train_indices],\n+    x[train_indices:],\n+    y[:train_indices],\n+    y[train_indices:],\n+)\n+\n+\"\"\"\n+## Create the model\n+\n+We embed both users and movies in to 50-dimensional vectors.\n+\n+The model computes a match score between user and movie embeddings via a dot product,\n+and adds a per-movie and per-user bias. The match score is scaled to the `[0, 1]`\n+interval via a sigmoid (since our ratings are normalized to this range).\n+\"\"\"\n+EMBEDDING_SIZE = 50\n+\n+\n+class RecommenderNet(keras.Model):\n+    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n+        super().__init__(**kwargs)\n+        self.num_users = num_users\n+        self.num_movies = num_movies\n+        self.embedding_size = embedding_size\n+        self.user_embedding = layers.Embedding(\n+            num_users,\n+            embedding_size,\n+            embeddings_initializer=\"he_normal\",\n+            embeddings_regularizer=keras.regularizers.l2(1e-6),\n+        )\n+        self.user_bias = layers.Embedding(num_users, 1)\n+        self.movie_embedding = layers.Embedding(\n+            num_movies,\n+            embedding_size,\n+            embeddings_initializer=\"he_normal\",\n+            embeddings_regularizer=keras.regularizers.l2(1e-6),\n+        )\n+        self.movie_bias = layers.Embedding(num_movies, 1)\n+\n+    def call(self, inputs):\n+        user_vector = self.user_embedding(inputs[:, 0])\n+        user_bias = self.user_bias(inputs[:, 0])\n+        movie_vector = self.movie_embedding(inputs[:, 1])\n+        movie_bias = self.movie_bias(inputs[:, 1])\n+        dot_user_movie = ops.tensordot(user_vector, movie_vector, 2)\n+        # Add all the components (including bias)\n+        x = dot_user_movie + user_bias + movie_bias\n+        # The sigmoid activation forces the rating to between 0 and 1\n+        return ops.nn.sigmoid(x)\n+\n+\n+model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n+model.compile(\n+    loss=keras.losses.BinaryCrossentropy(),\n+    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n+)\n+\n+\"\"\"\n+## Train the model based on the data split\n+\"\"\"\n+history = model.fit(\n+    x=x_train,\n+    y=y_train,\n+    batch_size=64,\n+    epochs=5,\n+    verbose=1,\n+    validation_data=(x_val, y_val),\n+)\n+\n+\"\"\"\n+## Plot training and validation loss\n+\"\"\"\n+plt.plot(history.history[\"loss\"])\n+plt.plot(history.history[\"val_loss\"])\n+plt.title(\"model loss\")\n+plt.ylabel(\"loss\")\n+plt.xlabel(\"epoch\")\n+plt.legend([\"train\", \"test\"], loc=\"upper left\")\n+plt.show()\n+\n+\"\"\"\n+## Show top 10 movie recommendations to a user\n+\"\"\"\n+\n+movie_df = pd.read_csv(movielens_dir / \"movies.csv\")\n+\n+# Let us get a user and see the top recommendations.\n+user_id = df.userId.sample(1).iloc[0]\n+movies_watched_by_user = df[df.userId == user_id]\n+movies_not_watched = movie_df[\n+    ~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)\n+][\"movieId\"]\n+movies_not_watched = list(\n+    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n+)\n+movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n+user_encoder = user2user_encoded.get(user_id)\n+user_movie_array = np.hstack(\n+    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n+)\n+ratings = model.predict(user_movie_array).flatten()\n+top_ratings_indices = ratings.argsort()[-10:][::-1]\n+recommended_movie_ids = [\n+    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices\n+]\n+\n+print(\"Showing recommendations for user: {}\".format(user_id))\n+print(\"====\" * 9)\n+print(\"Movies with high ratings from user\")\n+print(\"----\" * 8)\n+top_movies_user = (\n+    movies_watched_by_user.sort_values(by=\"rating\", ascending=False)\n+    .head(5)\n+    .movieId.values\n+)\n+movie_df_rows = movie_df[movie_df[\"movieId\"].isin(top_movies_user)]\n+for row in movie_df_rows.itertuples():\n+    print(row.title, \":\", row.genres)\n+\n+print(\"----\" * 8)\n+print(\"Top 10 movie recommendations\")\n+print(\"----\" * 8)\n+recommended_movies = movie_df[movie_df[\"movieId\"].isin(recommended_movie_ids)]\n+for row in recommended_movies.itertuples():\n+    print(row.title, \":\", row.genres)\n+\n+\"\"\"\n+**Example available on HuggingFace**\n+\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Collaborative%20Filtering-black.svg)](https://huggingface.co/keras-io/collaborative-filtering-movielens) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Collaborative%20Filtering-black.svg)](https://huggingface.co/spaces/keras-io/collaborative-filtering-movielens) |\n+\"\"\"\n\n@@ -0,0 +1,176 @@\n+\"\"\"\n+Title: Timeseries classification with a Transformer model\n+Author: [Theodoros Ntakouris](https://github.com/ntakouris)\n+Date created: 2021/06/25\n+Last modified: 2021/08/05\n+Description: This notebook demonstrates how to do timeseries classification using a Transformer model.\n+Accelerator: GPU\n+\"\"\"\n+\n+\n+\"\"\"\n+## Introduction\n+\n+This is the Transformer architecture from\n+[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n+applied to timeseries instead of natural language.\n+\n+This example requires TensorFlow 2.4 or higher.\n+\n+## Load the dataset\n+\n+We are going to use the same dataset and preprocessing as the\n+[TimeSeries Classification from Scratch](https://keras.io/examples/timeseries/timeseries_classification_from_scratch)\n+example.\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def readucr(filename):\n+    data = np.loadtxt(filename, delimiter=\"\\t\")\n+    y = data[:, 0]\n+    x = data[:, 1:]\n+    return x, y.astype(int)\n+\n+\n+root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n+\n+x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n+x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n+\n+x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n+x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n+\n+n_classes = len(np.unique(y_train))\n+\n+idx = np.random.permutation(len(x_train))\n+x_train = x_train[idx]\n+y_train = y_train[idx]\n+\n+y_train[y_train == -1] = 0\n+y_test[y_test == -1] = 0\n+\n+\"\"\"\n+## Build the model\n+\n+Our model processes a tensor of shape `(batch size, sequence length, features)`,\n+where `sequence length` is the number of time steps and `features` is each input\n+timeseries.\n+\n+You can replace your classification RNN layers with this one: the\n+inputs are fully compatible!\n+\"\"\"\n+\n+import keras_core as keras\n+from keras_core import layers\n+\n+\"\"\"\n+We include residual connections, layer normalization, and dropout.\n+The resulting layer can be stacked multiple times.\n+\n+The projection layers are implemented through `keras.layers.Conv1D`.\n+\"\"\"\n+\n+\n+def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n+    # Attention and Normalization\n+    x = layers.MultiHeadAttention(\n+        key_dim=head_size, num_heads=num_heads, dropout=dropout\n+    )(inputs, inputs)\n+    x = layers.Dropout(dropout)(x)\n+    x = layers.LayerNormalization(epsilon=1e-6)(x)\n+    res = x + inputs\n+\n+    # Feed Forward Part\n+    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n+    x = layers.Dropout(dropout)(x)\n+    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n+    x = layers.LayerNormalization(epsilon=1e-6)(x)\n+    return x + res\n+\n+\n+\"\"\"\n+The main part of our model is now complete. We can stack multiple of those\n+`transformer_encoder` blocks and we can also proceed to add the final\n+Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n+layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n+our model down to a vector of features for each data point in the current\n+batch. A common way to achieve this is to use a pooling layer. For\n+this example, a `GlobalAveragePooling1D` layer is sufficient.\n+\"\"\"\n+\n+\n+def build_model(\n+    input_shape,\n+    head_size,\n+    num_heads,\n+    ff_dim,\n+    num_transformer_blocks,\n+    mlp_units,\n+    dropout=0,\n+    mlp_dropout=0,\n+):\n+    inputs = keras.Input(shape=input_shape)\n+    x = inputs\n+    for _ in range(num_transformer_blocks):\n+        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n+\n+    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n+    for dim in mlp_units:\n+        x = layers.Dense(dim, activation=\"relu\")(x)\n+        x = layers.Dropout(mlp_dropout)(x)\n+    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n+    return keras.Model(inputs, outputs)\n+\n+\n+\"\"\"\n+## Train and evaluate\n+\"\"\"\n+\n+input_shape = x_train.shape[1:]\n+\n+model = build_model(\n+    input_shape,\n+    head_size=256,\n+    num_heads=4,\n+    ff_dim=4,\n+    num_transformer_blocks=4,\n+    mlp_units=[128],\n+    mlp_dropout=0.4,\n+    dropout=0.25,\n+)\n+\n+model.compile(\n+    loss=\"sparse_categorical_crossentropy\",\n+    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n+    metrics=[\"sparse_categorical_accuracy\"],\n+)\n+model.summary()\n+\n+callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n+\n+model.fit(\n+    x_train,\n+    y_train,\n+    validation_split=0.2,\n+    epochs=2,\n+    batch_size=64,\n+    callbacks=callbacks,\n+)\n+\n+model.evaluate(x_test, y_test, verbose=1)\n+\n+\"\"\"\n+## Conclusions\n+\n+In about 110-120 epochs (25s each on Colab), the model reaches a training\n+accuracy of ~0.95, validation accuracy of ~84 and a testing\n+accuracy of ~85, without hyperparameter tuning. And that is for a model\n+with less than 100k parameters. Of course, parameter count and accuracy could be\n+improved by a hyperparameter search and a more sophisticated learning rate\n+schedule, or a different optimizer.\n+\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/timeseries_transformer_classification)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification).\n+\"\"\"\n\n@@ -0,0 +1,272 @@\n+\"\"\"\n+Title: Image segmentation with a U-Net-like architecture\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2019/03/20\n+Last modified: 2020/04/20\n+Description: Image segmentation model trained from scratch on the Oxford Pets dataset.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Download the data\n+\"\"\"\n+\n+\"\"\"shell\n+!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n+!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n+\n+curl -O https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\n+curl -O https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\n+\n+tar -xf images.tar.gz\n+tar -xf annotations.tar.gz\n+\"\"\"\n+\n+\"\"\"\n+## Prepare paths of input images and target segmentation masks\n+\"\"\"\n+\n+import os\n+\n+input_dir = \"images/\"\n+target_dir = \"annotations/trimaps/\"\n+img_size = (160, 160)\n+num_classes = 3\n+batch_size = 32\n+\n+input_img_paths = sorted(\n+    [\n+        os.path.join(input_dir, fname)\n+        for fname in os.listdir(input_dir)\n+        if fname.endswith(\".jpg\")\n+    ]\n+)\n+target_img_paths = sorted(\n+    [\n+        os.path.join(target_dir, fname)\n+        for fname in os.listdir(target_dir)\n+        if fname.endswith(\".png\") and not fname.startswith(\".\")\n+    ]\n+)\n+\n+print(\"Number of samples:\", len(input_img_paths))\n+\n+for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n+    print(input_path, \"|\", target_path)\n+\n+\"\"\"\n+## What does one input image and corresponding segmentation mask look like?\n+\"\"\"\n+\n+from IPython.display import Image, display\n+from keras_core.utils import load_img\n+from PIL import ImageOps\n+\n+# Display input image #7\n+display(Image(filename=input_img_paths[9]))\n+\n+# Display auto-contrast version of corresponding target (per-pixel categories)\n+img = ImageOps.autocontrast(load_img(target_img_paths[9]))\n+display(img)\n+\n+\"\"\"\n+## Prepare dataset to load & vectorize batches of data\n+\"\"\"\n+\n+import keras_core as keras\n+import numpy as np\n+from tensorflow import data as tf_data\n+from tensorflow import image as tf_image\n+from tensorflow import io as tf_io\n+\n+\n+def get_dataset(\n+    batch_size,\n+    img_size,\n+    input_img_paths,\n+    target_img_paths,\n+    max_dataset_len=None,\n+):\n+    \"\"\"Returns a TF Dataset.\"\"\"\n+\n+    def load_img_masks(input_img_path, target_img_path):\n+        input_img = tf_io.read_file(input_img_path)\n+        input_img = tf_io.decode_png(input_img, channels=3)\n+        input_img = tf_image.resize(input_img, img_size)\n+        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n+\n+        target_img = tf_io.read_file(target_img_path)\n+        target_img = tf_io.decode_png(target_img, channels=1)\n+        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n+        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n+\n+        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n+        target_img -= 1\n+        return input_img, target_img\n+\n+    # For faster debugging, limit the size of data\n+    if max_dataset_len:\n+        input_img_paths = input_img_paths[:max_dataset_len]\n+        target_img_paths = target_img_paths[:max_dataset_len]\n+    dataset = tf_data.Dataset.from_tensor_slices(\n+        (input_img_paths, target_img_paths)\n+    )\n+    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n+    return dataset.batch(batch_size)\n+\n+\n+\"\"\"\n+## Prepare U-Net Xception-style model\n+\"\"\"\n+\n+from keras_core import layers\n+\n+\n+def get_model(img_size, num_classes):\n+    inputs = keras.Input(shape=img_size + (3,))\n+\n+    ### [First half of the network: downsampling inputs] ###\n+\n+    # Entry block\n+    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n+    x = layers.BatchNormalization()(x)\n+    x = layers.Activation(\"relu\")(x)\n+\n+    previous_block_activation = x  # Set aside residual\n+\n+    # Blocks 1, 2, 3 are identical apart from the feature depth.\n+    for filters in [64, 128, 256]:\n+        x = layers.Activation(\"relu\")(x)\n+        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n+        x = layers.BatchNormalization()(x)\n+\n+        x = layers.Activation(\"relu\")(x)\n+        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n+        x = layers.BatchNormalization()(x)\n+\n+        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n+\n+        # Project residual\n+        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n+            previous_block_activation\n+        )\n+        x = layers.add([x, residual])  # Add back residual\n+        previous_block_activation = x  # Set aside next residual\n+\n+    ### [Second half of the network: upsampling inputs] ###\n+\n+    for filters in [256, 128, 64, 32]:\n+        x = layers.Activation(\"relu\")(x)\n+        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n+        x = layers.BatchNormalization()(x)\n+\n+        x = layers.Activation(\"relu\")(x)\n+        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n+        x = layers.BatchNormalization()(x)\n+\n+        x = layers.UpSampling2D(2)(x)\n+\n+        # Project residual\n+        residual = layers.UpSampling2D(2)(previous_block_activation)\n+        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n+        x = layers.add([x, residual])  # Add back residual\n+        previous_block_activation = x  # Set aside next residual\n+\n+    # Add a per-pixel classification layer\n+    outputs = layers.Conv2D(\n+        num_classes, 3, activation=\"softmax\", padding=\"same\"\n+    )(x)\n+\n+    # Define the model\n+    model = keras.Model(inputs, outputs)\n+    return model\n+\n+\n+# Build model\n+model = get_model(img_size, num_classes)\n+model.summary()\n+\n+\"\"\"\n+## Set aside a validation split\n+\"\"\"\n+\n+import random\n+\n+# Split our img paths into a training and a validation set\n+val_samples = 1000\n+random.Random(1337).shuffle(input_img_paths)\n+random.Random(1337).shuffle(target_img_paths)\n+train_input_img_paths = input_img_paths[:-val_samples]\n+train_target_img_paths = target_img_paths[:-val_samples]\n+val_input_img_paths = input_img_paths[-val_samples:]\n+val_target_img_paths = target_img_paths[-val_samples:]\n+\n+# Instantiate dataset for each split\n+# Limit input files in `max_dataset_len` for faster epoch training time.\n+# Remove the `max_dataset_len` arg when running with full dataset.\n+train_dataset = get_dataset(\n+    batch_size,\n+    img_size,\n+    train_input_img_paths,\n+    train_target_img_paths,\n+    max_dataset_len=1000,\n+)\n+valid_dataset = get_dataset(\n+    batch_size, img_size, val_input_img_paths, val_target_img_paths\n+)\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+# Configure the model for training.\n+# We use the \"sparse\" version of categorical_crossentropy\n+# because our target data is integers.\n+model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n+\n+callbacks = [\n+    keras.callbacks.ModelCheckpoint(\n+        \"oxford_segmentation.keras\", save_best_only=True\n+    )\n+]\n+\n+# Train the model, doing validation at the end of each epoch.\n+epochs = 15\n+model.fit(\n+    train_dataset,\n+    epochs=epochs,\n+    validation_data=valid_dataset,\n+    callbacks=callbacks,\n+)\n+\n+\"\"\"\n+## Visualize predictions\n+\"\"\"\n+\n+# Generate predictions for all images in the validation set\n+\n+val_dataset = get_dataset(\n+    batch_size, img_size, val_input_img_paths, val_target_img_paths\n+)\n+val_preds = model.predict(val_dataset)\n+\n+\n+def display_mask(i):\n+    \"\"\"Quick utility to display a model's prediction.\"\"\"\n+    mask = np.argmax(val_preds[i], axis=-1)\n+    mask = np.expand_dims(mask, axis=-1)\n+    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n+    display(img)\n+\n+\n+# Display results for validation image #10\n+i = 10\n+\n+# Display input image\n+display(Image(filename=val_input_img_paths[i]))\n+\n+# Display ground-truth target mask\n+img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n+display(img)\n+\n+# Display mask predicted by our model\n+display_mask(i)  # Note that the model only sees inputs at 150x150.\n\n@@ -154,6 +154,12 @@ class UpSampling2D(Layer):\n \n         if data_format == \"channels_first\":\n             x = ops.transpose(x, [0, 2, 3, 1])\n+        # https://github.com/keras-team/keras-core/issues/294\n+        # Use `ops.repeat` for `nearest` interpolation\n+        if interpolation == \"nearest\":\n+            x = ops.repeat(x, height_factor, axis=1)\n+            x = ops.repeat(x, width_factor, axis=2)\n+        else:\n             x = ops.image.resize(x, new_shape, method=interpolation)\n         if data_format == \"channels_first\":\n             x = ops.transpose(x, [0, 3, 1, 2])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
