{"custom_id": "scrapy#2dc20bb56e83c3420cdd9715de6a0f9334f1eab8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 224 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,29 +118,34 @@ class Command(ScrapyCommand):\n         guids_missing = guids_before - guids_now\n         guids_both = guids_now & guids_before\n \n-        chcount, chreport = self._report_differences(self.before_db, self.now_db, guids_both)\n+        changed_items, chreport = self._report_differences(self.before_db, self.now_db, guids_both)\n \n-        if chcount == 0 and opts.quiet:\n+        ok_items = len(guids_both) - changed_items\n+        new_items = len(guids_new)\n+        missing_items = len(guids_missing)\n+\n+        if (new_items - missing_items - changed_items) == 0 and opts.quiet:\n             s = \"\"\n         else:\n             s = \"CRAWLING DIFFERENCES REPORT\\n\\n\"\n \n-            s += \"Total items     : %d\\n\" % (len(guids_both) + len(guids_new) + len(guids_missing))\n-            s += \"  Items OK      : %d\\n\" % (len(guids_both) - chcount)\n-            s += \"  New items     : %d\\n\" % len(guids_new)\n-            s += \"  Missing items : %d\\n\" % len(guids_missing)\n-            s += \"  Changed items : %d\\n\" % chcount\n+            s += \"Total items     : %d\\n\" % (len(guids_both) + new_items + missing_items)\n+            s += \"  Items OK      : %d\\n\" % ok_items\n+            s += \"  New items     : %d\\n\" % new_items\n+            s += \"  Missing items : %d\\n\" % missing_items\n+            s += \"  Changed items : %d\\n\" % changed_items\n \n             s += \"\\n\"\n-            s += \"- NEW ITEMS (%d) -----------------------------------------\\n\" % len(guids_new)\n+            s += \"- NEW ITEMS (%d) -----------------------------------------\\n\" % new_items\n             s += self._format_items([self.now_db[g] for g in guids_new])\n \n             s += \"\\n\"\n-            s += \"- MISSING ITEMS (%d) -------------------------------------\\n\" % len(guids_missing)\n+            s += \"- MISSING ITEMS (%d) -------------------------------------\\n\" % missing_items\n             s += self._format_items([self.before_db[g] for g in guids_missing])\n \n             s += \"\\n\"\n-            s += \"- CHANGED ITEMS (%d) -------------------------------------\\n\" % chcount\n+            s += \"- CHANGED ITEMS (%d) -------------------------------------\\n\" % changed_items\n+\n             s += chreport\n \n         return s\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d612701b66b8868fe4613f1279fd35981f7a9666", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 119 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 15 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 119 | Churn Cumulative: 119 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,119 @@\n+from __future__ import with_statement\n+\n+from twisted.internet import defer\n+from twisted.python import failure\n+from pprint import pprint\n+\n+from scrapy.core import log\n+from scrapy.http import Request\n+from scrapy.core.engine import scrapyengine\n+from scrapy.spider import spiders\n+from scrapy.core.exceptions import DropItem, NotConfigured\n+from scrapy.core.exceptions import HttpException\n+from scrapy.stats import stats\n+from scrapy.utils.misc import chain_deferred, mustbe_deferred\n+from scrapy.conf import settings\n+\n+class DomainInfo(object):\n+    def __init__(self, domain):\n+        self.domain = domain\n+        self.spider = spiders.fromdomain(domain)\n+        self.downloading = {}\n+        self.downloaded = {}\n+        self.waiting = {}\n+\n+\n+class MediaPipeline(object):\n+    def __init__(self):\n+        self.cache = {}\n+\n+    def process_item(self, domain, response, item):\n+        urls = self.get_urls_from_item(item)\n+        info = self.cache[domain]\n+\n+        ulist = []\n+        for url in urls:\n+            dfd = self._enqueue(url, info)\n+            dfd.addCallbacks(\n+                    callback=self.new_item_media,\n+                    callbackArgs=(item,),\n+                    errback=self.failed_item_media,\n+                    errbackArgs=(item,),\n+                    )\n+            ulist.append(dfd)\n+\n+        dlist = defer.DeferredList(ulist, consumeErrors=False)\n+        dlist.addBoth(self._item_completed, item)\n+        return dlist\n+\n+    def _enqueue(self, url, info):\n+        wad = defer.Deferred()\n+        wad.addBoth(lambda _: pprint(_))\n+\n+        request = Request(url=url)\n+        fp = request.fingerprint()\n+        waiting = info.waiting.setdefault(fp, []).append(wad)\n+\n+        if fp in info.downloading:\n+            return\n+\n+        dwld = scrapyengine.schedule(request, info.spider, priority=0)\n+        dwld.addCallbacks(self.media_downloaded, self.media_failure)\n+        dwld.addBoth(self._download_finished, info, fp)\n+        info.downloading[fp] = (request, dwld)\n+        return wad\n+\n+    def _download_finished(self, result, info, fp):\n+        del info.downloading[fp]\n+        info.downloaded[fp] = result # cache result\n+        waiting = info.waiting[fp]\n+\n+        isfail = isinstance(result, failure.Failure)\n+        for wad in waiting:\n+            tocall = wad.errback if isfail else wad.callback\n+            pprint(tocall)\n+            tocall(result)\n+        return result\n+\n+    def _item_completed(self, result, item):\n+        return self.item_completed(item) #dummy as hell\n+\n+    def open_domain(self, domain):\n+        self.cache[domain] = DomainInfo(domain)\n+\n+    def close_domain(self, domain):\n+        print '##### closing'\n+        del self.cache[domain]\n+\n+    ### Overradiable Interface\n+\n+    def get_urls_from_item(self, item):\n+        print '##### get_urls_from_item'\n+        return item.image_urls\n+\n+    def media_to_download(self, url):\n+        print '##### media_to_download'\n+        pass\n+\n+    def media_downloaded(self, response):\n+        print '##### media_downloaded'\n+        pass\n+\n+    def media_failure(self, _failure):\n+        print '##### media_failure'\n+        return _failure\n+\n+    def new_item_media(self, result, item):\n+        print '##### new_item_media'\n+        pass\n+\n+    def failed_item_media(self, _failure, item):\n+        print '##### failed_item_media'\n+        pass\n+\n+    def item_completed(self, item):\n+        print '##### item_completed'\n+        return item\n+\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2fd5cf644e61c55fd62edd7dc17ffcef84af4fb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 36 | Churn Cumulative: 1382 | Contributors (this commit): 2 | Commits (past 90d): 49 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,17 +35,19 @@ class Broker(pb.Referenceable):\n         else:\n             deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n             \n-    def status_as_dict(self, verbosity=0):\n+    def status_as_dict(self, verbosity=1):\n+        if verbosity == 0:\n+            return\n         status = {\"alive\": self.alive}\n         if self.alive:\n-            if verbosity == 0:\n+            if verbosity == 1:\n                 #dont show spider settings\n                 status[\"running\"] = []\n                 for proc in self.running:\n                     proccopy = proc.copy()\n                     del proccopy[\"settings\"]\n                     status[\"running\"].append(proccopy)\n-            else:\n+            elif verbosity == 2:\n                 status[\"running\"] = self.running\n             status[\"maxproc\"] = self.maxproc\n             status[\"freeslots\"] = self.maxproc - len(self.running)\n@@ -307,16 +309,17 @@ class ClusterMaster:\n             if domain == p['domain']:\n                 return p\n \n-    def print_pending(self, verbosity=0):\n-        if verbosity == 0:\n+    def print_pending(self, verbosity=1):\n+        if verbosity == 1:\n             pending = []\n             for p in self.pending:\n                 pp = p.copy()\n                 del pp[\"settings\"]\n                 pending.append(pp)\n             return pending\n-        else:\n+        elif verbosity == 2:\n             return self.pending\n+        return\n \n     def _engine_started(self):\n         self.load_nodes()\n\n@@ -76,7 +76,7 @@ class ClusterMasterWeb(ClusterMaster):\n \n             self.schedule(domains, spider_settings, priority)\n             if ws:\n-                return self.ws_status(wc_request)\n+                return self.ws_status(wc_request, verbosity=0)\n \n         if \"stop\" in args:\n             if ws:\n@@ -217,19 +217,21 @@ class ClusterMasterWeb(ClusterMaster):\n     def webconsole_discover_module(self):\n         return self\n \n-    def ws_status(self, wc_request):\n+    def ws_status(self, wc_request, verbosity=1):\n         format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n-        verbosity = wc_request.args['verbosity'][0] if 'verbosity' in wc_request.args else '0'\n+        verbosity = int(wc_request.args['verbosity'][0]) if 'verbosity' in wc_request.args else verbosity\n         wc_request.setHeader('content-type', 'text/plain')\n         status = {}\n         nodes_status = {}\n+        if verbosity > 0:\n             for d, n in self.nodes.iteritems():\n-            nodes_status[d] = n.status_as_dict(int(verbosity))\n+                nodes_status[d] = n.status_as_dict(verbosity)\n             status[\"nodes\"] = nodes_status\n-        status[\"pending\"] = self.print_pending(int(verbosity))\n+            status[\"pending\"] = self.print_pending(verbosity)\n             status[\"loading\"] = self.loading\n             content = serialize(status, format)\n             return content\n+        return \"\"\n \n     def ws_statistics(self, wc_request):\n         format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n\n@@ -21,13 +21,15 @@ def main():\n     parser.add_option(\"--status\", dest=\"status\", action=\"store_true\", help=\"Print cluster master status and quit.\")\n     parser.add_option(\"--statistics\", dest=\"statistics\", action=\"store_true\", help=\"Print cluster statistics\")\n     parser.add_option(\"--stop\", dest=\"stop\", action=\"store_true\", help=\"Stops a running domain.\")\n-    parser.add_option(\"--verbosity\", dest=\"verbosity\", type=\"int\", help=\"Sets the report status verbosity.\", default=0)\n+    parser.add_option(\"--verbosity\", dest=\"verbosity\", type=\"int\", help=\"Sets the report status verbosity.\")\n     (opts, args) = parser.parse_args()\n     \n     output = \"\"\n     domains = []\n     urlstring = \"http://%s:%s/cluster_master/ws/\" % (opts.server, opts.port)\n-    post = {\"format\":opts.format, \"verbosity\":opts.verbosity}\n+    post = {\"format\":opts.format}\n+    if isinstance(opts.verbosity, int):\n+        post[\"verbosity\"] = opts.verbosity\n     \n     if args:\n         domains = \",\".join(args)\n@@ -63,7 +65,8 @@ def main():\n \n     f = urllib.urlopen(urlstring, urllib.urlencode(post))\n     output=f.read()\n-\n+    if not output:\n+        return\n     if not opts.output:\n         print output\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3803f6764516e00bbea24055a418c5902a17c350", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 16 | Methods Changed: 11 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 140 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,5 @@\n-from __future__ import with_statement\n-\n from twisted.internet import defer\n from twisted.python import failure\n-from pprint import pprint\n \n from scrapy.core import log\n from scrapy.http import Request\n@@ -14,6 +11,7 @@ from scrapy.stats import stats\n from scrapy.utils.misc import chain_deferred, mustbe_deferred\n from scrapy.conf import settings\n \n+\n class DomainInfo(object):\n     def __init__(self, domain):\n         self.domain = domain\n@@ -32,7 +30,7 @@ class MediaPipeline(object):\n         info = self.cache[domain]\n \n         ulist = []\n-        for url in urls:\n+        for url in urls or ():\n             dfd = self._enqueue(url, info)\n             dfd.addCallbacks(\n                     callback=self.new_item_media,\n@@ -48,15 +46,12 @@ class MediaPipeline(object):\n \n     def _enqueue(self, url, info):\n         wad = defer.Deferred()\n-        wad.addBoth(lambda _: pprint(_))\n \n         request = Request(url=url)\n         fp = request.fingerprint()\n         waiting = info.waiting.setdefault(fp, []).append(wad)\n \n-        if fp in info.downloading:\n-            return\n-\n+        if fp not in info.downloading:\n             dwld = scrapyengine.schedule(request, info.spider, priority=0)\n             dwld.addCallbacks(self.media_downloaded, self.media_failure)\n             dwld.addBoth(self._download_finished, info, fp)\n@@ -67,11 +62,11 @@ class MediaPipeline(object):\n         del info.downloading[fp]\n         info.downloaded[fp] = result # cache result\n         waiting = info.waiting[fp]\n+        del info.waiting[fp]\n \n         isfail = isinstance(result, failure.Failure)\n         for wad in waiting:\n             tocall = wad.errback if isfail else wad.callback\n-            pprint(tocall)\n             tocall(result)\n         return result\n \n@@ -82,37 +77,29 @@ class MediaPipeline(object):\n         self.cache[domain] = DomainInfo(domain)\n \n     def close_domain(self, domain):\n-        print '##### closing'\n         del self.cache[domain]\n \n     ### Overradiable Interface\n \n     def get_urls_from_item(self, item):\n-        print '##### get_urls_from_item'\n         return item.image_urls\n \n     def media_to_download(self, url):\n-        print '##### media_to_download'\n         pass\n \n     def media_downloaded(self, response):\n-        print '##### media_downloaded'\n         pass\n \n     def media_failure(self, _failure):\n-        print '##### media_failure'\n         return _failure\n \n     def new_item_media(self, result, item):\n-        print '##### new_item_media'\n         pass\n \n     def failed_item_media(self, _failure, item):\n-        print '##### failed_item_media'\n         pass\n \n     def item_completed(self, item):\n-        print '##### item_completed'\n         return item\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df2f4b21b8e21f5c32e0883ebe2cedacfd79e07c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 104 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -71,7 +71,7 @@ class ItemPipeline(object):\n                 return item\n \n             current_stage = pipeline.pop(0)\n-            log.msg(\"_%s_ Pipeline stage: %s\" % (item.guid, type(current_stage).__name__), log.TRACE, domain=domain)\n+            log.msg(\"_%s_ Pipeline stage: %s\" % (item, type(current_stage).__name__), log.TRACE, domain=domain)\n \n             d = mustbe_deferred(current_stage.process_item, domain, response, item)\n             d.addCallback(_next_stage)\n@@ -90,7 +90,7 @@ class ItemPipeline(object):\n                 log.msg('Error processing %s - %s' % (item, _failure), log.ERROR, domain=domain)\n \n         def _pipeline_finished(_):\n-            log.msg(\"_%s_ Pipeline finished\" % item.guid, log.TRACE, domain=domain)\n+            log.msg(\"_%s_ Pipeline finished\" % item, log.TRACE, domain=domain)\n             info.remove(item)\n             return _\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31e497e5ebd13f1a376873be27028b6e8b592a64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 36 | Files Changed: 2 | Hunks: 24 | Methods Changed: 15 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 79 | Churn Cumulative: 446 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ from scrapy.spider import spiders\n from scrapy.core.exceptions import DropItem, NotConfigured\n from scrapy.core.exceptions import HttpException\n from scrapy.stats import stats\n-from scrapy.utils.misc import chain_deferred, mustbe_deferred\n+from scrapy.utils.misc import chain_deferred, mustbe_deferred, defer_result\n from scrapy.conf import settings\n \n \n@@ -19,6 +19,7 @@ class DomainInfo(object):\n         self.downloading = {}\n         self.downloaded = {}\n         self.waiting = {}\n+        self.extra = {}\n \n \n class MediaPipeline(object):\n@@ -29,49 +30,55 @@ class MediaPipeline(object):\n         urls = self.get_urls_from_item(item)\n         info = self.cache[domain]\n \n-        ulist = []\n+        lst = []\n         for url in urls or ():\n             dfd = self._enqueue(url, info)\n             dfd.addCallbacks(\n                     callback=self.new_item_media,\n-                    callbackArgs=(item,),\n+                    callbackArgs=(item, url),\n                     errback=self.failed_item_media,\n-                    errbackArgs=(item,),\n+                    errbackArgs=(item, url),\n                     )\n-            ulist.append(dfd)\n+            lst.append(dfd)\n \n-        dlist = defer.DeferredList(ulist, consumeErrors=False)\n-        dlist.addBoth(self._item_completed, item)\n-        return dlist\n+        dlst = defer.DeferredList(lst, consumeErrors=False)\n+        dlst.addBoth(lambda _: self.item_completed(item))\n+        return dlst\n \n     def _enqueue(self, url, info):\n-        wad = defer.Deferred()\n+        request = self.media_to_download(url, info)\n+        if not isinstance(request, Request):\n+            return defer_result(request)\n \n-        request = Request(url=url)\n         fp = request.fingerprint()\n-        waiting = info.waiting.setdefault(fp, []).append(wad)\n+        if fp in info.downloaded:\n+            return defer_result(info.downloaded[fp])\n \n         if fp not in info.downloading:\n-            dwld = scrapyengine.schedule(request, info.spider, priority=0)\n+            self._download(request, info, fp)\n+\n+        wad = defer.Deferred()\n+        waiting = info.waiting.setdefault(fp, []).append(wad)\n+        return wad\n+\n+    def _download(self, request, info, fp):\n+        dwld = mustbe_deferred(self.request(request, info))\n         dwld.addCallbacks(self.media_downloaded, self.media_failure)\n         dwld.addBoth(self._download_finished, info, fp)\n         info.downloading[fp] = (request, dwld)\n-        return wad\n \n     def _download_finished(self, result, info, fp):\n-        del info.downloading[fp]\n         info.downloaded[fp] = result # cache result\n-        waiting = info.waiting[fp]\n+        del info.downloading[fp]\n+\n+        waiting = info.waiting[fp] # client list\n         del info.waiting[fp]\n \n-        isfail = isinstance(result, failure.Failure)\n         for wad in waiting:\n-            tocall = wad.errback if isfail else wad.callback\n-            tocall(result)\n-        return result\n+            defer_result(result).chainDeferred(wad)\n \n-    def _item_completed(self, result, item):\n-        return self.item_completed(item) #dummy as hell\n+    def request(self, request, info):\n+        return scrapyengine.schedule(request, info.spider, priority=0)\n \n     def open_domain(self, domain):\n         self.cache[domain] = DomainInfo(domain)\n@@ -80,12 +87,11 @@ class MediaPipeline(object):\n         del self.cache[domain]\n \n     ### Overradiable Interface\n-\n     def get_urls_from_item(self, item):\n         return item.image_urls\n \n-    def media_to_download(self, url):\n-        pass\n+    def media_to_download(self, url, info):\n+        return Request(url=url)\n \n     def media_downloaded(self, response):\n         pass\n@@ -93,10 +99,10 @@ class MediaPipeline(object):\n     def media_failure(self, _failure):\n         return _failure\n \n-    def new_item_media(self, result, item):\n+    def new_item_media(self, result, item, url):\n         pass\n \n-    def failed_item_media(self, _failure, item):\n+    def failed_item_media(self, _failure, item, url):\n         pass\n \n     def item_completed(self, item):\n@@ -104,3 +110,4 @@ class MediaPipeline(object):\n \n \n \n+\n\n@@ -42,22 +42,22 @@ def defer_succeed(result):\n     reactor.callLater(0, d.callback, result)\n     return d\n \n-\n-def mustbe_deferred(f, *args, **kw):\n-    \"\"\"same as twisted.internet.defer.maybeDeferred, but delay calling callback/errback\"\"\"\n-    deferred = None\n-    try:\n-        result = f(*args, **kw)\n-    except:\n-        return defer_fail(failure.Failure())\n-    else:\n+def defer_result(result):\n     if isinstance(result, defer.Deferred):\n         return result\n     elif isinstance(result, failure.Failure):\n         return defer_fail(result)\n     else:\n         return defer_succeed(result)\n-    return deferred\n+\n+def mustbe_deferred(f, *args, **kw):\n+    \"\"\"same as twisted.internet.defer.maybeDeferred, but delay calling callback/errback\"\"\"\n+    try:\n+        result = f(*args, **kw)\n+    except:\n+        return defer_fail(failure.Failure())\n+    else:\n+        return defer_result(result)\n \n \n def chain_deferred(d1, d2):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4263c3a2e15f55a0feb97d2bb73bbd657684de56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 41 | Files Changed: 1 | Hunks: 27 | Methods Changed: 21 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 141 | Churn Cumulative: 340 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,14 +1,11 @@\n from twisted.internet import defer\n-from twisted.python import failure\n \n+from scrapy.utils.misc import mustbe_deferred, defer_result\n from scrapy.core import log\n-from scrapy.http import Request\n from scrapy.core.engine import scrapyengine\n-from scrapy.spider import spiders\n from scrapy.core.exceptions import DropItem, NotConfigured\n-from scrapy.core.exceptions import HttpException\n-from scrapy.stats import stats\n-from scrapy.utils.misc import chain_deferred, mustbe_deferred, defer_result\n+from scrapy.http import Request\n+from scrapy.spider import spiders\n from scrapy.conf import settings\n \n \n@@ -26,18 +23,27 @@ class MediaPipeline(object):\n     def __init__(self):\n         self.cache = {}\n \n+    def open_domain(self, domain):\n+        self.cache[domain] = DomainInfo(domain)\n+\n+    def close_domain(self, domain):\n+        del self.cache[domain]\n+\n     def process_item(self, domain, response, item):\n-        urls = self.get_urls_from_item(item)\n         info = self.cache[domain]\n+        urls = self.get_urls_from_item(item)\n+        assert urls is None or hasattr(urls, '__iter__'), \\\n+                'get_urls_from_item should return None or iterable'\n \n         lst = []\n         for url in urls or ():\n-            dfd = self._enqueue(url, info)\n+            request = url if isinstance(url, Request) else Request(url=url)\n+            dfd = self._enqueue(request, info)\n             dfd.addCallbacks(\n                     callback=self.new_item_media,\n-                    callbackArgs=(item, url),\n+                    callbackArgs=(item, request),\n                     errback=self.failed_item_media,\n-                    errbackArgs=(item, url),\n+                    errbackArgs=(item, request),\n                     )\n             lst.append(dfd)\n \n@@ -45,10 +51,10 @@ class MediaPipeline(object):\n         dlst.addBoth(lambda _: self.item_completed(item))\n         return dlst\n \n-    def _enqueue(self, url, info):\n-        request = self.media_to_download(url, info)\n-        if not isinstance(request, Request):\n-            return defer_result(request)\n+    def _enqueue(self, request, info):\n+        result = self.media_to_download(request, info)\n+        if result is not None:\n+            return defer_result(result)\n \n         fp = request.fingerprint()\n         if fp in info.downloaded:\n@@ -62,52 +68,105 @@ class MediaPipeline(object):\n         return wad\n \n     def _download(self, request, info, fp):\n-        dwld = mustbe_deferred(self.request(request, info))\n+        dwld = mustbe_deferred(self.download(request, info))\n         dwld.addCallbacks(self.media_downloaded, self.media_failure)\n-        dwld.addBoth(self._download_finished, info, fp)\n+        dwld.addBoth(self._downloaded, info, fp)\n         info.downloading[fp] = (request, dwld)\n \n-    def _download_finished(self, result, info, fp):\n+    def _downloaded(self, result, info, fp):\n         info.downloaded[fp] = result # cache result\n-        del info.downloading[fp]\n-\n         waiting = info.waiting[fp] # client list\n         del info.waiting[fp]\n-\n+        del info.downloading[fp]\n         for wad in waiting:\n             defer_result(result).chainDeferred(wad)\n \n-    def request(self, request, info):\n-        return scrapyengine.schedule(request, info.spider, priority=0)\n-\n-    def open_domain(self, domain):\n-        self.cache[domain] = DomainInfo(domain)\n-\n-    def close_domain(self, domain):\n-        del self.cache[domain]\n \n     ### Overradiable Interface\n+    def download(self, request, info):\n+        \"\"\" Defines how to request the download of media\n+\n+        Default gives high priority to media requests and use scheduler,\n+        shouldn't be necessary to override.\n+\n+        This methods is called only if result for request isn't cached,\n+        request fingerprint is used as cache key.\n+\n+        \"\"\"\n+        return scrapyengine.schedule(request, info.spider, priority=0)\n+\n+    def media_to_download(self, request, info):\n+        \"\"\" Ongoing request hook pre-cache\n+\n+        This method is called every time an item media is enqueue for download.\n+\n+        returning a non-None value implies:\n+            - call `new_item_media` with this value as input unless value is Failure instance\n+            - call `failed_item_media` if value is Failure instance\n+            - prevent downloading, this means calling `download` method.\n+            - prevent taking the value from the cached result for this request.\n+\n+        Take in count that this method doesn't cache returned value as request result.\n+\n+        Override `download` method if you want to hook after checking for cached\n+        result, and if you want to store returned valued as result for request.\n+\n+        \"\"\"\n+\n     def get_urls_from_item(self, item):\n+        \"\"\" Return the urls or Request objects to download for this item\n+\n+        Should return None or an iterable\n+\n+        Defaults return None (no media to download)\n+\n+        \"\"\"\n         return item.image_urls\n \n-    def media_to_download(self, url, info):\n-        return Request(url=url)\n-\n     def media_downloaded(self, response):\n-        pass\n+        \"\"\" Method called on success download of media request\n \n-    def media_failure(self, _failure):\n-        return _failure\n+        Return value is cached and used as input for `new_item_media` method.\n+        Default implementation returns None.\n \n-    def new_item_media(self, result, item, url):\n-        pass\n+        WARNING: returning the response object can eat your memory.\n \n-    def failed_item_media(self, _failure, item, url):\n-        pass\n+        \"\"\"\n+\n+    def media_failure(self, failure):\n+        \"\"\" Method called when media request failed due to any kind of download error.\n+\n+        Return value is cached and used as input for `failed_item_media` method.\n+        Default implementation returns same Failure object.\n+        \"\"\"\n+        return failure\n+\n+    def new_item_media(self, result, item, request):\n+        \"\"\" Method to handle result of requested media for item.\n+\n+        result is the return value of `media_downloaded` hook, or the non-Failure instance\n+        returned by `media_failure` hook.\n+\n+        return value of this method isn't important and is recommended to return None.\n+        \"\"\"\n+\n+    def failed_item_media(self, failure, item, request):\n+        \"\"\" Method to handle failed result of requested media for item.\n+\n+        result is the returned Failure instance of `media_failure` hook, or Failure instance\n+        of an exception raised by `media_downloaded` hook.\n+\n+        return value of this method isn't important and is recommended to return None.\n+        \"\"\"\n \n     def item_completed(self, item):\n+        \"\"\" Method called when all media requests for a single item has returned a result or failure.\n+\n+        The return value of this method is used as output of pipeline stage.\n+\n+        `item_completed` can return item itself or raise DropItem exception.\n+\n+        Default returns item\n+        \"\"\"\n         return item\n \n-\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af0808be95fde0a63fb9c4cde7134e121673d76d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 11 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 367 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,7 +31,7 @@ class MediaPipeline(object):\n \n     def process_item(self, domain, response, item):\n         info = self.cache[domain]\n-        urls = self.get_urls_from_item(item)\n+        urls = self.get_urls_from_item(item, info)\n         assert urls is None or hasattr(urls, '__iter__'), \\\n                 'get_urls_from_item should return None or iterable'\n \n@@ -41,14 +41,14 @@ class MediaPipeline(object):\n             dfd = self._enqueue(request, info)\n             dfd.addCallbacks(\n                     callback=self.new_item_media,\n-                    callbackArgs=(item, request),\n+                    callbackArgs=(item, request, info),\n                     errback=self.failed_item_media,\n-                    errbackArgs=(item, request),\n+                    errbackArgs=(item, request, info),\n                     )\n             lst.append(dfd)\n \n         dlst = defer.DeferredList(lst, consumeErrors=False)\n-        dlst.addBoth(lambda _: self.item_completed(item))\n+        dlst.addBoth(lambda _: self.item_completed(item, info))\n         return dlst\n \n     def _enqueue(self, request, info):\n@@ -69,7 +69,12 @@ class MediaPipeline(object):\n \n     def _download(self, request, info, fp):\n         dwld = mustbe_deferred(self.download(request, info))\n-        dwld.addCallbacks(self.media_downloaded, self.media_failure)\n+        dwld.addCallbacks(\n+                callback=self.media_downloaded,\n+                callbackArgs=(request, info),\n+                errback=self.media_failure,\n+                errbackArgs=(request, info),\n+                )\n         dwld.addBoth(self._downloaded, info, fp)\n         info.downloading[fp] = (request, dwld)\n \n@@ -113,7 +118,7 @@ class MediaPipeline(object):\n \n         \"\"\"\n \n-    def get_urls_from_item(self, item):\n+    def get_urls_from_item(self, item, info):\n         \"\"\" Return the urls or Request objects to download for this item\n \n         Should return None or an iterable\n@@ -123,7 +128,7 @@ class MediaPipeline(object):\n         \"\"\"\n         return item.image_urls\n \n-    def media_downloaded(self, response):\n+    def media_downloaded(self, response, request, info):\n         \"\"\" Method called on success download of media request\n \n         Return value is cached and used as input for `new_item_media` method.\n@@ -133,7 +138,7 @@ class MediaPipeline(object):\n \n         \"\"\"\n \n-    def media_failure(self, failure):\n+    def media_failure(self, failure, request, info):\n         \"\"\" Method called when media request failed due to any kind of download error.\n \n         Return value is cached and used as input for `failed_item_media` method.\n@@ -141,7 +146,7 @@ class MediaPipeline(object):\n         \"\"\"\n         return failure\n \n-    def new_item_media(self, result, item, request):\n+    def new_item_media(self, result, item, request, info):\n         \"\"\" Method to handle result of requested media for item.\n \n         result is the return value of `media_downloaded` hook, or the non-Failure instance\n@@ -150,7 +155,7 @@ class MediaPipeline(object):\n         return value of this method isn't important and is recommended to return None.\n         \"\"\"\n \n-    def failed_item_media(self, failure, item, request):\n+    def failed_item_media(self, failure, item, request, info):\n         \"\"\" Method to handle failed result of requested media for item.\n \n         result is the returned Failure instance of `media_failure` hook, or Failure instance\n@@ -159,7 +164,7 @@ class MediaPipeline(object):\n         return value of this method isn't important and is recommended to return None.\n         \"\"\"\n \n-    def item_completed(self, item):\n+    def item_completed(self, item, info):\n         \"\"\" Method called when all media requests for a single item has returned a result or failure.\n \n         The return value of this method is used as output of pipeline stage.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6407a744e3dfbc05bfcc529f2c6d038fffe17d2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 368 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -126,7 +126,6 @@ class MediaPipeline(object):\n         Defaults return None (no media to download)\n \n         \"\"\"\n-        return item.image_urls\n \n     def media_downloaded(self, response, request, info):\n         \"\"\" Method called on success download of media request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ca695c8c79a0cb00f88486fcfc1e57b1ac33955e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 379 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,6 +35,9 @@ class MediaPipeline(object):\n         assert urls is None or hasattr(urls, '__iter__'), \\\n                 'get_urls_from_item should return None or iterable'\n \n+        def _bugtrap(_failure, request):\n+            log.msg('MediaPipeline Unhandled ERROR in %s: %s' % (request, _failure), log.ERROR, domain=domain)\n+\n         lst = []\n         for url in urls or ():\n             request = url if isinstance(url, Request) else Request(url=url)\n@@ -45,6 +48,7 @@ class MediaPipeline(object):\n                     errback=self.failed_item_media,\n                     errbackArgs=(item, request, info),\n                     )\n+            dfd.addErrback(_bugtrap, request)\n             lst.append(dfd)\n \n         dlst = defer.DeferredList(lst, consumeErrors=False)\n@@ -68,16 +72,21 @@ class MediaPipeline(object):\n         return wad\n \n     def _download(self, request, info, fp):\n-        dwld = mustbe_deferred(self.download(request, info))\n+        def _bugtrap(_failure, request):\n+            log.msg('MediaPipeline Unhandled ERROR in %s: %s' % (request, _failure), log.ERROR, domain=info.domain)\n+\n+        dwld = mustbe_deferred(self.download, request, info)\n         dwld.addCallbacks(\n                 callback=self.media_downloaded,\n                 callbackArgs=(request, info),\n                 errback=self.media_failure,\n                 errbackArgs=(request, info),\n                 )\n+        dwld.addErrback(_bugtrap, request)\n         dwld.addBoth(self._downloaded, info, fp)\n         info.downloading[fp] = (request, dwld)\n \n+\n     def _downloaded(self, result, info, fp):\n         info.downloaded[fp] = result # cache result\n         waiting = info.waiting[fp] # client list\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4944762341ec10b5839e3cf4baef2cbdb3e8bb4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 389 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ class MediaPipeline(object):\n                 'get_urls_from_item should return None or iterable'\n \n         def _bugtrap(_failure, request):\n-            log.msg('MediaPipeline Unhandled ERROR in %s: %s' % (request, _failure), log.ERROR, domain=domain)\n+            log.msg('Unhandled ERROR in MediaPipeline.{new,failed}_item_media for %s: %s' % (request, _failure), log.ERROR, domain=domain)\n \n         lst = []\n         for url in urls or ():\n@@ -72,9 +72,6 @@ class MediaPipeline(object):\n         return wad\n \n     def _download(self, request, info, fp):\n-        def _bugtrap(_failure, request):\n-            log.msg('MediaPipeline Unhandled ERROR in %s: %s' % (request, _failure), log.ERROR, domain=info.domain)\n-\n         dwld = mustbe_deferred(self.download, request, info)\n         dwld.addCallbacks(\n                 callback=self.media_downloaded,\n@@ -82,10 +79,13 @@ class MediaPipeline(object):\n                 errback=self.media_failure,\n                 errbackArgs=(request, info),\n                 )\n-        dwld.addErrback(_bugtrap, request)\n         dwld.addBoth(self._downloaded, info, fp)\n         info.downloading[fp] = (request, dwld)\n \n+        def _bugtrap(_failure):\n+            log.msg('Unhandled ERROR in MediaPipeline._downloaded: %s' % (_failure), log.ERROR, domain=info.domain)\n+        dwld.errback(_bugtrap)\n+\n \n     def _downloaded(self, result, info, fp):\n         info.downloaded[fp] = result # cache result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#08feb4df1cd6622aaceb7bfccfb05ffd8b7be37a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 16 | Methods Changed: 13 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 40 | Churn Cumulative: 429 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,21 +31,20 @@ class MediaPipeline(object):\n \n     def process_item(self, domain, response, item):\n         info = self.cache[domain]\n-        urls = self.get_urls_from_item(item, info)\n-        assert urls is None or hasattr(urls, '__iter__'), \\\n-                'get_urls_from_item should return None or iterable'\n+        requests = self.get_media_requests(item, info)\n+        assert requests is None or hasattr(requests, '__iter__'), \\\n+                'get_media_requests should return None or iterable'\n \n         def _bugtrap(_failure, request):\n-            log.msg('Unhandled ERROR in MediaPipeline.{new,failed}_item_media for %s: %s' % (request, _failure), log.ERROR, domain=domain)\n+            log.msg('Unhandled ERROR in MediaPipeline.item_media_{downloaded,failed} for %s: %s' % (request, _failure), log.ERROR, domain=domain)\n \n         lst = []\n-        for url in urls or ():\n-            request = url if isinstance(url, Request) else Request(url=url)\n+        for request in requests or ():\n             dfd = self._enqueue(request, info)\n             dfd.addCallbacks(\n-                    callback=self.new_item_media,\n+                    callback=self.item_media_downloaded,\n                     callbackArgs=(item, request, info),\n-                    errback=self.failed_item_media,\n+                    errback=self.item_media_failed,\n                     errbackArgs=(item, request, info),\n                     )\n             dfd.addErrback(_bugtrap, request)\n@@ -76,7 +75,7 @@ class MediaPipeline(object):\n         dwld.addCallbacks(\n                 callback=self.media_downloaded,\n                 callbackArgs=(request, info),\n-                errback=self.media_failure,\n+                errback=self.media_failed,\n                 errbackArgs=(request, info),\n                 )\n         dwld.addBoth(self._downloaded, info, fp)\n@@ -86,7 +85,6 @@ class MediaPipeline(object):\n             log.msg('Unhandled ERROR in MediaPipeline._downloaded: %s' % (_failure), log.ERROR, domain=info.domain)\n         dwld.errback(_bugtrap)\n \n-\n     def _downloaded(self, result, info, fp):\n         info.downloaded[fp] = result # cache result\n         waiting = info.waiting[fp] # client list\n@@ -115,8 +113,8 @@ class MediaPipeline(object):\n         This method is called every time an item media is enqueue for download.\n \n         returning a non-None value implies:\n-            - call `new_item_media` with this value as input unless value is Failure instance\n-            - call `failed_item_media` if value is Failure instance\n+            - call `item_media_downloaded` with this value as input unless value is Failure instance\n+            - call `item_media_failed` if value is Failure instance\n             - prevent downloading, this means calling `download` method.\n             - prevent taking the value from the cached result for this request.\n \n@@ -127,8 +125,8 @@ class MediaPipeline(object):\n \n         \"\"\"\n \n-    def get_urls_from_item(self, item, info):\n-        \"\"\" Return the urls or Request objects to download for this item\n+    def get_media_requests(self, item, info):\n+        \"\"\" Return a list of Request objects to download for this item\n \n         Should return None or an iterable\n \n@@ -139,34 +137,34 @@ class MediaPipeline(object):\n     def media_downloaded(self, response, request, info):\n         \"\"\" Method called on success download of media request\n \n-        Return value is cached and used as input for `new_item_media` method.\n+        Return value is cached and used as input for `item_media_downloaded` method.\n         Default implementation returns None.\n \n         WARNING: returning the response object can eat your memory.\n \n         \"\"\"\n \n-    def media_failure(self, failure, request, info):\n+    def media_failed(self, failure, request, info):\n         \"\"\" Method called when media request failed due to any kind of download error.\n \n-        Return value is cached and used as input for `failed_item_media` method.\n+        Return value is cached and used as input for `item_media_failed` method.\n         Default implementation returns same Failure object.\n         \"\"\"\n         return failure\n \n-    def new_item_media(self, result, item, request, info):\n+    def item_media_downloaded(self, result, item, request, info):\n         \"\"\" Method to handle result of requested media for item.\n \n         result is the return value of `media_downloaded` hook, or the non-Failure instance\n-        returned by `media_failure` hook.\n+        returned by `media_failed` hook.\n \n         return value of this method isn't important and is recommended to return None.\n         \"\"\"\n \n-    def failed_item_media(self, failure, item, request, info):\n+    def item_media_failed(self, failure, item, request, info):\n         \"\"\" Method to handle failed result of requested media for item.\n \n-        result is the returned Failure instance of `media_failure` hook, or Failure instance\n+        result is the returned Failure instance of `media_failed` hook, or Failure instance\n         of an exception raised by `media_downloaded` hook.\n \n         return value of this method isn't important and is recommended to return None.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2cb300b9260d7662957d4c34f2d3438f637d2704", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1379 | Files Changed: 16 | Hunks: 15 | Methods Changed: 55 | Complexity Δ (Sum/Max): -205/0 | Churn Δ: 1379 | Churn Cumulative: 2758 | Contributors (this commit): 1 | Commits (past 90d): 32 | Contributors (cumulative): 16 | DMM Complexity: 0.5782983970406905\n\nDIFF:\n@@ -1,287 +0,0 @@\n-r\"\"\"\n-A simple, fast, extensible JSON encoder and decoder\n-\n-JSON (JavaScript Object Notation) <http://json.org> is a subset of\n-JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data\n-interchange format.\n-\n-simplejson exposes an API familiar to uses of the standard library\n-marshal and pickle modules.\n-\n-Encoding basic Python object hierarchies::\n-    \n-    >>> import simplejson\n-    >>> simplejson.dumps(['foo', {'bar': ('baz', None, 1.0, 2)}])\n-    '[\"foo\", {\"bar\": [\"baz\", null, 1.0, 2]}]'\n-    >>> print simplejson.dumps(\"\\\"foo\\bar\")\n-    \"\\\"foo\\bar\"\n-    >>> print simplejson.dumps(u'\\u1234')\n-    \"\\u1234\"\n-    >>> print simplejson.dumps('\\\\')\n-    \"\\\\\"\n-    >>> print simplejson.dumps({\"c\": 0, \"b\": 0, \"a\": 0}, sort_keys=True)\n-    {\"a\": 0, \"b\": 0, \"c\": 0}\n-    >>> from StringIO import StringIO\n-    >>> io = StringIO()\n-    >>> simplejson.dump(['streaming API'], io)\n-    >>> io.getvalue()\n-    '[\"streaming API\"]'\n-\n-Compact encoding::\n-\n-    >>> import simplejson\n-    >>> simplejson.dumps([1,2,3,{'4': 5, '6': 7}], separators=(',',':'))\n-    '[1,2,3,{\"4\":5,\"6\":7}]'\n-\n-Pretty printing::\n-\n-    >>> import simplejson\n-    >>> print simplejson.dumps({'4': 5, '6': 7}, sort_keys=True, indent=4)\n-    {\n-        \"4\": 5, \n-        \"6\": 7\n-    }\n-\n-Decoding JSON::\n-    \n-    >>> import simplejson\n-    >>> simplejson.loads('[\"foo\", {\"bar\":[\"baz\", null, 1.0, 2]}]')\n-    [u'foo', {u'bar': [u'baz', None, 1.0, 2]}]\n-    >>> simplejson.loads('\"\\\\\"foo\\\\bar\"')\n-    u'\"foo\\x08ar'\n-    >>> from StringIO import StringIO\n-    >>> io = StringIO('[\"streaming API\"]')\n-    >>> simplejson.load(io)\n-    [u'streaming API']\n-\n-Specializing JSON object decoding::\n-\n-    >>> import simplejson\n-    >>> def as_complex(dct):\n-    ...     if '__complex__' in dct:\n-    ...         return complex(dct['real'], dct['imag'])\n-    ...     return dct\n-    ... \n-    >>> simplejson.loads('{\"__complex__\": true, \"real\": 1, \"imag\": 2}',\n-    ...     object_hook=as_complex)\n-    (1+2j)\n-\n-Extending JSONEncoder::\n-    \n-    >>> import simplejson\n-    >>> class ComplexEncoder(simplejson.JSONEncoder):\n-    ...     def default(self, obj):\n-    ...         if isinstance(obj, complex):\n-    ...             return [obj.real, obj.imag]\n-    ...         return simplejson.JSONEncoder.default(self, obj)\n-    ... \n-    >>> dumps(2 + 1j, cls=ComplexEncoder)\n-    '[2.0, 1.0]'\n-    >>> ComplexEncoder().encode(2 + 1j)\n-    '[2.0, 1.0]'\n-    >>> list(ComplexEncoder().iterencode(2 + 1j))\n-    ['[', '2.0', ', ', '1.0', ']']\n-    \n-\n-Note that the JSON produced by this module's default settings\n-is a subset of YAML, so it may be used as a serializer for that as well.\n-\"\"\"\n-__version__ = '1.7.2'\n-__all__ = [\n-    'dump', 'dumps', 'load', 'loads',\n-    'JSONDecoder', 'JSONEncoder',\n-]\n-\n-from decoder import JSONDecoder\n-from encoder import JSONEncoder\n-\n-_default_encoder = JSONEncoder(\n-    skipkeys=False,\n-    ensure_ascii=True,\n-    check_circular=True,\n-    allow_nan=True,\n-    indent=None,\n-    separators=None,\n-    encoding='utf-8'\n-)\n-\n-def dump(obj, fp, skipkeys=False, ensure_ascii=True, check_circular=True,\n-        allow_nan=True, cls=None, indent=None, separators=None,\n-        encoding='utf-8', **kw):\n-    \"\"\"\n-    Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n-    ``.write()``-supporting file-like object).\n-\n-    If ``skipkeys`` is ``True`` then ``dict`` keys that are not basic types\n-    (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``) \n-    will be skipped instead of raising a ``TypeError``.\n-\n-    If ``ensure_ascii`` is ``False``, then the some chunks written to ``fp``\n-    may be ``unicode`` instances, subject to normal Python ``str`` to\n-    ``unicode`` coercion rules. Unless ``fp.write()`` explicitly\n-    understands ``unicode`` (as in ``codecs.getwriter()``) this is likely\n-    to cause an error.\n-\n-    If ``check_circular`` is ``False``, then the circular reference check\n-    for container types will be skipped and a circular reference will\n-    result in an ``OverflowError`` (or worse).\n-\n-    If ``allow_nan`` is ``False``, then it will be a ``ValueError`` to\n-    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``)\n-    in strict compliance of the JSON specification, instead of using the\n-    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n-\n-    If ``indent`` is a non-negative integer, then JSON array elements and object\n-    members will be pretty-printed with that indent level. An indent level\n-    of 0 will only insert newlines. ``None`` is the most compact representation.\n-\n-    If ``separators`` is an ``(item_separator, dict_separator)`` tuple\n-    then it will be used instead of the default ``(', ', ': ')`` separators.\n-    ``(',', ':')`` is the most compact JSON representation.\n-\n-    ``encoding`` is the character encoding for str instances, default is UTF-8.\n-\n-    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n-    ``.default()`` method to serialize additional types), specify it with\n-    the ``cls`` kwarg.\n-    \"\"\"\n-    # cached encoder\n-    if (skipkeys is False and ensure_ascii is True and\n-        check_circular is True and allow_nan is True and\n-        cls is None and indent is None and separators is None and\n-        encoding == 'utf-8' and not kw):\n-        iterable = _default_encoder.iterencode(obj)\n-    else:\n-        if cls is None:\n-            cls = JSONEncoder\n-        iterable = cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n-            check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n-            separators=separators, encoding=encoding, **kw).iterencode(obj)\n-    # could accelerate with writelines in some versions of Python, at\n-    # a debuggability cost\n-    for chunk in iterable:\n-        fp.write(chunk)\n-\n-\n-def dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True,\n-        allow_nan=True, cls=None, indent=None, separators=None,\n-        encoding='utf-8', **kw):\n-    \"\"\"\n-    Serialize ``obj`` to a JSON formatted ``str``.\n-\n-    If ``skipkeys`` is ``True`` then ``dict`` keys that are not basic types\n-    (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``) \n-    will be skipped instead of raising a ``TypeError``.\n-\n-    If ``ensure_ascii`` is ``False``, then the return value will be a\n-    ``unicode`` instance subject to normal Python ``str`` to ``unicode``\n-    coercion rules instead of being escaped to an ASCII ``str``.\n-\n-    If ``check_circular`` is ``False``, then the circular reference check\n-    for container types will be skipped and a circular reference will\n-    result in an ``OverflowError`` (or worse).\n-\n-    If ``allow_nan`` is ``False``, then it will be a ``ValueError`` to\n-    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in\n-    strict compliance of the JSON specification, instead of using the\n-    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n-\n-    If ``indent`` is a non-negative integer, then JSON array elements and\n-    object members will be pretty-printed with that indent level. An indent\n-    level of 0 will only insert newlines. ``None`` is the most compact\n-    representation.\n-\n-    If ``separators`` is an ``(item_separator, dict_separator)`` tuple\n-    then it will be used instead of the default ``(', ', ': ')`` separators.\n-    ``(',', ':')`` is the most compact JSON representation.\n-\n-    ``encoding`` is the character encoding for str instances, default is UTF-8.\n-\n-    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n-    ``.default()`` method to serialize additional types), specify it with\n-    the ``cls`` kwarg.\n-    \"\"\"\n-    # cached encoder\n-    if (skipkeys is False and ensure_ascii is True and\n-        check_circular is True and allow_nan is True and\n-        cls is None and indent is None and separators is None and\n-        encoding == 'utf-8' and not kw):\n-        return _default_encoder.encode(obj)\n-    if cls is None:\n-        cls = JSONEncoder\n-    return cls(\n-        skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n-        check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n-        separators=separators, encoding=encoding,\n-        **kw).encode(obj)\n-\n-_default_decoder = JSONDecoder(encoding=None, object_hook=None)\n-\n-def load(fp, encoding=None, cls=None, object_hook=None, **kw):\n-    \"\"\"\n-    Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\n-    a JSON document) to a Python object.\n-\n-    If the contents of ``fp`` is encoded with an ASCII based encoding other\n-    than utf-8 (e.g. latin-1), then an appropriate ``encoding`` name must\n-    be specified. Encodings that are not ASCII based (such as UCS-2) are\n-    not allowed, and should be wrapped with\n-    ``codecs.getreader(fp)(encoding)``, or simply decoded to a ``unicode``\n-    object and passed to ``loads()``\n-\n-    ``object_hook`` is an optional function that will be called with the\n-    result of any object literal decode (a ``dict``). The return value of\n-    ``object_hook`` will be used instead of the ``dict``. This feature\n-    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n-    \n-    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n-    kwarg.\n-    \"\"\"\n-    return loads(fp.read(),\n-        encoding=encoding, cls=cls, object_hook=object_hook, **kw)\n-\n-def loads(s, encoding=None, cls=None, object_hook=None, **kw):\n-    \"\"\"\n-    Deserialize ``s`` (a ``str`` or ``unicode`` instance containing a JSON\n-    document) to a Python object.\n-\n-    If ``s`` is a ``str`` instance and is encoded with an ASCII based encoding\n-    other than utf-8 (e.g. latin-1) then an appropriate ``encoding`` name\n-    must be specified. Encodings that are not ASCII based (such as UCS-2)\n-    are not allowed and should be decoded to ``unicode`` first.\n-\n-    ``object_hook`` is an optional function that will be called with the\n-    result of any object literal decode (a ``dict``). The return value of\n-    ``object_hook`` will be used instead of the ``dict``. This feature\n-    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n-\n-    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n-    kwarg.\n-    \"\"\"\n-    if cls is None and encoding is None and object_hook is None and not kw:\n-        return _default_decoder.decode(s)\n-    if cls is None:\n-        cls = JSONDecoder\n-    if object_hook is not None:\n-        kw['object_hook'] = object_hook\n-    return cls(encoding=encoding, **kw).decode(s)\n-\n-def read(s):\n-    \"\"\"\n-    json-py API compatibility hook. Use loads(s) instead.\n-    \"\"\"\n-    import warnings\n-    warnings.warn(\"simplejson.loads(s) should be used instead of read(s)\",\n-        DeprecationWarning)\n-    return loads(s)\n-\n-def write(obj):\n-    \"\"\"\n-    json-py API compatibility hook. Use dumps(s) instead.\n-    \"\"\"\n-    import warnings\n-    warnings.warn(\"simplejson.dumps(s) should be used instead of write(s)\",\n-        DeprecationWarning)\n-    return dumps(obj)\n-\n-\n\n@@ -1,273 +0,0 @@\n-\"\"\"\n-Implementation of JSONDecoder\n-\"\"\"\n-import re\n-\n-from simplejson.scanner import Scanner, pattern\n-\n-FLAGS = re.VERBOSE | re.MULTILINE | re.DOTALL\n-\n-def _floatconstants():\n-    import struct\n-    import sys\n-    _BYTES = '7FF80000000000007FF0000000000000'.decode('hex')\n-    if sys.byteorder != 'big':\n-        _BYTES = _BYTES[:8][::-1] + _BYTES[8:][::-1]\n-    nan, inf = struct.unpack('dd', _BYTES)\n-    return nan, inf, -inf\n-\n-NaN, PosInf, NegInf = _floatconstants()\n-\n-def linecol(doc, pos):\n-    lineno = doc.count('\\n', 0, pos) + 1\n-    if lineno == 1:\n-        colno = pos\n-    else:\n-        colno = pos - doc.rindex('\\n', 0, pos)\n-    return lineno, colno\n-\n-def errmsg(msg, doc, pos, end=None):\n-    lineno, colno = linecol(doc, pos)\n-    if end is None:\n-        return '%s: line %d column %d (char %d)' % (msg, lineno, colno, pos)\n-    endlineno, endcolno = linecol(doc, end)\n-    return '%s: line %d column %d - line %d column %d (char %d - %d)' % (\n-        msg, lineno, colno, endlineno, endcolno, pos, end)\n-\n-_CONSTANTS = {\n-    '-Infinity': NegInf,\n-    'Infinity': PosInf,\n-    'NaN': NaN,\n-    'true': True,\n-    'false': False,\n-    'null': None,\n-}\n-\n-def JSONConstant(match, context, c=_CONSTANTS):\n-    return c[match.group(0)], None\n-pattern('(-?Infinity|NaN|true|false|null)')(JSONConstant)\n-\n-def JSONNumber(match, context):\n-    match = JSONNumber.regex.match(match.string, *match.span())\n-    integer, frac, exp = match.groups()\n-    if frac or exp:\n-        res = float(integer + (frac or '') + (exp or ''))\n-    else:\n-        res = int(integer)\n-    return res, None\n-pattern(r'(-?(?:0|[1-9]\\d*))(\\.\\d+)?([eE][-+]?\\d+)?')(JSONNumber)\n-\n-STRINGCHUNK = re.compile(r'(.*?)([\"\\\\])', FLAGS)\n-BACKSLASH = {\n-    '\"': u'\"', '\\\\': u'\\\\', '/': u'/',\n-    'b': u'\\b', 'f': u'\\f', 'n': u'\\n', 'r': u'\\r', 't': u'\\t',\n-}\n-\n-DEFAULT_ENCODING = \"utf-8\"\n-\n-def scanstring(s, end, encoding=None, _b=BACKSLASH, _m=STRINGCHUNK.match):\n-    if encoding is None:\n-        encoding = DEFAULT_ENCODING\n-    chunks = []\n-    _append = chunks.append\n-    begin = end - 1\n-    while 1:\n-        chunk = _m(s, end)\n-        if chunk is None:\n-            raise ValueError(\n-                errmsg(\"Unterminated string starting at\", s, begin))\n-        end = chunk.end()\n-        content, terminator = chunk.groups()\n-        if content:\n-            if not isinstance(content, unicode):\n-                content = unicode(content, encoding)\n-            _append(content)\n-        if terminator == '\"':\n-            break\n-        try:\n-            esc = s[end]\n-        except IndexError:\n-            raise ValueError(\n-                errmsg(\"Unterminated string starting at\", s, begin))\n-        if esc != 'u':\n-            try:\n-                m = _b[esc]\n-            except KeyError:\n-                raise ValueError(\n-                    errmsg(\"Invalid \\\\escape: %r\" % (esc,), s, end))\n-            end += 1\n-        else:\n-            esc = s[end + 1:end + 5]\n-            try:\n-                m = unichr(int(esc, 16))\n-                if len(esc) != 4 or not esc.isalnum():\n-                    raise ValueError\n-            except ValueError:\n-                raise ValueError(errmsg(\"Invalid \\\\uXXXX escape\", s, end))\n-            end += 5\n-        _append(m)\n-    return u''.join(chunks), end\n-\n-def JSONString(match, context):\n-    encoding = getattr(context, 'encoding', None)\n-    return scanstring(match.string, match.end(), encoding)\n-pattern(r'\"')(JSONString)\n-\n-WHITESPACE = re.compile(r'\\s*', FLAGS)\n-\n-def JSONObject(match, context, _w=WHITESPACE.match):\n-    pairs = {}\n-    s = match.string\n-    end = _w(s, match.end()).end()\n-    nextchar = s[end:end + 1]\n-    # trivial empty object\n-    if nextchar == '}':\n-        return pairs, end + 1\n-    if nextchar != '\"':\n-        raise ValueError(errmsg(\"Expecting property name\", s, end))\n-    end += 1\n-    encoding = getattr(context, 'encoding', None)\n-    iterscan = JSONScanner.iterscan\n-    while True:\n-        key, end = scanstring(s, end, encoding)\n-        end = _w(s, end).end()\n-        if s[end:end + 1] != ':':\n-            raise ValueError(errmsg(\"Expecting : delimiter\", s, end))\n-        end = _w(s, end + 1).end()\n-        try:\n-            value, end = iterscan(s, idx=end, context=context).next()\n-        except StopIteration:\n-            raise ValueError(errmsg(\"Expecting object\", s, end))\n-        pairs[key] = value\n-        end = _w(s, end).end()\n-        nextchar = s[end:end + 1]\n-        end += 1\n-        if nextchar == '}':\n-            break\n-        if nextchar != ',':\n-            raise ValueError(errmsg(\"Expecting , delimiter\", s, end - 1))\n-        end = _w(s, end).end()\n-        nextchar = s[end:end + 1]\n-        end += 1\n-        if nextchar != '\"':\n-            raise ValueError(errmsg(\"Expecting property name\", s, end - 1))\n-    object_hook = getattr(context, 'object_hook', None)\n-    if object_hook is not None:\n-        pairs = object_hook(pairs)\n-    return pairs, end\n-pattern(r'{')(JSONObject)\n-            \n-def JSONArray(match, context, _w=WHITESPACE.match):\n-    values = []\n-    s = match.string\n-    end = _w(s, match.end()).end()\n-    # look-ahead for trivial empty array\n-    nextchar = s[end:end + 1]\n-    if nextchar == ']':\n-        return values, end + 1\n-    iterscan = JSONScanner.iterscan\n-    while True:\n-        try:\n-            value, end = iterscan(s, idx=end, context=context).next()\n-        except StopIteration:\n-            raise ValueError(errmsg(\"Expecting object\", s, end))\n-        values.append(value)\n-        end = _w(s, end).end()\n-        nextchar = s[end:end + 1]\n-        end += 1\n-        if nextchar == ']':\n-            break\n-        if nextchar != ',':\n-            raise ValueError(errmsg(\"Expecting , delimiter\", s, end))\n-        end = _w(s, end).end()\n-    return values, end\n-pattern(r'\\[')(JSONArray)\n- \n-ANYTHING = [\n-    JSONObject,\n-    JSONArray,\n-    JSONString,\n-    JSONConstant,\n-    JSONNumber,\n-]\n-\n-JSONScanner = Scanner(ANYTHING)\n-\n-class JSONDecoder(object):\n-    \"\"\"\n-    Simple JSON <http://json.org> decoder\n-\n-    Performs the following translations in decoding:\n-    \n-    +---------------+-------------------+\n-    | JSON          | Python            |\n-    +===============+===================+\n-    | object        | dict              |\n-    +---------------+-------------------+\n-    | array         | list              |\n-    +---------------+-------------------+\n-    | string        | unicode           |\n-    +---------------+-------------------+\n-    | number (int)  | int, long         |\n-    +---------------+-------------------+\n-    | number (real) | float             |\n-    +---------------+-------------------+\n-    | true          | True              |\n-    +---------------+-------------------+\n-    | false         | False             |\n-    +---------------+-------------------+\n-    | null          | None              |\n-    +---------------+-------------------+\n-\n-    It also understands ``NaN``, ``Infinity``, and ``-Infinity`` as\n-    their corresponding ``float`` values, which is outside the JSON spec.\n-    \"\"\"\n-\n-    _scanner = Scanner(ANYTHING)\n-    __all__ = ['__init__', 'decode', 'raw_decode']\n-\n-    def __init__(self, encoding=None, object_hook=None):\n-        \"\"\"\n-        ``encoding`` determines the encoding used to interpret any ``str``\n-        objects decoded by this instance (utf-8 by default).  It has no\n-        effect when decoding ``unicode`` objects.\n-        \n-        Note that currently only encodings that are a superset of ASCII work,\n-        strings of other encodings should be passed in as ``unicode``.\n-\n-        ``object_hook``, if specified, will be called with the result\n-        of every JSON object decoded and its return value will be used in\n-        place of the given ``dict``.  This can be used to provide custom\n-        deserializations (e.g. to support JSON-RPC class hinting).\n-        \"\"\"\n-        self.encoding = encoding\n-        self.object_hook = object_hook\n-\n-    def decode(self, s, _w=WHITESPACE.match):\n-        \"\"\"\n-        Return the Python representation of ``s`` (a ``str`` or ``unicode``\n-        instance containing a JSON document)\n-        \"\"\"\n-        obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n-        end = _w(s, end).end()\n-        if end != len(s):\n-            raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\n-        return obj\n-\n-    def raw_decode(self, s, **kw):\n-        \"\"\"\n-        Decode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning\n-        with a JSON document) and return a 2-tuple of the Python\n-        representation and the index in ``s`` where the document ended.\n-\n-        This can be used to decode a JSON document from a string that may\n-        have extraneous data at the end.\n-        \"\"\"\n-        kw.setdefault('context', self)\n-        try:\n-            obj, end = self._scanner.iterscan(s, **kw).next()\n-        except StopIteration:\n-            raise ValueError(\"No JSON object could be decoded\")\n-        return obj, end\n-\n-__all__ = ['JSONDecoder']\n\n@@ -1,371 +0,0 @@\n-\"\"\"\n-Implementation of JSONEncoder\n-\"\"\"\n-import re\n-try:\n-    from simplejson import _speedups\n-except ImportError:\n-    _speedups = None\n-\n-ESCAPE = re.compile(r'[\\x00-\\x19\\\\\"\\b\\f\\n\\r\\t]')\n-ESCAPE_ASCII = re.compile(r'([\\\\\"/]|[^\\ -~])')\n-ESCAPE_DCT = {\n-    # escape all forward slashes to prevent </script> attack\n-    '/': '\\\\/',\n-    '\\\\': '\\\\\\\\',\n-    '\"': '\\\\\"',\n-    '\\b': '\\\\b',\n-    '\\f': '\\\\f',\n-    '\\n': '\\\\n',\n-    '\\r': '\\\\r',\n-    '\\t': '\\\\t',\n-}\n-for i in range(0x20):\n-    ESCAPE_DCT.setdefault(chr(i), '\\\\u%04x' % (i,))\n-\n-# assume this produces an infinity on all machines (probably not guaranteed)\n-INFINITY = float('1e66666')\n-\n-def floatstr(o, allow_nan=True):\n-    # Check for specials.  Note that this type of test is processor- and/or\n-    # platform-specific, so do tests which don't depend on the internals.\n-\n-    if o != o:\n-        text = 'NaN'\n-    elif o == INFINITY:\n-        text = 'Infinity'\n-    elif o == -INFINITY:\n-        text = '-Infinity'\n-    else:\n-        return str(o)\n-\n-    if not allow_nan:\n-        raise ValueError(\"Out of range float values are not JSON compliant: %r\"\n-            % (o,))\n-\n-    return text\n-\n-\n-def encode_basestring(s):\n-    \"\"\"\n-    Return a JSON representation of a Python string\n-    \"\"\"\n-    def replace(match):\n-        return ESCAPE_DCT[match.group(0)]\n-    return '\"' + ESCAPE.sub(replace, s) + '\"'\n-\n-def encode_basestring_ascii(s):\n-    def replace(match):\n-        s = match.group(0)\n-        try:\n-            return ESCAPE_DCT[s]\n-        except KeyError:\n-            n = ord(s)\n-            if n < 0x10000:\n-                return '\\\\u%04x' % (n,)\n-            else:\n-                # surrogate pair\n-                n -= 0x10000\n-                s1 = 0xd800 | ((n >> 10) & 0x3ff)\n-                s2 = 0xdc00 | (n & 0x3ff)\n-                return '\\\\u%04x\\\\u%04x' % (s1, s2)\n-    return '\"' + str(ESCAPE_ASCII.sub(replace, s)) + '\"'\n-        \n-try:\n-    encode_basestring_ascii = _speedups.encode_basestring_ascii\n-    _need_utf8 = True\n-except AttributeError:\n-    _need_utf8 = False\n-\n-class JSONEncoder(object):\n-    \"\"\"\n-    Extensible JSON <http://json.org> encoder for Python data structures.\n-\n-    Supports the following objects and types by default:\n-    \n-    +-------------------+---------------+\n-    | Python            | JSON          |\n-    +===================+===============+\n-    | dict              | object        |\n-    +-------------------+---------------+\n-    | list, tuple       | array         |\n-    +-------------------+---------------+\n-    | str, unicode      | string        |\n-    +-------------------+---------------+\n-    | int, long, float  | number        |\n-    +-------------------+---------------+\n-    | True              | true          |\n-    +-------------------+---------------+\n-    | False             | false         |\n-    +-------------------+---------------+\n-    | None              | null          |\n-    +-------------------+---------------+\n-\n-    To extend this to recognize other objects, subclass and implement a\n-    ``.default()`` method with another method that returns a serializable\n-    object for ``o`` if possible, otherwise it should call the superclass\n-    implementation (to raise ``TypeError``).\n-    \"\"\"\n-    __all__ = ['__init__', 'default', 'encode', 'iterencode']\n-    item_separator = ', '\n-    key_separator = ': '\n-    def __init__(self, skipkeys=False, ensure_ascii=True,\n-            check_circular=True, allow_nan=True, sort_keys=False,\n-            indent=None, separators=None, encoding='utf-8'):\n-        \"\"\"\n-        Constructor for JSONEncoder, with sensible defaults.\n-\n-        If skipkeys is False, then it is a TypeError to attempt\n-        encoding of keys that are not str, int, long, float or None.  If\n-        skipkeys is True, such items are simply skipped.\n-\n-        If ensure_ascii is True, the output is guaranteed to be str\n-        objects with all incoming unicode characters escaped.  If\n-        ensure_ascii is false, the output will be unicode object.\n-\n-        If check_circular is True, then lists, dicts, and custom encoded\n-        objects will be checked for circular references during encoding to\n-        prevent an infinite recursion (which would cause an OverflowError).\n-        Otherwise, no such check takes place.\n-\n-        If allow_nan is True, then NaN, Infinity, and -Infinity will be\n-        encoded as such.  This behavior is not JSON specification compliant,\n-        but is consistent with most JavaScript based encoders and decoders.\n-        Otherwise, it will be a ValueError to encode such floats.\n-\n-        If sort_keys is True, then the output of dictionaries will be\n-        sorted by key; this is useful for regression tests to ensure\n-        that JSON serializations can be compared on a day-to-day basis.\n-\n-        If indent is a non-negative integer, then JSON array\n-        elements and object members will be pretty-printed with that\n-        indent level.  An indent level of 0 will only insert newlines.\n-        None is the most compact representation.\n-\n-        If specified, separators should be a (item_separator, key_separator)\n-        tuple. The default is (', ', ': '). To get the most compact JSON\n-        representation you should specify (',', ':') to eliminate whitespace.\n-\n-        If encoding is not None, then all input strings will be\n-        transformed into unicode using that encoding prior to JSON-encoding. \n-        The default is UTF-8.\n-        \"\"\"\n-\n-        self.skipkeys = skipkeys\n-        self.ensure_ascii = ensure_ascii\n-        self.check_circular = check_circular\n-        self.allow_nan = allow_nan\n-        self.sort_keys = sort_keys\n-        self.indent = indent\n-        self.current_indent_level = 0\n-        if separators is not None:\n-            self.item_separator, self.key_separator = separators\n-        self.encoding = encoding\n-\n-    def _newline_indent(self):\n-        return '\\n' + (' ' * (self.indent * self.current_indent_level))\n-\n-    def _iterencode_list(self, lst, markers=None):\n-        if not lst:\n-            yield '[]'\n-            return\n-        if markers is not None:\n-            markerid = id(lst)\n-            if markerid in markers:\n-                raise ValueError(\"Circular reference detected\")\n-            markers[markerid] = lst\n-        yield '['\n-        if self.indent is not None:\n-            self.current_indent_level += 1\n-            newline_indent = self._newline_indent()\n-            separator = self.item_separator + newline_indent\n-            yield newline_indent\n-        else:\n-            newline_indent = None\n-            separator = self.item_separator\n-        first = True\n-        for value in lst:\n-            if first:\n-                first = False\n-            else:\n-                yield separator\n-            for chunk in self._iterencode(value, markers):\n-                yield chunk\n-        if newline_indent is not None:\n-            self.current_indent_level -= 1\n-            yield self._newline_indent()\n-        yield ']'\n-        if markers is not None:\n-            del markers[markerid]\n-\n-    def _iterencode_dict(self, dct, markers=None):\n-        if not dct:\n-            yield '{}'\n-            return\n-        if markers is not None:\n-            markerid = id(dct)\n-            if markerid in markers:\n-                raise ValueError(\"Circular reference detected\")\n-            markers[markerid] = dct\n-        yield '{'\n-        key_separator = self.key_separator\n-        if self.indent is not None:\n-            self.current_indent_level += 1\n-            newline_indent = self._newline_indent()\n-            item_separator = self.item_separator + newline_indent\n-            yield newline_indent\n-        else:\n-            newline_indent = None\n-            item_separator = self.item_separator\n-        first = True\n-        if self.ensure_ascii:\n-            encoder = encode_basestring_ascii\n-        else:\n-            encoder = encode_basestring\n-        allow_nan = self.allow_nan\n-        if self.sort_keys:\n-            keys = dct.keys()\n-            keys.sort()\n-            items = [(k, dct[k]) for k in keys]\n-        else:\n-            items = dct.iteritems()\n-        _encoding = self.encoding\n-        _do_decode = (_encoding is not None\n-            and not (_need_utf8 and _encoding == 'utf-8'))\n-        for key, value in items:\n-            if isinstance(key, str):\n-                if _do_decode:\n-                    key = key.decode(_encoding)\n-            elif isinstance(key, basestring):\n-                pass\n-            # JavaScript is weakly typed for these, so it makes sense to\n-            # also allow them.  Many encoders seem to do something like this.\n-            elif isinstance(key, float):\n-                key = floatstr(key, allow_nan)\n-            elif isinstance(key, (int, long)):\n-                key = str(key)\n-            elif key is True:\n-                key = 'true'\n-            elif key is False:\n-                key = 'false'\n-            elif key is None:\n-                key = 'null'\n-            elif self.skipkeys:\n-                continue\n-            else:\n-                raise TypeError(\"key %r is not a string\" % (key,))\n-            if first:\n-                first = False\n-            else:\n-                yield item_separator\n-            yield encoder(key)\n-            yield key_separator\n-            for chunk in self._iterencode(value, markers):\n-                yield chunk\n-        if newline_indent is not None:\n-            self.current_indent_level -= 1\n-            yield self._newline_indent()\n-        yield '}'\n-        if markers is not None:\n-            del markers[markerid]\n-\n-    def _iterencode(self, o, markers=None):\n-        if isinstance(o, basestring):\n-            if self.ensure_ascii:\n-                encoder = encode_basestring_ascii\n-            else:\n-                encoder = encode_basestring\n-            _encoding = self.encoding\n-            if (_encoding is not None and isinstance(o, str)\n-                    and not (_need_utf8 and _encoding == 'utf-8')):\n-                o = o.decode(_encoding)\n-            yield encoder(o)\n-        elif o is None:\n-            yield 'null'\n-        elif o is True:\n-            yield 'true'\n-        elif o is False:\n-            yield 'false'\n-        elif isinstance(o, (int, long)):\n-            yield str(o)\n-        elif isinstance(o, float):\n-            yield floatstr(o, self.allow_nan)\n-        elif isinstance(o, (list, tuple)):\n-            for chunk in self._iterencode_list(o, markers):\n-                yield chunk\n-        elif isinstance(o, dict):\n-            for chunk in self._iterencode_dict(o, markers):\n-                yield chunk\n-        else:\n-            if markers is not None:\n-                markerid = id(o)\n-                if markerid in markers:\n-                    raise ValueError(\"Circular reference detected\")\n-                markers[markerid] = o\n-            for chunk in self._iterencode_default(o, markers):\n-                yield chunk\n-            if markers is not None:\n-                del markers[markerid]\n-\n-    def _iterencode_default(self, o, markers=None):\n-        newobj = self.default(o)\n-        return self._iterencode(newobj, markers)\n-\n-    def default(self, o):\n-        \"\"\"\n-        Implement this method in a subclass such that it returns\n-        a serializable object for ``o``, or calls the base implementation\n-        (to raise a ``TypeError``).\n-\n-        For example, to support arbitrary iterators, you could\n-        implement default like this::\n-            \n-            def default(self, o):\n-                try:\n-                    iterable = iter(o)\n-                except TypeError:\n-                    pass\n-                else:\n-                    return list(iterable)\n-                return JSONEncoder.default(self, o)\n-        \"\"\"\n-        raise TypeError(\"%r is not JSON serializable\" % (o,))\n-\n-    def encode(self, o):\n-        \"\"\"\n-        Return a JSON string representation of a Python data structure.\n-\n-        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n-        '{\"foo\":[\"bar\", \"baz\"]}'\n-        \"\"\"\n-        # This is for extremely simple cases and benchmarks...\n-        if isinstance(o, basestring):\n-            if isinstance(o, str):\n-                _encoding = self.encoding\n-                if (_encoding is not None \n-                        and not (_encoding == 'utf-8' and _need_utf8)):\n-                    o = o.decode(_encoding)\n-            return encode_basestring_ascii(o)\n-        # This doesn't pass the iterator directly to ''.join() because it\n-        # sucks at reporting exceptions.  It's going to do this internally\n-        # anyway because it uses PySequence_Fast or similar.\n-        chunks = list(self.iterencode(o))\n-        return ''.join(chunks)\n-\n-    def iterencode(self, o):\n-        \"\"\"\n-        Encode the given object and yield each string\n-        representation as available.\n-        \n-        For example::\n-            \n-            for chunk in JSONEncoder().iterencode(bigobject):\n-                mysocket.write(chunk)\n-        \"\"\"\n-        if self.check_circular:\n-            markers = {}\n-        else:\n-            markers = None\n-        return self._iterencode(o, markers)\n-\n-__all__ = ['JSONEncoder']\n\n@@ -1,40 +0,0 @@\n-import simplejson\n-import cgi\n-\n-class JSONFilter(object):\n-    def __init__(self, app, mime_type='text/x-json'):\n-        self.app = app\n-        self.mime_type = mime_type\n-\n-    def __call__(self, environ, start_response):\n-        # Read JSON POST input to jsonfilter.json if matching mime type\n-        response = {'status': '200 OK', 'headers': []}\n-        def json_start_response(status, headers):\n-            response['status'] = status\n-            response['headers'].extend(headers)\n-        environ['jsonfilter.mime_type'] = self.mime_type\n-        if environ.get('REQUEST_METHOD', '') == 'POST':\n-            if environ.get('CONTENT_TYPE', '') == self.mime_type:\n-                args = [_ for _ in [environ.get('CONTENT_LENGTH')] if _]\n-                data = environ['wsgi.input'].read(*map(int, args))\n-                environ['jsonfilter.json'] = simplejson.loads(data)\n-        res = simplejson.dumps(self.app(environ, json_start_response))\n-        jsonp = cgi.parse_qs(environ.get('QUERY_STRING', '')).get('jsonp')\n-        if jsonp:\n-            content_type = 'text/javascript'\n-            res = ''.join(jsonp + ['(', res, ')'])\n-        elif 'Opera' in environ.get('HTTP_USER_AGENT', ''):\n-            # Opera has bunk XMLHttpRequest support for most mime types\n-            content_type = 'text/plain'\n-        else:\n-            content_type = self.mime_type\n-        headers = [\n-            ('Content-type', content_type),\n-            ('Content-length', len(res)),\n-        ]\n-        headers.extend(response['headers'])\n-        start_response(response['status'], headers)\n-        return [res]\n-\n-def factory(app, global_conf, **kw):\n-    return JSONFilter(app, **kw)\n\n@@ -1,63 +0,0 @@\n-\"\"\"\n-Iterator based sre token scanner\n-\"\"\"\n-import sre_parse, sre_compile, sre_constants\n-from sre_constants import BRANCH, SUBPATTERN\n-from re import VERBOSE, MULTILINE, DOTALL\n-import re\n-\n-__all__ = ['Scanner', 'pattern']\n-\n-FLAGS = (VERBOSE | MULTILINE | DOTALL)\n-class Scanner(object):\n-    def __init__(self, lexicon, flags=FLAGS):\n-        self.actions = [None]\n-        # combine phrases into a compound pattern\n-        s = sre_parse.Pattern()\n-        s.flags = flags\n-        p = []\n-        for idx, token in enumerate(lexicon):\n-            phrase = token.pattern\n-            try:\n-                subpattern = sre_parse.SubPattern(s,\n-                    [(SUBPATTERN, (idx + 1, sre_parse.parse(phrase, flags)))])\n-            except sre_constants.error:\n-                raise\n-            p.append(subpattern)\n-            self.actions.append(token)\n-\n-        p = sre_parse.SubPattern(s, [(BRANCH, (None, p))])\n-        self.scanner = sre_compile.compile(p)\n-\n-\n-    def iterscan(self, string, idx=0, context=None):\n-        \"\"\"\n-        Yield match, end_idx for each match\n-        \"\"\"\n-        match = self.scanner.scanner(string, idx).match\n-        actions = self.actions\n-        lastend = idx\n-        end = len(string)\n-        while True:\n-            m = match()\n-            if m is None:\n-                break\n-            matchbegin, matchend = m.span()\n-            if lastend == matchend:\n-                break\n-            action = actions[m.lastindex]\n-            if action is not None:\n-                rval, next_pos = action(m, context)\n-                if next_pos is not None and next_pos != matchend:\n-                    # \"fast forward\" the scanner\n-                    matchend = next_pos\n-                    match = self.scanner.scanner(string, matchend).match\n-                yield rval, matchend\n-            lastend = matchend\n-            \n-def pattern(pattern, flags=FLAGS):\n-    def decorator(fn):\n-        fn.pattern = pattern\n-        fn.regex = re.compile(pattern, flags)\n-        return fn\n-    return decorator\n\n\n@@ -1,6 +0,0 @@\n-def test_script_close_attack():\n-    import simplejson\n-    res = simplejson.dumps('</script>')\n-    assert '</script>' not in res\n-    res = simplejson.dumps(simplejson.loads('\"</script>\"'))\n-    assert '</script>' not in res\n\n@@ -1,10 +0,0 @@\n-from cStringIO import StringIO\n-import simplejson as S\n-\n-def test_dump():\n-    sio = StringIO()\n-    S.dump({}, sio)\n-    assert sio.getvalue() == '{}'\n-    \n-def test_dumps():\n-    assert S.dumps({}) == '{}'\n\n@@ -1,70 +0,0 @@\n-# Fri Dec 30 18:57:26 2005\n-JSONDOCS = [\n-    # http://json.org/JSON_checker/test/fail1.json\n-    '\"A JSON payload should be an object or array, not a string.\"',\n-    # http://json.org/JSON_checker/test/fail2.json\n-    '[\"Unclosed array\"',\n-    # http://json.org/JSON_checker/test/fail3.json\n-    '{unquoted_key: \"keys must be quoted}',\n-    # http://json.org/JSON_checker/test/fail4.json\n-    '[\"extra comma\",]',\n-    # http://json.org/JSON_checker/test/fail5.json\n-    '[\"double extra comma\",,]',\n-    # http://json.org/JSON_checker/test/fail6.json\n-    '[   , \"<-- missing value\"]',\n-    # http://json.org/JSON_checker/test/fail7.json\n-    '[\"Comma after the close\"],',\n-    # http://json.org/JSON_checker/test/fail8.json\n-    '[\"Extra close\"]]',\n-    # http://json.org/JSON_checker/test/fail9.json\n-    '{\"Extra comma\": true,}',\n-    # http://json.org/JSON_checker/test/fail10.json\n-    '{\"Extra value after close\": true} \"misplaced quoted value\"',\n-    # http://json.org/JSON_checker/test/fail11.json\n-    '{\"Illegal expression\": 1 + 2}',\n-    # http://json.org/JSON_checker/test/fail12.json\n-    '{\"Illegal invocation\": alert()}',\n-    # http://json.org/JSON_checker/test/fail13.json\n-    '{\"Numbers cannot have leading zeroes\": 013}',\n-    # http://json.org/JSON_checker/test/fail14.json\n-    '{\"Numbers cannot be hex\": 0x14}',\n-    # http://json.org/JSON_checker/test/fail15.json\n-    '[\"Illegal backslash escape: \\\\x15\"]',\n-    # http://json.org/JSON_checker/test/fail16.json\n-    '[\"Illegal backslash escape: \\\\\\'\"]',\n-    # http://json.org/JSON_checker/test/fail17.json\n-    '[\"Illegal backslash escape: \\\\017\"]',\n-    # http://json.org/JSON_checker/test/fail18.json\n-    '[[[[[[[[[[[[[[[[[[[[\"Too deep\"]]]]]]]]]]]]]]]]]]]]',\n-    # http://json.org/JSON_checker/test/fail19.json\n-    '{\"Missing colon\" null}',\n-    # http://json.org/JSON_checker/test/fail20.json\n-    '{\"Double colon\":: null}',\n-    # http://json.org/JSON_checker/test/fail21.json\n-    '{\"Comma instead of colon\", null}',\n-    # http://json.org/JSON_checker/test/fail22.json\n-    '[\"Colon instead of comma\": false]',\n-    # http://json.org/JSON_checker/test/fail23.json\n-    '[\"Bad value\", truth]',\n-    # http://json.org/JSON_checker/test/fail24.json\n-    \"['single quote']\",\n-]\n-\n-SKIPS = {\n-    1: \"why not have a string payload?\",\n-    18: \"spec doesn't specify any nesting limitations\",\n-}\n-\n-def test_failures():\n-    import simplejson\n-    for idx, doc in enumerate(JSONDOCS):\n-        idx = idx + 1\n-        if idx in SKIPS:\n-            simplejson.loads(doc)\n-            continue\n-        try:\n-            simplejson.loads(doc)\n-        except ValueError:\n-            pass\n-        else:\n-            assert False, \"Expected failure for fail%d.json: %r\" % (idx, doc)\n\n@@ -1,41 +0,0 @@\n-\n-\n-\n-def test_indent():\n-    import simplejson\n-    import textwrap\n-    \n-    h = [['blorpie'], ['whoops'], [], 'd-shtaeou', 'd-nthiouh', 'i-vhbjkhnth',\n-         {'nifty': 87}, {'field': 'yes', 'morefield': False} ]\n-\n-    expect = textwrap.dedent(\"\"\"\\\n-    [\n-      [\n-        \"blorpie\"\n-      ],\n-      [\n-        \"whoops\"\n-      ],\n-      [],\n-      \"d-shtaeou\",\n-      \"d-nthiouh\",\n-      \"i-vhbjkhnth\",\n-      {\n-        \"nifty\": 87\n-      },\n-      {\n-        \"field\": \"yes\",\n-        \"morefield\": false\n-      }\n-    ]\"\"\")\n-\n-\n-    d1 = simplejson.dumps(h)\n-    d2 = simplejson.dumps(h, indent=2, sort_keys=True, separators=(',', ': '))\n-\n-    h1 = simplejson.loads(d1)\n-    h2 = simplejson.loads(d2)\n-\n-    assert h1 == h\n-    assert h2 == h\n-    assert d2 == expect\n\n@@ -1,72 +0,0 @@\n-# from http://json.org/JSON_checker/test/pass1.json\n-JSON = r'''\n-[\n-    \"JSON Test Pattern pass1\",\n-    {\"object with 1 member\":[\"array with 1 element\"]},\n-    {},\n-    [],\n-    -42,\n-    true,\n-    false,\n-    null,\n-    {\n-        \"integer\": 1234567890,\n-        \"real\": -9876.543210,\n-        \"e\": 0.123456789e-12,\n-        \"E\": 1.234567890E+34,\n-        \"\":  23456789012E666,\n-        \"zero\": 0,\n-        \"one\": 1,\n-        \"space\": \" \",\n-        \"quote\": \"\\\"\",\n-        \"backslash\": \"\\\\\",\n-        \"controls\": \"\\b\\f\\n\\r\\t\",\n-        \"slash\": \"/ & \\/\",\n-        \"alpha\": \"abcdefghijklmnopqrstuvwyz\",\n-        \"ALPHA\": \"ABCDEFGHIJKLMNOPQRSTUVWYZ\",\n-        \"digit\": \"0123456789\",\n-        \"special\": \"`1~!@#$%^&*()_+-={':[,]}|;.</>?\",\n-        \"hex\": \"\\u0123\\u4567\\u89AB\\uCDEF\\uabcd\\uef4A\",\n-        \"true\": true,\n-        \"false\": false,\n-        \"null\": null,\n-        \"array\":[  ],\n-        \"object\":{  },\n-        \"address\": \"50 St. James Street\",\n-        \"url\": \"http://www.JSON.org/\",\n-        \"comment\": \"// /* <!-- --\",\n-        \"# -- --> */\": \" \",\n-        \" s p a c e d \" :[1,2 , 3\n-\n-,\n-\n-4 , 5        ,          6           ,7        ],\n-        \"compact\": [1,2,3,4,5,6,7],\n-        \"jsontext\": \"{\\\"object with 1 member\\\":[\\\"array with 1 element\\\"]}\",\n-        \"quotes\": \"&#34; \\u0022 %22 0x22 034 &#x22;\",\n-        \"\\/\\\\\\\"\\uCAFE\\uBABE\\uAB98\\uFCDE\\ubcda\\uef4A\\b\\f\\n\\r\\t`1~!@#$%^&*()_+-=[]{}|;:',./<>?\"\n-: \"A key can be any string\"\n-    },\n-    0.5 ,98.6\n-,\n-99.44\n-,\n-\n-1066\n-\n-\n-,\"rosebud\"]\n-'''\n-\n-def test_parse():\n-    # test in/out equivalence and parsing\n-    import simplejson\n-    res = simplejson.loads(JSON)\n-    out = simplejson.dumps(res)\n-    assert res == simplejson.loads(out)\n-    try:\n-        simplejson.dumps(res, allow_nan=False)\n-    except ValueError:\n-        pass\n-    else:\n-        assert False, \"23456789012E666 should be out of range\"\n\n@@ -1,11 +0,0 @@\n-# from http://json.org/JSON_checker/test/pass2.json\n-JSON = r'''\n-[[[[[[[[[[[[[[[[[[[\"Not too deep\"]]]]]]]]]]]]]]]]]]]\n-'''\n-\n-def test_parse():\n-    # test in/out equivalence and parsing\n-    import simplejson\n-    res = simplejson.loads(JSON)\n-    out = simplejson.dumps(res)\n-    assert res == simplejson.loads(out)\n\n@@ -1,16 +0,0 @@\n-# from http://json.org/JSON_checker/test/pass3.json\n-JSON = r'''\n-{\n-    \"JSON Test Pattern pass3\": {\n-        \"The outermost value\": \"must be an object or array.\",\n-        \"In this test\": \"It is an object.\"\n-    }\n-}\n-'''\n-\n-def test_parse():\n-    # test in/out equivalence and parsing\n-    import simplejson\n-    res = simplejson.loads(JSON)\n-    out = simplejson.dumps(res)\n-    assert res == simplejson.loads(out)\n\n@@ -1,62 +0,0 @@\n-import simplejson\n-\n-def test_listrecursion():\n-    x = []\n-    x.append(x)\n-    try:\n-        simplejson.dumps(x)\n-    except ValueError:\n-        pass\n-    else:\n-        assert False, \"didn't raise ValueError on list recursion\"\n-    x = []\n-    y = [x]\n-    x.append(y)\n-    try:\n-        simplejson.dumps(x)\n-    except ValueError:\n-        pass\n-    else:\n-        assert False, \"didn't raise ValueError on alternating list recursion\"\n-    y = []\n-    x = [y, y]\n-    # ensure that the marker is cleared\n-    simplejson.dumps(x)\n-\n-def test_dictrecursion():\n-    x = {}\n-    x[\"test\"] = x\n-    try:\n-        simplejson.dumps(x)\n-    except ValueError:\n-        pass\n-    else:\n-        assert False, \"didn't raise ValueError on dict recursion\"\n-    x = {}\n-    y = {\"a\": x, \"b\": x}\n-    # ensure that the marker is cleared\n-    simplejson.dumps(x)\n-\n-class TestObject:\n-    pass\n-\n-class RecursiveJSONEncoder(simplejson.JSONEncoder):\n-    recurse = False\n-    def default(self, o):\n-        if o is TestObject:\n-            if self.recurse:\n-                return [TestObject]\n-            else:\n-                return 'TestObject'\n-        simplejson.JSONEncoder.default(o)\n-\n-def test_defaultrecursion():\n-    enc = RecursiveJSONEncoder()\n-    assert enc.encode(TestObject) == '\"TestObject\"'\n-    enc.recurse = True\n-    try:\n-        enc.encode(TestObject)\n-    except ValueError:\n-        pass\n-    else:\n-        assert False, \"didn't raise ValueError on default recursion\"\n\n@@ -1,41 +0,0 @@\n-\n-\n-\n-def test_separators():\n-    import simplejson\n-    import textwrap\n-    \n-    h = [['blorpie'], ['whoops'], [], 'd-shtaeou', 'd-nthiouh', 'i-vhbjkhnth',\n-         {'nifty': 87}, {'field': 'yes', 'morefield': False} ]\n-\n-    expect = textwrap.dedent(\"\"\"\\\n-    [\n-      [\n-        \"blorpie\"\n-      ] ,\n-      [\n-        \"whoops\"\n-      ] ,\n-      [] ,\n-      \"d-shtaeou\" ,\n-      \"d-nthiouh\" ,\n-      \"i-vhbjkhnth\" ,\n-      {\n-        \"nifty\" : 87\n-      } ,\n-      {\n-        \"field\" : \"yes\" ,\n-        \"morefield\" : false\n-      }\n-    ]\"\"\")\n-\n-\n-    d1 = simplejson.dumps(h)\n-    d2 = simplejson.dumps(h, indent=2, sort_keys=True, separators=(' ,', ' : '))\n-\n-    h1 = simplejson.loads(d1)\n-    h2 = simplejson.loads(d2)\n-\n-    assert h1 == h\n-    assert h2 == h\n-    assert d2 == expect\n\n@@ -1,16 +0,0 @@\n-import simplejson as S\n-\n-def test_encoding1():\n-    encoder = S.JSONEncoder(encoding='utf-8')\n-    u = u'\\N{GREEK SMALL LETTER ALPHA}\\N{GREEK CAPITAL LETTER OMEGA}'\n-    s = u.encode('utf-8')\n-    ju = encoder.encode(u)\n-    js = encoder.encode(s)\n-    assert ju == js\n-    \n-def test_encoding2():\n-    u = u'\\N{GREEK SMALL LETTER ALPHA}\\N{GREEK CAPITAL LETTER OMEGA}'\n-    s = u.encode('utf-8')\n-    ju = S.dumps(u, encoding='utf-8')\n-    js = S.dumps(s, encoding='utf-8')\n-    assert ju == js\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
