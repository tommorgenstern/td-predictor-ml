{"custom_id": "keras#0a109d5b7413975fad51ee787925996ae93a365e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 233 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -28,6 +28,7 @@ from keras.optimizers.optimizer_experimental import adam as adam_experimental\n from keras.optimizers.optimizer_experimental import adamax as adamax_experimental\n from keras.optimizers.optimizer_experimental import adamw as adamw_experimental\n from keras.optimizers.optimizer_experimental import ftrl as ftrl_experimental\n+from keras.optimizers.optimizer_experimental import nadam as nadam_experimental\n from keras.optimizers.optimizer_experimental import rmsprop as rmsprop_experimental\n from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n from keras.optimizers.optimizer_v1 import Optimizer\n\n@@ -17,10 +17,13 @@\n from keras.optimizers.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n \n \n # pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n+@keras_export('keras.optimizers.experimental.Nadam', v1=[])\n class Nadam(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the Nadam algorithm.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f85a252b44a30f8b793e77c3bf7188ea8058217a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 33 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -18,3 +18,8 @@ See [the mixed precision guide](\n   https://www.tensorflow.org/guide/keras/mixed_precision) to learn how to\n use the API.\n \"\"\"\n+\n+from keras.mixed_precision.loss_scale_optimizer import LossScaleOptimizer\n+from keras.mixed_precision.policy import global_policy\n+from keras.mixed_precision.policy import Policy\n+from keras.mixed_precision.policy import set_global_policy\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5fe8ba168dd9c0cb517cfa0df39aa447df30acbc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 36 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 72 | Churn Cumulative: 1451 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -147,27 +147,19 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n \n     Args:\n         depth: Depth of ResNet network.\n-        dropout_rate: dropout rate before final classifier layer.\n-        bn_momentum: Momentum parameter for Batch Normalization layers.\n-        bn_epsilon: Epsilon parameter for Batch Normalization layers.\n-        activation: activation function.\n-        block_args: list of dicts, parameters to construct block modules.\n-        se_ratio: Squeeze and Excitation layer ratio.\n-        model_name: name of the model.\n-        drop_connect_rate: dropout rate at skip connections.\n-        include_top: whether to include the fully-connected layer at the top of\n-        the network.\n-        weights: one of `None` (random initialization), `'imagenet'`\n-            (pre-training on ImageNet), or the path to the weights file to be loaded.\n-            Note: one model can have multiple imagenet variants depending on\n-            input shape it was trained with. For input_shape 224x224 pass\n-            `imagenet-i224` as argument. By default, highest input shape weights are\n-            downloaded.\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n-            use as image input for the model.\n         input_shape: optional shape tuple. It should have exactly 3 inputs\n             channels, and width and height should be no smaller than 32.\n             E.g. (200, 200, 3) would be one valid value.\n+        bn_momentum: Momentum parameter for Batch Normalization layers.\n+        bn_epsilon: Epsilon parameter for Batch Normalization layers.\n+        activation: activation function.\n+        se_ratio: Squeeze and Excitation layer ratio.\n+        dropout_rate: dropout rate before final classifier layer.\n+        drop_connect_rate: dropout rate at skip connections.\n+        include_top: whether to include the fully-connected layer at the top of\n+        the network.\n+        block_args: list of dicts, parameters to construct block modules.\n+        model_name: name of the model.\n         pooling: optional pooling mode for feature extraction when `include_top`\n             is `False`.\n             - `None` means that the output of the model will be\n@@ -179,6 +171,14 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n                 the output of the model will be a 2D tensor.\n             - `max` means that global max pooling will\n                 be applied.\n+        weights: one of `None` (random initialization), `'imagenet'`\n+            (pre-training on ImageNet), or the path to the weights file to be loaded.\n+            Note: one model can have multiple imagenet variants depending on\n+            input shape it was trained with. For input_shape 224x224 pass\n+            `imagenet-i224` as argument. By default, highest input shape weights are\n+            downloaded.\n+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n+            use as image input for the model.\n         classes: optional number of classes to classify images into, only to be\n             specified if `include_top` is True, and if no `weights` argument is\n             specified.\n@@ -524,27 +524,19 @@ def ResNetRS(\n \n     Args:\n         depth: Depth of ResNet network.\n-        dropout_rate: dropout rate before final classifier layer.\n-        bn_momentum: Momentum parameter for Batch Normalization layers.\n-        bn_epsilon: Epsilon parameter for Batch Normalization layers.\n-        activation: activation function.\n-        block_args: list of dicts, parameters to construct block modules.\n-        se_ratio: Squeeze and Excitation layer ratio.\n-        model_name: name of the model.\n-        drop_connect_rate: dropout rate at skip connections.\n-        include_top: whether to include the fully-connected layer at the top of\n-        the network.\n-        weights: one of `None` (random initialization), `'imagenet'`\n-            (pre-training on ImageNet), or the path to the weights file to be loaded.\n-            Note: one model can have multiple imagenet variants depending on\n-            input shape it was trained with. For input_shape 224x224 pass\n-            `imagenet-i224` as argument. By default, highest input shape weights are\n-            downloaded.\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n-            use as image input for the model.\n         input_shape: optional shape tuple. It should have exactly 3 inputs\n             channels, and width and height should be no smaller than 32.\n             E.g. (200, 200, 3) would be one valid value.\n+        bn_momentum: Momentum parameter for Batch Normalization layers.\n+        bn_epsilon: Epsilon parameter for Batch Normalization layers.\n+        activation: activation function.\n+        se_ratio: Squeeze and Excitation layer ratio.\n+        dropout_rate: dropout rate before final classifier layer.\n+        drop_connect_rate: dropout rate at skip connections.\n+        include_top: whether to include the fully-connected layer at the top of\n+        the network.\n+        block_args: list of dicts, parameters to construct block modules.\n+        model_name: name of the model.\n         pooling: optional pooling mode for feature extraction when `include_top`\n             is `False`.\n             - `None` means that the output of the model will be\n@@ -556,6 +548,14 @@ def ResNetRS(\n                 the output of the model will be a 2D tensor.\n             - `max` means that global max pooling will\n                 be applied.\n+        weights: one of `None` (random initialization), `'imagenet'`\n+            (pre-training on ImageNet), or the path to the weights file to be loaded.\n+            Note: one model can have multiple imagenet variants depending on\n+            input shape it was trained with. For input_shape 224x224 pass\n+            `imagenet-i224` as argument. By default, highest input shape weights are\n+            downloaded.\n+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n+            use as image input for the model.\n         classes: optional number of classes to classify images into, only to be\n             specified if `include_top` is True, and if no `weights` argument is\n             specified.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#707d8d81d833b31b8998ae5d7c6ee806121c6987", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 47 | Churn Cumulative: 175 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -12,4 +12,49 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Keras Applications are canned architectures with pre-trained weights.\"\"\"\n+\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\n+# pylint: disable=g-bad-import-order\n+\n+from keras.applications.densenet import DenseNet121\n+from keras.applications.densenet import DenseNet169\n+from keras.applications.densenet import DenseNet201\n+\n+from keras.applications.efficientnet import EfficientNetB0\n+from keras.applications.efficientnet import EfficientNetB1\n+from keras.applications.efficientnet import EfficientNetB2\n+from keras.applications.efficientnet import EfficientNetB3\n+from keras.applications.efficientnet import EfficientNetB4\n+from keras.applications.efficientnet import EfficientNetB5\n+from keras.applications.efficientnet import EfficientNetB6\n+from keras.applications.efficientnet import EfficientNetB7\n+\n+from keras.applications.efficientnet_v2 import EfficientNetV2B0\n+from keras.applications.efficientnet_v2 import EfficientNetV2B1\n+from keras.applications.efficientnet_v2 import EfficientNetV2B2\n+from keras.applications.efficientnet_v2 import EfficientNetV2B3\n+from keras.applications.efficientnet_v2 import EfficientNetV2L\n+from keras.applications.efficientnet_v2 import EfficientNetV2M\n+from keras.applications.efficientnet_v2 import EfficientNetV2S\n+\n+from keras.applications.inception_resnet_v2 import InceptionResNetV2\n+from keras.applications.inception_v3 import InceptionV3\n+\n+from keras.applications.mobilenet import MobileNet\n+from keras.applications.mobilenet_v2 import MobileNetV2\n+from keras.applications.mobilenet_v3 import MobileNetV3Small\n+from keras.applications.mobilenet_v3 import MobileNetV3Large\n+\n+from keras.applications.nasnet import NASNetLarge\n+from keras.applications.nasnet import NASNetMobile\n+\n+from keras.applications.resnet import ResNet50\n+from keras.applications.resnet import ResNet101\n+from keras.applications.resnet import ResNet152\n+from keras.applications.resnet_v2 import ResNet50V2\n+from keras.applications.resnet_v2 import ResNet101V2\n+from keras.applications.resnet_v2 import ResNet152V2\n+\n+from keras.applications.vgg16 import VGG16\n+from keras.applications.vgg19 import VGG19\n+\n+from keras.applications.xception import Xception\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#980a6be629610ee58c1eae5a65a4724ce650597b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 187 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 48 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 188 | Churn Cumulative: 2909 | Contributors (this commit): 4 | Commits (past 90d): 34 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,92 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for metrics.\"\"\"\n+\n+from absl.testing import parameterized\n+from keras import metrics\n+from keras.dtensor import dtensor_api as dtensor\n+from keras.utils import tf_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from keras.dtensor.tests import test_util\n+\n+\n+class MetricsTest(test_util.DTensorBaseTest):\n+\n+  def setUp(self):\n+    super(MetricsTest, self).setUp()\n+    global_ids = test_util.create_device_ids_array((2, 2))\n+    local_device_ids = np.ravel(global_ids).tolist()\n+    mesh_dict = {\n+        'CPU':\n+            dtensor.Mesh(['X', 'Y'], global_ids, local_device_ids,\n+                         test_util.create_device_list((2, 2), 'CPU'))\n+    }\n+    self.mesh = self.configTestMesh(mesh_dict)\n+    tf_utils.set_random_seed(1337)\n+\n+  @parameterized.parameters(\n+      (metrics.Accuracy, {}),\n+      (metrics.AUC, {}),\n+      (metrics.BinaryAccuracy, {}),\n+      (metrics.BinaryCrossentropy, {}),\n+      (metrics.BinaryIoU, {}),\n+      (metrics.CategoricalAccuracy, {}),\n+      (metrics.CategoricalCrossentropy, {}),\n+      (metrics.CategoricalHinge, {}),\n+      (metrics.CosineSimilarity, {}),\n+      (metrics.FalseNegatives, {}),\n+      (metrics.FalsePositives, {}),\n+      (metrics.Hinge, {}),\n+      (metrics.IoU, {'num_classes': 3, 'target_class_ids': [1]}),\n+      (metrics.KLDivergence, {}),\n+      (metrics.LogCoshError, {}),\n+      (metrics.Mean, {}),\n+      (metrics.MeanAbsoluteError, {}),\n+      (metrics.MeanAbsolutePercentageError, {}),\n+      (metrics.MeanIoU, {'num_classes': 3}),\n+      (metrics.MeanRelativeError, {'normalizer': [1, 3, 2, 3]}),\n+      (metrics.MeanSquaredError, {}),\n+      (metrics.MeanSquaredLogarithmicError, {}),\n+      (metrics.OneHotIoU, {'num_classes': 3, 'target_class_ids': [1]}),\n+      (metrics.OneHotMeanIoU, {'num_classes': 3}),\n+      (metrics.Poisson, {}),\n+      (metrics.Precision, {}),\n+      (metrics.PrecisionAtRecall, {'recall': 0.5}),\n+      (metrics.Recall, {}),\n+      (metrics.RecallAtPrecision, {'precision': 0.5}),\n+      (metrics.RootMeanSquaredError, {}),\n+      (metrics.SensitivityAtSpecificity, {'specificity': 0.5}),\n+      (metrics.SparseCategoricalAccuracy, {}),\n+      (metrics.SparseCategoricalCrossentropy, {}),\n+      (metrics.SparseTopKCategoricalAccuracy, {}),\n+      (metrics.SpecificityAtSensitivity, {'sensitivity': 0.5}),\n+      (metrics.SquaredHinge, {}),\n+      (metrics.Sum, {}),\n+      (metrics.TopKCategoricalAccuracy, {}),\n+      (metrics.TrueNegatives, {}),\n+      (metrics.TruePositives, {}),\n+  )\n+  def test_metric_layout(self, metric_cls, init_args):\n+    metric = metric_cls(**init_args, mesh=self.mesh)\n+\n+    for weight in metric.non_trainable_weights:\n+      self.assertIsInstance(weight, dtensor.DVariable)\n+      self.assertTrue(weight.layout.is_fully_replicated())\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -103,6 +103,47 @@ def allow_initializer_layout(init_method):\n       target=init_method, decorator_func=_wrap_function)\n \n \n+def inject_mesh(init_method):\n+  \"\"\"Inject DTensor mesh information to an object.\n+\n+  This is useful for keras object like `Metric` and `Optimizer` which need\n+  DTensor mesh to create the weights, but doesn't want to change the current\n+  public API interface.\n+\n+  This is for temporary usage and eventually the mesh/layout information will be\n+  public arguments in the `__init__` method\n+\n+  Sample usage:\n+  ```python\n+  class Accuracy(tf.keras.metrics.Metric):\n+\n+    @inject_mesh\n+    def __init__(self, name='accuracy', dtype=None):\n+       super().__init__(**kwargs)\n+\n+    acc = Accuracy(mesh=mesh)\n+    assert acc._mesh == mesh\n+  ```\n+\n+  Args:\n+    init_method: the `__init__` method of the Keras class to annotate.\n+\n+  Returns:\n+    the annotated __init__ method.\n+  \"\"\"\n+  def _wrap_function(instance, *args, **kwargs):\n+    mesh = kwargs.pop(\"mesh\", None)\n+    # Note that the injection of _mesh need to happen before the invocation of\n+    # __init__, since the class might need the mesh to create weights in the\n+    # __init__.\n+    if mesh is not None:\n+      instance._mesh = mesh  # pylint: disable=protected-access\n+    init_method(instance, *args, **kwargs)\n+\n+  return tf.__internal__.decorator.make_decorator(\n+      target=init_method, decorator_func=_wrap_function)\n+\n+\n def call_with_layout(fn, layout, *args, **kwargs):\n   \"\"\"Invoke the function with inputs and relayout the result.\n \n\n@@ -22,6 +22,8 @@ import types\n import warnings\n \n from keras import backend\n+from keras.dtensor import dtensor_api as dtensor\n+from keras.dtensor import utils as dtensor_utils\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n from keras.engine import keras_tensor\n@@ -337,6 +339,13 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n     # TODO(b/120571621): Make `ON_READ` work with Keras metrics on TPU.\n     if backend.is_tpu_strategy(strategy):\n       synchronization = tf.VariableSynchronization.ON_WRITE\n+    if getattr(self, '_mesh', None) is not None:\n+      # When self._mesh is set, it means this metric is used for DTensor.\n+      additional_kwargs = {\n+          'layout': dtensor.Layout.replicated(self._mesh,\n+                                              tf.TensorShape(shape).rank)}\n+    else:\n+      additional_kwargs = {}\n \n     with tf.init_scope():\n       return super(Metric, self).add_weight(\n@@ -347,7 +356,8 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n           initializer=initializer,\n           collections=[],\n           synchronization=synchronization,\n-          aggregation=aggregation)\n+          aggregation=aggregation,\n+          **additional_kwargs)\n \n   ### End: For use by subclasses ###\n \n@@ -520,6 +530,7 @@ class Sum(Reduce):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='sum', dtype=None):\n     super(Sum, self).__init__(reduction=metrics_utils.Reduction.SUM,\n                               name=name, dtype=dtype)\n@@ -562,6 +573,7 @@ class Mean(Reduce):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean', dtype=None):\n     super(Mean, self).__init__(\n         reduction=metrics_utils.Reduction.WEIGHTED_MEAN, name=name, dtype=dtype)\n@@ -595,6 +607,7 @@ class MeanMetricWrapper(Mean):\n     **kwargs: Keyword arguments to pass on to `fn`.\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, fn, name=None, dtype=None, **kwargs):\n     super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\n     self._fn = fn\n@@ -695,6 +708,7 @@ class MeanTensor(Metric):\n   array([[2., 3., 4., 5.]])\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_tensor', dtype=None, shape=None):\n     super(MeanTensor, self).__init__(name=name, dtype=dtype)\n     self._shape = None\n\n@@ -21,6 +21,7 @@ from typing import List, Tuple, Union\n \n from keras import activations\n from keras import backend\n+from keras.dtensor import utils as dtensor_utils\n from keras.losses import binary_crossentropy\n from keras.losses import categorical_crossentropy\n from keras.losses import categorical_hinge\n@@ -83,6 +84,7 @@ class MeanRelativeError(base_metric.Mean):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, normalizer, name=None, dtype=None):\n     super(MeanRelativeError, self).__init__(name=name, dtype=dtype)\n     normalizer = tf.cast(normalizer, self._dtype)\n@@ -163,6 +165,7 @@ class Accuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='accuracy', dtype=None):\n     super(Accuracy, self).__init__(accuracy, name, dtype=dtype)\n \n@@ -207,6 +210,7 @@ class BinaryAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='binary_accuracy', dtype=None, threshold=0.5):\n     super(BinaryAccuracy, self).__init__(\n         binary_accuracy, name, dtype=dtype, threshold=threshold)\n@@ -259,6 +263,7 @@ class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='categorical_accuracy', dtype=None):\n     super(CategoricalAccuracy, self).__init__(\n         categorical_accuracy, name, dtype=dtype)\n@@ -310,6 +315,7 @@ class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='sparse_categorical_accuracy', dtype=None):\n     super(SparseCategoricalAccuracy, self).__init__(\n         sparse_categorical_accuracy, name, dtype=dtype)\n@@ -375,6 +381,7 @@ class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, k=5, name='top_k_categorical_accuracy', dtype=None):\n     super(TopKCategoricalAccuracy, self).__init__(\n         top_k_categorical_accuracy, name, dtype=dtype, k=k)\n@@ -413,6 +420,7 @@ class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, k=5, name='sparse_top_k_categorical_accuracy', dtype=None):\n     super(SparseTopKCategoricalAccuracy, self).__init__(\n         sparse_top_k_categorical_accuracy, name, dtype=dtype, k=k)\n@@ -532,6 +540,7 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(FalsePositives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES,\n@@ -581,6 +590,7 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(FalseNegatives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES,\n@@ -630,6 +640,7 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(TrueNegatives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES,\n@@ -679,6 +690,7 @@ class TruePositives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(TruePositives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES,\n@@ -756,6 +768,7 @@ class Precision(base_metric.Metric):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                thresholds=None,\n                top_k=None,\n@@ -886,6 +899,7 @@ class Recall(base_metric.Metric):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                thresholds=None,\n                top_k=None,\n@@ -1129,6 +1143,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                specificity,\n                num_thresholds=200,\n@@ -1225,6 +1240,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                sensitivity,\n                num_thresholds=200,\n@@ -1313,6 +1329,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                recall,\n                num_thresholds=200,\n@@ -1401,6 +1418,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                precision,\n                num_thresholds=200,\n@@ -1554,6 +1572,7 @@ class AUC(base_metric.Metric):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                num_thresholds=200,\n                curve='ROC',\n@@ -1963,6 +1982,7 @@ class CosineSimilarity(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='cosine_similarity', dtype=None, axis=-1):\n     super(CosineSimilarity, self).__init__(\n         cosine_similarity, name, dtype=dtype, axis=axis)\n@@ -1999,6 +2019,7 @@ class MeanAbsoluteError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_absolute_error', dtype=None):\n     super(MeanAbsoluteError, self).__init__(\n         mean_absolute_error, name, dtype=dtype)\n@@ -2035,6 +2056,7 @@ class MeanAbsolutePercentageError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_absolute_percentage_error', dtype=None):\n     super(MeanAbsolutePercentageError, self).__init__(\n         mean_absolute_percentage_error, name, dtype=dtype)\n@@ -2071,6 +2093,7 @@ class MeanSquaredError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_squared_error', dtype=None):\n     super(MeanSquaredError, self).__init__(\n         mean_squared_error, name, dtype=dtype)\n@@ -2107,6 +2130,7 @@ class MeanSquaredLogarithmicError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_squared_logarithmic_error', dtype=None):\n     super(MeanSquaredLogarithmicError, self).__init__(\n         mean_squared_logarithmic_error, name, dtype=dtype)\n@@ -2143,6 +2167,7 @@ class Hinge(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='hinge', dtype=None):\n     super(Hinge, self).__init__(hinge, name, dtype=dtype)\n \n@@ -2181,6 +2206,7 @@ class SquaredHinge(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='squared_hinge', dtype=None):\n     super(SquaredHinge, self).__init__(squared_hinge, name, dtype=dtype)\n \n@@ -2216,6 +2242,7 @@ class CategoricalHinge(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='categorical_hinge', dtype=None):\n     super(CategoricalHinge, self).__init__(categorical_hinge, name, dtype=dtype)\n \n@@ -2247,6 +2274,7 @@ class RootMeanSquaredError(base_metric.Mean):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='root_mean_squared_error', dtype=None):\n     super(RootMeanSquaredError, self).__init__(name, dtype=dtype)\n \n@@ -2307,6 +2335,7 @@ class LogCoshError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='logcosh', dtype=None):\n     super(LogCoshError, self).__init__(logcosh, name, dtype=dtype)\n \n@@ -2343,6 +2372,7 @@ class Poisson(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='poisson', dtype=None):\n     super(Poisson, self).__init__(poisson, name, dtype=dtype)\n \n@@ -2379,6 +2409,7 @@ class KLDivergence(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='kullback_leibler_divergence', dtype=None):\n     super(KLDivergence, self).__init__(\n         kullback_leibler_divergence, name, dtype=dtype)\n@@ -2534,6 +2565,7 @@ class IoU(_IoUBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       num_classes: int,\n@@ -2656,6 +2688,7 @@ class BinaryIoU(IoU):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),\n@@ -2762,6 +2795,7 @@ class MeanIoU(IoU):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, num_classes, name=None, dtype=None):\n     target_class_ids = list(range(num_classes))\n     super(MeanIoU, self).__init__(\n@@ -2852,6 +2886,7 @@ class OneHotIoU(IoU):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       num_classes: int,\n@@ -2956,6 +2991,7 @@ class OneHotMeanIoU(MeanIoU):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       num_classes: int,\n@@ -3028,6 +3064,7 @@ class BinaryCrossentropy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                name='binary_crossentropy',\n                dtype=None,\n@@ -3092,6 +3129,7 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                name='categorical_crossentropy',\n                dtype=None,\n@@ -3163,6 +3201,7 @@ class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                name='sparse_categorical_crossentropy',\n                dtype=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#521c780f0b1dacf850a8a4809cc6d9d67a6d8959", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 20 | Churn Cumulative: 1156 | Contributors (this commit): 1 | Commits (past 90d): 18 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -375,9 +375,14 @@ def _create_dvariable(layout_map, object_path, variable):\n     # The init value is probably already created as a tensor, we will just copy\n     # it to mesh and give it a proper layout.\n     init_val = dtensor.copy_to_mesh(init_val, layout)\n+  # Use the original variable name for new DVariable creation. TF was adding\n+  # \":0\" suffix to it.\n+  variable_name = variable.name\n+  if variable_name.endswith(':0'):\n+    variable_name = variable_name[:-2]\n   new_variable = dtensor.DVariable(init_val,\n                                    trainable=variable.trainable,\n-                                   name=variable.name)\n+                                   name=variable_name)\n   return new_variable\n \n \n\n@@ -312,6 +312,19 @@ class ObjectPathMappingTest(test_util.DTensorBaseTest):\n     # kernel shape [20, 30] with all \"1\", timed by 0.01 from l2\n     self.assertAllClose(model.losses[1], 6.0)\n \n+  def test_dvariable_name(self):\n+    layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n+    with layout_map_lib.layout_map_scope(layout_map):\n+      model = tf.keras.Sequential([\n+          layers.Dense(20, name='d1', input_shape=(10,)),\n+          layers.Dropout(0.1),\n+          layers.Dense(30, name='d2')\n+      ])\n+\n+    self.assertLen(model.layers, 3)\n+    self.assertEqual(model.layers[0].kernel.name, 'd1/kernel:0')\n+    self.assertEqual(model.layers[0].bias.name, 'd1/bias:0')\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#483302988849838b2789142b8599c0992b16f7e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 308 | Lines Deleted: 9 | Files Changed: 21 | Hunks: 21 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 317 | Churn Cumulative: 546 | Contributors (this commit): 6 | Commits (past 90d): 63 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,15 @@ from keras.optimizers.optimizer_experimental import ftrl as ftrl_experimental\n from keras.optimizers.optimizer_experimental import nadam as nadam_experimental\n from keras.optimizers.optimizer_experimental import rmsprop as rmsprop_experimental\n from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n+from keras.optimizers.legacy import optimizer as optimizer_legacy\n+from keras.optimizers.legacy import adadelta as adadelta_legacy\n+from keras.optimizers.legacy import adagrad as adagrad_legacy\n+from keras.optimizers.legacy import adam as adam_legacy\n+from keras.optimizers.legacy import adamax as adamax_legacy\n+from keras.optimizers.legacy import ftrl as ftrl_legacy\n+from keras.optimizers.legacy import nadam as nadam_legacy\n+from keras.optimizers.legacy import rmsprop as rmsprop_legacy\n+from keras.optimizers.legacy import sgd as sgd_legacy\n from keras.optimizers.optimizer_v1 import Optimizer\n from keras.optimizers.optimizer_v1 import TFOptimizer\n from keras.optimizers.optimizer_v2 import adadelta as adadelta_v2\n\n@@ -0,0 +1,15 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy optimizer package.\"\"\"\n\n@@ -0,0 +1,24 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy Adadelta optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_v2 import adadelta\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.optimizers.legacy.Adadelta')\n+class Adadelta(adadelta.Adadelta):\n+  pass\n\n@@ -0,0 +1,24 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy Adagrad optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_v2 import adagrad\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.optimizers.legacy.Adagrad')\n+class Adagrad(adagrad.Adagrad):\n+  pass\n\n@@ -0,0 +1,24 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy Adam optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_v2 import adam\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.optimizers.legacy.Adam')\n+class Adam(adam.Adam):\n+  pass\n\n@@ -0,0 +1,24 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy Adamax optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_v2 import adamax\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.optimizers.legacy.Adamax')\n+class Adamax(adamax.Adamax):\n+  pass\n\n@@ -0,0 +1,24 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy Ftrl optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_v2 import ftrl\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.optimizers.legacy.Ftrl')\n+class Ftrl(ftrl.Ftrl):\n+  pass\n\n@@ -0,0 +1,24 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy Nadam optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_v2 import nadam\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.optimizers.legacy.Nadam')\n+class Nadam(nadam.Nadam):\n+  pass\n\n@@ -0,0 +1,24 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy Adam optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.optimizers.legacy.Optimizer')\n+class Optimizer(optimizer_v2.OptimizerV2):\n+  pass\n\n@@ -0,0 +1,60 @@\n+\"\"\"Tests for optimizer.\"\"\"\n+\n+from absl.testing import parameterized\n+import keras\n+from keras.optimizers.legacy import adadelta\n+from keras.optimizers.legacy import adagrad\n+from keras.optimizers.legacy import adam\n+from keras.optimizers.legacy import adamax\n+from keras.optimizers.legacy import ftrl\n+from keras.optimizers.legacy import nadam\n+from keras.optimizers.legacy import rmsprop\n+from keras.optimizers.legacy import sgd\n+import tensorflow.compat.v2 as tf\n+\n+adadelta_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"adadelta\", lambda: adadelta.Adadelta(0.002))\n+adagrad_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"adagrad\", lambda: adagrad.Adagrad(0.002))\n+adam_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"adam\", lambda: adam.Adam(0.002))\n+adamax_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"adamax\", lambda: adamax.Adamax(0.002))\n+ftrl_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"ftrl\", lambda: ftrl.Ftrl(0.002))\n+gradient_descent_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"sgd\", lambda: sgd.SGD(0.002))\n+nadam_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"nadam\", lambda: nadam.Nadam(0.002))\n+rmsprop_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"rmsprop\", lambda: rmsprop.RMSprop(0.002))\n+\n+OPTIMIZER_FN = [\n+    adadelta_fn,\n+    adagrad_fn,\n+    adam_fn,\n+    adamax_fn,\n+    ftrl_fn,\n+    gradient_descent_fn,\n+    nadam_fn,\n+    rmsprop_fn,\n+]\n+\n+\n+class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n+  \"\"\"Test the functionality of optimizer.\"\"\"\n+\n+  @parameterized.product(optimizer_fn=OPTIMIZER_FN)\n+  def testModelFit(self, optimizer_fn):\n+    model = keras.Sequential(\n+        [keras.layers.Input(shape=(1,)),\n+         keras.layers.Dense(1)])\n+    optimizer = optimizer_fn()\n+    x = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n+    y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n+    model.compile(loss=\"mse\", optimizer=optimizer)\n+    model.fit(x, y, epochs=1, steps_per_epoch=5)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.__internal__.distribute.multi_process_runner.test_main()\n\n@@ -0,0 +1,24 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy RMSprop optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_v2 import rmsprop\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.optimizers.legacy.RMSprop')\n+class RMSprop(rmsprop.RMSprop):\n+  pass\n\n@@ -0,0 +1,24 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Legacy SGD optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_v2 import gradient_descent\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.optimizers.legacy.SGD')\n+class SGD(gradient_descent.SGD):\n+  pass\n\n@@ -24,7 +24,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n # pylint: disable=g-classes-have-attributes\n-@keras_export('keras.optimizers.Adadelta', 'keras.optimizers_legacy.Adadelta')\n+@keras_export('keras.optimizers.Adadelta')\n class Adadelta(optimizer_v2.OptimizerV2):\n   r\"\"\"Optimizer that implements the Adadelta algorithm.\n \n\n@@ -24,7 +24,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n # pylint: disable=g-classes-have-attributes\n-@keras_export('keras.optimizers.Adagrad', 'keras.optimizers_legacy.Adagrad')\n+@keras_export('keras.optimizers.Adagrad')\n class Adagrad(optimizer_v2.OptimizerV2):\n   r\"\"\"Optimizer that implements the Adagrad algorithm.\n \n\n@@ -21,7 +21,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n # pylint: disable=g-classes-have-attributes\n-@keras_export('keras.optimizers.Adam', 'keras.optimizers_legacy.Adam')\n+@keras_export('keras.optimizers.Adam')\n class Adam(optimizer_v2.OptimizerV2):\n   r\"\"\"Optimizer that implements the Adam algorithm.\n \n\n@@ -21,7 +21,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n # pylint: disable=g-classes-have-attributes\n-@keras_export('keras.optimizers.Adamax', 'keras.optimizers_legacy.Adamax')\n+@keras_export('keras.optimizers.Adamax')\n class Adamax(optimizer_v2.OptimizerV2):\n   \"\"\"Optimizer that implements the Adamax algorithm.\n \n\n@@ -22,7 +22,7 @@ from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-d\n \n \n # pylint: disable=g-classes-have-attributes\n-@keras_export('keras.optimizers.Ftrl', 'keras.optimizers_legacy.Ftrl')\n+@keras_export('keras.optimizers.Ftrl')\n class Ftrl(optimizer_v2.OptimizerV2):\n   r\"\"\"Optimizer that implements the FTRL algorithm.\n \n\n@@ -21,7 +21,7 @@ from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-d\n \n \n # pylint: disable=g-classes-have-attributes\n-@keras_export(\"keras.optimizers.SGD\", \"keras.optimizers_legacy.SGD\")\n+@keras_export(\"keras.optimizers.SGD\")\n class SGD(optimizer_v2.OptimizerV2):\n   r\"\"\"Gradient descent (with momentum) optimizer.\n \n\n@@ -22,7 +22,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n # pylint: disable=g-classes-have-attributes\n-@keras_export('keras.optimizers.Nadam', 'keras.optimizers_legacy.Nadam')\n+@keras_export('keras.optimizers.Nadam')\n class Nadam(optimizer_v2.OptimizerV2):\n   r\"\"\"Optimizer that implements the NAdam algorithm.\n   Much like Adam is essentially RMSprop with momentum, Nadam is Adam with\n\n@@ -94,7 +94,6 @@ def name_scope_only_in_function_or_graph(name):\n \n @keras_export(\n     \"keras.optimizers.Optimizer\",\n-    \"keras.optimizers_legacy.Optimizer\",\n     metaclass=abc.ABCMeta)\n class OptimizerV2(tf.__internal__.tracking.Trackable):\n   \"\"\"Base class for Keras optimizers.\n\n@@ -24,7 +24,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n # pylint: disable=g-classes-have-attributes\n-@keras_export(\"keras.optimizers.RMSprop\", \"keras.optimizers_legacy.RMSprop\")\n+@keras_export(\"keras.optimizers.RMSprop\")\n class RMSprop(optimizer_v2.OptimizerV2):\n   r\"\"\"Optimizer that implements the RMSprop algorithm.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
