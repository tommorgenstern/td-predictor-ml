{"custom_id": "keras#4ece03af3f7ce4fc6367e2e82d61792c3f870a56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 9 | Churn Cumulative: 8215 | Contributors (this commit): 14 | Commits (past 90d): 36 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3992,6 +3992,9 @@ class Nonzero(Operation):\n     def call(self, x):\n         return backend.numpy.nonzero(x)\n \n+    def compute_output_spec(self, x):\n+        return KerasTensor([None] * len(x.shape))\n+\n \n @keras_export([\"keras.ops.nonzero\", \"keras.ops.numpy.nonzero\"])\n def nonzero(x):\n@@ -4003,6 +4006,8 @@ def nonzero(x):\n     Returns:\n         Indices of elements that are non-zero.\n     \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Nonzero().symbolic_call(x)\n     return backend.numpy.nonzero(x)\n \n \n\n@@ -1311,6 +1311,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.ndim(x).shape, (2,))\n \n+    def test_nonzero(self):\n+        x = KerasTensor((None, 5, 6))\n+        self.assertEqual(knp.nonzero(x).shape, (None, None, None))\n+\n     def test_ones_like(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.ones_like(x).shape, (None, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ec5eadf21e0cc67615e147a87c1adad6c9669421", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 365 | Contributors (this commit): 10 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -23,8 +23,8 @@ class KerasVariable:\n         initializer: Initial value or callable for initialization.\n             If a callable is used, it should take the arguments\n             `shape` and `dtype`.\n-        shape: Optional. Tuple for the variable's shape. Required if `initializer`\n-            is a callable.\n+        shape: Optional. Tuple for the variable's shape.\n+            Required if `initializer` is a callable.\n         dtype: Optional. Data type of the variable. Defaults to the global float\n             dtype type (`\"float32\"` if never configured).\n         trainable: Optional. Boolean indicating if variable is trainable.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8961e3f20f6d879ecb2ed2699a6fff14688b4129", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 71 | Churn Cumulative: 300 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.02\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import warnings\n+\n import tensorflow as tf\n \n from keras import backend\n@@ -124,6 +126,8 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n     def _distributed_tf_update_step(\n         self, distribution, grads_and_vars, learning_rate\n     ):\n+        grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n+\n         def apply_grad_to_update_var(var, grad):\n             return self.update_step(grad, var, learning_rate)\n \n@@ -132,12 +136,48 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n                 var, apply_grad_to_update_var, args=(grad,), group=False\n             )\n \n+    def _all_reduce_sum_gradients(self, grads_and_vars):\n+        \"\"\"Returns all-reduced gradients aggregated via summation.\n+\n+        Args:\n+            grads_and_vars: List of (gradient, variable) pairs.\n+\n+        Returns:\n+            List of (gradient, variable) pairs\n+            where gradients have been all-reduced.\n+        \"\"\"\n+        replica_context = tf.distribute.get_replica_context()\n+        if not replica_context:\n+            return grads_and_vars\n+\n+        grads_and_vars = list(grads_and_vars)\n+        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n+        if filtered_grads_and_vars:\n+            grads = [pair[0] for pair in filtered_grads_and_vars]\n+            reduced = tf.distribute.get_replica_context().all_reduce(\n+                tf.distribute.ReduceOp.SUM, grads\n+            )\n+        else:\n+            reduced = []\n+        # Copy 'reduced' but add None gradients back in\n+        reduced_with_nones = []\n+        reduced_pos = 0\n+        for g, v in grads_and_vars:\n+            if g is None:\n+                reduced_with_nones.append((None, v))\n+            else:\n+                reduced_with_nones.append((reduced[reduced_pos], v))\n+                reduced_pos += 1\n+        assert reduced_pos == len(reduced), \"Failed to add all gradients\"\n+        return reduced_with_nones\n+\n     def _overwrite_model_variables_with_average_value(\n         self, trainable_variables\n     ):\n         \"\"\"Overwrite model variables with their moving average values.\n \n         This function overwrites variables on each device.\n+\n         Args:\n           var_list: list of model variables.\n         \"\"\"\n@@ -178,3 +218,34 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n         # We need to use TF-specific OP to support the case,\n         # when `values` are `tf.IndexedSlices`.\n         return tf.clip_by_norm(values, self.clipnorm, axes)\n+\n+\n+def filter_empty_gradients(grads_and_vars):\n+    \"\"\"Filter out `(grad, var)` pairs that have a gradient equal to `None`.\"\"\"\n+    grads_and_vars = tuple(grads_and_vars)\n+    if not grads_and_vars:\n+        return grads_and_vars\n+\n+    filtered = []\n+    vars_with_empty_grads = []\n+    for grad, var in grads_and_vars:\n+        if grad is None:\n+            vars_with_empty_grads.append(var)\n+        else:\n+            filtered.append((grad, var))\n+    filtered = tuple(filtered)\n+\n+    if not filtered:\n+        variable = ([v.name for _, v in grads_and_vars],)\n+        raise ValueError(\n+            f\"No gradients provided for any variable: {variable}. \"\n+            f\"Provided `grads_and_vars` is {grads_and_vars}.\"\n+        )\n+    if vars_with_empty_grads:\n+        warnings.warn(\n+            \"Gradients do not exist for variables %s when minimizing the \"\n+            \"loss. If you're using `model.compile()`, did you forget to \"\n+            \"provide a `loss` argument?\",\n+            ([v.name for v in vars_with_empty_grads]),\n+        )\n+    return filtered\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e62b2d617e3aa91bd246be710cb3b291c4d5691f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 722 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -23,6 +23,7 @@ from keras.utils.naming import to_snake_case\n ALL_OBJECTS = {\n     Initializer,\n     Constant,\n+    Identity,\n     Ones,\n     Zeros,\n     GlorotNormal,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4de2c745ad1de82e691ae090ea125e681eb30d74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 153 | Lines Deleted: 77 | Files Changed: 3 | Hunks: 27 | Methods Changed: 12 | Complexity Δ (Sum/Max): 3/5 | Churn Δ: 230 | Churn Cumulative: 3589 | Contributors (this commit): 15 | Commits (past 90d): 8 | Contributors (cumulative): 21 | DMM Complexity: 0.9482758620689655\n\nDIFF:\n@@ -75,11 +75,14 @@ class BaseOptimizer:\n         self.ema_momentum = ema_momentum\n         self.ema_overwrite_frequency = ema_overwrite_frequency\n \n-        if self.clipnorm is not None and self.global_clipnorm is not None:\n+        clip_args_sum = sum(\n+            a is not None for a in [clipnorm, clipvalue, global_clipnorm]\n+        )\n+        if clip_args_sum > 1:\n             raise ValueError(\n-                \"Only one of `clipnorm` and `global_clipnorm` can \"\n-                f\"be set. Received: clipnorm={self.clipnorm}, \"\n-                f\"global_clipnorm={self.global_clipnorm}\"\n+                \"Only one of `clipnorm`, `clipvalue` and `global_clipnorm` can \"\n+                f\"be set. Received: clipnorm={clipnorm}, \"\n+                f\"clipvalue={clipvalue}, global_clipnorm={global_clipnorm}\"\n             )\n         self.built = False\n \n@@ -568,50 +571,39 @@ class BaseOptimizer:\n         return self._learning_rate\n \n     def _filter_empty_gradients(self, grads, vars):\n-        for grad in grads:\n-            if grad is None:\n-                # Filtering is required.\n-                filtered = [\n-                    (g, v) for g, v in zip(grads, vars) if g is not None\n-                ]\n-                if not filtered:\n+        filtered_grads = list(grads)\n+        filtered_vars = list(vars)\n+        missing_grad_vars = []\n+\n+        # Iterate from right to left for safe popping\n+        for i in range(len(filtered_grads) - 1, -1, -1):\n+            if filtered_grads[i] is None:\n+                filtered_grads.pop(i)\n+                v = filtered_vars.pop(i)\n+                missing_grad_vars.append(v.name)\n+\n+        if not filtered_grads:\n             raise ValueError(\"No gradients provided for any variable.\")\n-                if len(filtered) < len(grads):\n-                    missing_grad_vars = [\n-                        v for g, v in zip(grads, vars) if g is None\n-                    ]\n+        if missing_grad_vars:\n             warnings.warn(\n                 \"Gradients do not exist for variables \"\n-                        f\"{[v.name for v in missing_grad_vars]} when \"\n-                        \"minimizing the loss. If using `model.compile()`, \"\n-                        \"did you forget to provide a `loss` argument?\"\n+                f\"{list(reversed(missing_grad_vars))} when minimizing the loss.\"\n+                \" If using `model.compile()`, did you forget to provide a \"\n+                \"`loss` argument?\"\n             )\n-                return zip(*filtered)\n-        return grads, vars\n+        return filtered_grads, filtered_vars\n \n     def _clip_gradients(self, grads):\n         if self.clipnorm and self.clipnorm > 0:\n-            clipped_grads = []\n-            for g in grads:\n-                if g is None:\n-                    clipped_grads.append(g)\n-                else:\n-                    clipped_grads.append(self._clip_by_norm(g))\n-            return clipped_grads\n-\n-        if self.global_clipnorm and self.global_clipnorm > 0:\n+            return [\n+                self._clip_by_norm(g) if g is not None else g for g in grads\n+            ]\n+        elif self.global_clipnorm and self.global_clipnorm > 0:\n             return clip_by_global_norm(grads, self.global_clipnorm)\n-\n-        if self.clipvalue and self.clipvalue > 0:\n-            clipped_grads = []\n-            for g in grads:\n-                if g is None:\n-                    clipped_grads.append(g)\n+        elif self.clipvalue and self.clipvalue > 0:\n+            v = self.clipvalue\n+            return [ops.clip(g, -v, v) if g is not None else g for g in grads]\n         else:\n-                    clipped_grads.append(\n-                        ops.clip(g, -self.clipvalue, self.clipvalue)\n-                    )\n-            return clipped_grads\n             return grads\n \n     def exclude_from_weight_decay(self, var_list=None, var_names=None):\n@@ -623,7 +615,7 @@ class BaseOptimizer:\n         name, then the variable is excluded.\n \n         Args:\n-            var_list: A list of `tf.Variable`s to exclude from weight decay.\n+            var_list: A list of `Variable`s to exclude from weight decay.\n             var_names: A list of strings. If any string in `var_names` appear\n                 in the model variable's name, then this model variable is\n                 excluded from weight decay. For example, `var_names=['bias']`\n@@ -635,28 +627,52 @@ class BaseOptimizer:\n                 \"the optimizer is built.\"\n             )\n \n+        # Use a `set` for the ids of `var_list` to speed up the searching\n         if var_list:\n-            self._exclude_from_weight_decay = [\n+            self._exclude_from_weight_decay = set(\n                 self._var_key(variable) for variable in var_list\n-            ]\n+            )\n         else:\n-            self._exclude_from_weight_decay = []\n-        self._exclude_from_weight_decay_names = var_names or []\n+            self._exclude_from_weight_decay = set()\n+\n+        # Precompile the pattern for `var_names` to speed up the searching\n+        if var_names and len(var_names) > 0:\n+            self._exclude_from_weight_decay_pattern = re.compile(\n+                \"|\".join(set(var_names))\n+            )\n+        else:\n+            self._exclude_from_weight_decay_pattern = None\n+\n+        # Reset cache\n+        self._exclude_from_weight_decay_cache = dict()\n \n     def _use_weight_decay(self, variable):\n-        exclude_from_weight_decay = getattr(\n-            self, \"_exclude_from_weight_decay\", []\n-        )\n-        exclude_from_weight_decay_names = getattr(\n-            self, \"_exclude_from_weight_decay_names\", []\n-        )\n         variable_id = self._var_key(variable)\n-        for exclude_id in exclude_from_weight_decay:\n-            if variable_id == exclude_id:\n+\n+        # Immediately return the value if `variable_id` hits the cache\n+        if not hasattr(self, \"_exclude_from_weight_decay_cache\"):\n+            self._exclude_from_weight_decay_cache = dict()\n+        if variable_id in self._exclude_from_weight_decay_cache:\n+            return self._exclude_from_weight_decay_cache[variable_id]\n+\n+        # Determine whether the variable should apply weight decay or not\n+        exclude_from_weight_decay = getattr(\n+            self, \"_exclude_from_weight_decay\", set()\n+        )\n+        exclude_from_weight_decay_pattern = getattr(\n+            self, \"_exclude_from_weight_decay_pattern\", None\n+        )\n+        if variable_id in exclude_from_weight_decay:\n+            self._exclude_from_weight_decay_cache[variable_id] = False\n             return False\n-        for name in exclude_from_weight_decay_names:\n-            if re.search(name, variable.name) is not None:\n+        if exclude_from_weight_decay_pattern is not None:\n+            if (\n+                re.search(exclude_from_weight_decay_pattern, variable.name)\n+                is not None\n+            ):\n+                self._exclude_from_weight_decay_cache[variable_id] = False\n                 return False\n+        self._exclude_from_weight_decay_cache[variable_id] = True\n         return True\n \n     def _apply_weight_decay(self, variables):\n@@ -868,10 +884,9 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n \n def global_norm(value_list):\n     \"\"\"Computes the global norm of multiple tensors.\"\"\"\n-    squared_norms = []\n-    for v in value_list:\n-        if v is not None:\n-            squared_norms.append(ops.sum(ops.square(v)))\n+    squared_norms = [\n+        ops.sum(ops.square(v)) for v in value_list if v is not None\n+    ]\n     squared_norm = ops.sum(ops.stack(squared_norms))\n     return ops.sqrt(squared_norm)\n \n@@ -883,10 +898,4 @@ def clip_by_global_norm(value_list, clip_norm):\n     # If use_norm is any finite number, this is a no-op. For inf/-inf/NaN,\n     # this will make scale NaN.\n     scale = scale_for_finite + (use_norm - use_norm)\n-    values_clipped = []\n-    for v in value_list:\n-        if v is None:\n-            values_clipped.append(None)\n-        else:\n-            values_clipped.append(v * scale)\n-    return values_clipped\n+    return [v * scale if v is not None else v for v in value_list]\n\n@@ -3,25 +3,24 @@ from keras.api_export import keras_export\n from keras.optimizers import base_optimizer\n \n if backend.backend() == \"tensorflow\":\n-    from keras.backend.tensorflow.optimizer import TFOptimizer\n-\n-    BackendOptimizer = TFOptimizer\n+    from keras.backend.tensorflow.optimizer import (\n+        TFOptimizer as BackendOptimizer,\n+    )\n elif backend.backend() == \"torch\":\n-    from keras.backend.torch.optimizers import TorchOptimizer\n-\n-    BackendOptimizer = TorchOptimizer\n+    from keras.backend.torch.optimizers import (\n+        TorchOptimizer as BackendOptimizer,\n+    )\n elif backend.backend() == \"jax\":\n-    from keras.backend.jax.optimizer import JaxOptimizer\n-\n-    BackendOptimizer = JaxOptimizer\n+    from keras.backend.jax.optimizer import JaxOptimizer as BackendOptimizer\n else:\n-    BackendOptimizer = base_optimizer.BaseOptimizer\n+\n+    class BackendOptimizer(base_optimizer.BaseOptimizer):\n+        pass\n \n \n @keras_export([\"keras.Optimizer\", \"keras.optimizers.Optimizer\"])\n-class Optimizer(BackendOptimizer):\n+class Optimizer(BackendOptimizer, base_optimizer.BaseOptimizer):\n     pass\n \n \n base_optimizer_keyword_args = base_optimizer.base_optimizer_keyword_args\n-Optimizer.__doc__ = base_optimizer.BaseOptimizer.__doc__\n\n@@ -22,6 +22,74 @@ class OptimizerTest(testing.TestCase):\n         optimizer.apply_gradients([(grads, v)])\n         self.assertAllClose(optimizer.iterations, 2)\n \n+    def test_empty_gradients(self):\n+        # Test no valid gradient\n+        v = backend.Variable([[3.0, 4.0], [5.0, 6.0]])\n+        grads = None\n+        optimizer = optimizers.SGD(learning_rate=1.0)\n+        with self.assertRaisesRegexp(\n+            ValueError, \"No gradients provided for any variable.\"\n+        ):\n+            optimizer.apply_gradients([(grads, v)])\n+\n+        # Test filtering of empty gradients\n+        v2 = backend.Variable([[3.0, 4.0], [5.0, 6.0]])\n+        grads2 = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n+        optimizer = optimizers.SGD(learning_rate=1.0)\n+        with self.assertWarns(Warning):\n+            optimizer.apply_gradients([(grads, v), (grads2, v2)])\n+        self.assertAllClose(v, [[3.0, 4.0], [5.0, 6.0]])\n+        self.assertAllClose(v2, [[2.0, 3.0], [4.0, 5.0]])\n+\n+    def test_clip_args(self):\n+        optimizer = optimizers.SGD(learning_rate=1.0, clipnorm=0.1)\n+        self.assertEqual(optimizer.clipnorm, 0.1)\n+        optimizer = optimizers.SGD(learning_rate=1.0, clipvalue=0.1)\n+        self.assertEqual(optimizer.clipvalue, 0.1)\n+        optimizer = optimizers.SGD(learning_rate=1.0, global_clipnorm=0.1)\n+        self.assertEqual(optimizer.global_clipnorm, 0.1)\n+\n+        # Test invalid arguments\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Only one of `clipnorm`, `clipvalue` and `global_clipnorm` can\",\n+        ):\n+            optimizers.SGD(\n+                learning_rate=1.0,\n+                clipnorm=0.1,\n+                clipvalue=0.1,\n+            )\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Only one of `clipnorm`, `clipvalue` and `global_clipnorm` can\",\n+        ):\n+            optimizers.SGD(\n+                learning_rate=1.0,\n+                clipnorm=0.1,\n+                global_clipnorm=0.1,\n+            )\n+\n+    def test_clip_norm(self):\n+        optimizer = optimizers.SGD(clipnorm=1)\n+        grad = backend.convert_to_tensor([100.0, 100.0])\n+        clipped_grad = optimizer._clip_gradients([grad])\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = optimizers.SGD(clipvalue=1)\n+        grad = backend.convert_to_tensor([100.0, 100.0])\n+        clipped_grad = optimizer._clip_gradients([grad])\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n+\n+    def test_global_clip_norm(self):\n+        optimizer = optimizers.SGD(global_clipnorm=1)\n+        grad = np.array([50.0, 100.0], dtype=\"float32\")\n+        global_norm = np.linalg.norm(grad)\n+        clipped_grad = optimizer._clip_gradients(\n+            [backend.convert_to_tensor(grad)]\n+        )\n+        self.assertAllClose(clipped_grad[0], grad / global_norm)\n+\n     def test_ema(self):\n         v = backend.Variable([[3.0, 4.0], [5.0, 6.0]])\n         grads = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a013ebf51c25a5fdf5918b72a2ee497b2964e7d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 105 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -357,9 +357,9 @@ def hard_sigmoid(x):\n \n     The hard sigmoid activation is defined as:\n \n-    - `0` if `if x < -2.5`\n-    - `1` if `x > 2.5`\n-    - `0.2 * x + 0.5` if `-2.5 <= x <= 2.5`\n+    - `0` if `if x <= -3`\n+    - `1` if `x >= 3`\n+    - `(x/6) + 0.5` if `-3 < x < 3`\n \n     It's a faster, piecewise linear approximation\n     of the sigmoid activation.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ad7f9b4e3b49edbefe2fa590ae04f8d9b4ddc5b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 35 | Churn Cumulative: 677 | Contributors (this commit): 10 | Commits (past 90d): 30 | Contributors (cumulative): 16 | DMM Complexity: 0.6428571428571429\n\nDIFF:\n@@ -292,10 +292,10 @@ class Layer(BackendLayer, Operation):\n         self._build_shapes_dict = None\n         # Parent path\n         self._parent_path = None\n-        self._initializer_tracker()\n+        self._initialize_tracker()\n \n     @tracking.no_automatic_dependency_tracking\n-    def _initializer_tracker(self):\n+    def _initialize_tracker(self):\n         if hasattr(self, \"_tracker\"):\n             return\n \n@@ -325,7 +325,8 @@ class Layer(BackendLayer, Operation):\n                     lambda x: isinstance(x, backend.random.SeedGenerator),\n                     seed_generators,\n                 ),\n-            }\n+            },\n+            exclusions={\"non_trainable_variables\": [\"trainable_variables\"]},\n         )\n         if backend.backend() == \"tensorflow\":\n             # Remove attribute tracking for lists (TF-specific attribute)\n@@ -1322,7 +1323,7 @@ class Layer(BackendLayer, Operation):\n         if hasattr(self, \"_tracker\"):\n             value = self._tracker.track(value)\n         elif name != \"_tracker\":\n-            self._initializer_tracker()\n+            self._initialize_tracker()\n         return super().__setattr__(name, value)\n \n     def _check_super_called(self):\n\n@@ -912,10 +912,16 @@ class LayerTest(testing.TestCase):\n         model = models.Model(inputs, outputs)\n \n         self.assertFalse(layer.trainable)\n-        self.assertLen(layer.trainable_variables, 0)\n-        self.assertLen(model.trainable_variables, 0)\n+        self.assertLen(layer._trainable_variables, 2)\n+        self.assertLen(layer._non_trainable_variables, 0)\n+        self.assertLen(layer.trainable_weights, 0)\n+        self.assertLen(model.trainable_weights, 0)\n+        self.assertLen(model.non_trainable_weights, 2)\n \n         layer.trainable = True\n         self.assertTrue(layer.trainable)\n-        self.assertLen(layer.trainable_variables, 2)\n-        self.assertLen(model.trainable_variables, 2)\n+        self.assertLen(layer._trainable_variables, 2)\n+        self.assertLen(layer._non_trainable_variables, 0)\n+        self.assertLen(layer.trainable_weights, 2)\n+        self.assertLen(model.trainable_weights, 2)\n+        self.assertLen(model.non_trainable_weights, 0)\n\n@@ -62,11 +62,12 @@ class Tracker:\n     ```\n     \"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config, exclusions=None):\n         self.config = config\n         self.stored_ids = {name: set() for name in self.config.keys()}\n         self.locked = False\n         self._lock_violation_msg = None\n+        self.exclusions = exclusions or {}\n \n     def track(self, attr):\n         if not is_tracking_enabled():\n@@ -74,7 +75,11 @@ class Tracker:\n \n         for store_name, (is_attr_type, _) in self.config.items():\n             if is_attr_type(attr):\n-                if id(attr) not in self.stored_ids[store_name]:\n+                if store_name in self.exclusions:\n+                    for excl in self.exclusions[store_name]:\n+                        if self.is_in_store(excl, attr):\n+                            return attr\n+                if not self.is_in_store(store_name, attr):\n                     self.add_to_store(store_name, attr)\n                 return attr\n         if isinstance(attr, tuple):\n@@ -112,6 +117,9 @@ class Tracker:\n         self.config[store_name][1].append(value)\n         self.stored_ids[store_name].add(id(value))\n \n+    def is_in_store(self, store_name, value):\n+        return id(value) in self.stored_ids[store_name]\n+\n \n @optree.register_pytree_node_class(namespace=\"keras\")\n class TrackedList(list):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#29130d3a14d4b673339cbdaddb464359b84ba9c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 29 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.2.0\"\n+__version__ = \"3.2.1\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6a9bc4c051f0e4ee5e4ff48f08fd14230036dc46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 144 | Lines Deleted: 100 | Files Changed: 3 | Hunks: 39 | Methods Changed: 11 | Complexity Δ (Sum/Max): 15/8 | Churn Δ: 244 | Churn Cumulative: 9086 | Contributors (this commit): 14 | Commits (past 90d): 49 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,18 +23,27 @@ def add(x1, x2):\n \n \n def bincount(x, weights=None, minlength=0, sparse=False):\n-    if sparse:\n+    # Note: bincount is never tracable / jittable because the output shape\n+    # depends on the values in x.\n+    if sparse or isinstance(x, jax_sparse.BCOO):\n+        if isinstance(x, jax_sparse.BCOO):\n+            if weights is not None:\n+                if not isinstance(weights, jax_sparse.BCOO):\n+                    raise ValueError(\"`x` and `weights` must both be BCOOs\")\n+                if x.indices is not weights.indices:\n+                    # This test works in eager mode only\n+                    if not jnp.all(jnp.equal(x.indices, weights.indices)):\n+                        raise ValueError(\n+                            \"`x` and `weights` BCOOs must have the same indices\"\n+                        )\n+                weights = weights.data\n+            x = x.data\n         reduction_axis = 1 if len(x.shape) > 1 else 0\n         maxlength = jnp.maximum(jnp.max(x) + 1, minlength)\n-        one_hot_encoding = nn.one_hot(x, maxlength, sparse=sparse)\n+        one_hot_encoding = nn.one_hot(x, maxlength, sparse=True)\n         if weights is not None:\n-            split_weights = split(\n-                weights,\n-                weights.shape[reduction_axis],\n-                reduction_axis,\n-            )\n-            stacked_weights = stack(split_weights, axis=reduction_axis)\n-            one_hot_encoding = one_hot_encoding * stacked_weights\n+            expanded_weights = jnp.expand_dims(weights, reduction_axis + 1)\n+            one_hot_encoding = one_hot_encoding * expanded_weights\n \n         outputs = jax_sparse.bcoo_reduce_sum(\n             one_hot_encoding,\n@@ -485,6 +494,7 @@ def diff(a, n=1, axis=-1):\n     return jnp.diff(a, n=n, axis=axis)\n \n \n+@sparse.elementwise_unary(linear=False)\n def digitize(x, bins):\n     x = convert_to_tensor(x)\n     bins = convert_to_tensor(bins)\n\n@@ -1263,8 +1263,11 @@ class Bincount(Operation):\n             dtype = dtypes.result_type(*dtypes_to_resolve)\n         else:\n             dtype = \"int32\"\n+        x_sparse = getattr(x, \"sparse\", False)\n         return KerasTensor(\n-            list(x.shape[:-1]) + [None], dtype=dtype, sparse=self.sparse\n+            list(x.shape[:-1]) + [None],\n+            dtype=dtype,\n+            sparse=x_sparse or self.sparse,\n         )\n \n \n@@ -2072,7 +2075,8 @@ class Digitize(Operation):\n                 f\"`bins` must be a 1D array. Received: bins={bins} \"\n                 f\"with shape bins.shape={bins_shape}\"\n             )\n-        return KerasTensor(x.shape, dtype=\"int32\")\n+        sparse = getattr(x, \"sparse\", False)\n+        return KerasTensor(x.shape, dtype=\"int32\", sparse=sparse)\n \n \n @keras_export([\"keras.ops.digitize\", \"keras.ops.numpy.digitize\"])\n\n@@ -3117,59 +3117,68 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         )\n \n     @parameterized.named_parameters(\n-        [\n-            {\"testcase_name\": \"dense\", \"sparse\": False},\n-            {\"testcase_name\": \"sparse\", \"sparse\": True},\n-        ]\n+        named_product(sparse_input=(False, True), sparse_arg=(False, True))\n     )\n-    def test_bincount(self, sparse):\n-        if sparse and not backend.SUPPORTS_SPARSE_TENSORS:\n+    def test_bincount(self, sparse_input, sparse_arg):\n+        if (sparse_input or sparse_arg) and not backend.SUPPORTS_SPARSE_TENSORS:\n             pytest.skip(\"Backend does not support sparse tensors\")\n+        if testing.tensorflow_uses_gpu():\n+            self.skipTest(\"bincount does not work in tensorflow gpu\")\n+\n+        x = x_np = np.array([1, 1, 2, 3, 2, 4, 4, 6])\n+        weights = weights_np = np.array([0, 0, 3, 2, 1, 1, 4, 2])\n+        if sparse_input:\n+            indices = np.array([[1], [3], [5], [7], [9], [11], [13], [15]])\n+\n             if backend.backend() == \"tensorflow\":\n                 import tensorflow as tf\n \n-            if tf.test.is_gpu_available():\n-                self.skipTest(\"bincount does not work in tensorflow gpu\")\n+                x = tf.SparseTensor(indices, x, (16,))\n+                weights = tf.SparseTensor(indices, weights, (16,))\n+            elif backend.backend() == \"jax\":\n+                from jax.experimental import sparse as jax_sparse\n+\n+                x = jax_sparse.BCOO((x, indices), shape=(16,))\n+                weights = jax_sparse.BCOO((weights, indices), shape=(16,))\n \n-        x = np.array([1, 1, 2, 3, 2, 4, 4, 5])\n-        weights = np.array([0, 0, 3, 2, 1, 1, 4, 2])\n         minlength = 3\n         output = knp.bincount(\n-            x, weights=weights, minlength=minlength, sparse=sparse\n+            x, weights=weights, minlength=minlength, sparse=sparse_arg\n         )\n         self.assertAllClose(\n-            output, np.bincount(x, weights=weights, minlength=minlength)\n+            output, np.bincount(x_np, weights=weights_np, minlength=minlength)\n         )\n-        self.assertSparse(output, sparse)\n+        self.assertSparse(output, sparse_input or sparse_arg)\n         output = knp.Bincount(\n-            weights=weights, minlength=minlength, sparse=sparse\n+            weights=weights, minlength=minlength, sparse=sparse_arg\n         )(x)\n         self.assertAllClose(\n-            output, np.bincount(x, weights=weights, minlength=minlength)\n+            output, np.bincount(x_np, weights=weights_np, minlength=minlength)\n         )\n-        self.assertSparse(output, sparse)\n+        self.assertSparse(output, sparse_input or sparse_arg)\n \n-        x = np.array([[1, 1, 2, 3, 2, 4, 4, 5]])\n-        weights = np.array([[0, 0, 3, 2, 1, 1, 4, 2]])\n-        expected_output = np.array([[0, 0, 4, 2, 5, 2]])\n+        x = knp.expand_dims(x, 0)\n+        weights = knp.expand_dims(weights, 0)\n+\n+        expected_output = np.array([[0, 0, 4, 2, 5, 0, 2]])\n         output = knp.bincount(\n-            x, weights=weights, minlength=minlength, sparse=sparse\n+            x, weights=weights, minlength=minlength, sparse=sparse_arg\n         )\n         self.assertAllClose(output, expected_output)\n-        self.assertSparse(output, sparse)\n+        self.assertSparse(output, sparse_input or sparse_arg)\n         output = knp.Bincount(\n-            weights=weights, minlength=minlength, sparse=sparse\n+            weights=weights, minlength=minlength, sparse=sparse_arg\n         )(x)\n         self.assertAllClose(output, expected_output)\n-        self.assertSparse(output, sparse)\n+        self.assertSparse(output, sparse_input or sparse_arg)\n \n         # test with weights=None\n-        expected_output = np.array([[0, 2, 2, 1, 2, 1]])\n-        output = knp.Bincount(weights=None, minlength=minlength, sparse=sparse)(\n-            x\n-        )\n+        expected_output = np.array([[0, 2, 2, 1, 2, 0, 1]])\n+        output = knp.Bincount(\n+            weights=None, minlength=minlength, sparse=sparse_arg\n+        )(x)\n         self.assertAllClose(output, expected_output)\n-        self.assertSparse(output, sparse)\n+        self.assertSparse(output, sparse_input or sparse_arg)\n \n     def test_broadcast_to(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n@@ -4382,57 +4391,62 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         }\n         for op in ELEMENTWISE_UNARY_OPS\n     ]\n+    OTHER_UNARY_OPS_ARGS = [\n+        (\"digitize\", \"\", {}, {\"bins\": np.array([0.1, 0.2, 1.0])}, (4, 2, 3)),\n+        (\"mean\", \"none\", {\"axis\": None}, {}, (4, 2, 3)),\n+        (\"mean\", \"none_k\", {\"axis\": None, \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"mean\", \"empty\", {\"axis\": ()}, {}, (4, 2, 3)),\n+        (\"mean\", \"empty_k\", {\"axis\": (), \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"mean\", \"0\", {\"axis\": 0}, {}, (4, 2, 3)),\n+        (\"mean\", \"0_k\", {\"axis\": 0, \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"mean\", \"1\", {\"axis\": 1}, {}, (4, 2, 3)),\n+        (\"mean\", \"1_k\", {\"axis\": 1, \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"mean\", \"01\", {\"axis\": (0, 1)}, {}, (4, 2, 3)),\n+        (\"mean\", \"01_k\", {\"axis\": (0, 1), \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"mean\", \"02\", {\"axis\": (1, 2)}, {}, (4, 2, 3)),\n+        (\"mean\", \"02_k\", {\"axis\": (1, 2), \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"mean\", \"all\", {\"axis\": (0, 1, 2)}, {}, (4, 2, 3)),\n+        (\"mean\", \"all_k\", {\"axis\": (0, 1, 2), \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"sum\", \"none\", {\"axis\": None}, {}, (4, 2, 3)),\n+        (\"sum\", \"none_k\", {\"axis\": None, \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"sum\", \"empty\", {\"axis\": ()}, {}, (4, 2, 3)),\n+        (\"sum\", \"empty_k\", {\"axis\": (), \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"sum\", \"0\", {\"axis\": 0}, {}, (4, 2, 3)),\n+        (\"sum\", \"0_k\", {\"axis\": 0, \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"sum\", \"1\", {\"axis\": 1}, {}, (4, 2, 3)),\n+        (\"sum\", \"1_k\", {\"axis\": 1, \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"sum\", \"01\", {\"axis\": (0, 1)}, {}, (4, 2, 3)),\n+        (\"sum\", \"01_k\", {\"axis\": (0, 1), \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"sum\", \"02\", {\"axis\": (1, 2)}, {}, (4, 2, 3)),\n+        (\"sum\", \"02_k\", {\"axis\": (1, 2), \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"sum\", \"all\", {\"axis\": (0, 1, 2)}, {}, (4, 2, 3)),\n+        (\"sum\", \"all_k\", {\"axis\": (0, 1, 2), \"keepdims\": True}, {}, (4, 2, 3)),\n+        (\"expand_dims\", \"zero\", {\"axis\": 0}, {}, (2, 3)),\n+        (\"expand_dims\", \"one\", {\"axis\": 1}, {}, (2, 3)),\n+        (\"expand_dims\", \"minus_two\", {\"axis\": -2}, {}, (2, 3)),\n+        (\"reshape\", \"basic\", {\"newshape\": (4, 3, 2)}, {}, (4, 2, 3)),\n+        (\"reshape\", \"minus_one\", {\"newshape\": (4, 3, -1)}, {}, (4, 2, 3)),\n+        (\"reshape\", \"fewer_dims\", {\"newshape\": (4, 6)}, {}, (4, 2, 3)),\n+        (\"squeeze\", \"no_axis_no_op\", {}, {}, (2, 3)),\n+        (\"squeeze\", \"one\", {\"axis\": 1}, {}, (2, 1, 3)),\n+        (\"squeeze\", \"minus_two\", {\"axis\": -2}, {}, (2, 1, 3)),\n+        (\"squeeze\", \"no_axis\", {}, {}, (2, 1, 3)),\n+        (\"transpose\", \"no_axes\", {}, {}, (1, 2, 3, 4)),\n+        (\"transpose\", \"axes\", {\"axes\": (0, 3, 2, 1)}, {}, (1, 2, 3, 4)),\n+    ]\n     OTHER_UNARY_OPS_TESTS = [\n         {\n             \"testcase_name\": \"_\".join([op, testcase_name]),\n             \"op_function\": getattr(knp, op),\n             \"op_class\": getattr(knp, snake_to_pascal_case(op)),\n             \"np_op\": getattr(np, op),\n+            \"init_kwargs\": init_kwargs,\n             \"op_kwargs\": op_kwargs,\n             \"input_shape\": input_shape,\n         }\n-        for op, testcase_name, op_kwargs, input_shape in [\n-            (\"mean\", \"none\", {\"axis\": None}, (4, 2, 3)),\n-            (\"mean\", \"none_k\", {\"axis\": None, \"keepdims\": True}, (4, 2, 3)),\n-            (\"mean\", \"empty\", {\"axis\": ()}, (4, 2, 3)),\n-            (\"mean\", \"empty_k\", {\"axis\": (), \"keepdims\": True}, (4, 2, 3)),\n-            (\"mean\", \"0\", {\"axis\": 0}, (4, 2, 3)),\n-            (\"mean\", \"0_k\", {\"axis\": 0, \"keepdims\": True}, (4, 2, 3)),\n-            (\"mean\", \"1\", {\"axis\": 1}, (4, 2, 3)),\n-            (\"mean\", \"1_k\", {\"axis\": 1, \"keepdims\": True}, (4, 2, 3)),\n-            (\"mean\", \"01\", {\"axis\": (0, 1)}, (4, 2, 3)),\n-            (\"mean\", \"01_k\", {\"axis\": (0, 1), \"keepdims\": True}, (4, 2, 3)),\n-            (\"mean\", \"02\", {\"axis\": (1, 2)}, (4, 2, 3)),\n-            (\"mean\", \"02_k\", {\"axis\": (1, 2), \"keepdims\": True}, (4, 2, 3)),\n-            (\"mean\", \"all\", {\"axis\": (0, 1, 2)}, (4, 2, 3)),\n-            (\"mean\", \"all_k\", {\"axis\": (0, 1, 2), \"keepdims\": True}, (4, 2, 3)),\n-            (\"sum\", \"none\", {\"axis\": None}, (4, 2, 3)),\n-            (\"sum\", \"none_k\", {\"axis\": None, \"keepdims\": True}, (4, 2, 3)),\n-            (\"sum\", \"empty\", {\"axis\": ()}, (4, 2, 3)),\n-            (\"sum\", \"empty_k\", {\"axis\": (), \"keepdims\": True}, (4, 2, 3)),\n-            (\"sum\", \"0\", {\"axis\": 0}, (4, 2, 3)),\n-            (\"sum\", \"0_k\", {\"axis\": 0, \"keepdims\": True}, (4, 2, 3)),\n-            (\"sum\", \"1\", {\"axis\": 1}, (4, 2, 3)),\n-            (\"sum\", \"1_k\", {\"axis\": 1, \"keepdims\": True}, (4, 2, 3)),\n-            (\"sum\", \"01\", {\"axis\": (0, 1)}, (4, 2, 3)),\n-            (\"sum\", \"01_k\", {\"axis\": (0, 1), \"keepdims\": True}, (4, 2, 3)),\n-            (\"sum\", \"02\", {\"axis\": (1, 2)}, (4, 2, 3)),\n-            (\"sum\", \"02_k\", {\"axis\": (1, 2), \"keepdims\": True}, (4, 2, 3)),\n-            (\"sum\", \"all\", {\"axis\": (0, 1, 2)}, (4, 2, 3)),\n-            (\"sum\", \"all_k\", {\"axis\": (0, 1, 2), \"keepdims\": True}, (4, 2, 3)),\n-            (\"expand_dims\", \"zero\", {\"axis\": 0}, (2, 3)),\n-            (\"expand_dims\", \"one\", {\"axis\": 1}, (2, 3)),\n-            (\"expand_dims\", \"minus_two\", {\"axis\": -2}, (2, 3)),\n-            (\"reshape\", \"basic\", {\"newshape\": (4, 3, 2)}, (4, 2, 3)),\n-            (\"reshape\", \"minus_one\", {\"newshape\": (4, 3, -1)}, (4, 2, 3)),\n-            (\"reshape\", \"fewer_dims\", {\"newshape\": (4, 6)}, (4, 2, 3)),\n-            (\"squeeze\", \"no_axis_no_op\", {}, (2, 3)),\n-            (\"squeeze\", \"one\", {\"axis\": 1}, (2, 1, 3)),\n-            (\"squeeze\", \"minus_two\", {\"axis\": -2}, (2, 1, 3)),\n-            (\"squeeze\", \"no_axis\", {}, (2, 1, 3)),\n-            (\"transpose\", \"no_axes\", {}, (1, 2, 3, 4)),\n-            (\"transpose\", \"axes\", {\"axes\": (0, 3, 2, 1)}, (1, 2, 3, 4)),\n-        ]\n+        for op, testcase_name, init_kwargs, op_kwargs, input_shape in (\n+            OTHER_UNARY_OPS_ARGS\n+        )\n     ]\n \n     BINARY_OPS = [\n@@ -4483,30 +4497,38 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n \n     @parameterized.named_parameters(OTHER_UNARY_OPS_TESTS)\n     def test_other_unary_symbolic_static_shape(\n-        self, op_function, op_class, np_op, op_kwargs, input_shape\n+        self, op_function, op_class, np_op, init_kwargs, op_kwargs, input_shape\n     ):\n         expected_shape = op_function(\n-            KerasTensor(input_shape), **op_kwargs\n+            KerasTensor(input_shape), **init_kwargs, **op_kwargs\n         ).shape\n         x = KerasTensor(input_shape, sparse=True)\n-        self.assertEqual(op_function(x, **op_kwargs).shape, expected_shape)\n-        self.assertTrue(op_function(x, **op_kwargs).sparse)\n-        self.assertEqual(op_class(**op_kwargs)(x).shape, expected_shape)\n-        self.assertTrue(op_class(**op_kwargs)(x).sparse)\n+        self.assertEqual(\n+            op_function(x, **init_kwargs, **op_kwargs).shape, expected_shape\n+        )\n+        self.assertTrue(op_function(x, **init_kwargs, **op_kwargs).sparse)\n+        self.assertEqual(\n+            op_class(**init_kwargs)(x, **op_kwargs).shape, expected_shape\n+        )\n+        self.assertTrue(op_class(**init_kwargs)(x, **op_kwargs).sparse)\n \n     @parameterized.named_parameters(OTHER_UNARY_OPS_TESTS)\n     def test_other_unary_symbolic_dynamic_shape(\n-        self, op_function, op_class, np_op, op_kwargs, input_shape\n+        self, op_function, op_class, np_op, init_kwargs, op_kwargs, input_shape\n     ):\n         input_shape = (None,) + input_shape[1:]\n         expected_shape = op_function(\n-            KerasTensor(input_shape), **op_kwargs\n+            KerasTensor(input_shape), **init_kwargs, **op_kwargs\n         ).shape\n         x = KerasTensor(input_shape, sparse=True)\n-        self.assertEqual(op_function(x, **op_kwargs).shape, expected_shape)\n-        self.assertTrue(op_function(x, **op_kwargs).sparse)\n-        self.assertEqual(op_class(**op_kwargs)(x).shape, expected_shape)\n-        self.assertTrue(op_class(**op_kwargs)(x).sparse)\n+        self.assertEqual(\n+            op_function(x, **init_kwargs, **op_kwargs).shape, expected_shape\n+        )\n+        self.assertTrue(op_function(x, **init_kwargs, **op_kwargs).sparse)\n+        self.assertEqual(\n+            op_class(**init_kwargs)(x, **op_kwargs).shape, expected_shape\n+        )\n+        self.assertTrue(op_class(**init_kwargs)(x, **op_kwargs).sparse)\n \n     @parameterized.named_parameters(DENSIFYING_UNARY_OPS_TESTS)\n     def test_densifying_unary_sparse_correctness(\n@@ -4564,10 +4586,10 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n \n     @parameterized.named_parameters(OTHER_UNARY_OPS_TESTS)\n     @pytest.mark.skipif(\n-        backend.backend() == \"tensorflow\", reason=\"Temporary, XLA error\"\n+        testing.tensorflow_uses_gpu(), reason=\"Temporary, XLA error\"\n     )\n-    def test_other_unary_symbolic_sparse_correctness(\n-        self, op_function, op_class, np_op, op_kwargs, input_shape\n+    def test_other_unary_sparse_correctness(\n+        self, op_function, op_class, np_op, init_kwargs, op_kwargs, input_shape\n     ):\n         x = np.random.random(input_shape)\n         if op_function is knp.mean:\n@@ -4577,14 +4599,22 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         x_np = backend.convert_to_numpy(x)\n \n         self.assertAllClose(\n-            op_function(x, **op_kwargs), np_op(x_np, **op_kwargs)\n+            op_function(x, **init_kwargs, **op_kwargs),\n+            np_op(x_np, **init_kwargs, **op_kwargs),\n+        )\n+        self.assertAllClose(\n+            op_class(**init_kwargs)(x, **op_kwargs),\n+            np_op(x_np, **init_kwargs, **op_kwargs),\n         )\n-        self.assertAllClose(op_class(**op_kwargs)(x), np_op(x_np, **op_kwargs))\n         # Reduction operations have complex and backend dependent rules about\n         # when the result is sparse and it is dense.\n         if op_function is not knp.mean:\n-            self.assertSameSparseness(op_function(x, **op_kwargs), x)\n-            self.assertSameSparseness(op_class(**op_kwargs)(x), x)\n+            self.assertSameSparseness(\n+                op_function(x, **init_kwargs, **op_kwargs), x\n+            )\n+            self.assertSameSparseness(\n+                op_class(**init_kwargs)(x, **op_kwargs), x\n+            )\n \n     @parameterized.named_parameters(\n         named_product(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f7adff1b4d51fd68973ad70f638819f351a068c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 11 | Churn Cumulative: 103 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,13 +23,22 @@ def list_devices(device_type=None):\n     Return:\n         List of devices that are available for distribute computation.\n     \"\"\"\n-    device_type = device_type.upper() if device_type else \"CPU\"\n+    device_type = device_type.upper() if device_type else None\n \n     # DTensor doesn't support getting global devices, even when knowing the\n     # Mesh. Use TF API instead to get global devices. Coordinator service is\n     # enabled by default with DTensor, so that list_logical_devices() returns\n     # a list of global devices. More context can be found in b/254911601.\n     tf_devices = tf.config.list_logical_devices(device_type=device_type)\n+    cpu_devices = []\n+    other_devices = []\n+    for device in tf_devices:\n+        if device.device_type.lower() == \"cpu\":\n+            cpu_devices.append(device)\n+        else:\n+            other_devices.append(device)\n+    if device_type is None:\n+        tf_devices = other_devices if len(other_devices) > 0 else cpu_devices\n     return [\n         f\"{device.device_type.lower()}:{device.name.split(':')[-1]}\"\n         for device in tf_devices\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c73a1160c4bcf4701f6812b12b578879ab642523", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 307 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -128,12 +128,15 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n     ):\n         grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n \n-        def apply_grad_to_update_var(var, grad):\n+        def apply_grad_to_update_var(var, grad, learning_rate):\n             return self.update_step(grad, var, learning_rate)\n \n         for grad, var in grads_and_vars:\n             distribution.extended.update(\n-                var, apply_grad_to_update_var, args=(grad,), group=False\n+                var,\n+                apply_grad_to_update_var,\n+                args=(grad, learning_rate),\n+                group=False,\n             )\n \n     def _all_reduce_sum_gradients(self, grads_and_vars):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
