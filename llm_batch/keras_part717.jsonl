{"custom_id": "keras#5e1c58b60b2c7d7018cc98539c17fe7286e72cce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 283 | Lines Deleted: 7 | Files Changed: 5 | Hunks: 14 | Methods Changed: 25 | Complexity Δ (Sum/Max): 36/16 | Churn Δ: 290 | Churn Cumulative: 1606 | Contributors (this commit): 13 | Commits (past 90d): 20 | Contributors (cumulative): 23 | DMM Complexity: 0.8157894736842105\n\nDIFF:\n@@ -32,12 +32,16 @@ from keras_core.utils.io_utils import print_msg\n if backend() == \"tensorflow\":\n     print_msg(\"Using TensorFlow backend\")\n     from keras_core.backend.tensorflow import *  # noqa: F403\n+\n+    distribution_lib = None\n elif backend() == \"jax\":\n     print_msg(\"Using JAX backend.\")\n     from keras_core.backend.jax import *  # noqa: F403\n elif backend() == \"torch\":\n     print_msg(\"Using PyTorch backend.\")\n     from keras_core.backend.torch import *  # noqa: F403\n+\n+    distribution_lib = None\n elif backend() == \"numpy\":\n     print_msg(\n         \"Using NumPy backend.\\nThe NumPy backend does not support \"\n@@ -45,5 +49,7 @@ elif backend() == \"numpy\":\n         \"and debugging.\"\n     )\n     from keras_core.backend.numpy import *  # noqa: F403\n+\n+    distribution_lib = None\n else:\n     raise ValueError(f\"Unable to import backend : {backend()}\")\n\n@@ -1,4 +1,5 @@\n from keras_core.backend.jax import core\n+from keras_core.backend.jax import distribution_lib\n from keras_core.backend.jax import image\n from keras_core.backend.jax import math\n from keras_core.backend.jax import nn\n\n@@ -0,0 +1,56 @@\n+\"\"\"!!!DO NOT USE!!!\n+\n+Distribution related class for JAX backend.\n+\n+This is just a prototype and we might want to unify it\n+with other backends in the future.\n+\"\"\"\n+import jax\n+\n+\n+def list_devices(device_type=None):\n+    \"\"\"Return all the available devices based on the device type.\n+\n+    Note that this should return the global devices in a distributed setting.\n+\n+    Args:\n+        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Default to `gpu` or\n+            `tpu` if available when device_type is not provided. Otherwise\n+            will return the `cpu` devices.\n+\n+    Return:\n+        List of devices that are available for distribute computation.\n+    \"\"\"\n+    device_type = device_type.lower() if device_type else None\n+    return jax.devices(backend=device_type)\n+\n+\n+def to_jax_mesh(device_mesh):\n+    \"\"\"Convert the DeviceMesh to JAX backend specific Mesh.\n+\n+    Args:\n+        device_mesh: DeviceMesh instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.Mesh` instance.\n+    \"\"\"\n+    return jax.sharding.Mesh(device_mesh.devices, device_mesh.axis_names)\n+\n+\n+def to_jax_layout(tensor_layout):\n+    \"\"\"Convert the TensorLayout to JAX backend specific Sharding.\n+\n+    Args:\n+        tensor_layout: TensorLayout instance to convert.\n+\n+    Returns:\n+        A `jax.sharding.NamedSharding` instance.\n+    \"\"\"\n+    if tensor_layout.device_mesh is None:\n+        raise ValueError(\n+            \"Cannot create sharding when device mesh is not set \"\n+            \"for TensorLayout.\"\n+        )\n+    partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n+    jax_mesh = to_jax_mesh(tensor_layout.device_mesh)\n+    return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n\n@@ -7,7 +7,9 @@ will be implemented in future (via tf.dtensor API).\n \"\"\"\n \n import contextlib\n+import numpy as np\n \n+from keras_core.backend import distribution_lib\n from keras_core.backend.common import global_state\n \n GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n@@ -19,14 +21,14 @@ def list_devices(device_type=None):\n     Note that this should return the global devices in a distributed setting.\n \n     Args:\n-        device_type: string of 'CPU', 'GPU' or 'TPU'. Default to GPU or TPU if\n-            available when device_type is not provided. Otherwise will return\n-            the CPU devices.\n+        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Default to `gpu` or\n+            `tpu` if available when device_type is not provided. Otherwise\n+            will return the `cpu` devices.\n \n     Return:\n         List of devices that are available for distribute computation.\n     \"\"\"\n-    pass\n+    return distribution_lib.list_devices(device_type)\n \n \n class DeviceMesh:\n@@ -61,7 +63,42 @@ class DeviceMesh:\n             devices: Optional list of devices. Default to all the available\n                 devices locally from `list_devices()`.\n         \"\"\"\n-        pass\n+        if not shape or not axis_names:\n+            raise ValueError(\n+                \"Shape and axis_names cannot be empty. Got \"\n+                f\"shape: {shape}, axis_names: {axis_names}\"\n+            )\n+\n+        if len(shape) != len(axis_names):\n+            raise ValueError(\n+                \"Shape and axis_names should have same size,\"\n+                f\"got shape: {shape} and axis_names: {axis_names}\"\n+            )\n+        if not devices:\n+            devices = list_devices()\n+        devices = np.array(devices)\n+        if np.prod(shape) != np.prod(devices.shape):\n+            raise ValueError(\n+                \"Shape does not match the number of devices. \"\n+                f\"Got shape: {shape}, and shape of the \"\n+                f\"devices: {devices.shape}\"\n+            )\n+\n+        self._shape = shape\n+        self._axis_names = axis_names\n+        self._devices = np.reshape(devices, shape)\n+\n+    @property\n+    def shape(self):\n+        return self._shape\n+\n+    @property\n+    def axis_names(self):\n+        return self._axis_names\n+\n+    @property\n+    def devices(self):\n+        return self._devices\n \n \n class TensorLayout:\n@@ -78,15 +115,48 @@ class TensorLayout:\n         https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout).\n     \"\"\"\n \n-    def __init__(self, axes):\n+    def __init__(self, axes, device_mesh=None):\n         \"\"\"Initialize the TensorLayout with axis names.\n \n         Args:\n             axes: list of strings that should map to the `axis_names` in\n                 `DeviceMesh`. For any dimentions that doesn't need any sharding,\n                 A `None` can be used a placeholder.\n+            device_mesh: Optional `DeviceMesh` that will be used to create\n+                the layout. The actual mapping of tensor to physical device\n+                is not known until the mesh is specified.\n         \"\"\"\n-        pass\n+        self._axes = axes\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    @property\n+    def axes(self):\n+        return self._axes\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    @device_mesh.setter\n+    def device_mesh(self, device_mesh):\n+        if self._device_mesh is not None:\n+            raise ValueError(\n+                \"Cannot override device mesh value. Existing \"\n+                f\"value is {self._device_mesh}\"\n+            )\n+        self._device_mesh = device_mesh\n+        self._validate_axes()\n+\n+    def _validate_axes(self):\n+        if self._device_mesh:\n+            valid_axis_names = set(self._device_mesh.axis_names)\n+            axis_names = set(self._axes) - set([None])\n+            if axis_names - valid_axis_names:\n+                raise ValueError(\n+                    \"Invalid axis names for Layout. Valid axis \"\n+                    f\"names: {valid_axis_names}, Got {axis_names}\"\n+                )\n \n \n class Distribution:\n\n@@ -0,0 +1,143 @@\n+\"\"\"Test for distribution_lib.py.\"\"\"\n+\n+import os\n+\n+import jax\n+import pytest\n+\n+from keras_core import backend\n+from keras_core.backend import distribution_lib as backend_dlib\n+from keras_core.distribution import distribution_lib\n+from keras_core import testing\n+\n+if backend.backend() == \"jax\":\n+    # Due to https://github.com/google/jax/issues/17188, we can't\n+    # override the XLA flag after the JAX back init. We have to\n+    # run this at top level to let JAX pick the flag value.\n+    xla_flags = os.getenv(\"XLA_FLAGS\") or \"\"\n+    # Don't override user-specified device count, or other XLA flags.\n+    if \"xla_force_host_platform_device_count\" not in xla_flags:\n+        os.environ[\"XLA_FLAGS\"] = (\n+            xla_flags + \" --xla_force_host_platform_device_count=8\"\n+        )\n+\n+\n+class DeviceMeshTest(testing.TestCase):\n+    def test_mesh_creation(self):\n+        devices = [\"CPU:{i}\" for i in range(8)]\n+        shape = (4, 2)\n+        axis_names = [\"batch\", \"model\"]\n+\n+        mesh = distribution_lib.DeviceMesh(shape, axis_names, devices)\n+        self.assertEqual(mesh.shape, shape)\n+        self.assertEqual(mesh.axis_names, axis_names)\n+        self.assertEqual(mesh.devices.shape, shape)\n+\n+    def test_input_validation(self):\n+        devices = [\"CPU:{i}\" for i in range(4)]\n+        with self.assertRaisesRegex(\n+            ValueError, \"Shape and axis_names cannot be empty\"\n+        ):\n+            distribution_lib.DeviceMesh((4,), \"\", devices)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Shape and axis_names should have same size\"\n+        ):\n+            distribution_lib.DeviceMesh((4, 2), [\"batch\"], devices)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Shape does not match the number of devices\"\n+        ):\n+            distribution_lib.DeviceMesh((4, 2), [\"batch\", \"model\"], devices)\n+\n+\n+class TensorLayoutTest(testing.TestCase):\n+    def setUp(self):\n+        self.mesh = distribution_lib.DeviceMesh(\n+            (4, 2), [\"data\", \"model\"], [f\"CPU:{i}\" for i in range(8)]\n+        )\n+\n+    def test_tensor_layout_creation(self):\n+        axes = [\"data\", None]\n+        layout = distribution_lib.TensorLayout(axes, self.mesh)\n+\n+        self.assertEqual(layout.device_mesh, self.mesh)\n+        self.assertEqual(layout.axes, axes)\n+\n+    def test_tensor_layout_validation(self):\n+        axes = [\"data\", \"unknown\", None]\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid axis names for Layout\"\n+        ):\n+            distribution_lib.TensorLayout(axes, self.mesh)\n+\n+    def test_lazy_device_mesh_injection(self):\n+        axes = [\"data\", None]\n+        layout = distribution_lib.TensorLayout(axes, None)\n+\n+        self.assertIsNone(layout.device_mesh)\n+        self.assertEqual(layout.axes, axes)\n+\n+        layout.device_mesh = self.mesh\n+\n+        self.assertEqual(layout.device_mesh, self.mesh)\n+        self.assertEqual(layout.axes, axes)\n+\n+    def test_lazy_device_mesh_validation(self):\n+        axes = [\"data\", \"unknown\", None]\n+        layout = distribution_lib.TensorLayout(axes, None)\n+\n+        self.assertIsNone(layout.device_mesh)\n+        self.assertEqual(layout.axes, axes)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid axis names for Layout\"\n+        ):\n+            layout.device_mesh = self.mesh\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() != \"jax\",\n+    reason=\"Backend specific test\",\n+)\n+class JaxDistributionLibTest(testing.TestCase):\n+    def test_list_devices(self):\n+        self.assertEqual(len(distribution_lib.list_devices()), 8)\n+        self.assertEqual(len(distribution_lib.list_devices(\"cpu\")), 8)\n+        self.assertEqual(len(distribution_lib.list_devices(\"CPU\")), 8)\n+\n+    def test_to_jax_mesh(self):\n+        devices = [\"CPU:{i}\" for i in range(8)]\n+        shape = (4, 2)\n+        axis_names = [\"batch\", \"model\"]\n+\n+        mesh = distribution_lib.DeviceMesh(shape, axis_names, devices)\n+        jax_mesh = backend_dlib.to_jax_mesh(mesh)\n+\n+        self.assertIsInstance(jax_mesh, jax.sharding.Mesh)\n+        self.assertEqual(jax_mesh.devices.shape, shape)\n+        self.assertEqual(jax_mesh.axis_names, (\"batch\", \"model\"))\n+\n+    def test_to_jax_layout(self):\n+        axes = [\"data\", None]\n+        mesh = distribution_lib.DeviceMesh(\n+            (4, 2), [\"data\", \"model\"], [f\"CPU:{i}\" for i in range(8)]\n+        )\n+        layout = distribution_lib.TensorLayout(axes, mesh)\n+        jax_sharding = backend_dlib.to_jax_layout(layout)\n+        jax_mesh = backend_dlib.to_jax_mesh(mesh)\n+        self.assertEqual(\n+            jax_sharding,\n+            jax.sharding.NamedSharding(\n+                jax_mesh, jax.sharding.PartitionSpec(\"data\", None)\n+            ),\n+        )\n+\n+    def test_validation_for_device_mesh(self):\n+        axes = [\"data\", None]\n+        layout = distribution_lib.TensorLayout(axes, device_mesh=None)\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"Cannot create sharding when device mesh is not set\"\n+        ):\n+            backend_dlib.to_jax_layout(layout)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01e49f770d728860d1fbcd1d44aca4340b5ee68b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 310 | Lines Deleted: 43 | Files Changed: 1 | Hunks: 18 | Methods Changed: 6 | Complexity Δ (Sum/Max): 30/30 | Churn Δ: 353 | Churn Cumulative: 1727 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,11 +1,16 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n \n from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_output_shape,\n+)\n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_padding,\n+)\n \n \n class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n@@ -250,6 +255,279 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    def np_conv1d_transpose(\n+        self,\n+        x,\n+        kernel_weights,\n+        bias_weights,\n+        strides,\n+        padding,\n+        output_padding,\n+        data_format,\n+        dilation_rate,\n+    ):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 1))\n+        if isinstance(strides, (tuple, list)):\n+            h_stride = strides[0]\n+        else:\n+            h_stride = strides\n+        if isinstance(dilation_rate, (tuple, list)):\n+            h_dilation = dilation_rate[0]\n+        else:\n+            h_dilation = dilation_rate\n+\n+        h_kernel, ch_out, ch_in = kernel_weights.shape\n+        n_batch, h_x, _ = x.shape\n+        # Get output shape and padding\n+        _, h_out, _ = compute_conv_transpose_output_shape(\n+            x.shape,\n+            kernel_weights.shape,\n+            ch_out,\n+            strides,\n+            padding,\n+            output_padding,\n+            data_format,\n+            dilation_rate,\n+        )\n+        (h_pad,) = compute_conv_transpose_padding(\n+            x.shape,\n+            kernel_weights.shape,\n+            strides,\n+            padding,\n+            output_padding,\n+            data_format,\n+            dilation_rate,\n+        )\n+        h_pad_side1 = h_kernel - 1 - h_pad[0]\n+\n+        if h_dilation > 1:\n+            # Increase kernel size\n+            new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n+            new_kenel_size_tuple = (new_h_kernel,)\n+            new_kernel_weights = np.zeros(\n+                (*new_kenel_size_tuple, ch_out, ch_in),\n+                dtype=kernel_weights.dtype,\n+            )\n+            new_kernel_weights[::h_dilation] = kernel_weights\n+            kernel_weights = new_kernel_weights\n+            h_kernel = kernel_weights.shape[0]\n+\n+        # Compute output\n+        output = np.zeros([n_batch, h_out + h_kernel, ch_out])\n+        for nb in range(n_batch):\n+            for h_x_idx in range(h_x):\n+                h_out_idx = h_x_idx * h_stride  # Index in output\n+                output[nb, h_out_idx : h_out_idx + h_kernel, :] += np.sum(\n+                    kernel_weights[:, :, :] * x[nb, h_x_idx, :], axis=-1\n+                )\n+        output = output + bias_weights\n+\n+        # Cut padding results from output\n+        output = output[:, h_pad_side1 : h_out + h_pad_side1]\n+        if data_format == \"channels_first\":\n+            output = output.transpose((0, 2, 1))\n+        return output\n+\n+    def np_conv2d_transpose(\n+        self,\n+        x,\n+        kernel_weights,\n+        bias_weights,\n+        strides,\n+        padding,\n+        output_padding,\n+        data_format,\n+        dilation_rate,\n+    ):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 1))\n+        if isinstance(strides, (tuple, list)):\n+            h_stride, w_stride = strides\n+        else:\n+            h_stride = strides\n+            w_stride = strides\n+        if isinstance(dilation_rate, (tuple, list)):\n+            h_dilation, w_dilation = dilation_rate\n+        else:\n+            h_dilation = dilation_rate\n+            w_dilation = dilation_rate\n+\n+        h_kernel, w_kernel, ch_out, ch_in = kernel_weights.shape\n+        n_batch, h_x, w_x, _ = x.shape\n+        # Get output shape and padding\n+        _, h_out, w_out, _ = compute_conv_transpose_output_shape(\n+            x.shape,\n+            kernel_weights.shape,\n+            ch_out,\n+            strides,\n+            padding,\n+            output_padding,\n+            data_format,\n+            dilation_rate,\n+        )\n+        h_pad, w_pad = compute_conv_transpose_padding(\n+            x.shape,\n+            kernel_weights.shape,\n+            strides,\n+            padding,\n+            output_padding,\n+            data_format,\n+            dilation_rate,\n+        )\n+        h_pad_side1 = h_kernel - 1 - h_pad[0]\n+        w_pad_side1 = w_kernel - 1 - w_pad[0]\n+\n+        if h_dilation > 1 or w_dilation > 1:\n+            # Increase kernel size\n+            new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n+            new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n+            new_kenel_size_tuple = (new_h_kernel, new_w_kernel)\n+            new_kernel_weights = np.zeros(\n+                (*new_kenel_size_tuple, ch_out, ch_in),\n+                dtype=kernel_weights.dtype,\n+            )\n+            new_kernel_weights[::h_dilation, ::w_dilation] = kernel_weights\n+            kernel_weights = new_kernel_weights\n+            h_kernel, w_kernel = kernel_weights.shape[:2]\n+\n+        # Compute output\n+        output = np.zeros([n_batch, h_out + h_kernel, w_out + w_kernel, ch_out])\n+        for nb in range(n_batch):\n+            for h_x_idx in range(h_x):\n+                h_out_idx = h_x_idx * h_stride  # Index in output\n+                for w_x_idx in range(w_x):\n+                    w_out_idx = w_x_idx * w_stride\n+                    output[\n+                        nb,\n+                        h_out_idx : h_out_idx + h_kernel,\n+                        w_out_idx : w_out_idx + w_kernel,\n+                        :,\n+                    ] += np.sum(\n+                        kernel_weights[:, :, :, :] * x[nb, h_x_idx, w_x_idx, :],\n+                        axis=-1,\n+                    )\n+        output = output + bias_weights\n+\n+        # Cut padding results from output\n+        output = output[\n+            :,\n+            h_pad_side1 : h_out + h_pad_side1,\n+            w_pad_side1 : w_out + w_pad_side1,\n+        ]\n+        if data_format == \"channels_first\":\n+            output = output.transpose((0, 3, 1, 2))\n+        return output\n+\n+    def np_conv3d_transpose(\n+        self,\n+        x,\n+        kernel_weights,\n+        bias_weights,\n+        strides,\n+        padding,\n+        output_padding,\n+        data_format,\n+        dilation_rate,\n+    ):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 4, 1))\n+        if isinstance(strides, (tuple, list)):\n+            h_stride, w_stride, d_stride = strides\n+        else:\n+            h_stride = strides\n+            w_stride = strides\n+            d_stride = strides\n+        if isinstance(dilation_rate, (tuple, list)):\n+            h_dilation, w_dilation, d_dilation = dilation_rate\n+        else:\n+            h_dilation = dilation_rate\n+            w_dilation = dilation_rate\n+            d_dilation = dilation_rate\n+\n+        h_kernel, w_kernel, d_kernel, ch_out, ch_in = kernel_weights.shape\n+        n_batch, h_x, w_x, d_x, _ = x.shape\n+        # Get output shape and padding\n+        _, h_out, w_out, d_out, _ = compute_conv_transpose_output_shape(\n+            x.shape,\n+            kernel_weights.shape,\n+            ch_out,\n+            strides,\n+            padding,\n+            output_padding,\n+            data_format,\n+            dilation_rate,\n+        )\n+        h_pad, w_pad, d_pad = compute_conv_transpose_padding(\n+            x.shape,\n+            kernel_weights.shape,\n+            strides,\n+            padding,\n+            output_padding,\n+            data_format,\n+            dilation_rate,\n+        )\n+        h_pad_side1 = h_kernel - 1 - h_pad[0]\n+        w_pad_side1 = w_kernel - 1 - w_pad[0]\n+        d_pad_side1 = d_kernel - 1 - d_pad[0]\n+\n+        if h_dilation > 1 or w_dilation > 1 or d_dilation > 1:\n+            # Increase kernel size\n+            new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n+            new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n+            new_d_kernel = d_kernel + (d_dilation - 1) * (d_kernel - 1)\n+            new_kenel_size_tuple = (new_h_kernel, new_w_kernel, new_d_kernel)\n+            new_kernel_weights = np.zeros(\n+                (*new_kenel_size_tuple, ch_out, ch_in),\n+                dtype=kernel_weights.dtype,\n+            )\n+            new_kernel_weights[\n+                ::h_dilation, ::w_dilation, ::d_dilation\n+            ] = kernel_weights\n+            kernel_weights = new_kernel_weights\n+            h_kernel, w_kernel, d_kernel = kernel_weights.shape[:3]\n+\n+        # Compute output\n+        output = np.zeros(\n+            [\n+                n_batch,\n+                h_out + h_kernel,\n+                w_out + w_kernel,\n+                d_out + d_kernel,\n+                ch_out,\n+            ]\n+        )\n+        for nb in range(n_batch):\n+            for h_x_idx in range(h_x):\n+                h_out_idx = h_x_idx * h_stride  # Index in output\n+                for w_x_idx in range(w_x):\n+                    w_out_idx = w_x_idx * w_stride\n+                    for d_x_idx in range(d_x):\n+                        d_out_idx = d_x_idx * d_stride\n+                        output[\n+                            nb,\n+                            h_out_idx : h_out_idx + h_kernel,\n+                            w_out_idx : w_out_idx + w_kernel,\n+                            d_out_idx : d_out_idx + d_kernel,\n+                            :,\n+                        ] += np.sum(\n+                            kernel_weights[:, :, :, :, :]\n+                            * x[nb, h_x_idx, w_x_idx, d_x_idx, :],\n+                            axis=-1,\n+                        )\n+        output = output + bias_weights\n+\n+        # Cut padding results from output\n+        output = output[\n+            :,\n+            h_pad_side1 : h_out + h_pad_side1,\n+            w_pad_side1 : w_out + w_pad_side1,\n+            d_pad_side1 : d_out + d_pad_side1,\n+        ]\n+        if data_format == \"channels_first\":\n+            output = output.transpose((0, 4, 1, 2, 3))\n+        return output\n+\n     @parameterized.parameters(\n         {\n             \"filters\": 5,\n@@ -298,31 +576,27 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n         )\n-        tf_keras_layer = tf.keras.layers.Conv1DTranspose(\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            output_padding=output_padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-        )\n \n         inputs = np.random.normal(size=[2, 8, 4])\n         layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n \n         kernel_shape = layer.kernel.shape\n         kernel_weights = np.random.normal(size=kernel_shape)\n         bias_weights = np.random.normal(size=(filters,))\n         layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n         layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n \n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self.np_conv1d_transpose(\n+            inputs,\n+            kernel_weights,\n+            bias_weights,\n+            strides,\n+            padding,\n+            output_padding,\n+            data_format,\n+            dilation_rate,\n+        )\n         self.assertAllClose(outputs, expected, atol=1e-5)\n \n     @parameterized.parameters(\n@@ -382,31 +656,27 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n         )\n-        tf_keras_layer = tf.keras.layers.Conv2DTranspose(\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            output_padding=output_padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-        )\n \n         inputs = np.random.normal(size=[2, 14, 14, 4])\n         layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n \n         kernel_shape = layer.kernel.shape\n         kernel_weights = np.random.normal(size=kernel_shape)\n         bias_weights = np.random.normal(size=(filters,))\n         layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n         layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n \n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self.np_conv2d_transpose(\n+            inputs,\n+            kernel_weights,\n+            bias_weights,\n+            strides,\n+            padding,\n+            output_padding,\n+            data_format,\n+            dilation_rate,\n+        )\n         self.assertAllClose(outputs, expected, atol=1e-5)\n \n     @parameterized.parameters(\n@@ -457,28 +727,25 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n         )\n-        tf_keras_layer = tf.keras.layers.Conv3DTranspose(\n-            filters=filters,\n-            kernel_size=kernel_size,\n-            strides=strides,\n-            padding=padding,\n-            output_padding=output_padding,\n-            data_format=data_format,\n-            dilation_rate=dilation_rate,\n-        )\n \n         inputs = np.random.normal(size=[2, 8, 8, 8, 4])\n         layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n \n         kernel_shape = layer.kernel.shape\n         kernel_weights = np.random.normal(size=kernel_shape)\n         bias_weights = np.random.normal(size=(filters,))\n         layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n         layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n+\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self.np_conv3d_transpose(\n+            inputs,\n+            kernel_weights,\n+            bias_weights,\n+            strides,\n+            padding,\n+            output_padding,\n+            data_format,\n+            dilation_rate,\n+        )\n         self.assertAllClose(outputs, expected, atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#03f711ae82392c7c6190213d5efbbd409ee5b12c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 1214 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,9 +6,9 @@ import jax\n import pytest\n \n from keras_core import backend\n+from keras_core import testing\n from keras_core.backend import distribution_lib as backend_dlib\n from keras_core.distribution import distribution_lib\n-from keras_core import testing\n \n if backend.backend() == \"jax\":\n     # Due to https://github.com/google/jax/issues/17188, we can't\n\n@@ -827,7 +827,9 @@ class IndexLookup(Layer):\n \n     def _uninitialized_lookup_table(self):\n         with tf.init_scope():\n-            initializer = NullInitializer(self._key_dtype, self._value_dtype)\n+            initializer = get_null_initializer()(\n+                self._key_dtype, self._value_dtype\n+            )\n             return tf.lookup.StaticHashTable(initializer, self._default_value)\n \n     def _lookup_table_from_tokens(self, tokens):\n@@ -955,8 +957,9 @@ class IndexLookup(Layer):\n         return vocabulary.numpy()\n \n \n+def get_null_initializer():\n     class NullInitializer(tf.lookup.KeyValueTensorInitializer):\n-    \"\"\"A placeholder initializer for restoring this layer from a SavedModel.\"\"\"\n+        \"\"\"A placeholder initializer for restoring from a SavedModel.\"\"\"\n \n         def __init__(self, key_dtype, value_dtype):\n             \"\"\"Construct a table initializer object.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5199fdcf04fe809c7057674dba636f0b86aac9a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 40 | Files Changed: 5 | Hunks: 27 | Methods Changed: 15 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 102 | Churn Cumulative: 2708 | Contributors (this commit): 7 | Commits (past 90d): 28 | Contributors (cumulative): 12 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -1,11 +1,11 @@\n import numpy as np\n \n-from keras_core import Model\n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import metrics\n-from keras_core import optimizers\n-import keras_core as keras\n+from keras import Model\n+from keras import layers\n+from keras import losses\n+from keras import metrics\n+from keras import optimizers\n+import keras as keras\n \n keras.config.disable_traceback_filtering()\n \n@@ -43,12 +43,12 @@ model.compile(\n     ],\n )\n \n-# print(\"\\nTrain model\")\n-# history = model.fit(\n-#     x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n-# )\n-# print(\"\\nHistory:\")\n-# print(history.history)\n+print(\"\\nTrain model\")\n+history = model.fit(\n+    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n+)\n+print(\"\\nHistory:\")\n+print(history.history)\n \n print(\"\\nEvaluate model\")\n scores = model.evaluate(x, y, return_dict=True)\n\n@@ -1,10 +1,11 @@\n import numpy as np\n-from tensorflow import nest\n+import tree\n \n from keras_core.backend.common import KerasVariable\n from keras_core.backend.common import standardize_dtype\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n+from keras_core.utils.nest import pack_sequence_as\n \n \n class Variable(KerasVariable):\n@@ -76,7 +77,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                 return None in x.shape\n             return False\n \n-        none_in_shape = any(map(has_none_shape, nest.flatten((args, kwargs))))\n+        none_in_shape = any(map(has_none_shape, tree.flatten((args, kwargs))))\n \n         def convert_keras_tensor_to_numpy(x, fill_value=None):\n             if isinstance(x, KerasTensor):\n@@ -91,7 +92,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                 )\n             return x\n \n-        args_1, kwargs_1 = nest.map_structure(\n+        args_1, kwargs_1 = tree.map_structure(\n             lambda x: convert_keras_tensor_to_numpy(x, fill_value=83),\n             (args, kwargs),\n         )\n@@ -100,14 +101,14 @@ def compute_output_spec(fn, *args, **kwargs):\n         outputs = outputs_1\n \n         if none_in_shape:\n-            args_2, kwargs_2 = nest.map_structure(\n+            args_2, kwargs_2 = tree.map_structure(\n                 lambda x: convert_keras_tensor_to_numpy(x, fill_value=89),\n                 (args, kwargs),\n             )\n             outputs_2 = fn(*args_2, **kwargs_2)\n \n-            flat_out_1 = nest.flatten(outputs_1)\n-            flat_out_2 = nest.flatten(outputs_2)\n+            flat_out_1 = tree.flatten(outputs_1)\n+            flat_out_2 = tree.flatten(outputs_2)\n \n             flat_out = []\n             for x1, x2 in zip(flat_out_1, flat_out_2):\n@@ -116,14 +117,14 @@ def compute_output_spec(fn, *args, **kwargs):\n                     if e != shape[i]:\n                         shape[i] = None\n                 flat_out.append(KerasTensor(shape, standardize_dtype(x1.dtype)))\n-            outputs = nest.pack_sequence_as(outputs_1, flat_out)\n+            outputs = pack_sequence_as(outputs_1, flat_out)\n \n         def convert_numpy_to_keras_tensor(x):\n             if is_tensor(x):\n                 return KerasTensor(x.shape, standardize_dtype(x.dtype))\n             return x\n \n-        output_spec = nest.map_structure(convert_numpy_to_keras_tensor, outputs)\n+        output_spec = tree.map_structure(convert_numpy_to_keras_tensor, outputs)\n     return output_spec\n \n \n\n@@ -10,7 +10,7 @@ from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.export.ExportArchive\")\n-class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n+class ExportArchive:\n     \"\"\"ExportArchive is used to write SavedModel artifacts (e.g. for inference).\n \n     If you have a Keras model or layer that you want to export as SavedModel for\n@@ -80,11 +80,24 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n         self._endpoint_names = []\n         self._endpoint_signatures = {}\n         self.tensorflow_version = tf.__version__\n-        self.variables = []\n-        self.trainable_variables = []\n-        self.non_trainable_variables = []\n \n-    @tf.__internal__.tracking.no_automatic_dependency_tracking\n+        self._tf_trackable = tf.__internal__.tracking.AutoTrackable()\n+        self._tf_trackable.variables = []\n+        self._tf_trackable.trainable_variables = []\n+        self._tf_trackable.non_trainable_variables = []\n+\n+    @property\n+    def variables(self):\n+        return self._tf_trackable.variables\n+\n+    @property\n+    def trainable_variables(self):\n+        return self._tf_trackable.trainable_variables\n+\n+    @property\n+    def non_trainable_variables(self):\n+        return self._tf_trackable.non_trainable_variables\n+\n     def track(self, resource):\n         \"\"\"Track the variables (and other assets) of a layer or model.\"\"\"\n         if not isinstance(resource, tf.__internal__.tracking.Trackable):\n@@ -111,9 +124,13 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n         if isinstance(resource, Layer):\n             # Variables in the lists below are actually part of the trackables\n             # that get saved, because the lists are created in __init__.\n-            self.variables += resource.variables\n-            self.trainable_variables += resource.trainable_variables\n-            self.non_trainable_variables += resource.non_trainable_variables\n+            self._tf_trackable.variables += resource.variables\n+            self._tf_trackable.trainable_variables += (\n+                resource.trainable_variables\n+            )\n+            self._tf_trackable.non_trainable_variables += (\n+                resource.non_trainable_variables\n+            )\n \n     def add_endpoint(self, name, fn, input_signature=None):\n         \"\"\"Register a new serving endpoint.\n@@ -258,7 +275,7 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n                     \"    ],\\n\"\n                     \")\"\n                 )\n-        setattr(self, name, decorated_fn)\n+        setattr(self._tf_trackable, name, decorated_fn)\n         self._endpoint_names.append(name)\n \n     def add_variable_collection(self, name, variables):\n@@ -309,7 +326,7 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n                 \"`tf.Variable` instances. Found instead the following types: \"\n                 f\"{list(set(type(v) for v in variables))}\"\n             )\n-        setattr(self, name, list(variables))\n+        setattr(self._tf_trackable, name, list(variables))\n \n     def write_out(self, filepath, options=None):\n         \"\"\"Write the corresponding SavedModel to disk.\n@@ -342,11 +359,11 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n                 self._endpoint_names[0]\n             )\n         tf.saved_model.save(\n-            self, filepath, options=options, signatures=signatures\n+            self._tf_trackable, filepath, options=options, signatures=signatures\n         )\n         # Print out available endpoints\n         endpoints = \"\\n\\n\".join(\n-            _print_signature(getattr(self, name), name)\n+            _print_signature(getattr(self._tf_trackable, name), name)\n             for name in self._endpoint_names\n         )\n         io_utils.print_msg(\n@@ -358,9 +375,11 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n     def _get_concrete_fn(self, endpoint):\n         \"\"\"Workaround for some SavedModel quirks.\"\"\"\n         if endpoint in self._endpoint_signatures:\n-            return getattr(self, endpoint)\n+            return getattr(self._tf_trackable, endpoint)\n         else:\n-            traces = getattr(self, endpoint)._trackable_children(\"saved_model\")\n+            traces = getattr(self._tf_trackable, endpoint)._trackable_children(\n+                \"saved_model\"\n+            )\n             return list(traces.values())[0]\n \n     def _get_variables_used_by_endpoints(self):\n@@ -372,11 +391,11 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n         # Start by extracting variables from endpoints.\n         fns = [self._get_concrete_fn(name) for name in self._endpoint_names]\n         tvs, ntvs = _list_variables_used_by_fns(fns)\n-        self._all_variables = list(tvs + ntvs)\n+        self._tf_trackable._all_variables = list(tvs + ntvs)\n \n         # Next, track lookup tables.\n         # Hopefully, one day this will be automated at the tf.function level.\n-        self._misc_assets = []\n+        self._tf_trackable._misc_assets = []\n         from keras_core.layers import IntegerLookup\n         from keras_core.layers import StringLookup\n         from keras_core.layers import TextVectorization\n@@ -389,7 +408,7 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n                         trackable,\n                         (IntegerLookup, StringLookup, TextVectorization),\n                     ):\n-                        self._misc_assets.append(trackable)\n+                        self._tf_trackable._misc_assets.append(trackable)\n \n \n def export_model(model, filepath):\n\n@@ -326,7 +326,7 @@ class ExportArchiveTest(testing.TestCase):\n         export_archive.add_variable_collection(\n             \"my_vars\", model.layers[1].weights\n         )\n-        self.assertLen(export_archive.my_vars, 2)\n+        self.assertLen(export_archive._tf_trackable.my_vars, 2)\n         export_archive.write_out(temp_filepath)\n         revived_model = tf.saved_model.load(temp_filepath)\n         self.assertLen(revived_model.my_vars, 2)\n\n@@ -827,7 +827,7 @@ class IndexLookup(Layer):\n \n     def _uninitialized_lookup_table(self):\n         with tf.init_scope():\n-            initializer = get_null_initializer()(\n+            initializer = get_null_initializer(\n                 self._key_dtype, self._value_dtype\n             )\n             return tf.lookup.StaticHashTable(initializer, self._default_value)\n@@ -957,7 +957,7 @@ class IndexLookup(Layer):\n         return vocabulary.numpy()\n \n \n-def get_null_initializer():\n+def get_null_initializer(key_dtype, value_dtype):\n     class NullInitializer(tf.lookup.KeyValueTensorInitializer):\n         \"\"\"A placeholder initializer for restoring from a SavedModel.\"\"\"\n \n@@ -985,6 +985,8 @@ def get_null_initializer():\n             \"\"\"Returns the table initialization op.\"\"\"\n             pass\n \n+    return NullInitializer(key_dtype, value_dtype)\n+\n \n def listify_tensors(x):\n     \"\"\"Convert any tensors or numpy arrays to lists for config serialization.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#55c3ac1c98e5ef4f137134104ffbf2e56d6292df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 20 | Churn Cumulative: 165 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n \n@@ -40,6 +41,14 @@ class Rescaling(TFDataLayer):\n         dtype = self.compute_dtype\n         scale = self.backend.cast(self.scale, dtype)\n         offset = self.backend.cast(self.offset, dtype)\n+        scale_shape = self.backend.core.shape(scale)\n+        if (\n+            len(scale_shape) > 0\n+            and backend.image_data_format() == \"channels_first\"\n+        ):\n+            scale = self.backend.numpy.reshape(\n+                scale, scale_shape + (1,) * (3 - len(scale_shape))\n+            )\n         return self.backend.cast(inputs, dtype) * scale + offset\n \n     def compute_output_shape(self, input_shape):\n\n@@ -2,6 +2,7 @@ import numpy as np\n import pytest\n import tensorflow as tf\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -73,3 +74,13 @@ class RescalingTest(testing.TestCase):\n         ds = tf.data.Dataset.from_tensor_slices(x).batch(3).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n+\n+    def test_rescaling_with_channels_first_and_vector_scale(self):\n+        config = backend.image_data_format()\n+        backend.set_image_data_format(\"channels_first\")\n+        layer = layers.Rescaling(\n+            scale=[1.0 / 255, 1.5 / 255, 2.0 / 255], offset=0.5\n+        )\n+        x = np.random.random((2, 3, 10, 10)) * 255\n+        layer(x)\n+        backend.set_image_data_format(config)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#228c3e1f97f5009fce93681a8161a02461553c6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1369 | Contributors (this commit): 8 | Commits (past 90d): 18 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -763,7 +763,11 @@ def stft(\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return STFT(\n-            sequence_length, sequence_stride, fft_length, center, window\n+            sequence_length=sequence_length,\n+            sequence_stride=sequence_stride,\n+            fft_length=fft_length,\n+            window=window,\n+            center=center,\n         ).symbolic_call(x)\n     return backend.math.stft(\n         x,\n@@ -883,7 +887,11 @@ def istft(\n     \"\"\"\n     if any_symbolic_tensors(x):\n         return ISTFT(\n-            sequence_length, sequence_stride, fft_length, length, center, window\n+            sequence_length=sequence_length,\n+            sequence_stride=sequence_stride,\n+            fft_length=fft_length,\n+            window=window,\n+            center=center,\n         ).symbolic_call(x)\n     return backend.math.istft(\n         x,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#902d23fc9d9f2589cf16409b86e989d0b7c6d871", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 3368 | Contributors (this commit): 12 | Commits (past 90d): 33 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -5282,18 +5282,18 @@ class Squeeze(Operation):\n     def call(self, x):\n         return backend.numpy.squeeze(x, axis=self.axis)\n \n-    def compute_output_spec(self, x, axis=None):\n+    def compute_output_spec(self, x):\n         input_shape = list(x.shape)\n-        if axis is None:\n+        if self.axis is None:\n             output_shape = list(filter((1).__ne__, input_shape))\n             return KerasTensor(output_shape)\n         else:\n-            if input_shape[axis] != 1:\n+            if input_shape[self.axis] != 1:\n                 raise ValueError(\n-                    f\"Cannot squeeze axis {axis}, because the dimension is not \"\n-                    \"1.\"\n+                    f\"Cannot squeeze axis {self.axis}, because the dimension \"\n+                    \"is not 1.\"\n                 )\n-            del input_shape[axis]\n+            del input_shape[self.axis]\n             return KerasTensor(input_shape, dtype=x.dtype)\n \n \n@@ -5310,7 +5310,7 @@ def squeeze(x, axis=None):\n         length 1 removed.\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return Squeeze().symbolic_call(x, axis=axis)\n+        return Squeeze(axis=axis).symbolic_call(x)\n     return backend.numpy.squeeze(x, axis=axis)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d999f51016568c3beb0a4c36753f0c5c0ace90b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 35 | Files Changed: 16 | Hunks: 42 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 71 | Churn Cumulative: 2664 | Contributors (this commit): 14 | Commits (past 90d): 62 | Contributors (cumulative): 57 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ will be implemented in future (via tf.dtensor API).\n \"\"\"\n \n import contextlib\n+\n import numpy as np\n \n from keras_core.backend import distribution_lib\n\n@@ -1,5 +1,5 @@\n import numpy as np\n-import tensorflow as tf\n+from tensorflow import data as tf_data\n \n from keras_core import layers\n from keras_core import testing\n@@ -121,7 +121,7 @@ class CategoryEncodingTest(testing.TestCase):\n                 [0, 1, 0, 0],\n             ]\n         )\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(4).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(4).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, expected_output)\n\n@@ -1,7 +1,7 @@\n import os\n \n import numpy as np\n-import tensorflow as tf\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -73,7 +73,7 @@ class DicretizationTest(testing.TestCase):\n         layer = layers.Discretization(bin_boundaries=[0.0, 0.35, 0.5, 1.0])\n         x = np.array([[-1.0, 0.0, 0.1, 0.2, 0.4, 0.5, 1.0, 1.2, 0.98]])\n         self.assertAllClose(layer(x), np.array([[0, 1, 1, 1, 2, 3, 4, 4, 3]]))\n-        ds = tf.data.Dataset.from_tensor_slices(x).batch(1).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(x).batch(1).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, np.array([[0, 1, 1, 1, 2, 3, 4, 4, 3]]))\n@@ -84,7 +84,7 @@ class DicretizationTest(testing.TestCase):\n             np.random.random((32, 3)),\n         )\n         x = np.array([[0.0, 0.1, 0.3]])\n-        ds = tf.data.Dataset.from_tensor_slices(x).batch(1).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(x).batch(1).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n \n\n@@ -2,8 +2,8 @@ import os\n \n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -403,7 +403,7 @@ class IndexLookupLayerTest(testing.TestCase, parameterized.TestCase):\n \n     def test_adapt_with_tf_data(self):\n         # Case: adapt + list inputs\n-        adapt_data = tf.data.Dataset.from_tensor_slices(\n+        adapt_data = tf_data.Dataset.from_tensor_slices(\n             [\"one\", \"one\", \"one\", \"two\", \"two\", \"three\"]\n         ).batch(2)\n         input_data = [\"one\", \"two\", \"four\"]\n\n@@ -1,5 +1,5 @@\n import numpy as np\n-import tensorflow as tf\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -101,7 +101,7 @@ class IntegerLookupTest(testing.TestCase):\n             vocabulary=[1, 2, 3, 4],\n         )\n         input_data = [2, 3, 4, 5]\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(4).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(4).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, np.array([2, 3, 4, 0]))\n\n@@ -1,7 +1,7 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -64,7 +64,7 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         elif input_type == \"tensor\":\n             data = backend.convert_to_tensor(x)\n         elif input_type == \"tf.data\":\n-            data = tf.data.Dataset.from_tensor_slices(x).batch(8)\n+            data = tf_data.Dataset.from_tensor_slices(x).batch(8)\n \n         layer = layers.Normalization()\n         layer.adapt(data)\n@@ -81,7 +81,7 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         elif input_type == \"tensor\":\n             data = backend.convert_to_tensor(x)\n         elif input_type == \"tf.data\":\n-            data = tf.data.Dataset.from_tensor_slices(x).batch(8)\n+            data = tf_data.Dataset.from_tensor_slices(x).batch(8)\n \n         layer = layers.Normalization(axis=(1, 2))\n         layer.adapt(data)\n\n@@ -1,6 +1,6 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -55,6 +55,6 @@ class RandomBrightnessTest(testing.TestCase):\n     def test_tf_data_compatibility(self):\n         layer = layers.RandomBrightness(factor=0.5, seed=1337)\n         input_data = np.random.random((2, 8, 8, 3))\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n\n@@ -1,6 +1,6 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -42,6 +42,6 @@ class RandomContrastTest(testing.TestCase):\n     def test_tf_data_compatibility(self):\n         layer = layers.RandomContrast(factor=0.5, seed=1337)\n         input_data = np.random.random((2, 8, 8, 3))\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n\n@@ -1,5 +1,5 @@\n import numpy as np\n-import tensorflow as tf\n+from tensorflow import data as tf_data\n \n from keras_core import layers\n from keras_core import testing\n@@ -68,7 +68,7 @@ class RandomCropTest(testing.TestCase):\n     def test_tf_data_compatibility(self):\n         layer = layers.RandomCrop(8, 9)\n         input_data = np.random.random((2, 10, 12, 3))\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertEqual(list(output.shape), [2, 8, 9, 3])\n\n@@ -1,8 +1,8 @@\n import unittest.mock\n \n import numpy as np\n-import tensorflow as tf\n from absl.testing import parameterized\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -134,7 +134,7 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n         layer = layers.RandomFlip(\"vertical\", seed=42)\n         input_data = np.array([[[2, 3, 4]], [[5, 6, 7]]])\n         expected_output = np.array([[[5, 6, 7]], [[2, 3, 4]]])\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, expected_output)\n@@ -158,7 +158,7 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n                 [[[5, 6, 7]], [[2, 3, 4]]],\n             ]\n         )\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, expected_output)\n\n@@ -1,6 +1,6 @@\n import numpy as np\n-import tensorflow as tf\n from absl.testing import parameterized\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -55,7 +55,7 @@ class RandomRotationTest(testing.TestCase, parameterized.TestCase):\n         input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n         layer = layers.RandomRotation(factor=(0.5, 0.5))\n \n-        ds = tf.data.Dataset.from_tensor_slices(input_image).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_image).map(layer)\n         expected_output = np.asarray(\n             [\n                 [24, 23, 22, 21, 20],\n\n@@ -1,6 +1,6 @@\n import numpy as np\n-import tensorflow as tf\n from absl.testing import parameterized\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -377,6 +377,6 @@ class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n     def test_tf_data_compatibility(self):\n         layer = layers.RandomTranslation(0.2, 0.1)\n         input_data = np.random.random((1, 4, 4, 3))\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(1).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(1).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n\n@@ -1,6 +1,6 @@\n import numpy as np\n-import tensorflow as tf\n from absl.testing import parameterized\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -95,7 +95,7 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n             interpolation=\"nearest\",\n             fill_mode=\"constant\",\n         )\n-        ds = tf.data.Dataset.from_tensor_slices(input_image).batch(1).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_image).batch(1).map(layer)\n         expected_output = np.asarray(\n             [\n                 [0, 0, 0, 0, 0],\n\n@@ -1,6 +1,6 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -71,7 +71,7 @@ class RescalingTest(testing.TestCase):\n     def test_tf_data_compatibility(self):\n         layer = layers.Rescaling(scale=1.0 / 255, offset=0.5)\n         x = np.random.random((3, 10, 10, 3)) * 255\n-        ds = tf.data.Dataset.from_tensor_slices(x).batch(3).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(x).batch(3).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n \n\n@@ -1,5 +1,5 @@\n import numpy as np\n-import tensorflow as tf\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -54,7 +54,7 @@ class StringLookupTest(testing.TestCase):\n             vocabulary=[\"a\", \"b\", \"c\"],\n         )\n         input_data = [\"b\", \"c\", \"d\"]\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(3).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(3).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, np.array([2, 3, 0]))\n\n@@ -1,6 +1,6 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n+from tensorflow import data as tf_data\n \n from keras_core import backend\n from keras_core import layers\n@@ -71,7 +71,7 @@ class TextVectorizationTest(testing.TestCase):\n             vocabulary=[\"baz\", \"bar\", \"foo\"],\n         )\n         input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, np.array([[4, 1, 3, 0], [1, 2, 0, 0]]))\n@@ -83,7 +83,7 @@ class TextVectorizationTest(testing.TestCase):\n             output_sequence_length=max_len,\n         )\n         layer.adapt(input_data)\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n \n@@ -103,4 +103,4 @@ class TextVectorizationTest(testing.TestCase):\n                 layers.Embedding(5, 4),\n             ]\n         )\n-        model(tf.convert_to_tensor([[\"foo qux bar\"], [\"qux baz\"]]))\n+        model(backend.convert_to_tensor([[\"foo qux bar\"], [\"qux baz\"]]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
