{"custom_id": "keras#e61436934cd39bfd5a07f95943ae99c957c13fad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 2 | Churn Cumulative: 7396 | Contributors (this commit): 100 | Commits (past 90d): 15 | Contributors (cumulative): 100 | DMM Complexity: None\n\nDIFF:\n@@ -1253,7 +1253,7 @@ class ModelCheckpoint(Callback):\n       options: Optional `tf.train.CheckpointOptions` object if\n         `save_weights_only` is true or optional `tf.saved_model.SaveOptions`\n         object if `save_weights_only` is false.\n-      initial_best: Initial \"best\" value of the metric to be monitored.\n+      initial_value_threshold: Initial \"best\" value of the metric to be monitored.\n         Only overwrites the model weights already saved if the\n         performance of current model is better than this value.\n       **kwargs: Additional arguments for backwards compatibility. Possible key\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c719b1a2229d554dd7714c22ff7a5a8a84e0ae7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1565 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -639,34 +639,34 @@ def Stage(block_type, depth, group_width, filters_in, filters_out, name=None):\n                  filters_out,\n                  group_width,\n                  stride=2,\n-                 name=name + \"_XBlock_0\")(x)\n-      for i in range(depth - 1):\n+                 name=f\"{name}_XBlock_0\")(x)\n+      for i in range(1, depth):\n         x = XBlock(filters_out,\n                    filters_out,\n                    group_width,\n-                   name=name + \"_XBlock_\" + str(i+1))(x)\n+                   name=f\"{name}_XBlock_{i}\")(x)\n     elif block_type == \"Y\":\n       x = YBlock(filters_in,\n                  filters_out,\n                  group_width,\n                  stride=2,\n                  name=name + \"_YBlock_0\")(x)\n-      for i in range(depth - 1):\n+      for i in range(1, depth):\n         x = YBlock(filters_out,\n                    filters_out,\n                    group_width,\n-                   name=name + \"_YBlock_\" + str(i+1))(x)\n+                   name=f\"{name}_YBlock_{i}\")(x)\n     elif block_type == \"Z\":\n       x = ZBlock(filters_in,\n                  filters_out,\n                  group_width,\n                  stride=2,\n-                 name=name + \"_ZBlock_0\")(x)\n-      for i in range(depth - 1):\n+                 name=f\"{name}_ZBlock_0\")(x)\n+      for i in range(1, depth):\n         x = ZBlock(filters_out,\n                    filters_out,\n                    group_width,\n-                   name=name + \"_ZBlock_\" + str(i+1))(x)\n+                   name=f\"{name}_ZBlock_{i}\")(x)\n     else:\n       raise NotImplementedError(f\"\"\"Block type `{block_type}` not recognized. \n                                 block_type must be one of (\"X\", \"Y\", \"Z\"). \"\"\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e48b3af446a1025f0b4976a952f0094b7f7f3a8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1181 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -96,7 +96,7 @@ class ExponentialDecay(LearningRateSchedule):\n   the training progresses. This schedule applies an exponential decay function\n   to an optimizer step, given a provided initial learning rate.\n \n-  The schedule a 1-arg callable that produces a decayed learning\n+  The schedule is a 1-arg callable that produces a decayed learning\n   rate when passed the current optimizer step. This can be useful for changing\n   the learning rate value across different invocations of optimizer functions.\n   It is computed as:\n@@ -451,7 +451,7 @@ class InverseTimeDecay(LearningRateSchedule):\n   It requires a `step` value to compute the decayed learning rate. You can\n   just pass a TensorFlow variable that you increment at each training step.\n \n-  The schedule a 1-arg callable that produces a decayed learning\n+  The schedule is a 1-arg callable that produces a decayed learning\n   rate when passed the current optimizer step. This can be useful for changing\n   the learning rate value across different invocations of optimizer functions.\n   It is computed as:\n@@ -561,7 +561,7 @@ class CosineDecay(LearningRateSchedule):\n   It requires a `step` value to compute the decayed learning rate. You can\n   just pass a TensorFlow variable that you increment at each training step.\n \n-  The schedule a 1-arg callable that produces a decayed learning\n+  The schedule is a 1-arg callable that produces a decayed learning\n   rate when passed the current optimizer step. This can be useful for changing\n   the learning rate value across different invocations of optimizer functions.\n   It is computed as:\n@@ -655,7 +655,7 @@ class CosineDecayRestarts(LearningRateSchedule):\n   It requires a `step` value to compute the decayed learning rate. You can\n   just pass a TensorFlow variable that you increment at each training step.\n \n-  The schedule a 1-arg callable that produces a decayed learning\n+  The schedule is a 1-arg callable that produces a decayed learning\n   rate when passed the current optimizer step. This can be useful for changing\n   the learning rate value across different invocations of optimizer functions.\n \n@@ -788,7 +788,7 @@ class LinearCosineDecay(LearningRateSchedule):\n   It requires a `step` value to compute the decayed learning rate. You can\n   just pass a TensorFlow variable that you increment at each training step.\n \n-  The schedule a 1-arg callable that produces a decayed learning\n+  The schedule is a 1-arg callable that produces a decayed learning\n   rate when passed the current optimizer step. This can be useful for changing\n   the learning rate value across different invocations of optimizer functions.\n   It is computed as:\n@@ -906,7 +906,7 @@ class NoisyLinearCosineDecay(LearningRateSchedule):\n   It requires a `step` value to compute the decayed learning rate. You can\n   just pass a TensorFlow variable that you increment at each training step.\n \n-  The schedule a 1-arg callable that produces a decayed learning\n+  The schedule is a 1-arg callable that produces a decayed learning\n   rate when passed the current optimizer step. This can be useful for changing\n   the learning rate value across different invocations of optimizer functions.\n   It is computed as:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#acfed7be3d22594b5159f423d583b73fbc9dba08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 267 | Lines Deleted: 8 | Files Changed: 7 | Hunks: 14 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/1 | Churn Δ: 275 | Churn Cumulative: 8614 | Contributors (this commit): 34 | Commits (past 90d): 27 | Contributors (cumulative): 60 | DMM Complexity: 1.0\n\nDIFF:\n@@ -203,6 +203,11 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n      array([1.], dtype=float32),\n      array([2.], dtype=float32)]\n \n+    `adapt()` is meant only as a single machine utility to compute layer state.\n+    To analyze a dataset that cannot fit on a single machine, see\n+    [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n+    for a multi-machine, map-reduce solution.\n+\n     Arguments:\n         data: The data to train on. It can be passed either as a tf.data\n           Dataset, or as a numpy array.\n\n@@ -33,7 +33,13 @@ class Embedding(Layer):\n \n   e.g. `[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]`\n \n-  This layer can only be used as the first layer in a model.\n+  This layer can only be used on positive integer inputs of a fixed range. The\n+  `tf.keras.layers.TextVectorization`, `tf.keras.layers.StringLookup`,\n+  and `tf.keras.layers.IntegerLookup` preprocessing layers can help prepare\n+  inputs for an `Embedding` layer.\n+\n+  This layer accepts `tf.Tensor` and `tf.RaggedTensor` inputs. It cannot be\n+  called with `tf.SparseTensor` input.\n \n   Example:\n \n\n@@ -149,9 +149,9 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     bin_boundaries: A list of bin boundaries. The leftmost and rightmost bins\n       will always extend to `-inf` and `inf`, so `bin_boundaries=[0., 1., 2.]`\n       generates bins `(-inf, 0.)`, `[0., 1.)`, `[1., 2.)`, and `[2., +inf)`. If\n-      this option is set, `adapt` should not be called.\n+      this option is set, `adapt()` should not be called.\n     num_bins: The integer number of bins to compute. If this option is set,\n-      `adapt` should be called to learn the bin boundaries.\n+      `adapt()` should be called to learn the bin boundaries.\n     epsilon: Error tolerance, typically a small fraction close to zero (e.g.\n       0.01). Higher values of epsilon increase the quantile approximation, and\n       hence result in more unequal buckets, but could improve performance\n@@ -274,6 +274,52 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n         initializer=lambda shape, dtype: [[], []],  # pylint: disable=unused-arguments\n         trainable=False)\n \n+  # We override this method solely to generate a docstring.\n+  def adapt(self, data, batch_size=None, steps=None):\n+    \"\"\"Computes bin boundaries from quantiles in a input dataset.\n+\n+    Calling `adapt()` on a `Discretization` layer is an alternative to passing\n+    in a `bin_boundaries` argument during construction. A `Discretization` layer\n+    should always be either adapted over a dataset or passed `bin_boundaries`.\n+\n+    During `adapt()`, the layer will estimate the quantile boundaries of the\n+    input dataset. The number of quantiles can be controlled via the `num_bins`\n+    argument, and the error tolerance for quantile boundaries can be controlled\n+    via the `epsilon` argument.\n+\n+    In order to make `Discretization` efficient in any distribution context, the\n+    computed boundaries are kept static with respect to any compiled `tf.Graph`s\n+    that call the layer. As a consequence, if the layer is adapted a second\n+    time, any models using the layer should be re-compiled. For more information\n+    see `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+\n+    `adapt()` is meant only as a single machine utility to compute layer state.\n+    To analyze a dataset that cannot fit on a single machine, see\n+    [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n+    for a multi-machine, map-reduce solution.\n+\n+    Arguments:\n+      data: The data to train on. It can be passed either as a\n+          `tf.data.Dataset`, or as a numpy array.\n+      batch_size: Integer or `None`.\n+          Number of samples per state update.\n+          If unspecified, `batch_size` will default to 32.\n+          Do not specify the `batch_size` if your data is in the\n+          form of datasets, generators, or `keras.utils.Sequence` instances\n+          (since they generate batches).\n+      steps: Integer or `None`.\n+          Total number of steps (batches of samples)\n+          When training with input tensors such as\n+          TensorFlow data tensors, the default `None` is equal to\n+          the number of samples in your dataset divided by\n+          the batch size, or 1 if that cannot be determined. If x is a\n+          `tf.data` dataset, and 'steps' is None, the epoch will run until\n+          the input dataset is exhausted. When passing an infinitely\n+          repeating dataset, you must specify the `steps` argument. This\n+          argument is not supported with array inputs.\n+    \"\"\"\n+    super().adapt(data, batch_size=batch_size, steps=steps)\n+\n   def update_state(self, data):\n     if self.input_bin_boundaries is not None:\n       raise ValueError(\n\n@@ -85,7 +85,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n       file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\n       tensor containing the integer vocbulary terms. If passing a file path, the\n       file should contain one line per term in the vocabulary. If this argument\n-      is set, there is no need to `adapt` the layer.\n+      is set, there is no need to `adapt()` the layer.\n     vocabulary_dtype: The dtype of the vocabulary terms, for example\n       `\"int64\"` or `\"int32\"`. Defaults to `\"int64\"`.\n     idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list, 1D\n@@ -380,3 +380,53 @@ class IntegerLookup(index_lookup.IndexLookup):\n         pad_to_max_tokens=pad_to_max_tokens,\n         **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\"IntegerLookup\").set(True)\n+\n+  # We override this method solely to generate a docstring.\n+  def adapt(self, data, batch_size=None, steps=None):\n+    \"\"\"Computes a vocabulary of interger terms from tokens in a dataset.\n+\n+    Calling `adapt()` on an `IntegerLookup` layer is an alternative to passing\n+    in a precomputed vocabulary  on construction via the `vocabulary` argument.\n+    An `IntegerLookup` layer should always be either adapted over a dataset or\n+    supplied with a vocabulary.\n+\n+    During `adapt()`, the layer will build a vocabulary of all integer tokens\n+    seen in the dataset, sorted by occurance count, with ties broken by sort\n+    order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n+    is set, the voculary wil be truncated to `max_tokens` size. For example,\n+    adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n+    tokens occurring in the input dataset. If `output_mode='tf-idf'`, `adapt()`\n+    will also learn the document frequencies of each token in the input dataset.\n+\n+    In order to make `StringLookup` efficient in any distribution context, the\n+    vocabulary is kept static with respect to any compiled `tf.Graph`s that\n+    call the layer. As a consequence, if the layer is adapted a second time,\n+    any models using the layer should be re-compiled. For more information\n+    see `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+\n+    `adapt()` is meant only as a single machine utility to compute layer state.\n+    To analyze a dataset that cannot fit on a single machine, see\n+    [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n+    for a multi-machine, map-reduce solution.\n+\n+    Arguments:\n+      data: The data to train on. It can be passed either as a\n+          `tf.data.Dataset`, or as a numpy array.\n+      batch_size: Integer or `None`.\n+          Number of samples per state update.\n+          If unspecified, `batch_size` will default to 32.\n+          Do not specify the `batch_size` if your data is in the\n+          form of datasets, generators, or `keras.utils.Sequence` instances\n+          (since they generate batches).\n+      steps: Integer or `None`.\n+          Total number of steps (batches of samples)\n+          When training with input tensors such as\n+          TensorFlow data tensors, the default `None` is equal to\n+          the number of samples in your dataset divided by\n+          the batch size, or 1 if that cannot be determined. If x is a\n+          `tf.data` dataset, and 'steps' is None, the epoch will run until\n+          the input dataset is exhausted. When passing an infinitely\n+          repeating dataset, you must specify the `steps` argument. This\n+          argument is not supported with array inputs.\n+    \"\"\"\n+    super().adapt(data, batch_size=batch_size, steps=steps)\n\n@@ -193,6 +193,54 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n       self.mean = tf.cast(mean, self.compute_dtype)\n       self.variance = tf.cast(variance, self.compute_dtype)\n \n+  # We override this method solely to generate a docstring.\n+  def adapt(self, data, batch_size=None, steps=None):\n+    \"\"\"Computes the mean and variance of values in a dataset.\n+\n+    Calling `adapt()` on a `Normalization` layer is an alternative to passing in\n+    `mean` and `variance` arguments during layer construction. A `Normalization`\n+    layer should always either be adapted over a dataset or passed `mean` and\n+    `variance`.\n+\n+    During `adapt()`, the layer will compute a `mean` and `variance` separately\n+    for each position in each axis specified by the `axis` argument. To\n+    calculate a single `mean` and `variance` over the input data, simply pass\n+    `axis=None`.\n+\n+    In order to make `Normalization` efficient in any distribution context, the\n+    computed mean and variance are kept static with respect to any compiled\n+    `tf.Graph`s that call the layer. As a consequence, if the layer is adapted a\n+    second time, any models using the layer should be re-compiled. For more\n+    information see\n+    `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+\n+    `adapt()` is meant only as a single machine utility to compute layer state.\n+    To analyze a dataset that cannot fit on a single machine, see\n+    [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n+    for a multi-machine, map-reduce solution.\n+\n+    Arguments:\n+      data: The data to train on. It can be passed either as a\n+          `tf.data.Dataset`, or as a numpy array.\n+      batch_size: Integer or `None`.\n+          Number of samples per state update.\n+          If unspecified, `batch_size` will default to 32.\n+          Do not specify the `batch_size` if your data is in the\n+          form of datasets, generators, or `keras.utils.Sequence` instances\n+          (since they generate batches).\n+      steps: Integer or `None`.\n+          Total number of steps (batches of samples)\n+          When training with input tensors such as\n+          TensorFlow data tensors, the default `None` is equal to\n+          the number of samples in your dataset divided by\n+          the batch size, or 1 if that cannot be determined. If x is a\n+          `tf.data` dataset, and 'steps' is None, the epoch will run until\n+          the input dataset is exhausted. When passing an infinitely\n+          repeating dataset, you must specify the `steps` argument. This\n+          argument is not supported with array inputs.\n+    \"\"\"\n+    super().adapt(data, batch_size=batch_size, steps=steps)\n+\n   def update_state(self, data):\n     if self.input_mean is not None:\n       raise ValueError(\n\n@@ -31,7 +31,9 @@ class StringLookup(index_lookup.IndexLookup):\n   \"\"\"A preprocessing layer which maps string features to integer indices.\n \n   This layer translates a set of arbitrary strings into integer output via a\n-  table-based vocabulary lookup.\n+  table-based vocabulary lookup. This layer will perform no splitting or\n+  transformation of input strings. For a layer than can split and tokenize\n+  natural language, see the `TextVectorization` layer.\n \n   The vocabulary for the layer must be either supplied on construction or\n   learned via `adapt()`. During `adapt()`, the layer will analyze a data set,\n@@ -80,7 +82,7 @@ class StringLookup(index_lookup.IndexLookup):\n       file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\n       tensor containing the string vocbulary terms. If passing a file path, the\n       file should contain one line per term in the vocabulary. If this argument\n-      is set, there is no need to `adapt` the layer.\n+      is set, there is no need to `adapt()` the layer.\n     idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list, 1D\n       numpy array, or 1D tensor or the same length as the vocabulary, containing\n       the floating point inverse document frequency weights, which will be\n@@ -343,6 +345,56 @@ class StringLookup(index_lookup.IndexLookup):\n     del base_config[\"vocabulary_dtype\"]\n     return dict(list(base_config.items()) + list(config.items()))\n \n+  # We override this method solely to generate a docstring.\n+  def adapt(self, data, batch_size=None, steps=None):\n+    \"\"\"Computes a vocabulary of string terms from tokens in a dataset.\n+\n+    Calling `adapt()` on a `StringLookup` layer is an alternative to passing in\n+    a precomputed vocabulary on construction via the `vocabulary` argument. A\n+    `StringLookup` layer should always be either adapted over a dataset or\n+    supplied with a vocabulary.\n+\n+    During `adapt()`, the layer will build a vocabulary of all string tokens\n+    seen in the dataset, sorted by occurance count, with ties broken by sort\n+    order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n+    is set, the voculary wil be truncated to `max_tokens` size. For example,\n+    adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n+    tokens occurring in the input dataset. If `output_mode='tf-idf'`, `adapt()`\n+    will also learn the document frequencies of each token in the input dataset.\n+\n+    In order to make `StringLookup` efficient in any distribution context, the\n+    vocabulary is kept static with respect to any compiled `tf.Graph`s that\n+    call the layer. As a consequence, if the layer is adapted a second time,\n+    any models using the layer should be re-compiled. For more information\n+    see `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+\n+    `adapt()` is meant only as a single machine utility to compute layer state.\n+    To analyze a dataset that cannot fit on a single machine, see\n+    [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n+    for a multi-machine, map-reduce solution.\n+\n+    Arguments:\n+      data: The data to train on. It can be passed either as a\n+          `tf.data.Dataset`, or as a numpy array.\n+      batch_size: Integer or `None`.\n+          Number of samples per state update.\n+          If unspecified, `batch_size` will default to 32.\n+          Do not specify the `batch_size` if your data is in the\n+          form of datasets, generators, or `keras.utils.Sequence` instances\n+          (since they generate batches).\n+      steps: Integer or `None`.\n+          Total number of steps (batches of samples)\n+          When training with input tensors such as\n+          TensorFlow data tensors, the default `None` is equal to\n+          the number of samples in your dataset divided by\n+          the batch size, or 1 if that cannot be determined. If x is a\n+          `tf.data` dataset, and 'steps' is None, the epoch will run until\n+          the input dataset is exhausted. When passing an infinitely\n+          repeating dataset, you must specify the `steps` argument. This\n+          argument is not supported with array inputs.\n+    \"\"\"\n+    super().adapt(data, batch_size=batch_size, steps=steps)\n+\n   # Overridden methods from IndexLookup.\n   def _tensor_vocab_to_numpy(self, vocabulary):\n     vocabulary = vocabulary.numpy()\n\n@@ -57,7 +57,9 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n   a batch of strings (one example = one string) into either a list of token\n   indices (one example = 1D tensor of integer token indices) or a dense\n   representation (one example = 1D tensor of float values representing data\n-  about the example's tokens).\n+  about the example's tokens). This layer is meant to handle natural language\n+  inputs. To handle simple string inputs (categorical strings or pre-tokenized\n+  strings) see `tf.keras.layers.StringLookup`.\n \n   The vocabulary for the layer must be either supplied on construction or\n   learned via `adapt()`. When this layer is adapted, it will analyze the\n@@ -156,7 +158,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n       file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\n       tensor containing the string vocbulary terms. If passing a file path, the\n       file should contain one line per term in the vocabulary. If this argument\n-      is set, there is no need to `adapt` the layer.\n+      is set, there is no need to `adapt()` the layer.\n     idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list, 1D\n       numpy array, or 1D tensor or the same length as the vocabulary, containing\n       the floating point inverse document frequency weights, which will be\n@@ -375,6 +377,56 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n                     else backend.floatx())\n     return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n+  # We override this method solely to generate a docstring.\n+  def adapt(self, data, batch_size=None, steps=None):\n+    \"\"\"Computes a vocabulary of string terms from tokens in a dataset.\n+\n+    Calling `adapt()` on a `TextVectorization` layer is an alternative to\n+    passing in a precomputed vocabulary on construction via the `vocabulary`\n+    argument. A `TextVectorization` layer should always be either adapted over a\n+    dataset or supplied with a vocabulary.\n+\n+    During `adapt()`, the layer will build a vocabulary of all string tokens\n+    seen in the dataset, sorted by occurance count, with ties broken by sort\n+    order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n+    is set, the voculary wil be truncated to `max_tokens` size. For example,\n+    adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n+    tokens occurring in the input dataset. If `output_mode='tf-idf'`, `adapt()`\n+    will also learn the document frequencies of each token in the input dataset.\n+\n+    In order to make `TextVectorization` efficient in any distribution context,\n+    the vocabulary is kept static with respect to any compiled `tf.Graph`s that\n+    call the layer. As a consequence, if the layer is adapted a second time,\n+    any models using the layer should be re-compiled. For more information\n+    see `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+\n+    `adapt()` is meant only as a single machine utility to compute layer state.\n+    To analyze a dataset that cannot fit on a single machine, see\n+    [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n+    for a multi-machine, map-reduce solution.\n+\n+    Arguments:\n+      data: The data to train on. It can be passed either as a\n+          `tf.data.Dataset`, or as a numpy array.\n+      batch_size: Integer or `None`.\n+          Number of samples per state update.\n+          If unspecified, `batch_size` will default to 32.\n+          Do not specify the `batch_size` if your data is in the\n+          form of datasets, generators, or `keras.utils.Sequence` instances\n+          (since they generate batches).\n+      steps: Integer or `None`.\n+          Total number of steps (batches of samples)\n+          When training with input tensors such as\n+          TensorFlow data tensors, the default `None` is equal to\n+          the number of samples in your dataset divided by\n+          the batch size, or 1 if that cannot be determined. If x is a\n+          `tf.data` dataset, and 'steps' is None, the epoch will run until\n+          the input dataset is exhausted. When passing an infinitely\n+          repeating dataset, you must specify the `steps` argument. This\n+          argument is not supported with array inputs.\n+    \"\"\"\n+    super().adapt(data, batch_size=batch_size, steps=steps)\n+\n   def update_state(self, data):\n     self._lookup_layer.update_state(self._preprocess(data))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c178f0d4a803ed68d6f9a6e32eba35bab1a2fa1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 13 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 3079 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -61,6 +61,8 @@ class Resizing(base_layer.Layer):\n \n   This layer resizes an image input to a target height and width. The input\n   should be a 4D (batched) or 3D (unbatched) tensor in `\"channels_last\"` format.\n+  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and of\n+  interger or floating point dtype. By default, the layer will output floats.\n \n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -142,6 +144,9 @@ class CenterCrop(base_layer.Layer):\n   return the largest possible window in the image that matches the target aspect\n   ratio.\n \n+  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+  of interger or floating point dtype. By default, the layer will output floats.\n+\n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n@@ -217,6 +222,9 @@ class RandomCrop(base_layer.BaseRandomLayer):\n   to apply random cropping at inference time, set `training` to True when\n   calling the layer.\n \n+  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+  of interger or floating point dtype. By default, the layer will output floats.\n+\n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n@@ -289,13 +297,15 @@ class Rescaling(base_layer.Layer):\n \n   For instance:\n \n-  1. To rescale an input in the `[0, 255]` range\n+  1. To rescale an input in the ``[0, 255]`` range\n   to be in the `[0, 1]` range, you would pass `scale=1./255`.\n \n-  2. To rescale an input in the `[0, 255]` range to be in the `[-1, 1]` range,\n+  2. To rescale an input in the ``[0, 255]`` range to be in the `[-1, 1]` range,\n   you would pass `scale=1./127.5, offset=-1`.\n \n-  The rescaling is applied both during training and inference.\n+  The rescaling is applied both during training and inference. Inputs can be\n+  of integer or floating point dtype, and by default the layer will output\n+  floats.\n \n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -349,6 +359,9 @@ class RandomFlip(base_layer.BaseRandomLayer):\n   `mode` attribute. During inference time, the output will be identical to\n   input. Call the layer with `training=True` to flip the input.\n \n+  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+  of interger or floating point dtype. By default, the layer will output floats.\n+\n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n@@ -440,6 +453,9 @@ class RandomTranslation(base_layer.BaseRandomLayer):\n   This layer will apply random translations to each image during training,\n   filling empty space according to `fill_mode`.\n \n+  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+  of interger or floating point dtype. By default, the layer will output floats.\n+\n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n@@ -771,6 +787,9 @@ class RandomRotation(base_layer.BaseRandomLayer):\n   At inference time, the layer does nothing. If you need to apply random\n   rotations at inference time, set `training` to True when calling the layer.\n \n+  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+  of interger or floating point dtype. By default, the layer will output floats.\n+\n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n@@ -894,6 +913,9 @@ class RandomZoom(base_layer.BaseRandomLayer):\n   This layer will randomly zoom in or out on each axis of an image\n   independently, filling empty space according to `fill_mode`.\n \n+  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+  of interger or floating point dtype. By default, the layer will output floats.\n+\n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n@@ -1105,6 +1127,9 @@ class RandomContrast(base_layer.BaseRandomLayer):\n   channel and then adjusts each component `x` of each pixel to\n   `(x - mean) * contrast_factor + mean`.\n \n+  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+  of interger or floating point dtype. By default, the layer will output floats.\n+\n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n@@ -1179,7 +1204,10 @@ class RandomHeight(base_layer.BaseRandomLayer):\n \n   This layer adjusts the height of a batch of images by a random factor.\n   The input should be a 3D (unbatched) or 4D (batched) tensor in the\n-  `\"channels_last\"` image data format.\n+  `\"channels_last\"` image data format. Input pixel values can be of any range\n+  (e.g. `[0., 1.)` or `[0, 255]`) and of interger or floating point dtype. By\n+  default, the layer will output floats.\n+\n \n   By default, this layer is inactive during inference.\n \n@@ -1287,7 +1315,9 @@ class RandomWidth(base_layer.BaseRandomLayer):\n \n   This layer will randomly adjusts the width of a batch of images of a\n   batch of images by a random factor. The input should be a 3D (unbatched) or\n-  4D (batched) tensor in the `\"channels_last\"` image data format.\n+  4D (batched) tensor in the `\"channels_last\"` image data format. Input pixel\n+  values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and of interger or\n+  floating point dtype. By default, the layer will output floats.\n \n   By default, this layer is inactive during inference.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f34e136e917eb0e8ec9403f719bbaee5ba2ab1fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2192 | Contributors (this commit): 29 | Commits (past 90d): 1 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -191,12 +191,14 @@ class KerasClassifier(BaseWrapper):\n   \"\"\"Implementation of the scikit-learn classifier API for Keras.\n \n   DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n+  See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n   \"\"\"\n \n   def __init__(self, build_fn=None, **sk_params):\n     warnings.warn(\n         'KerasClassifier is deprecated, '\n-        'use Sci-Keras (https://github.com/adriangb/scikeras) instead.',\n+        'use Sci-Keras (https://github.com/adriangb/scikeras) instead. '\n+        'See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.',\n         DeprecationWarning,\n         stacklevel=2)\n     super().__init__(build_fn, **sk_params)\n@@ -327,13 +329,15 @@ class KerasRegressor(BaseWrapper):\n   \"\"\"Implementation of the scikit-learn regressor API for Keras.\n \n   DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n+  See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n   \"\"\"\n \n   @doc_controls.do_not_doc_inheritable\n   def __init__(self, build_fn=None, **sk_params):\n     warnings.warn(\n         'KerasRegressor is deprecated, '\n-        'use Sci-Keras (https://github.com/adriangb/scikeras) instead.',\n+        'use Sci-Keras (https://github.com/adriangb/scikeras) instead. '\n+        'See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.',\n         DeprecationWarning,\n         stacklevel=2)\n     super().__init__(build_fn, **sk_params)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2604af8ea1dfaa90fa975c4c669c922e10ef6243", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 53 | Churn Cumulative: 2132 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: 0.7647058823529411\n\nDIFF:\n@@ -23,6 +23,7 @@ from keras import backend\n from keras.engine import base_layer\n from keras.utils import control_flow_util\n import tensorflow.compat.v2 as tf\n+from tensorflow.python.platform import tf_logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -40,6 +41,9 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n       flow of information from the future towards the past.\n     dropout: Float between 0 and 1. Fraction of the units to drop for the\n       attention scores.\n+    return_attention_scores: bool, if `True`, layer returns the attention scores.\n+      If `True`, the layer is incompatible with TimeDistributed wrapper.\n+      Default is `False`.\n \n   Call Args:\n \n@@ -58,7 +62,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n         `mask==False` do not contribute to the result.\n     training: Python boolean indicating whether the layer should behave in\n       training mode (adding dropout) or in inference mode (no dropout).\n-    return_attention_scores: bool, it `True`, returns the attention scores\n+    return_attention_scores: bool, (deprecated) if `True`, returns the attention scores\n       (after masking and softmax) as an additional output argument.\n \n   Output:\n@@ -68,10 +72,11 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n       `[batch_size, Tq, Tv]`.\n   \"\"\"\n \n-  def __init__(self, causal=False, dropout=0.0, **kwargs):\n+  def __init__(self, causal=False, dropout=0.0, return_attention_scores=False, **kwargs):\n     super(BaseDenseAttention, self).__init__(**kwargs)\n     self.causal = causal\n     self.dropout = dropout\n+    self.return_attention_scores = return_attention_scores\n     self.supports_masking = True\n \n   def _calculate_scores(self, query, key):\n@@ -137,7 +142,16 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n            inputs,\n            mask=None,\n            training=None,\n-           return_attention_scores=False):\n+           return_attention_scores=None):\n+    if return_attention_scores is not None:\n+      tf_logging.warn(\n+        'return_attention_scores in call method is deprecated, '\n+        'you can set it in constructor. Now return_attention_scores '\n+        'in call method will override return_attention_scores value that '\n+        'has been set in constructor. It could cause unexpected behavior.',\n+        DeprecationWarning)\n+      self.return_attention_scores = return_attention_scores\n+\n     self._validate_call_args(inputs=inputs, mask=mask)\n     q = inputs[0]\n     v = inputs[1]\n@@ -167,7 +181,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n       # Mask of shape [batch_size, Tq, 1].\n       q_mask = tf.expand_dims(q_mask, axis=-1)\n       result *= tf.cast(q_mask, dtype=result.dtype)\n-    if return_attention_scores:\n+    if self.return_attention_scores:\n       return result, attention_scores\n     return result\n \n@@ -180,6 +194,15 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n       return tf.convert_to_tensor(q_mask)\n     return None\n \n+  def compute_output_shape(self, input_shape):\n+     attention_outputs_shape = tf.TensorShape(input_shape[0])\n+     values_shape = tf.TensorShape(input_shape[1])\n+     if self.return_attention_scores:\n+       attention_scores_shape = tf.TensorShape(\n+         attention_outputs_shape[:2] + [values_shape[2]])\n+       return attention_outputs_shape, attention_scores_shape\n+     return attention_outputs_shape\n+\n   def _validate_call_args(self, inputs, mask):\n     \"\"\"Validates arguments of the call method.\"\"\"\n     class_name = self.__class__.__name__\n\n@@ -514,6 +514,28 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n         epochs=1,\n         batch_size=3)\n \n+  def test_TimeDistributed_Attention(self):\n+     query_input = keras.layers.Input(shape=(None, 1, 10), dtype='float32')\n+     value_input = keras.layers.Input(shape=(None, 4, 10), dtype='float32')\n+\n+     # Query-value attention of shape [batch_size, Tq, filters].\n+     query_value_attention_seq = keras.layers.TimeDistributed(\n+       keras.layers.Attention())([query_input, value_input])\n+     model = keras.models.Model(\n+       [query_input, value_input], query_value_attention_seq)\n+     model.compile(optimizer='rmsprop', loss='mse')\n+     model.fit(\n+       [np.random.random((10, 8, 1, 10)), np.random.random((10, 8, 4, 10))],\n+       np.random.random((10, 8, 1, 10)),\n+       epochs=1,\n+       batch_size=10\n+     )\n+\n+     # test config and serialization/deserialization\n+     model.get_config()\n+     model = keras.models.model_from_json(model.to_json())\n+     model.summary()\n+\n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#341716eec11e1dd125d5fb0fbdb6168865f86abe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 568 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -158,6 +158,10 @@ class MinimizeLossStepTest(tf.test.TestCase, parameterized.TestCase):\n \n     def appending_creator(next_creator, **kwargs):\n       v = next_creator(**kwargs)\n+      # Skip the StateVar created in the tf.random.Generator, which is used by\n+      # keras initializers.\n+      if \"StateVar\" in v.name:\n+        return v\n       created_variables.append(v.name)\n       if \"trainable\" in kwargs and kwargs[\"trainable\"]:\n         trainable_variables.append(v.name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f7a02700c666c4246ac5d2d0c1f7b7d022ca5b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 18 | Churn Cumulative: 1433 | Contributors (this commit): 3 | Commits (past 90d): 26 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -547,8 +547,11 @@ class Optimizer(_BaseOptimizer):\n     if self._ema_option and self._ema_option.use_ema:\n       _, var_list = zip(*grads_and_vars)\n       self._update_model_variables_moving_average(var_list)\n+\n+      if self._ema_option.ema_overwrite_frequency:\n+        # Only when self._ema_option.ema_overwrite_frequency is not None, we\n+        # overwrite the model variables.\n         should_overwrite_model_vars = (\n-          self._ema_option and self._ema_option.ema_overwrite_frequency and\n             self.iterations % self._ema_option.ema_overwrite_frequency == 0)\n         tf.cond(\n             should_overwrite_model_vars,\n\n@@ -72,8 +72,8 @@ class EMAOption:\n       if ema_momentum > 1 or ema_momentum < 0:\n         raise ValueError(\"`ema_momentum` must be in the range [0, 1]. \"\n                          f\"Received: ema_momentum={ema_momentum}\")\n-      if ema_overwrite_frequency and not isinstance(\n-          ema_overwrite_frequency, int) or ema_overwrite_frequency < 1:\n+      if ema_overwrite_frequency and (not isinstance(\n+          ema_overwrite_frequency, int) or ema_overwrite_frequency < 1):\n         raise ValueError(\n             \"`ema_overwrite_frequency` must be an integer > 1 or None. \"\n             f\"Received: ema_overwrite_frequency={ema_overwrite_frequency}\")\n\n@@ -36,7 +36,11 @@ STRATEGIES = [\n ]\n \n adadelta_new_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"experimentaladadelta\", lambda: adadelta_new.Adadelta(0.002))\n+    \"experimentaladadelta\",\n+    lambda: adadelta_new.Adadelta(  # pylint: disable=g-long-lambda\n+        0.002,\n+        ema_option=optimizer_lib.EMAOption(\n+            use_ema=True, ema_overwrite_frequency=None)))\n adagrad_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladagrad\", lambda: adagrad_new.Adagrad(0.002))\n adam_new_fn = tf.__internal__.test.combinations.NamedObject(\n@@ -45,7 +49,8 @@ sgd_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalsgdaverage\",\n     lambda: sgd_new.SGD(  # pylint: disable=g-long-lambda\n         0.002,\n-        ema_option=optimizer_lib.EMAOption(use_ema=True)))\n+        ema_option=optimizer_lib.EMAOption(\n+            use_ema=True, ema_overwrite_frequency=1)))\n \n OPTIMIZER_FN = [\n     adadelta_new_fn,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fb05277973379da77376499eabb54e7bd5be52cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 11 | Methods Changed: 11 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 88 | Churn Cumulative: 1653 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -216,7 +216,7 @@ MODEL_CONFIGS = {\n     }\n }\n \n-BASE_DOCSTRING = \"\"\"Instantiates Regnet architecture.\n+BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n \n   References:\n     - [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n@@ -385,8 +385,10 @@ def XBlock(filters_in, filters_out, group_width, stride=1, name=None):\n   def apply(inputs):\n     if filters_in != filters_out and stride == 1:\n       raise ValueError(\n-          \"\"\"Input filters and output filters are not equal for stride\n-                          1. Please check inputs and try again.\"\"\")\n+          f\"Input filters({filters_in}) and output filters({filters_out}) \"\n+          f\"are not equal for stride {stride}. Input and output filters must \"\n+          f\"be equal for stride={stride}.\"\n+      )\n \n     # Declare layers\n     groups = filters_out // group_width\n@@ -470,8 +472,9 @@ def YBlock(filters_in,\n   def apply(inputs):\n     if filters_in != filters_out and stride == 1:\n       raise ValueError(\n-          f\"\"\"Input filters({filters_in}) and output filters({filters_out}) are not equal for stride\n-                          {stride}. Input and output filters must be equal for stride={stride}.\"\"\"\n+          f\"Input filters({filters_in}) and output filters({filters_out}) \"\n+          f\"are not equal for stride {stride}. Input and output filters must  \"\n+          f\"be equal for stride={stride}.\"\n       )\n \n     groups = filters_out // group_width\n@@ -513,7 +516,7 @@ def YBlock(filters_in,\n                                   name=name + \"_conv_3x3_bn\")(x)\n     x = layers.ReLU(name=name + \"_conv_3x3_relu\")(x)\n \n-    # Squeeze-Excitation block (https://arxiv.org/abs/1709.01507)\n+    # Squeeze-Excitation block\n     x = SqueezeAndExciteBlock(filters_out, se_filters, name=name)(x)\n \n     # conv_1x1_2\n@@ -561,8 +564,9 @@ def ZBlock(filters_in,\n   def apply(inputs):\n     if filters_in != filters_out and stride == 1:\n       raise ValueError(\n-          f\"\"\"Input filters({filters_in}) and output filters({filters_out}) are not equal for stride\n-                          {stride}. Input and output filters must be equal for stride={stride}.\"\"\"\n+          f\"Input filters({filters_in}) and output filters({filters_out})\"\n+          f\"are not equal for stride {stride}. Input and output filters must be\"\n+          f\" equal for stride={stride}.\"\n       )\n \n     groups = filters_out // group_width\n@@ -594,7 +598,7 @@ def ZBlock(filters_in,\n                                   name=name + \"_conv_3x3_bn\")(x)\n     x = tf.nn.silu(x)\n \n-    # Squeeze-Excitation block (https://arxiv.org/abs/1709.01507)\n+    # Squeeze-Excitation block\n     x = SqueezeAndExciteBlock(inv_btlneck_filters, se_filters, name=name)\n \n     # conv_1x1_2\n@@ -668,8 +672,8 @@ def Stage(block_type, depth, group_width, filters_in, filters_out, name=None):\n                    group_width,\n                    name=f\"{name}_ZBlock_{i}\")(x)\n     else:\n-      raise NotImplementedError(f\"\"\"Block type `{block_type}` not recognized. \n-                                block_type must be one of (\"X\", \"Y\", \"Z\"). \"\"\")\n+      raise NotImplementedError(f\"Block type `{block_type}` not recognized.\" \n+                                f\"block_type must be one of (`X`, `Y`, `Z`). \")\n     return x\n \n   return apply\n@@ -758,6 +762,7 @@ def RegNet(depths,\n   \n   Returns:\n     A `keras.Model` instance.\n+  \n   Raises:\n       ValueError: in case of invalid argument for `weights`,\n         or invalid input shape.\n@@ -834,10 +839,10 @@ def RegNet(depths,\n   if weights == \"imagenet\":\n     if include_top:\n       file_suffix = \".h5\"\n-      file_hash = WEIGHTS_HASHES[model_name[-2:]][0]\n+      file_hash = WEIGHTS_HASHES[model_name[-4:]][0]\n     else:\n       file_suffix = \"_notop.h5\"\n-      file_hash = WEIGHTS_HASHES[model_name[-2:]][1]\n+      file_hash = WEIGHTS_HASHES[model_name[-4:]][1]\n     file_name = model_name + file_suffix\n     weights_path = data_utils.get_file(file_name,\n                                        BASE_WEIGHTS_PATH + file_name,\n@@ -1547,3 +1552,60 @@ def RegNetY320(model_name=\"regnety320\",\n       classes=classes,\n       classifier_activation=classifier_activation\n   )\n+\n+\n+RegNetX002.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX002\")\n+RegNetX004.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX004\")\n+RegNetX006.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX006\")\n+RegNetX008.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX008\")\n+RegNetX016.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX016\")\n+RegNetX032.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX032\")\n+RegNetX040.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX040\")\n+RegNetX064.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX064\")\n+RegNetX080.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX080\")\n+RegNetX120.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX120\")\n+RegNetX160.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX160\")\n+RegNetX320.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX320\")\n+\n+RegNetY002.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY002\")\n+RegNetY004.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY004\")\n+RegNetY006.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY006\")\n+RegNetY008.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY008\")\n+RegNetY016.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY016\")\n+RegNetY032.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY032\")\n+RegNetY040.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY040\")\n+RegNetY064.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY064\")\n+RegNetY080.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY080\")\n+RegNetY120.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY120\")\n+RegNetY160.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY160\")\n+RegNetY320.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY320\")\n+\n+\n+@keras_export('keras.applications.regnet.preprocess_input')\n+def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n+  \"\"\"A placeholder method for backward compatibility.\n+\n+  The preprocessing logic has been included in the efficientnet model\n+  implementation. Users are no longer required to call this method to normalize\n+  the input data. This method does nothing and only kept as a placeholder to\n+  align the API surface between old and new version of model.\n+\n+  Args:\n+    x: A floating point `numpy.array` or a `tf.Tensor`.\n+    data_format: Optional data format of the image tensor/array. Defaults to\n+      None, in which case the global setting\n+      `tf.keras.backend.image_data_format()` is used (unless you changed it,\n+      it defaults to \"channels_last\").{mode}\n+\n+  Returns:\n+    Unchanged `numpy.array` or `tf.Tensor`.\n+  \"\"\"\n+  return x\n+\n+\n+@keras_export('keras.applications.regnet.decode_predictions')\n+def decode_predictions(preds, top=5):\n+  return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
