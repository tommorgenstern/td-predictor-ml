{"custom_id": "keras#0cdd15d917cfb5078889140ff8a985bf1e349053", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 278 | Lines Deleted: 257 | Files Changed: 17 | Hunks: 57 | Methods Changed: 51 | Complexity Δ (Sum/Max): -20/0 | Churn Δ: 535 | Churn Cumulative: 6810 | Contributors (this commit): 11 | Commits (past 90d): 255 | Contributors (cumulative): 82 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,7 +38,7 @@ model.predict_on_batch(x)\n inputs = keras_core.Input(shape=(32, 32, 3))\n outputs = layers.Conv2D(filters=10, kernel_size=3)(inputs)\n outputs = layers.GlobalAveragePooling2D()(outputs)\n-outputs = layers.Dense(1, activation=\"sigmoid\")(outputs)\n+outputs = layers.Dense(1)(outputs)\n model = keras_core.Model(inputs, outputs)\n model.compile(\n     loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", \"accuracy\"]\n\n@@ -15,7 +15,6 @@ from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.backend.common.variables import AutocastScope\n from keras_core.backend.common.variables import get_autocast_scope\n from keras_core.backend.common.variables import is_float_dtype\n-from keras_core.backend.common.variables import is_int_dtype\n from keras_core.backend.common.variables import standardize_dtype\n from keras_core.backend.common.variables import standardize_shape\n from keras_core.backend.config import epsilon\n\n@@ -3,7 +3,6 @@ from keras_core.backend.common.variables import AutocastScope\n from keras_core.backend.common.variables import KerasVariable\n from keras_core.backend.common.variables import get_autocast_scope\n from keras_core.backend.common.variables import is_float_dtype\n-from keras_core.backend.common.variables import is_int_dtype\n from keras_core.backend.common.variables import standardize_dtype\n from keras_core.backend.common.variables import standardize_shape\n from keras_core.random import random\n\n@@ -460,11 +460,6 @@ def is_float_dtype(dtype):\n     return dtype.startswith(\"float\") or dtype.startswith(\"bfloat\")\n \n \n-def is_int_dtype(dtype):\n-    dtype = standardize_dtype(dtype)\n-    return dtype.startswith(\"int\") or dtype.startswith(\"uint\")\n-\n-\n def get_autocast_scope():\n     return global_state.get_global_attribute(\"autocast_scope\")\n \n\n@@ -202,12 +202,14 @@ def scatter_update(inputs, indices, updates):\n     return inputs\n \n \n-def slice(inputs, start_indices, shape):\n-    return jax.lax.dynamic_slice(inputs, start_indices, shape)\n-\n-\n-def slice_update(inputs, start_indices, updates):\n-    return jax.lax.dynamic_update_slice(inputs, updates, start_indices)\n+def block_update(inputs, start_indices, updates):\n+    update_shape = updates.shape\n+    slices = [\n+        slice(start_index, start_index + update_length)\n+        for start_index, update_length in zip(start_indices, update_shape)\n+    ]\n+    inputs[tuple(slices)] = updates\n+    return inputs\n \n \n def while_loop(\n\n@@ -7,6 +7,26 @@ from keras_core.random.seed_generator import make_default_seed\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Draw random samples from a normal (Gaussian) distribution.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n+            to generate.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.backend.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.backend.set_floatx(float_dtype)`).\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n     dtype = dtype or floatx()\n     seed = draw_seed(seed)\n     sample = jax.random.normal(seed, shape=shape, dtype=dtype)\n@@ -14,6 +34,34 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n+    \"\"\"Draw samples from a uniform distribution.\n+\n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n+    For floats, the default range is `[0, 1)`.  For ints, at least `maxval`\n+    must be specified explicitly.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        minval: Floats, defaults to 0. Lower bound of the range of\n+            random values to generate (inclusive).\n+        maxval: Floats, defaults to 1. Upper bound of the range of\n+            random values to generate (exclusive).\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.backend.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.backend.set_floatx(float_dtype)`)\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n     dtype = dtype or floatx()\n     seed = draw_seed(seed)\n     return jax.random.uniform(\n@@ -21,14 +69,31 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     )\n \n \n-def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n-    seed = draw_seed(seed)\n-    return jax.random.randint(\n-        seed, shape=shape, dtype=dtype, minval=minval, maxval=maxval\n-    )\n-\n-\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Draw samples from a truncated normal distribution.\n+\n+    The values are drawn from a normal distribution with specified mean and\n+    standard deviation, discarding and re-drawing any samples that are more\n+    than two standard deviations from the mean.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n+            to generate.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.backend.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.backend.set_floatx(float_dtype)`)\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n     dtype = dtype or floatx()\n     seed = draw_seed(seed)\n     sample = jax.random.truncated_normal(\n\n@@ -141,11 +141,7 @@ def scatter_update(inputs, indices, updates):\n     return tf.tensor_scatter_nd_update(inputs, indices, updates)\n \n \n-def slice(inputs, start_indices, shape):\n-    return tf.slice(inputs, start_indices, shape)\n-\n-\n-def slice_update(inputs, start_indices, updates):\n+def block_update(inputs, start_indices, updates):\n     return dynamic_update_slice(inputs, updates, start_indices)\n \n \n\n@@ -1,6 +1,5 @@\n import tensorflow as tf\n \n-from keras_core.backend.common import standardize_dtype\n from keras_core.backend.config import floatx\n from keras_core.random.seed_generator import SeedGenerator\n from keras_core.random.seed_generator import draw_seed\n@@ -13,6 +12,26 @@ def tf_draw_seed(seed):\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Draw random samples from a normal (Gaussian) distribution.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n+            to generate.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.backend.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.backend.set_floatx(float_dtype)`).\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n     return tf.random.stateless_normal(\n@@ -21,6 +40,34 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n+    \"\"\"Draw samples from a uniform distribution.\n+\n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n+    For floats, the default range is `[0, 1)`.  For ints, at least `maxval`\n+    must be specified explicitly.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        minval: Floats, defaults to 0. Lower bound of the range of\n+            random values to generate (inclusive).\n+        maxval: Floats, defaults to 1. Upper bound of the range of\n+            random values to generate (exclusive).\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.backend.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.backend.set_floatx(float_dtype)`)\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n     return tf.random.stateless_uniform(\n@@ -32,22 +79,31 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     )\n \n \n-def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n-    intemediate_dtype = dtype\n-    if standardize_dtype(dtype) not in [\"int32\", \"int64\"]:\n-        intemediate_dtype = \"int64\"\n-    seed = tf_draw_seed(seed)\n-    output = tf.random.stateless_uniform(\n-        shape=shape,\n-        minval=minval,\n-        maxval=maxval,\n-        dtype=intemediate_dtype,\n-        seed=seed,\n-    )\n-    return tf.cast(output, dtype)\n-\n-\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Draw samples from a truncated normal distribution.\n+\n+    The values are drawn from a normal distribution with specified mean and\n+    standard deviation, discarding and re-drawing any samples that are more\n+    than two standard deviations from the mean.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n+            to generate.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.backend.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.backend.set_floatx(float_dtype)`)\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n     return tf.random.stateless_truncated_normal(\n\n@@ -18,7 +18,6 @@ TORCH_DTYPES = {\n     \"float32\": torch.float32,\n     \"float64\": torch.float64,\n     \"uint8\": torch.uint8,\n-    \"uint16\": torch.int32,  # TODO: Torch doesn't have `uint16` dtype.\n     \"uint32\": torch.int64,  # TODO: Torch doesn't have `uint32` dtype.\n     \"int8\": torch.int8,\n     \"int16\": torch.int16,\n@@ -59,7 +58,7 @@ def to_torch_dtype(dtype):\n class Variable(KerasVariable):\n     def _initialize(self, value):\n         self._value = torch.nn.Parameter(\n-            convert_to_tensor(value, dtype=self._dtype).to(get_device()),\n+            convert_to_tensor(value, dtype=self._dtype),\n             requires_grad=self.trainable,\n         ).to(get_device())\n \n@@ -85,14 +84,16 @@ class Variable(KerasVariable):\n         return func(*args, **kwargs)\n \n     def __array__(self, dtype=None):\n-        return _prepare_for_numpy(self.value).__array__(dtype)\n+        if self.value.requires_grad:\n+            return self.value.detach().__array__(dtype)\n+        return super().__array__(dtype)\n \n     @property\n     def value(self):\n         value = super().value\n         # Create and use a symbolic tensor stub in symbolic calls.\n         if get_device() == \"meta\" and value.device != \"meta\":\n-            return torch.empty(\n+            return torch.randn(\n                 size=value.shape,\n                 dtype=value.dtype,\n                 device=\"meta\",\n@@ -104,7 +105,6 @@ def convert_to_tensor(x, dtype=None):\n     dtype = to_torch_dtype(dtype or getattr(x, \"dtype\", None))\n     if isinstance(x, Variable):\n         x = x.value\n-        return x\n     if is_tensor(x):\n         if dtype and dtype != x.dtype:\n             x = x.to(dtype)\n@@ -120,18 +120,15 @@ def convert_to_tensor(x, dtype=None):\n     return torch.as_tensor(x, dtype=dtype, device=get_device())\n \n \n-def _prepare_for_numpy(x):\n-    if is_tensor(x):\n-        if x.requires_grad:\n-            x = x.detach()\n-        # Tensor has to be moved to CPU before converting to numpy.\n-        if x.is_cuda:\n-            x = x.cpu()\n-    return x\n-\n-\n def convert_to_numpy(x):\n-    return np.array(_prepare_for_numpy(x))\n+    if isinstance(x, KerasVariable):\n+        x = x.value\n+    if is_tensor(x) and x.is_cuda:\n+        # Tensor has to be moved to CPU before converting to numpy.\n+        x = x.cpu()\n+    if is_tensor(x) and x.requires_grad:\n+        return x.detach().numpy()\n+    return np.array(x)\n \n \n def is_tensor(x):\n@@ -173,7 +170,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                     for i, e in enumerate(shape):\n                         if e is None:\n                             shape[i] = fill_value\n-                return torch.empty(\n+                return torch.randn(\n                     size=shape,\n                     dtype=TORCH_DTYPES[x.dtype],\n                     device=get_device(),\n@@ -254,30 +251,15 @@ def scatter_update(inputs, indices, updates):\n     return inputs\n \n \n-def slice(inputs, start_indices, shape):\n-    shape_dtype = to_torch_dtype(\"int64\")\n+def block_update(inputs, start_indices, updates):\n     inputs = convert_to_tensor(inputs)\n-    start_indices = convert_to_tensor(start_indices).to(shape_dtype)\n-    shape = convert_to_tensor(shape).to(shape_dtype)\n-\n-    python_slice = __builtins__[\"slice\"]\n-    slices = [\n-        python_slice(start_index, start_index + length)\n-        for start_index, length in zip(start_indices, shape)\n-    ]\n-    return inputs[slices]\n-\n-\n-def slice_update(inputs, start_indices, updates):\n-    shape_dtype = to_torch_dtype(\"int64\")\n-    inputs = convert_to_tensor(inputs)\n-    start_indices = convert_to_tensor(start_indices).to(shape_dtype)\n+    start_indices = convert_to_tensor(start_indices, dtype=\"int64\")\n     updates = convert_to_tensor(updates)\n \n-    python_slice = __builtins__[\"slice\"]\n+    update_shape = updates.shape\n     slices = [\n-        python_slice(start_index, start_index + update_length)\n-        for start_index, update_length in zip(start_indices, updates.shape)\n+        slice(start_index, start_index + update_length)\n+        for start_index, update_length in zip(start_indices, update_shape)\n     ]\n     inputs[slices] = updates\n     return inputs\n\n@@ -16,6 +16,26 @@ def torch_seed_generator(seed):\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Produce random number based on the normal distribution.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n+            to generate.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.backend.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.backend.set_floatx(float_dtype)`).\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n     generator = torch_seed_generator(seed)\n@@ -25,6 +45,31 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n+    \"\"\"Produce random number based on the uniform distribution.\n+\n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        minval: Floats, defaults to 0. Lower bound of the range of\n+            random values to generate (inclusive).\n+        maxval: Floats, defaults to 1. Upper bound of the range of\n+            random values to generate (exclusive).\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.backend.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.backend.set_floatx(float_dtype)`)\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n     generator = torch_seed_generator(seed)\n@@ -33,15 +78,31 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     ) + minval\n \n \n-def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n-    dtype = to_torch_dtype(dtype)\n-    generator = torch_seed_generator(seed)\n-    return torch.randint(\n-        low=minval, high=maxval, size=shape, generator=generator, dtype=dtype\n-    )\n-\n-\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n+    \"\"\"Produce random number based on the truncated normal distribution.\n+\n+    The values are drawn from a normal distribution with specified mean and\n+    standard deviation, discarding and re-drawing any samples that are more\n+    than two standard deviations from the mean.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n+            to generate.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.backend.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.backend.set_floatx(float_dtype)`)\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n     # Take a larger standard normal dist, discard values outside 2 * stddev\n     # Offset by mean and stddev\n     x = normal(shape + (4,), mean=0, stddev=1, dtype=dtype, seed=seed)\n\n@@ -4,7 +4,6 @@ import numpy as np\n import tensorflow as tf\n import torch\n \n-from keras_core import backend\n from keras_core import callbacks as callbacks_module\n from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n@@ -387,7 +386,6 @@ class TorchTrainer(base_trainer.Trainer):\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n-        outputs = tf.nest.map_structure(backend.convert_to_numpy, outputs)\n         return tf.__internal__.nest.map_structure_up_to(\n             batch_outputs, np.concatenate, outputs\n         )\n@@ -443,6 +441,6 @@ class TorchTrainer(base_trainer.Trainer):\n         self.make_predict_function()\n         batch_outputs = self.predict_function((x,))\n         batch_outputs = tf.nest.map_structure(\n-            backend.convert_to_numpy, batch_outputs\n+            lambda x: np.array(x), batch_outputs\n         )\n         return batch_outputs\n\n@@ -135,9 +135,11 @@ class Metric:\n         self._tracker.add_to_store(\"variables\", variable)\n         return variable\n \n-    def add_weight(self, *args, **kwargs):\n+    def add_weight(self, shape=(), initializer=None, dtype=None, name=None):\n         # Backwards compatibility alias\n-        return self.add_variable(*args, **kwargs)\n+        return self.add_variable(\n+            shape=shape, initializer=initializer, dtype=dtype, name=name\n+        )\n \n     @property\n     def variables(self):\n\n@@ -1,8 +1,7 @@\n \"\"\"\n scatter\n scatter_update\n-slice\n-slice_update\n+block_update\n while_loop\n \"\"\"\n \n@@ -88,56 +87,17 @@ def scatter_update(inputs, indices, updates):\n     return backend.core.scatter_update(inputs, indices, updates)\n \n \n-class Slice(Operation):\n-    def call(self, inputs, start_indices, shape):\n-        return backend.core.slice(inputs, start_indices, shape)\n-\n-    def compute_output_spec(self, inputs, start_indices, shape):\n-        return KerasTensor(shape, dtype=inputs.dtype)\n-\n-\n-@keras_core_export(\"keras_core.operations.slice\")\n-def slice(inputs, start_indices, shape):\n-    \"\"\"Return a slice of an input tensor.\n-\n-    At a high level, this operation is an explicit replacement for array slicing\n-    e.g. `inputs[start_indices: start_indices + shape]`.\n-    Unlike slicing via brackets, this operation will accept tensor start\n-    indices on all backends, which is useful when indices dynamically computed\n-    via other tensor operations.\n-\n-    ```python\n-    inputs = np.zeros((5, 5))\n-    start_indices = np.array([3, 3])\n-    shape = np.array([2, 2])\n-    inputs = keras_core.operations.slice(inputs, start_indices, updates)\n-    ```\n-\n-    Args:\n-        inputs: A tensor, the tensor to be updated.\n-        start_indices: A list/tuple of shape `(inputs.ndim,)`, specifying\n-            the starting indices for updating.\n-        shape: The full shape of the returned slice.\n-\n-    Returns:\n-        A tensor, has the same shape and dtype as `inputs`.\n-    \"\"\"\n-    if any_symbolic_tensors((inputs, start_indices, shape)):\n-        return Slice().symbolic_call(inputs, start_indices, shape)\n-    return backend.core.slice(inputs, start_indices, shape)\n-\n-\n-class SliceUpdate(Operation):\n+class BlockUpdate(Operation):\n     def call(self, inputs, start_indices, updates):\n-        return backend.core.slice_update(inputs, start_indices, updates)\n+        return backend.core.block_update(inputs, start_indices, updates)\n \n     def compute_output_spec(self, inputs, start_indices, updates):\n         return KerasTensor(inputs.shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\"keras_core.operations.slice_update\")\n-def slice_update(inputs, start_indices, updates):\n-    \"\"\"Update an input by slicing in a tensor of updated values.\n+@keras_core_export(\"keras_core.operations.block_update\")\n+def block_update(inputs, start_indices, updates):\n+    \"\"\"Update inputs block.\n \n     At a high level, this operation does\n     `inputs[start_indices: start_indices + updates.shape] = updates`.\n@@ -153,7 +113,7 @@ def slice_update(inputs, start_indices, updates):\n     inputs = np.zeros((5, 5))\n     start_indices = [3, 3]\n     updates = np.ones((2, 2))\n-    inputs = keras_core.operations.slice_update(inputs, start_indices, updates)\n+    inputs = keras_core.operations.block_update(inputs, start_indices, updates)\n     ```\n \n     Args:\n@@ -167,8 +127,8 @@ def slice_update(inputs, start_indices, updates):\n         A tensor, has the same shape and dtype as `inputs`.\n     \"\"\"\n     if any_symbolic_tensors((inputs, start_indices, updates)):\n-        return SliceUpdate().symbolic_call(inputs, start_indices, updates)\n-    return backend.core.slice_update(inputs, start_indices, updates)\n+        return BlockUpdate().symbolic_call(inputs, start_indices, updates)\n+    return backend.core.block_update(inputs, start_indices, updates)\n \n \n class WhileLoop(Operation):\n@@ -270,3 +230,12 @@ def shape(x):\n     if any_symbolic_tensors((x,)):\n         return x.shape\n     return backend.core.shape(x)\n+\n+\n+@keras_core_export(\"keras_core.operations.cast\")\n+def cast(x, dtype):\n+    \"\"\"Cast a tensor to the desired dtype.\"\"\"\n+    dtype = backend.standardize_dtype(dtype)\n+    if any_symbolic_tensors((x,)):\n+        return backend.KerasTensor(shape=x.shape, dtype=dtype)\n+    return backend.core.cast(x, dtype)\n\n@@ -32,19 +32,19 @@ class CoreOpsStaticShapeTest(testing.TestCase):\n             core.scatter_update(inputs, indices, updates).shape, (4, 4, 4)\n         )\n \n-    def test_slice_update(self):\n+    def test_block_update(self):\n         inputs = KerasTensor((4, 4))\n         start_indices = KerasTensor((2,))\n         updates = KerasTensor((2, 2))\n         self.assertEqual(\n-            core.slice_update(inputs, start_indices, updates).shape, (4, 4)\n+            core.block_update(inputs, start_indices, updates).shape, (4, 4)\n         )\n \n         inputs = KerasTensor((4, 4, 4))\n         start_indices = KerasTensor((3,))\n         updates = KerasTensor((2, 2, 2))\n         self.assertEqual(\n-            core.slice_update(inputs, start_indices, updates).shape, (4, 4, 4)\n+            core.block_update(inputs, start_indices, updates).shape, (4, 4, 4)\n         )\n \n \n@@ -119,53 +119,13 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(outputs[1, 1, :], [0, 1, 2, 3])\n         self.assertAllClose(outputs[2, 2, :], [3, 2, 1, 0])\n \n-    def test_slice(self):\n-        # Test 1D.\n-        inputs = np.arange(10)\n-        start_indices = np.array([1])\n-        shape = np.array([4])\n-        self.assertAllClose(\n-            core.slice(inputs, start_indices, shape),\n-            [1, 2, 3, 4],\n-        )\n-\n-        # Test 2D.\n-        inputs = np.broadcast_to(np.arange(10), (4, 10))\n-        start_indices = np.array([1, 1])\n-        shape = np.array([2, 4])\n-        self.assertAllClose(\n-            core.slice(inputs, start_indices, shape),\n-            [[1, 2, 3, 4], [1, 2, 3, 4]],\n-        )\n-\n-        # Test N-D.\n-        inputs = np.broadcast_to(np.arange(10), (4, 4, 4, 10))\n-        start_indices = np.array([1, 1, 1, 1])\n-        shape = np.array([1, 2, 3, 4])\n-        outputs = core.slice(inputs, start_indices, shape)\n-        expected = np.broadcast_to(np.arange(1, 5), (1, 2, 3, 4))\n-        self.assertAllClose(outputs, expected)\n-\n-    def test_dynamic_slice(self):\n-        def cond(index, inputs, sum):\n-            return index < 10\n-\n-        def body(index, inputs, sum):\n-            sum = sum + core.slice(inputs, [index], [1])\n-            index = index + 1\n-            return index, inputs, sum\n-\n-        index, inputs, sum = 0, np.arange(10), np.array([0])\n-        index, inputs, sum = core.while_loop(cond, body, (index, inputs, sum))\n-        self.assertAllClose(sum, [45])\n-\n-    def test_slice_update(self):\n+    def test_block_update(self):\n         # Test 1D.\n         inputs = np.array([0, 0, 0, 0, 0, 0, 0, 0])\n         start_indices = [1]\n         updates = np.array([9, 10, 11, 12])\n         self.assertAllClose(\n-            core.slice_update(inputs, start_indices, updates),\n+            core.block_update(inputs, start_indices, updates),\n             [0, 9, 10, 11, 12, 0, 0, 0],\n         )\n \n@@ -174,7 +134,7 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         start_indices = [1, 0]\n         updates = np.array([[2, 2], [2, 2]])\n         self.assertAllClose(\n-            core.slice_update(inputs, start_indices, updates),\n+            core.block_update(inputs, start_indices, updates),\n             [[1, 1], [2, 2], [2, 2]],\n         )\n \n@@ -182,7 +142,7 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         inputs = np.ones([4, 4, 4, 4])\n         start_indices = [1, 1, 2, 2]\n         updates = np.zeros([2, 2, 2, 2])\n-        outputs = core.slice_update(inputs, start_indices, updates)\n+        outputs = core.block_update(inputs, start_indices, updates)\n         self.assertAllClose(outputs[1:3, 1:3, 2:4, 2:4], np.zeros([2, 2, 2, 2]))\n \n     def test_while_loop(self):\n\n@@ -37,7 +37,8 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     `[minval, maxval)`. The lower bound `minval` is included in the range,\n     while the upper bound `maxval` is excluded.\n \n-    `dtype` must be a floating point type, the default range is `[0, 1)`.\n+    For floats, the default range is `[0, 1)`.  For ints, at least `maxval`\n+    must be specified explicitly.\n \n     Args:\n         shape: The shape of the random values to generate.\n@@ -58,55 +59,11 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n             across multiple calls, use as seed an instance\n             of `keras_core.random.SeedGenerator`.\n     \"\"\"\n-    if dtype and not backend.is_float_dtype(dtype):\n-        raise ValueError(\n-            \"`keras_core.random.uniform` requires a floating point `dtype`. \"\n-            f\"Received: dtype={dtype} \"\n-        )\n     return backend.random.uniform(\n         shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed\n     )\n \n \n-@keras_core_export(\"keras_core.random.randint\")\n-def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n-    \"\"\"Draw random integers from a uniform distribution.\n-\n-    The generated values follow a uniform distribution in the range\n-    `[minval, maxval)`. The lower bound `minval` is included in the range,\n-    while the upper bound `maxval` is excluded.\n-\n-    `dtype` must be an integer type.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        minval: Floats, defaults to 0. Lower bound of the range of\n-            random values to generate (inclusive).\n-        maxval: Floats, defaults to 1. Upper bound of the range of\n-            random values to generate (exclusive).\n-        dtype: Optional dtype of the tensor. Only integer types are\n-            supported. If not specified, `keras_core.config.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras_core.config.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n-            `keras_core.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.random.SeedGenerator`.\n-    \"\"\"\n-    if dtype and not backend.is_int_dtype(dtype):\n-        raise ValueError(\n-            \"`keras_core.random.randint` requires an integer `dtype`. \"\n-            f\"Received: dtype={dtype} \"\n-        )\n-    return backend.random.randint(\n-        shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed\n-    )\n-\n-\n @keras_core_export(\"keras_core.random.truncated_normal\")\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Draw samples from a truncated normal distribution.\n\n@@ -3,7 +3,6 @@ import pytest\n from absl.testing import parameterized\n \n import keras_core\n-from keras_core import backend\n from keras_core import testing\n from keras_core.operations import numpy as knp\n from keras_core.random import random\n@@ -40,27 +39,6 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertLessEqual(knp.max(res), maxval)\n         self.assertGreaterEqual(knp.max(res), minval)\n \n-    @parameterized.parameters(\n-        {\"seed\": 10, \"shape\": (5,), \"min\": 0, \"max\": 10, \"dtype\": \"uint16\"},\n-        {\"seed\": 10, \"shape\": (2, 3), \"min\": 0, \"max\": 10, \"dtype\": \"uint32\"},\n-        {\"seed\": 10, \"shape\": (2, 3, 4), \"min\": 0, \"max\": 2, \"dtype\": \"int8\"},\n-        {\"seed\": 10, \"shape\": (2, 3), \"min\": -1, \"max\": 1, \"dtype\": \"int16\"},\n-        {\"seed\": 10, \"shape\": (2, 3), \"min\": 1, \"max\": 3, \"dtype\": \"int32\"},\n-    )\n-    def test_randint(self, seed, shape, min, max, dtype):\n-        np.random.seed(seed)\n-        np_res = np.random.randint(low=min, high=max, size=shape)\n-        res = random.randint(\n-            shape, minval=min, maxval=max, seed=seed, dtype=dtype\n-        )\n-        self.assertEqual(res.shape, shape)\n-        self.assertEqual(res.shape, np_res.shape)\n-        self.assertLessEqual(knp.max(res), max)\n-        self.assertGreaterEqual(knp.max(res), min)\n-        # Torch has incomplete dtype support for uints; will remap some dtypes.\n-        if keras_core.backend.backend() != \"torch\":\n-            self.assertEqual(backend.standardize_dtype(res.dtype), dtype)\n-\n     @parameterized.parameters(\n         {\"seed\": 10, \"shape\": (5,), \"mean\": 0, \"stddev\": 1},\n         {\"seed\": 10, \"shape\": (2, 3), \"mean\": 0, \"stddev\": 1},\n\n@@ -205,10 +205,12 @@ def train_validation_split(arrays, validation_split):\n \n def class_weight_to_sample_weights(y, class_weight):\n     sample_weight = np.ones(shape=(y.shape[0],), dtype=y.dtype)\n-    if len(y.shape) > 1 and y.shape[-1] != 1:\n+    if len(y.shape) > 1:\n+        if y.shape[-1] != 1:\n             y = np.argmax(y, axis=-1)\n         else:\n-        y = np.round(np.squeeze(y, axis=-1)).astype(\"int32\")\n+            y = np.squeeze(y, axis=-1)\n+    y = np.round(y).astype(\"int32\")\n     for i in range(y.shape[0]):\n         sample_weight[i] = class_weight.get(int(y[i]), 1.0)\n     return sample_weight\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#22d7835b4a72e1000b90c3c1ab0afcf2e5d8f9fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 252 | Lines Deleted: 260 | Files Changed: 15 | Hunks: 53 | Methods Changed: 47 | Complexity Δ (Sum/Max): 22/5 | Churn Δ: 512 | Churn Cumulative: 7063 | Contributors (this commit): 11 | Commits (past 90d): 253 | Contributors (cumulative): 77 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,7 +38,7 @@ model.predict_on_batch(x)\n inputs = keras_core.Input(shape=(32, 32, 3))\n outputs = layers.Conv2D(filters=10, kernel_size=3)(inputs)\n outputs = layers.GlobalAveragePooling2D()(outputs)\n-outputs = layers.Dense(1)(outputs)\n+outputs = layers.Dense(1, activation=\"sigmoid\")(outputs)\n model = keras_core.Model(inputs, outputs)\n model.compile(\n     loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", \"accuracy\"]\n\n@@ -15,6 +15,7 @@ from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.backend.common.variables import AutocastScope\n from keras_core.backend.common.variables import get_autocast_scope\n from keras_core.backend.common.variables import is_float_dtype\n+from keras_core.backend.common.variables import is_int_dtype\n from keras_core.backend.common.variables import standardize_dtype\n from keras_core.backend.common.variables import standardize_shape\n from keras_core.backend.config import epsilon\n\n@@ -3,6 +3,7 @@ from keras_core.backend.common.variables import AutocastScope\n from keras_core.backend.common.variables import KerasVariable\n from keras_core.backend.common.variables import get_autocast_scope\n from keras_core.backend.common.variables import is_float_dtype\n+from keras_core.backend.common.variables import is_int_dtype\n from keras_core.backend.common.variables import standardize_dtype\n from keras_core.backend.common.variables import standardize_shape\n from keras_core.random import random\n\n@@ -460,6 +460,11 @@ def is_float_dtype(dtype):\n     return dtype.startswith(\"float\") or dtype.startswith(\"bfloat\")\n \n \n+def is_int_dtype(dtype):\n+    dtype = standardize_dtype(dtype)\n+    return dtype.startswith(\"int\") or dtype.startswith(\"uint\")\n+\n+\n def get_autocast_scope():\n     return global_state.get_global_attribute(\"autocast_scope\")\n \n\n@@ -202,14 +202,12 @@ def scatter_update(inputs, indices, updates):\n     return inputs\n \n \n-def block_update(inputs, start_indices, updates):\n-    update_shape = updates.shape\n-    slices = [\n-        slice(start_index, start_index + update_length)\n-        for start_index, update_length in zip(start_indices, update_shape)\n-    ]\n-    inputs[tuple(slices)] = updates\n-    return inputs\n+def slice(inputs, start_indices, shape):\n+    return jax.lax.dynamic_slice(inputs, start_indices, shape)\n+\n+\n+def slice_update(inputs, start_indices, updates):\n+    return jax.lax.dynamic_update_slice(inputs, updates, start_indices)\n \n \n def while_loop(\n\n@@ -7,26 +7,6 @@ from keras_core.random.seed_generator import make_default_seed\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Draw random samples from a normal (Gaussian) distribution.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        mean: Floats, defaults to 0. Mean of the random values to generate.\n-        stddev: Floats, defaults to 1. Standard deviation of the random values\n-            to generate.\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`).\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n     dtype = dtype or floatx()\n     seed = draw_seed(seed)\n     sample = jax.random.normal(seed, shape=shape, dtype=dtype)\n@@ -34,34 +14,6 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n-    \"\"\"Draw samples from a uniform distribution.\n-\n-    The generated values follow a uniform distribution in the range\n-    `[minval, maxval)`. The lower bound `minval` is included in the range,\n-    while the upper bound `maxval` is excluded.\n-\n-    For floats, the default range is `[0, 1)`.  For ints, at least `maxval`\n-    must be specified explicitly.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        minval: Floats, defaults to 0. Lower bound of the range of\n-            random values to generate (inclusive).\n-        maxval: Floats, defaults to 1. Upper bound of the range of\n-            random values to generate (exclusive).\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n     dtype = dtype or floatx()\n     seed = draw_seed(seed)\n     return jax.random.uniform(\n@@ -69,31 +21,14 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     )\n \n \n+def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n+    seed = draw_seed(seed)\n+    return jax.random.randint(\n+        seed, shape=shape, dtype=dtype, minval=minval, maxval=maxval\n+    )\n+\n+\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Draw samples from a truncated normal distribution.\n-\n-    The values are drawn from a normal distribution with specified mean and\n-    standard deviation, discarding and re-drawing any samples that are more\n-    than two standard deviations from the mean.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        mean: Floats, defaults to 0. Mean of the random values to generate.\n-        stddev: Floats, defaults to 1. Standard deviation of the random values\n-            to generate.\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n     dtype = dtype or floatx()\n     seed = draw_seed(seed)\n     sample = jax.random.truncated_normal(\n\n@@ -141,7 +141,11 @@ def scatter_update(inputs, indices, updates):\n     return tf.tensor_scatter_nd_update(inputs, indices, updates)\n \n \n-def block_update(inputs, start_indices, updates):\n+def slice(inputs, start_indices, shape):\n+    return tf.slice(inputs, start_indices, shape)\n+\n+\n+def slice_update(inputs, start_indices, updates):\n     return dynamic_update_slice(inputs, updates, start_indices)\n \n \n\n@@ -1,5 +1,6 @@\n import tensorflow as tf\n \n+from keras_core.backend.common import standardize_dtype\n from keras_core.backend.config import floatx\n from keras_core.random.seed_generator import SeedGenerator\n from keras_core.random.seed_generator import draw_seed\n@@ -12,26 +13,6 @@ def tf_draw_seed(seed):\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Draw random samples from a normal (Gaussian) distribution.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        mean: Floats, defaults to 0. Mean of the random values to generate.\n-        stddev: Floats, defaults to 1. Standard deviation of the random values\n-            to generate.\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`).\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n     return tf.random.stateless_normal(\n@@ -40,34 +21,6 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n-    \"\"\"Draw samples from a uniform distribution.\n-\n-    The generated values follow a uniform distribution in the range\n-    `[minval, maxval)`. The lower bound `minval` is included in the range,\n-    while the upper bound `maxval` is excluded.\n-\n-    For floats, the default range is `[0, 1)`.  For ints, at least `maxval`\n-    must be specified explicitly.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        minval: Floats, defaults to 0. Lower bound of the range of\n-            random values to generate (inclusive).\n-        maxval: Floats, defaults to 1. Upper bound of the range of\n-            random values to generate (exclusive).\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n     return tf.random.stateless_uniform(\n@@ -79,31 +32,22 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     )\n \n \n+def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n+    intemediate_dtype = dtype\n+    if standardize_dtype(dtype) not in [\"int32\", \"int64\"]:\n+        intemediate_dtype = \"int64\"\n+    seed = tf_draw_seed(seed)\n+    output = tf.random.stateless_uniform(\n+        shape=shape,\n+        minval=minval,\n+        maxval=maxval,\n+        dtype=intemediate_dtype,\n+        seed=seed,\n+    )\n+    return tf.cast(output, dtype)\n+\n+\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Draw samples from a truncated normal distribution.\n-\n-    The values are drawn from a normal distribution with specified mean and\n-    standard deviation, discarding and re-drawing any samples that are more\n-    than two standard deviations from the mean.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        mean: Floats, defaults to 0. Mean of the random values to generate.\n-        stddev: Floats, defaults to 1. Standard deviation of the random values\n-            to generate.\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n     return tf.random.stateless_truncated_normal(\n\n@@ -18,6 +18,7 @@ TORCH_DTYPES = {\n     \"float32\": torch.float32,\n     \"float64\": torch.float64,\n     \"uint8\": torch.uint8,\n+    \"uint16\": torch.int32,  # TODO: Torch doesn't have `uint16` dtype.\n     \"uint32\": torch.int64,  # TODO: Torch doesn't have `uint32` dtype.\n     \"int8\": torch.int8,\n     \"int16\": torch.int16,\n@@ -58,7 +59,7 @@ def to_torch_dtype(dtype):\n class Variable(KerasVariable):\n     def _initialize(self, value):\n         self._value = torch.nn.Parameter(\n-            convert_to_tensor(value, dtype=self._dtype),\n+            convert_to_tensor(value, dtype=self._dtype).to(get_device()),\n             requires_grad=self.trainable,\n         ).to(get_device())\n \n@@ -84,16 +85,14 @@ class Variable(KerasVariable):\n         return func(*args, **kwargs)\n \n     def __array__(self, dtype=None):\n-        if self.value.requires_grad:\n-            return self.value.detach().__array__(dtype)\n-        return super().__array__(dtype)\n+        return _prepare_for_numpy(self.value).__array__(dtype)\n \n     @property\n     def value(self):\n         value = super().value\n         # Create and use a symbolic tensor stub in symbolic calls.\n         if get_device() == \"meta\" and value.device != \"meta\":\n-            return torch.randn(\n+            return torch.empty(\n                 size=value.shape,\n                 dtype=value.dtype,\n                 device=\"meta\",\n@@ -105,6 +104,7 @@ def convert_to_tensor(x, dtype=None):\n     dtype = to_torch_dtype(dtype or getattr(x, \"dtype\", None))\n     if isinstance(x, Variable):\n         x = x.value\n+        return x\n     if is_tensor(x):\n         if dtype and dtype != x.dtype:\n             x = x.to(dtype)\n@@ -120,15 +120,18 @@ def convert_to_tensor(x, dtype=None):\n     return torch.as_tensor(x, dtype=dtype, device=get_device())\n \n \n-def convert_to_numpy(x):\n-    if isinstance(x, KerasVariable):\n-        x = x.value\n-    if is_tensor(x) and x.is_cuda:\n+def _prepare_for_numpy(x):\n+    if is_tensor(x):\n+        if x.requires_grad:\n+            x = x.detach()\n         # Tensor has to be moved to CPU before converting to numpy.\n+        if x.is_cuda:\n             x = x.cpu()\n-    if is_tensor(x) and x.requires_grad:\n-        return x.detach().numpy()\n-    return np.array(x)\n+    return x\n+\n+\n+def convert_to_numpy(x):\n+    return np.array(_prepare_for_numpy(x))\n \n \n def is_tensor(x):\n@@ -170,7 +173,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                     for i, e in enumerate(shape):\n                         if e is None:\n                             shape[i] = fill_value\n-                return torch.randn(\n+                return torch.empty(\n                     size=shape,\n                     dtype=TORCH_DTYPES[x.dtype],\n                     device=get_device(),\n@@ -251,15 +254,30 @@ def scatter_update(inputs, indices, updates):\n     return inputs\n \n \n-def block_update(inputs, start_indices, updates):\n+def slice(inputs, start_indices, shape):\n+    shape_dtype = to_torch_dtype(\"int64\")\n     inputs = convert_to_tensor(inputs)\n-    start_indices = convert_to_tensor(start_indices, dtype=\"int64\")\n+    start_indices = convert_to_tensor(start_indices).to(shape_dtype)\n+    shape = convert_to_tensor(shape).to(shape_dtype)\n+\n+    python_slice = __builtins__[\"slice\"]\n+    slices = [\n+        python_slice(start_index, start_index + length)\n+        for start_index, length in zip(start_indices, shape)\n+    ]\n+    return inputs[slices]\n+\n+\n+def slice_update(inputs, start_indices, updates):\n+    shape_dtype = to_torch_dtype(\"int64\")\n+    inputs = convert_to_tensor(inputs)\n+    start_indices = convert_to_tensor(start_indices).to(shape_dtype)\n     updates = convert_to_tensor(updates)\n \n-    update_shape = updates.shape\n+    python_slice = __builtins__[\"slice\"]\n     slices = [\n-        slice(start_index, start_index + update_length)\n-        for start_index, update_length in zip(start_indices, update_shape)\n+        python_slice(start_index, start_index + update_length)\n+        for start_index, update_length in zip(start_indices, updates.shape)\n     ]\n     inputs[slices] = updates\n     return inputs\n\n@@ -16,26 +16,6 @@ def torch_seed_generator(seed):\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the normal distribution.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        mean: Floats, defaults to 0. Mean of the random values to generate.\n-        stddev: Floats, defaults to 1. Standard deviation of the random values\n-            to generate.\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`).\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n     generator = torch_seed_generator(seed)\n@@ -45,31 +25,6 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the uniform distribution.\n-\n-    The generated values follow a uniform distribution in the range\n-    `[minval, maxval)`. The lower bound `minval` is included in the range,\n-    while the upper bound `maxval` is excluded.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        minval: Floats, defaults to 0. Lower bound of the range of\n-            random values to generate (inclusive).\n-        maxval: Floats, defaults to 1. Upper bound of the range of\n-            random values to generate (exclusive).\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n     generator = torch_seed_generator(seed)\n@@ -78,31 +33,15 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     ) + minval\n \n \n+def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n+    dtype = to_torch_dtype(dtype)\n+    generator = torch_seed_generator(seed)\n+    return torch.randint(\n+        low=minval, high=maxval, size=shape, generator=generator, dtype=dtype\n+    )\n+\n+\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the truncated normal distribution.\n-\n-    The values are drawn from a normal distribution with specified mean and\n-    standard deviation, discarding and re-drawing any samples that are more\n-    than two standard deviations from the mean.\n-\n-    Args:\n-        shape: The shape of the random values to generate.\n-        mean: Floats, defaults to 0. Mean of the random values to generate.\n-        stddev: Floats, defaults to 1. Standard deviation of the random values\n-            to generate.\n-        dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras.backend.floatx()` is used,\n-            which defaults to `float32` unless you configured it otherwise (via\n-            `keras.backend.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n     # Take a larger standard normal dist, discard values outside 2 * stddev\n     # Offset by mean and stddev\n     x = normal(shape + (4,), mean=0, stddev=1, dtype=dtype, seed=seed)\n\n@@ -4,6 +4,7 @@ import numpy as np\n import tensorflow as tf\n import torch\n \n+from keras_core import backend\n from keras_core import callbacks as callbacks_module\n from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n@@ -386,6 +387,7 @@ class TorchTrainer(base_trainer.Trainer):\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n+        outputs = tf.nest.map_structure(backend.convert_to_numpy, outputs)\n         return tf.__internal__.nest.map_structure_up_to(\n             batch_outputs, np.concatenate, outputs\n         )\n@@ -441,6 +443,6 @@ class TorchTrainer(base_trainer.Trainer):\n         self.make_predict_function()\n         batch_outputs = self.predict_function((x,))\n         batch_outputs = tf.nest.map_structure(\n-            lambda x: np.array(x), batch_outputs\n+            backend.convert_to_numpy, batch_outputs\n         )\n         return batch_outputs\n\n@@ -1,7 +1,8 @@\n \"\"\"\n scatter\n scatter_update\n-block_update\n+slice\n+slice_update\n while_loop\n \"\"\"\n \n@@ -87,17 +88,56 @@ def scatter_update(inputs, indices, updates):\n     return backend.core.scatter_update(inputs, indices, updates)\n \n \n-class BlockUpdate(Operation):\n+class Slice(Operation):\n+    def call(self, inputs, start_indices, shape):\n+        return backend.core.slice(inputs, start_indices, shape)\n+\n+    def compute_output_spec(self, inputs, start_indices, shape):\n+        return KerasTensor(shape, dtype=inputs.dtype)\n+\n+\n+@keras_core_export(\"keras_core.operations.slice\")\n+def slice(inputs, start_indices, shape):\n+    \"\"\"Return a slice of an input tensor.\n+\n+    At a high level, this operation is an explicit replacement for array slicing\n+    e.g. `inputs[start_indices: start_indices + shape]`.\n+    Unlike slicing via brackets, this operation will accept tensor start\n+    indices on all backends, which is useful when indices dynamically computed\n+    via other tensor operations.\n+\n+    ```python\n+    inputs = np.zeros((5, 5))\n+    start_indices = np.array([3, 3])\n+    shape = np.array([2, 2])\n+    inputs = keras_core.operations.slice(inputs, start_indices, updates)\n+    ```\n+\n+    Args:\n+        inputs: A tensor, the tensor to be updated.\n+        start_indices: A list/tuple of shape `(inputs.ndim,)`, specifying\n+            the starting indices for updating.\n+        shape: The full shape of the returned slice.\n+\n+    Returns:\n+        A tensor, has the same shape and dtype as `inputs`.\n+    \"\"\"\n+    if any_symbolic_tensors((inputs, start_indices, shape)):\n+        return Slice().symbolic_call(inputs, start_indices, shape)\n+    return backend.core.slice(inputs, start_indices, shape)\n+\n+\n+class SliceUpdate(Operation):\n     def call(self, inputs, start_indices, updates):\n-        return backend.core.block_update(inputs, start_indices, updates)\n+        return backend.core.slice_update(inputs, start_indices, updates)\n \n     def compute_output_spec(self, inputs, start_indices, updates):\n         return KerasTensor(inputs.shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\"keras_core.operations.block_update\")\n-def block_update(inputs, start_indices, updates):\n-    \"\"\"Update inputs block.\n+@keras_core_export(\"keras_core.operations.slice_update\")\n+def slice_update(inputs, start_indices, updates):\n+    \"\"\"Update an input by slicing in a tensor of updated values.\n \n     At a high level, this operation does\n     `inputs[start_indices: start_indices + updates.shape] = updates`.\n@@ -113,7 +153,7 @@ def block_update(inputs, start_indices, updates):\n     inputs = np.zeros((5, 5))\n     start_indices = [3, 3]\n     updates = np.ones((2, 2))\n-    inputs = keras_core.operations.block_update(inputs, start_indices, updates)\n+    inputs = keras_core.operations.slice_update(inputs, start_indices, updates)\n     ```\n \n     Args:\n@@ -127,8 +167,8 @@ def block_update(inputs, start_indices, updates):\n         A tensor, has the same shape and dtype as `inputs`.\n     \"\"\"\n     if any_symbolic_tensors((inputs, start_indices, updates)):\n-        return BlockUpdate().symbolic_call(inputs, start_indices, updates)\n-    return backend.core.block_update(inputs, start_indices, updates)\n+        return SliceUpdate().symbolic_call(inputs, start_indices, updates)\n+    return backend.core.slice_update(inputs, start_indices, updates)\n \n \n class WhileLoop(Operation):\n\n@@ -32,19 +32,19 @@ class CoreOpsStaticShapeTest(testing.TestCase):\n             core.scatter_update(inputs, indices, updates).shape, (4, 4, 4)\n         )\n \n-    def test_block_update(self):\n+    def test_slice_update(self):\n         inputs = KerasTensor((4, 4))\n         start_indices = KerasTensor((2,))\n         updates = KerasTensor((2, 2))\n         self.assertEqual(\n-            core.block_update(inputs, start_indices, updates).shape, (4, 4)\n+            core.slice_update(inputs, start_indices, updates).shape, (4, 4)\n         )\n \n         inputs = KerasTensor((4, 4, 4))\n         start_indices = KerasTensor((3,))\n         updates = KerasTensor((2, 2, 2))\n         self.assertEqual(\n-            core.block_update(inputs, start_indices, updates).shape, (4, 4, 4)\n+            core.slice_update(inputs, start_indices, updates).shape, (4, 4, 4)\n         )\n \n \n@@ -119,13 +119,53 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(outputs[1, 1, :], [0, 1, 2, 3])\n         self.assertAllClose(outputs[2, 2, :], [3, 2, 1, 0])\n \n-    def test_block_update(self):\n+    def test_slice(self):\n+        # Test 1D.\n+        inputs = np.arange(10)\n+        start_indices = np.array([1])\n+        shape = np.array([4])\n+        self.assertAllClose(\n+            core.slice(inputs, start_indices, shape),\n+            [1, 2, 3, 4],\n+        )\n+\n+        # Test 2D.\n+        inputs = np.broadcast_to(np.arange(10), (4, 10))\n+        start_indices = np.array([1, 1])\n+        shape = np.array([2, 4])\n+        self.assertAllClose(\n+            core.slice(inputs, start_indices, shape),\n+            [[1, 2, 3, 4], [1, 2, 3, 4]],\n+        )\n+\n+        # Test N-D.\n+        inputs = np.broadcast_to(np.arange(10), (4, 4, 4, 10))\n+        start_indices = np.array([1, 1, 1, 1])\n+        shape = np.array([1, 2, 3, 4])\n+        outputs = core.slice(inputs, start_indices, shape)\n+        expected = np.broadcast_to(np.arange(1, 5), (1, 2, 3, 4))\n+        self.assertAllClose(outputs, expected)\n+\n+    def test_dynamic_slice(self):\n+        def cond(index, inputs, sum):\n+            return index < 10\n+\n+        def body(index, inputs, sum):\n+            sum = sum + core.slice(inputs, [index], [1])\n+            index = index + 1\n+            return index, inputs, sum\n+\n+        index, inputs, sum = 0, np.arange(10), np.array([0])\n+        index, inputs, sum = core.while_loop(cond, body, (index, inputs, sum))\n+        self.assertAllClose(sum, [45])\n+\n+    def test_slice_update(self):\n         # Test 1D.\n         inputs = np.array([0, 0, 0, 0, 0, 0, 0, 0])\n         start_indices = [1]\n         updates = np.array([9, 10, 11, 12])\n         self.assertAllClose(\n-            core.block_update(inputs, start_indices, updates),\n+            core.slice_update(inputs, start_indices, updates),\n             [0, 9, 10, 11, 12, 0, 0, 0],\n         )\n \n@@ -134,7 +174,7 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         start_indices = [1, 0]\n         updates = np.array([[2, 2], [2, 2]])\n         self.assertAllClose(\n-            core.block_update(inputs, start_indices, updates),\n+            core.slice_update(inputs, start_indices, updates),\n             [[1, 1], [2, 2], [2, 2]],\n         )\n \n@@ -142,7 +182,7 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         inputs = np.ones([4, 4, 4, 4])\n         start_indices = [1, 1, 2, 2]\n         updates = np.zeros([2, 2, 2, 2])\n-        outputs = core.block_update(inputs, start_indices, updates)\n+        outputs = core.slice_update(inputs, start_indices, updates)\n         self.assertAllClose(outputs[1:3, 1:3, 2:4, 2:4], np.zeros([2, 2, 2, 2]))\n \n     def test_while_loop(self):\n\n@@ -37,8 +37,7 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     `[minval, maxval)`. The lower bound `minval` is included in the range,\n     while the upper bound `maxval` is excluded.\n \n-    For floats, the default range is `[0, 1)`.  For ints, at least `maxval`\n-    must be specified explicitly.\n+    `dtype` must be a floating point type, the default range is `[0, 1)`.\n \n     Args:\n         shape: The shape of the random values to generate.\n@@ -59,11 +58,55 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n             across multiple calls, use as seed an instance\n             of `keras_core.random.SeedGenerator`.\n     \"\"\"\n+    if dtype and not backend.is_float_dtype(dtype):\n+        raise ValueError(\n+            \"`keras_core.random.uniform` requires a floating point `dtype`. \"\n+            f\"Received: dtype={dtype} \"\n+        )\n     return backend.random.uniform(\n         shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed\n     )\n \n \n+@keras_core_export(\"keras_core.random.randint\")\n+def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n+    \"\"\"Draw random integers from a uniform distribution.\n+\n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n+    `dtype` must be an integer type.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        minval: Floats, defaults to 0. Lower bound of the range of\n+            random values to generate (inclusive).\n+        maxval: Floats, defaults to 1. Upper bound of the range of\n+            random values to generate (exclusive).\n+        dtype: Optional dtype of the tensor. Only integer types are\n+            supported. If not specified, `keras_core.config.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras_core.config.set_floatx(float_dtype)`)\n+        seed: A Python integer or instance of\n+            `keras_core.random.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.random.SeedGenerator`.\n+    \"\"\"\n+    if dtype and not backend.is_int_dtype(dtype):\n+        raise ValueError(\n+            \"`keras_core.random.randint` requires an integer `dtype`. \"\n+            f\"Received: dtype={dtype} \"\n+        )\n+    return backend.random.randint(\n+        shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed\n+    )\n+\n+\n @keras_core_export(\"keras_core.random.truncated_normal\")\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Draw samples from a truncated normal distribution.\n\n@@ -3,6 +3,7 @@ import pytest\n from absl.testing import parameterized\n \n import keras_core\n+from keras_core import backend\n from keras_core import testing\n from keras_core.operations import numpy as knp\n from keras_core.random import random\n@@ -39,6 +40,27 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertLessEqual(knp.max(res), maxval)\n         self.assertGreaterEqual(knp.max(res), minval)\n \n+    @parameterized.parameters(\n+        {\"seed\": 10, \"shape\": (5,), \"min\": 0, \"max\": 10, \"dtype\": \"uint16\"},\n+        {\"seed\": 10, \"shape\": (2, 3), \"min\": 0, \"max\": 10, \"dtype\": \"uint32\"},\n+        {\"seed\": 10, \"shape\": (2, 3, 4), \"min\": 0, \"max\": 2, \"dtype\": \"int8\"},\n+        {\"seed\": 10, \"shape\": (2, 3), \"min\": -1, \"max\": 1, \"dtype\": \"int16\"},\n+        {\"seed\": 10, \"shape\": (2, 3), \"min\": 1, \"max\": 3, \"dtype\": \"int32\"},\n+    )\n+    def test_randint(self, seed, shape, min, max, dtype):\n+        np.random.seed(seed)\n+        np_res = np.random.randint(low=min, high=max, size=shape)\n+        res = random.randint(\n+            shape, minval=min, maxval=max, seed=seed, dtype=dtype\n+        )\n+        self.assertEqual(res.shape, shape)\n+        self.assertEqual(res.shape, np_res.shape)\n+        self.assertLessEqual(knp.max(res), max)\n+        self.assertGreaterEqual(knp.max(res), min)\n+        # Torch has incomplete dtype support for uints; will remap some dtypes.\n+        if keras_core.backend.backend() != \"torch\":\n+            self.assertEqual(backend.standardize_dtype(res.dtype), dtype)\n+\n     @parameterized.parameters(\n         {\"seed\": 10, \"shape\": (5,), \"mean\": 0, \"stddev\": 1},\n         {\"seed\": 10, \"shape\": (2, 3), \"mean\": 0, \"stddev\": 1},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
