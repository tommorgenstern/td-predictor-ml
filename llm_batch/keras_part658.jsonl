{"custom_id": "keras#d029433538f3e12d109fe9b84fb077b839ed060b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 678 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 14 | Complexity Δ (Sum/Max): 35/35 | Churn Δ: 678 | Churn Cumulative: 678 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,678 @@\n+\"\"\"\n+Title: Making new layers and models via subclassing\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2019/03/01\n+Last modified: 2023/06/21\n+Description: Complete guide to writing `Layer` and `Model` objects from scratch.\n+Accelerator: None\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This guide will cover everything you need to know to build your own\n+subclassed layers and models. In particular, you'll learn about the following features:\n+\n+- The `Layer` class\n+- The `add_weight()` method\n+- Trainable and non-trainable weights\n+- The `build()` method\n+- Making sure your layers can be used with any backend\n+- The `add_loss()` method\n+- The `training` argument in `call()`\n+- The `mask` argument in `call()`\n+- Making sure your layers can be serialized\n+\n+Let's dive in.\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import keras_core as keras\n+from keras_core import operations as ops\n+from keras_core import layers\n+\n+\"\"\"\n+## The `Layer` class: the combination of state (weights) and some computation\n+\n+One of the central abstractions in Keras is the `Layer` class. A layer\n+encapsulates both a state (the layer's \"weights\") and a transformation from\n+inputs to outputs (a \"call\", the layer's forward pass).\n+\n+Here's a densely-connected layer. It has two state variables:\n+the variables `w` and `b`.\n+\"\"\"\n+\n+\n+class Linear(keras.layers.Layer):\n+    def __init__(self, units=32, input_dim=32):\n+        super().__init__()\n+        self.w = self.add_weight(\n+            shape=(input_dim, units),\n+            initializer=\"random_normal\",\n+            trainable=True,\n+        )\n+        self.b = self.add_weight(\n+            shape=(units,), initializer=\"zeros\", trainable=True\n+        )\n+\n+    def call(self, inputs):\n+        return ops.matmul(inputs, self.w) + self.b\n+\n+\n+\"\"\"\n+You would use a layer by calling it on some tensor input(s), much like a Python\n+function.\n+\"\"\"\n+\n+x = ops.ones((2, 2))\n+linear_layer = Linear(4, 2)\n+y = linear_layer(x)\n+print(y)\n+\n+\"\"\"\n+Note that the weights `w` and `b` are automatically tracked by the layer upon\n+being set as layer attributes:\n+\"\"\"\n+\n+assert linear_layer.weights == [linear_layer.w, linear_layer.b]\n+\n+\"\"\"\n+## Layers can have non-trainable weights\n+\n+Besides trainable weights, you can add non-trainable weights to a layer as\n+well. Such weights are meant not to be taken into account during\n+backpropagation, when you are training the layer.\n+\n+Here's how to add and use a non-trainable weight:\n+\"\"\"\n+\n+\n+class ComputeSum(keras.layers.Layer):\n+    def __init__(self, input_dim):\n+        super().__init__()\n+        self.total = self.add_weight(\n+            initializer=\"zeros\", shape=(input_dim,), trainable=False\n+        )\n+\n+    def call(self, inputs):\n+        self.total.assign_add(ops.sum(inputs, axis=0))\n+        return self.total\n+\n+\n+x = ops.ones((2, 2))\n+my_sum = ComputeSum(2)\n+y = my_sum(x)\n+print(y.numpy())\n+y = my_sum(x)\n+print(y.numpy())\n+\n+\"\"\"\n+It's part of `layer.weights`, but it gets categorized as a non-trainable weight:\n+\"\"\"\n+\n+print(\"weights:\", len(my_sum.weights))\n+print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))\n+\n+# It's not included in the trainable weights:\n+print(\"trainable_weights:\", my_sum.trainable_weights)\n+\n+\"\"\"\n+## Best practice: deferring weight creation until the shape of the inputs is known\n+\n+Our `Linear` layer above took an `input_dim` argument that was used to compute\n+the shape of the weights `w` and `b` in `__init__()`:\n+\"\"\"\n+\n+\n+class Linear(keras.layers.Layer):\n+    def __init__(self, units=32, input_dim=32):\n+        super().__init__()\n+        self.w = self.add_weight(\n+            shape=(input_dim, units),\n+            initializer=\"random_normal\",\n+            trainable=True,\n+        )\n+        self.b = self.add_weight(\n+            shape=(units,), initializer=\"zeros\", trainable=True\n+        )\n+\n+    def call(self, inputs):\n+        return ops.matmul(inputs, self.w) + self.b\n+\n+\n+\"\"\"\n+In many cases, you may not know in advance the size of your inputs, and you\n+would like to lazily create weights when that value becomes known, some time\n+after instantiating the layer.\n+\n+In the Keras API, we recommend creating layer weights in the\n+`build(self, inputs_shape)` method of your layer. Like this:\n+\"\"\"\n+\n+\n+class Linear(keras.layers.Layer):\n+    def __init__(self, units=32):\n+        super().__init__()\n+        self.units = units\n+\n+    def build(self, input_shape):\n+        self.w = self.add_weight(\n+            shape=(input_shape[-1], self.units),\n+            initializer=\"random_normal\",\n+            trainable=True,\n+        )\n+        self.b = self.add_weight(\n+            shape=(self.units,), initializer=\"random_normal\", trainable=True\n+        )\n+\n+    def call(self, inputs):\n+        return ops.matmul(inputs, self.w) + self.b\n+\n+\n+\"\"\"\n+The `__call__()` method of your layer will automatically run build the first time\n+it is called. You now have a layer that's lazy and thus easier to use:\n+\"\"\"\n+\n+# At instantiation, we don't know on what inputs this is going to get called\n+linear_layer = Linear(32)\n+\n+# The layer's weights are created dynamically the first time the layer is called\n+y = linear_layer(x)\n+\n+\"\"\"\n+Implementing `build()` separately as shown above nicely separates creating weights\n+only once from using weights in every call.\n+\"\"\"\n+\n+\"\"\"\n+## Layers are recursively composable\n+\n+If you assign a Layer instance as an attribute of another Layer, the outer layer\n+will start tracking the weights created by the inner layer.\n+\n+We recommend creating such sublayers in the `__init__()` method and leave it to\n+the first `__call__()` to trigger building their weights.\n+\"\"\"\n+\n+\n+class MLPBlock(keras.layers.Layer):\n+    def __init__(self):\n+        super().__init__()\n+        self.linear_1 = Linear(32)\n+        self.linear_2 = Linear(32)\n+        self.linear_3 = Linear(1)\n+\n+    def call(self, inputs):\n+        x = self.linear_1(inputs)\n+        x = keras.activations.relu(x)\n+        x = self.linear_2(x)\n+        x = keras.activations.relu(x)\n+        return self.linear_3(x)\n+\n+\n+mlp = MLPBlock()\n+y = mlp(\n+    ops.ones(shape=(3, 64))\n+)  # The first call to the `mlp` will create the weights\n+print(\"weights:\", len(mlp.weights))\n+print(\"trainable weights:\", len(mlp.trainable_weights))\n+\n+\"\"\"\n+## Backend-agnostic layers and backend-specific layers\n+\n+As long as a layer only uses APIs from the `keras.ops` namespace\n+(or other Keras namespaces such as `keras.activations`, `keras.random`, or `keras.layers`),\n+then it can be used with any backend -- TensorFlow, JAX, or PyTorch.\n+\n+All layers you've seen so far in this guide work with all Keras backends.\n+\n+The `keras.ops` namespace gives you access to:\n+\n+- The NumPy API, e.g. `ops.matmul`, `ops.sum`, `ops.reshape`, `ops.stack`, etc.\n+- Neural networks-specific APIs such as `ops.softmax`, `ops`.conv`, `ops.binary_crossentropy`, `ops.relu`, etc.\n+\n+You can also use backend-native APIs in your layers (such as `tf.nn` functions),\n+but if you do this, then your layer will only be usable with the backend in question.\n+For instance, you could write the following JAX-specific layer using `jax.numpy`:\n+\n+```python\n+import jax\n+\n+class Linear(keras.layers.Layer):\n+    ...\n+\n+    def call(self, inputs):\n+        return jax.numpy.matmul(inputs, self.w) + self.b\n+```\n+\n+This would be the equivalent TensorFlow-specific layer:\n+\n+```python\n+import tensorflow as tf\n+\n+class Linear(keras.layers.Layer):\n+    ...\n+\n+    def call(self, inputs):\n+        return tf.matmul(inputs, self.w) + self.b\n+```\n+\n+And this would be the equivalent PyTorch-specific layer:\n+\n+```python\n+import torch\n+\n+class Linear(keras.layers.Layer):\n+    ...\n+\n+    def call(self, inputs):\n+        return torch.matmul(inputs, self.w) + self.b\n+```\n+\n+Because cross-backend compatibility is a tremendously useful property, we strongly\n+recommend that you seek to always make your layers backend-agnostic by leveraging\n+only Keras APIs.\n+\"\"\"\n+\n+\"\"\"\n+## The `add_loss()` method\n+\n+When writing the `call()` method of a layer, you can create loss tensors that\n+you will want to use later, when writing your training loop. This is doable by\n+calling `self.add_loss(value)`:\n+\"\"\"\n+\n+\n+# A layer that creates an activity regularization loss\n+class ActivityRegularizationLayer(keras.layers.Layer):\n+    def __init__(self, rate=1e-2):\n+        super().__init__()\n+        self.rate = rate\n+\n+    def call(self, inputs):\n+        self.add_loss(self.rate * ops.mean(inputs))\n+        return inputs\n+\n+\n+\"\"\"\n+These losses (including those created by any inner layer) can be retrieved via\n+`layer.losses`. This property is reset at the start of every `__call__()` to\n+the top-level layer, so that `layer.losses` always contains the loss values\n+created during the last forward pass.\n+\"\"\"\n+\n+\n+class OuterLayer(keras.layers.Layer):\n+    def __init__(self):\n+        super().__init__()\n+        self.activity_reg = ActivityRegularizationLayer(1e-2)\n+\n+    def call(self, inputs):\n+        return self.activity_reg(inputs)\n+\n+\n+layer = OuterLayer()\n+assert (\n+    len(layer.losses) == 0\n+)  # No losses yet since the layer has never been called\n+\n+_ = layer(ops.zeros((1, 1)))\n+assert len(layer.losses) == 1  # We created one loss value\n+\n+# `layer.losses` gets reset at the start of each __call__\n+_ = layer(ops.zeros((1, 1)))\n+assert len(layer.losses) == 1  # This is the loss created during the call above\n+\n+\"\"\"\n+In addition, the `loss` property also contains regularization losses created\n+for the weights of any inner layer:\n+\"\"\"\n+\n+\n+class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n+    def __init__(self):\n+        super().__init__()\n+        self.dense = keras.layers.Dense(\n+            32, kernel_regularizer=keras.regularizers.l2(1e-3)\n+        )\n+\n+    def call(self, inputs):\n+        return self.dense(inputs)\n+\n+\n+layer = OuterLayerWithKernelRegularizer()\n+_ = layer(ops.zeros((1, 1)))\n+\n+# This is `1e-3 * sum(layer.dense.kernel ** 2)`,\n+# created by the `kernel_regularizer` above.\n+print(layer.losses)\n+\n+\"\"\"\n+These losses are meant to be taken into account when writing custom training loops.\n+\n+They also work seamlessly with `fit()` (they get automatically summed and added to the main loss, if any):\n+\"\"\"\n+\n+inputs = keras.Input(shape=(3,))\n+outputs = ActivityRegularizationLayer()(inputs)\n+model = keras.Model(inputs, outputs)\n+\n+# If there is a loss passed in `compile`, the regularization\n+# losses get added to it\n+model.compile(optimizer=\"adam\", loss=\"mse\")\n+model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n+\n+# It's also possible not to pass any loss in `compile`,\n+# since the model already has a loss to minimize, via the `add_loss`\n+# call during the forward pass!\n+model.compile(optimizer=\"adam\")\n+model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n+\n+\"\"\"\n+## You can optionally enable serialization on your layers\n+\n+If you need your custom layers to be serializable as part of a\n+[Functional model](/guides/functional_api/), you can optionally implement a `get_config()`\n+method:\n+\"\"\"\n+\n+\n+class Linear(keras.layers.Layer):\n+    def __init__(self, units=32):\n+        super().__init__()\n+        self.units = units\n+\n+    def build(self, input_shape):\n+        self.w = self.add_weight(\n+            shape=(input_shape[-1], self.units),\n+            initializer=\"random_normal\",\n+            trainable=True,\n+        )\n+        self.b = self.add_weight(\n+            shape=(self.units,), initializer=\"random_normal\", trainable=True\n+        )\n+\n+    def call(self, inputs):\n+        return ops.matmul(inputs, self.w) + self.b\n+\n+    def get_config(self):\n+        return {\"units\": self.units}\n+\n+\n+# Now you can recreate the layer from its config:\n+layer = Linear(64)\n+config = layer.get_config()\n+print(config)\n+new_layer = Linear.from_config(config)\n+\n+\"\"\"\n+Note that the `__init__()` method of the base `Layer` class takes some keyword\n+arguments, in particular a `name` and a `dtype`. It's good practice to pass\n+these arguments to the parent class in `__init__()` and to include them in the\n+layer config:\n+\"\"\"\n+\n+\n+class Linear(keras.layers.Layer):\n+    def __init__(self, units=32, **kwargs):\n+        super().__init__(**kwargs)\n+        self.units = units\n+\n+    def build(self, input_shape):\n+        self.w = self.add_weight(\n+            shape=(input_shape[-1], self.units),\n+            initializer=\"random_normal\",\n+            trainable=True,\n+        )\n+        self.b = self.add_weight(\n+            shape=(self.units,), initializer=\"random_normal\", trainable=True\n+        )\n+\n+    def call(self, inputs):\n+        return ops.matmul(inputs, self.w) + self.b\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"units\": self.units})\n+        return config\n+\n+\n+layer = Linear(64)\n+config = layer.get_config()\n+print(config)\n+new_layer = Linear.from_config(config)\n+\n+\"\"\"\n+If you need more flexibility when deserializing the layer from its config, you\n+can also override the `from_config()` class method. This is the base\n+implementation of `from_config()`:\n+\n+```python\n+def from_config(cls, config):\n+    return cls(**config)\n+```\n+\n+To learn more about serialization and saving, see the complete\n+[guide to saving and serializing models](/guides/serialization_and_saving/).\n+\"\"\"\n+\n+\"\"\"\n+## Privileged `training` argument in the `call()` method\n+\n+Some layers, in particular the `BatchNormalization` layer and the `Dropout`\n+layer, have different behaviors during training and inference. For such\n+layers, it is standard practice to expose a `training` (boolean) argument in\n+the `call()` method.\n+\n+By exposing this argument in `call()`, you enable the built-in training and\n+evaluation loops (e.g. `fit()`) to correctly use the layer in training and\n+inference.\n+\"\"\"\n+\n+\n+class CustomDropout(keras.layers.Layer):\n+    def __init__(self, rate, **kwargs):\n+        super().__init__(**kwargs)\n+        self.rate = rate\n+\n+    def call(self, inputs, training=None):\n+        if training:\n+            return keras.random.dropout(inputs, rate=self.rate)\n+        return inputs\n+\n+\n+\"\"\"\n+## Privileged `mask` argument in the `call()` method\n+\n+The other privileged argument supported by `call()` is the `mask` argument.\n+\n+You will find it in all Keras RNN layers. A mask is a boolean tensor (one\n+boolean value per timestep in the input) used to skip certain input timesteps\n+when processing timeseries data.\n+\n+Keras will automatically pass the correct `mask` argument to `__call__()` for\n+layers that support it, when a mask is generated by a prior layer.\n+Mask-generating layers are the `Embedding`\n+layer configured with `mask_zero=True`, and the `Masking` layer.\n+\"\"\"\n+\n+\"\"\"\n+## The `Model` class\n+\n+In general, you will use the `Layer` class to define inner computation blocks,\n+and will use the `Model` class to define the outer model -- the object you\n+will train.\n+\n+For instance, in a ResNet50 model, you would have several ResNet blocks\n+subclassing `Layer`, and a single `Model` encompassing the entire ResNet50\n+network.\n+\n+The `Model` class has the same API as `Layer`, with the following differences:\n+\n+- It exposes built-in training, evaluation, and prediction loops\n+(`model.fit()`, `model.evaluate()`, `model.predict()`).\n+- It exposes the list of its inner layers, via the `model.layers` property.\n+- It exposes saving and serialization APIs (`save()`, `save_weights()`...)\n+\n+Effectively, the `Layer` class corresponds to what we refer to in the\n+literature as a \"layer\" (as in \"convolution layer\" or \"recurrent layer\") or as\n+a \"block\" (as in \"ResNet block\" or \"Inception block\").\n+\n+Meanwhile, the `Model` class corresponds to what is referred to in the\n+literature as a \"model\" (as in \"deep learning model\") or as a \"network\" (as in\n+\"deep neural network\").\n+\n+So if you're wondering, \"should I use the `Layer` class or the `Model` class?\",\n+ask yourself: will I need to call `fit()` on it? Will I need to call `save()`\n+on it? If so, go with `Model`. If not (either because your class is just a block\n+in a bigger system, or because you are writing training & saving code yourself),\n+use `Layer`.\n+\n+For instance, we could take our mini-resnet example above, and use it to build\n+a `Model` that we could train with `fit()`, and that we could save with\n+`save_weights()`:\n+\"\"\"\n+\n+\"\"\"\n+```python\n+class ResNet(keras.Model):\n+\n+    def __init__(self, num_classes=1000):\n+        super().__init__()\n+        self.block_1 = ResNetBlock()\n+        self.block_2 = ResNetBlock()\n+        self.global_pool = layers.GlobalAveragePooling2D()\n+        self.classifier = Dense(num_classes)\n+\n+    def call(self, inputs):\n+        x = self.block_1(inputs)\n+        x = self.block_2(x)\n+        x = self.global_pool(x)\n+        return self.classifier(x)\n+\n+\n+resnet = ResNet()\n+dataset = ...\n+resnet.fit(dataset, epochs=10)\n+resnet.save(filepath.keras)\n+```\n+\"\"\"\n+\n+\"\"\"\n+## Putting it all together: an end-to-end example\n+\n+Here's what you've learned so far:\n+\n+- A `Layer` encapsulate a state (created in `__init__()` or `build()`) and some\n+computation (defined in `call()`).\n+- Layers can be recursively nested to create new, bigger computation blocks.\n+- Layers are backend-agnostic as long as they only use Keras APIs. You can use\n+backend-native APIs (such as `jax.numpy`, `torch.nn` or `tf.nn`), but then\n+your layer will only be usable with that specific backend.\n+- Layers can create and track losses (typically regularization losses)\n+via `add_loss()`.\n+- The outer container, the thing you want to train, is a `Model`. A `Model` is\n+just like a `Layer`, but with added training and serialization utilities.\n+\n+Let's put all of these things together into an end-to-end example: we're going\n+to implement a Variational AutoEncoder (VAE) in a backend-agnostic fashion\n+-- so that it runs the same with TensorFlow, JAX, and PyTorch.\n+We'll train it on MNIST digits.\n+\n+Our VAE will be a subclass of `Model`, built as a nested composition of layers\n+that subclass `Layer`. It will feature a regularization loss (KL divergence).\n+\"\"\"\n+\n+\n+class Sampling(layers.Layer):\n+    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n+\n+    def call(self, inputs):\n+        z_mean, z_log_var = inputs\n+        batch = ops.shape(z_mean)[0]\n+        dim = ops.shape(z_mean)[1]\n+        epsilon = keras.random.normal(shape=(batch, dim))\n+        return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n+\n+\n+class Encoder(layers.Layer):\n+    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n+\n+    def __init__(\n+        self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs\n+    ):\n+        super().__init__(name=name, **kwargs)\n+        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n+        self.dense_mean = layers.Dense(latent_dim)\n+        self.dense_log_var = layers.Dense(latent_dim)\n+        self.sampling = Sampling()\n+\n+    def call(self, inputs):\n+        x = self.dense_proj(inputs)\n+        z_mean = self.dense_mean(x)\n+        z_log_var = self.dense_log_var(x)\n+        z = self.sampling((z_mean, z_log_var))\n+        return z_mean, z_log_var, z\n+\n+\n+class Decoder(layers.Layer):\n+    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n+\n+    def __init__(\n+        self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs\n+    ):\n+        super().__init__(name=name, **kwargs)\n+        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n+        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n+\n+    def call(self, inputs):\n+        x = self.dense_proj(inputs)\n+        return self.dense_output(x)\n+\n+\n+class VariationalAutoEncoder(keras.Model):\n+    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n+\n+    def __init__(\n+        self,\n+        original_dim,\n+        intermediate_dim=64,\n+        latent_dim=32,\n+        name=\"autoencoder\",\n+        **kwargs\n+    ):\n+        super().__init__(name=name, **kwargs)\n+        self.original_dim = original_dim\n+        self.encoder = Encoder(\n+            latent_dim=latent_dim, intermediate_dim=intermediate_dim\n+        )\n+        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n+\n+    def call(self, inputs):\n+        z_mean, z_log_var, z = self.encoder(inputs)\n+        reconstructed = self.decoder(z)\n+        # Add KL divergence regularization loss.\n+        kl_loss = -0.5 * ops.mean(\n+            z_log_var - ops.square(z_mean) - ops.exp(z_log_var) + 1\n+        )\n+        self.add_loss(kl_loss)\n+        return reconstructed\n+\n+\n+\"\"\"\n+Let's train it on MNIST using the `fit()` API:\n+\"\"\"\n+\n+(x_train, _), _ = keras.datasets.mnist.load_data()\n+x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n+\n+original_dim = 784\n+vae = VariationalAutoEncoder(784, 64, 32)\n+\n+optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n+vae.compile(optimizer, loss=keras.losses.MeanSquaredError())\n+\n+vae.fit(x_train, x_train, epochs=2, batch_size=64)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d95dc4fdb9d898f367923360c283ea4b41236e8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 649 | Contributors (this commit): 7 | Commits (past 90d): 36 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -302,8 +302,9 @@ def slice_update(inputs, start_indices, updates):\n         python_slice(start_index, start_index + update_length)\n         for start_index, update_length in zip(start_indices, updates.shape)\n     ]\n-    inputs[slices] = updates\n-    return inputs\n+    outputs = torch.clone(inputs)\n+    outputs[slices] = updates\n+    return outputs\n \n \n def while_loop(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7748a6aebba4bbc77a7866c44c1293c4bb9e0808", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 190 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,6 +39,7 @@ class Softmax(Layer):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n         self.axis = axis\n+        self.built = True\n \n     def call(self, inputs, mask=None):\n         if mask is not None:\n\n@@ -51,6 +51,7 @@ class Dropout(Layer):\n         self.noise_shape = noise_shape\n         self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs, training=False):\n         if training and self.rate > 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f53cca04e1401ddec7a359ce78be3d484f9f0454", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 2370 | Contributors (this commit): 8 | Commits (past 90d): 65 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -211,7 +211,7 @@ class Functional(Function, Model):\n         if isinstance(inputs, dict):\n             ref_inputs = self._inputs_struct\n             if not nest.is_nested(ref_inputs):\n-                ref_inputs = [self._nested_inputs]\n+                ref_inputs = [self._inputs_struct]\n             if isinstance(ref_inputs, dict):\n                 # In the case that the graph is constructed with dict input\n                 # tensors, We will use the original dict key to map with the\n\n@@ -99,6 +99,24 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertEqual(out_val.shape, (2, 4))\n \n+    def test_named_input_dict_io(self):\n+        input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n+        x = layers.Dense(5)(input_a)\n+        outputs = layers.Dense(4)(x)\n+\n+        model = Functional(input_a, outputs)\n+\n+        # Eager call\n+        in_val = {\"a\": np.random.random((2, 3))}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n+        # Symbolic call\n+        input_a_2 = Input(shape=(3,), batch_size=2)\n+        in_val = {\"a\": input_a_2}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n     def test_layer_getters(self):\n         # Test mixing ops and layers\n         input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca8f90ac2e82344409e2f23b112fc8670aa1078d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 43 | Files Changed: 9 | Hunks: 21 | Methods Changed: 15 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 111 | Churn Cumulative: 9814 | Contributors (this commit): 14 | Commits (past 90d): 331 | Contributors (cumulative): 54 | DMM Complexity: 0.76\n\nDIFF:\n@@ -4,6 +4,7 @@ import tensorflow as tf  # for nest\n \n from keras_core import backend\n from keras_core import callbacks as callbacks_module\n+from keras_core import operations as ops\n from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n from keras_core.trainers.data_adapters import data_adapter_utils\n@@ -31,11 +32,16 @@ class JAXTrainer(base_trainer.Trainer):\n         kwargs = {}\n         if self._call_has_training_arg():\n             kwargs[\"training\"] = training\n-        y_pred, non_trainable_variables = self.stateless_call(\n-            trainable_variables, non_trainable_variables, x, **kwargs\n+        y_pred, non_trainable_variables, losses = self.stateless_call(\n+            trainable_variables,\n+            non_trainable_variables,\n+            x,\n+            return_losses=True,\n+            **kwargs,\n         )\n-\n-        loss = self.compute_loss(x, y, y_pred, sample_weight)\n+        loss = self.compute_loss(x, y, y_pred, sample_weight, allow_empty=True)\n+        if losses:\n+            loss += ops.sum(losses)\n         return loss, (y_pred, non_trainable_variables)\n \n     def _eager_build(self, data_batch):\n\n@@ -302,9 +302,8 @@ def slice_update(inputs, start_indices, updates):\n         python_slice(start_index, start_index + update_length)\n         for start_index, update_length in zip(start_indices, updates.shape)\n     ]\n-    outputs = torch.clone(inputs)\n-    outputs[slices] = updates\n-    return outputs\n+    inputs[slices] = updates\n+    return inputs\n \n \n def while_loop(\n\n@@ -39,7 +39,6 @@ class Softmax(Layer):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n         self.axis = axis\n-        self.built = True\n \n     def call(self, inputs, mask=None):\n         if mask is not None:\n\n@@ -247,6 +247,7 @@ class Layer(BackendLayer, Operation):\n \n         self._trainable = trainable\n         self._losses = []\n+        self._loss_ids = set()\n \n         self._call_signature_parameters = [\n             p.name for p in inspect.signature(self.call).parameters.values()\n@@ -844,14 +845,26 @@ class Layer(BackendLayer, Operation):\n             if scope.collect_losses:\n                 for x in losses:\n                     scope.add_loss(loss)\n+                    self._loss_ids.add(id(loss))\n         else:\n             self._losses.extend(losses)\n \n+    def _get_own_losses(self):\n+        if backend.in_stateless_scope():\n+            losses = []\n+            scope = backend.get_stateless_scope()\n+            for loss in scope.losses:\n+                if id(loss) in self._loss_ids:\n+                    losses.append(loss)\n+            return losses\n+        else:\n+            return self._losses[:]\n+\n     @property\n     def losses(self):\n-        losses = self._losses[:]\n+        losses = self._get_own_losses()\n         for layer in self._layers:\n-            losses.extend(layer._losses)\n+            losses.extend(layer._get_own_losses())\n         weight_regularization_losses = []\n         for v in self.trainable_weights:\n             regularizer = getattr(v, \"regularizer\", None)\n@@ -860,6 +873,18 @@ class Layer(BackendLayer, Operation):\n         losses.extend(weight_regularization_losses)\n         return losses\n \n+    def _clear_losses(self):\n+        if backend.in_stateless_scope():\n+            scope = backend.get_stateless_scope()\n+            if scope.collect_losses:\n+                for x in scope.losses:\n+                    if id(x) in self._loss_ids:\n+                        scope.losses.remove(x)\n+        self._losses.clear()\n+        self._loss_ids.clear()\n+        for layer in self._layers:\n+            layer._clear_losses()\n+\n     def save_own_variables(self, store):\n         \"\"\"Saves the state of the layer.\n \n@@ -917,17 +942,6 @@ class Layer(BackendLayer, Operation):\n         for i, v in enumerate(all_vars):\n             v.assign(store[f\"{i}\"])\n \n-    def _clear_losses(self):\n-        if backend.in_stateless_scope():\n-            scope = backend.get_stateless_scope()\n-            if scope.collect_losses:\n-                for x in scope.losses:\n-                    if x in self._losses:\n-                        scope.losses.remove(x)\n-        self._losses.clear()\n-        for layer in self._layers:\n-            layer._clear_losses()\n-\n     def _track_variable(self, variable):\n         if variable.trainable:\n             self._tracker.add_to_store(\"trainable_variables\", variable)\n\n@@ -51,7 +51,6 @@ class Dropout(Layer):\n         self.noise_shape = noise_shape\n         self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n-        self.built = True\n \n     def call(self, inputs, training=False):\n         if training and self.rate > 0:\n\n@@ -211,7 +211,7 @@ class Functional(Function, Model):\n         if isinstance(inputs, dict):\n             ref_inputs = self._inputs_struct\n             if not nest.is_nested(ref_inputs):\n-                ref_inputs = [self._inputs_struct]\n+                ref_inputs = [self._nested_inputs]\n             if isinstance(ref_inputs, dict):\n                 # In the case that the graph is constructed with dict input\n                 # tensors, We will use the original dict key to map with the\n\n@@ -99,24 +99,6 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertEqual(out_val.shape, (2, 4))\n \n-    def test_named_input_dict_io(self):\n-        input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n-        x = layers.Dense(5)(input_a)\n-        outputs = layers.Dense(4)(x)\n-\n-        model = Functional(input_a, outputs)\n-\n-        # Eager call\n-        in_val = {\"a\": np.random.random((2, 3))}\n-        out_val = model(in_val)\n-        self.assertEqual(out_val.shape, (2, 4))\n-\n-        # Symbolic call\n-        input_a_2 = Input(shape=(3,), batch_size=2)\n-        in_val = {\"a\": input_a_2}\n-        out_val = model(in_val)\n-        self.assertEqual(out_val.shape, (2, 4))\n-\n     def test_layer_getters(self):\n         # Test mixing ops and layers\n         input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n\n@@ -123,7 +123,9 @@ class Trainer:\n         for m in self.metrics:\n             m.reset_state()\n \n-    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n+    def compute_loss(\n+        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n+    ):\n         \"\"\"Compute the total loss, validate it, and return it.\n \n         Subclasses can optionally override this method to provide custom loss\n@@ -167,6 +169,9 @@ class Trainer:\n             y: Target data.\n             y_pred: Predictions returned by the model (output of `model(x)`)\n             sample_weight: Sample weights for weighting the loss function.\n+            allow_empty: If `False`, the method will error out if\n+                no loss has been computed by the model. If `True`, then\n+                if no loss is computed, the method returns 0.\n \n         Returns:\n             The total loss as a scalar tensor, or `None` if no loss results\n@@ -180,12 +185,14 @@ class Trainer:\n                 losses.append(loss)\n         for loss in self.losses:\n             losses.append(ops.cast(loss, dtype=backend.floatx()))\n-        if len(losses) == 0:\n+        if not allow_empty and len(losses) == 0:\n             raise ValueError(\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n             )\n         if len(losses) == 1:\n             total_loss = losses[0]\n+        elif len(losses) == 0:\n+            total_loss = ops.zeros(())\n         else:\n             total_loss = ops.sum(losses)\n         return total_loss\n\n@@ -8,6 +8,7 @@ from keras_core import initializers\n from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n+from keras_core import operations as ops\n from keras_core import optimizers\n from keras_core import testing\n from keras_core.callbacks.callback import Callback\n@@ -431,3 +432,21 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         )\n         model.evaluate(x_test, y_test, batch_size=4)\n         model.predict(x_test, batch_size=4)\n+\n+    def test_internal_only_loss(self):\n+        class LossLayer(layers.Layer):\n+            def call(self, x):\n+                self.add_loss(ops.sum(x))\n+                return x\n+\n+        model = keras_core.Sequential(\n+            [\n+                layers.Dense(2),\n+                LossLayer(),\n+                layers.Dense(1),\n+            ]\n+        )\n+        model.compile(optimizer=\"adam\")\n+        x = np.ones((16, 2))\n+        y = np.zeros((16, 1))\n+        model.fit(x, y, batch_size=4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a63f331d4e67e77adef2632e1df75d26529ca4f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 69 | Files Changed: 9 | Hunks: 23 | Methods Changed: 16 | Complexity Δ (Sum/Max): -7/2 | Churn Δ: 141 | Churn Cumulative: 9955 | Contributors (this commit): 14 | Commits (past 90d): 340 | Contributors (cumulative): 61 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ import tensorflow as tf  # for nest\n \n from keras_core import backend\n from keras_core import callbacks as callbacks_module\n-from keras_core import operations as ops\n from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n from keras_core.trainers.data_adapters import data_adapter_utils\n@@ -32,16 +31,11 @@ class JAXTrainer(base_trainer.Trainer):\n         kwargs = {}\n         if self._call_has_training_arg():\n             kwargs[\"training\"] = training\n-        y_pred, non_trainable_variables, losses = self.stateless_call(\n-            trainable_variables,\n-            non_trainable_variables,\n-            x,\n-            return_losses=True,\n-            **kwargs,\n+        y_pred, non_trainable_variables = self.stateless_call(\n+            trainable_variables, non_trainable_variables, x, **kwargs\n         )\n-        loss = self.compute_loss(x, y, y_pred, sample_weight, allow_empty=True)\n-        if losses:\n-            loss += ops.sum(losses)\n+\n+        loss = self.compute_loss(x, y, y_pred, sample_weight)\n         return loss, (y_pred, non_trainable_variables)\n \n     def _eager_build(self, data_batch):\n\n@@ -302,8 +302,9 @@ def slice_update(inputs, start_indices, updates):\n         python_slice(start_index, start_index + update_length)\n         for start_index, update_length in zip(start_indices, updates.shape)\n     ]\n-    inputs[slices] = updates\n-    return inputs\n+    outputs = torch.clone(inputs)\n+    outputs[slices] = updates\n+    return outputs\n \n \n def while_loop(\n\n@@ -39,6 +39,7 @@ class Softmax(Layer):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n         self.axis = axis\n+        self.built = True\n \n     def call(self, inputs, mask=None):\n         if mask is not None:\n\n@@ -247,7 +247,6 @@ class Layer(BackendLayer, Operation):\n \n         self._trainable = trainable\n         self._losses = []\n-        self._loss_ids = set()\n \n         self._call_signature_parameters = [\n             p.name for p in inspect.signature(self.call).parameters.values()\n@@ -845,26 +844,14 @@ class Layer(BackendLayer, Operation):\n             if scope.collect_losses:\n                 for x in losses:\n                     scope.add_loss(loss)\n-                    self._loss_ids.add(id(loss))\n         else:\n             self._losses.extend(losses)\n \n-    def _get_own_losses(self):\n-        if backend.in_stateless_scope():\n-            losses = []\n-            scope = backend.get_stateless_scope()\n-            for loss in scope.losses:\n-                if id(loss) in self._loss_ids:\n-                    losses.append(loss)\n-            return losses\n-        else:\n-            return self._losses[:]\n-\n     @property\n     def losses(self):\n-        losses = self._get_own_losses()\n+        losses = self._losses[:]\n         for layer in self._layers:\n-            losses.extend(layer._get_own_losses())\n+            losses.extend(layer._losses)\n         weight_regularization_losses = []\n         for v in self.trainable_weights:\n             regularizer = getattr(v, \"regularizer\", None)\n@@ -873,18 +860,6 @@ class Layer(BackendLayer, Operation):\n         losses.extend(weight_regularization_losses)\n         return losses\n \n-    def _clear_losses(self):\n-        if backend.in_stateless_scope():\n-            scope = backend.get_stateless_scope()\n-            if scope.collect_losses:\n-                for x in scope.losses:\n-                    if id(x) in self._loss_ids:\n-                        scope.losses.remove(x)\n-        self._losses.clear()\n-        self._loss_ids.clear()\n-        for layer in self._layers:\n-            layer._clear_losses()\n-\n     def save_own_variables(self, store):\n         \"\"\"Saves the state of the layer.\n \n@@ -942,6 +917,17 @@ class Layer(BackendLayer, Operation):\n         for i, v in enumerate(all_vars):\n             v.assign(store[f\"{i}\"])\n \n+    def _clear_losses(self):\n+        if backend.in_stateless_scope():\n+            scope = backend.get_stateless_scope()\n+            if scope.collect_losses:\n+                for x in scope.losses:\n+                    if x in self._losses:\n+                        scope.losses.remove(x)\n+        self._losses.clear()\n+        for layer in self._layers:\n+            layer._clear_losses()\n+\n     def _track_variable(self, variable):\n         if variable.trainable:\n             self._tracker.add_to_store(\"trainable_variables\", variable)\n\n@@ -51,6 +51,7 @@ class Dropout(Layer):\n         self.noise_shape = noise_shape\n         self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs, training=False):\n         if training and self.rate > 0:\n\n@@ -211,7 +211,7 @@ class Functional(Function, Model):\n         if isinstance(inputs, dict):\n             ref_inputs = self._inputs_struct\n             if not nest.is_nested(ref_inputs):\n-                ref_inputs = [self._nested_inputs]\n+                ref_inputs = [self._inputs_struct]\n             if isinstance(ref_inputs, dict):\n                 # In the case that the graph is constructed with dict input\n                 # tensors, We will use the original dict key to map with the\n@@ -225,7 +225,7 @@ class Functional(Function, Model):\n                 ]\n             # Raise an warning if there are more input data comparing to input\n             # tensor\n-            if allow_extra_keys and len(inputs) > len(ref_input_names):\n+            if not allow_extra_keys and len(inputs) > len(ref_input_names):\n                 warnings.warn(\n                     \"Input dict contained keys {} which did not match any \"\n                     \"model input. They will be ignored by the model.\".format(\n\n@@ -1,3 +1,5 @@\n+import warnings\n+\n import numpy as np\n \n from keras_core import backend\n@@ -99,6 +101,50 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertEqual(out_val.shape, (2, 4))\n \n+    def test_named_input_dict_io(self):\n+        input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n+        x = layers.Dense(5)(input_a)\n+        outputs = layers.Dense(4)(x)\n+\n+        model = Functional(input_a, outputs)\n+\n+        # Eager call\n+        in_val = {\"a\": np.random.random((2, 3))}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n+        # Symbolic call\n+        input_a_2 = Input(shape=(3,), batch_size=2)\n+        in_val = {\"a\": input_a_2}\n+        out_val = model(in_val)\n+        self.assertEqual(out_val.shape, (2, 4))\n+\n+    def test_input_dict_with_extra_field(self):\n+        input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n+        x = input_a * 5\n+        outputs = x + 2\n+\n+        model = Functional({\"a\": input_a}, outputs)\n+\n+        # Eager call\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            in_val = {\n+                \"a\": np.random.random((2, 3)),\n+                \"b\": np.random.random((2, 1)),\n+            }\n+            out_val = model(in_val)\n+            self.assertEqual(out_val.shape, (2, 3))\n+\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"error\")\n+            # Symbolic call\n+            input_a_2 = Input(shape=(3,), batch_size=2)\n+            input_b_2 = Input(shape=(1,), batch_size=2)\n+            in_val = {\"a\": input_a_2, \"b\": input_b_2}\n+            out_val = model(in_val)\n+            self.assertEqual(out_val.shape, (2, 3))\n+\n     def test_layer_getters(self):\n         # Test mixing ops and layers\n         input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n\n@@ -123,9 +123,7 @@ class Trainer:\n         for m in self.metrics:\n             m.reset_state()\n \n-    def compute_loss(\n-        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n-    ):\n+    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n         \"\"\"Compute the total loss, validate it, and return it.\n \n         Subclasses can optionally override this method to provide custom loss\n@@ -169,9 +167,6 @@ class Trainer:\n             y: Target data.\n             y_pred: Predictions returned by the model (output of `model(x)`)\n             sample_weight: Sample weights for weighting the loss function.\n-            allow_empty: If `False`, the method will error out if\n-                no loss has been computed by the model. If `True`, then\n-                if no loss is computed, the method returns 0.\n \n         Returns:\n             The total loss as a scalar tensor, or `None` if no loss results\n@@ -185,14 +180,12 @@ class Trainer:\n                 losses.append(loss)\n         for loss in self.losses:\n             losses.append(ops.cast(loss, dtype=backend.floatx()))\n-        if not allow_empty and len(losses) == 0:\n+        if len(losses) == 0:\n             raise ValueError(\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n             )\n         if len(losses) == 1:\n             total_loss = losses[0]\n-        elif len(losses) == 0:\n-            total_loss = ops.zeros(())\n         else:\n             total_loss = ops.sum(losses)\n         return total_loss\n\n@@ -8,7 +8,6 @@ from keras_core import initializers\n from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n-from keras_core import operations as ops\n from keras_core import optimizers\n from keras_core import testing\n from keras_core.callbacks.callback import Callback\n@@ -432,21 +431,3 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         )\n         model.evaluate(x_test, y_test, batch_size=4)\n         model.predict(x_test, batch_size=4)\n-\n-    def test_internal_only_loss(self):\n-        class LossLayer(layers.Layer):\n-            def call(self, x):\n-                self.add_loss(ops.sum(x))\n-                return x\n-\n-        model = keras_core.Sequential(\n-            [\n-                layers.Dense(2),\n-                LossLayer(),\n-                layers.Dense(1),\n-            ]\n-        )\n-        model.compile(optimizer=\"adam\")\n-        x = np.ones((16, 2))\n-        y = np.zeros((16, 1))\n-        model.fit(x, y, batch_size=4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca0acc4b71b3af34df99226d13c6b7c8cbebb4ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 16 | Methods Changed: 10 | Complexity Δ (Sum/Max): 9/4 | Churn Δ: 84 | Churn Cumulative: 6746 | Contributors (this commit): 12 | Commits (past 90d): 219 | Contributors (cumulative): 31 | DMM Complexity: 0.85\n\nDIFF:\n@@ -4,6 +4,7 @@ import tensorflow as tf  # for nest\n \n from keras_core import backend\n from keras_core import callbacks as callbacks_module\n+from keras_core import operations as ops\n from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n from keras_core.trainers.data_adapters import data_adapter_utils\n@@ -31,11 +32,16 @@ class JAXTrainer(base_trainer.Trainer):\n         kwargs = {}\n         if self._call_has_training_arg():\n             kwargs[\"training\"] = training\n-        y_pred, non_trainable_variables = self.stateless_call(\n-            trainable_variables, non_trainable_variables, x, **kwargs\n+        y_pred, non_trainable_variables, losses = self.stateless_call(\n+            trainable_variables,\n+            non_trainable_variables,\n+            x,\n+            return_losses=True,\n+            **kwargs,\n         )\n-\n-        loss = self.compute_loss(x, y, y_pred, sample_weight)\n+        loss = self.compute_loss(x, y, y_pred, sample_weight, allow_empty=True)\n+        if losses:\n+            loss += ops.sum(losses)\n         return loss, (y_pred, non_trainable_variables)\n \n     def _eager_build(self, data_batch):\n\n@@ -247,6 +247,7 @@ class Layer(BackendLayer, Operation):\n \n         self._trainable = trainable\n         self._losses = []\n+        self._loss_ids = set()\n \n         self._call_signature_parameters = [\n             p.name for p in inspect.signature(self.call).parameters.values()\n@@ -844,14 +845,26 @@ class Layer(BackendLayer, Operation):\n             if scope.collect_losses:\n                 for x in losses:\n                     scope.add_loss(loss)\n+                    self._loss_ids.add(id(loss))\n         else:\n             self._losses.extend(losses)\n \n+    def _get_own_losses(self):\n+        if backend.in_stateless_scope():\n+            losses = []\n+            scope = backend.get_stateless_scope()\n+            for loss in scope.losses:\n+                if id(loss) in self._loss_ids:\n+                    losses.append(loss)\n+            return losses\n+        else:\n+            return self._losses[:]\n+\n     @property\n     def losses(self):\n-        losses = self._losses[:]\n+        losses = self._get_own_losses()\n         for layer in self._layers:\n-            losses.extend(layer._losses)\n+            losses.extend(layer._get_own_losses())\n         weight_regularization_losses = []\n         for v in self.trainable_weights:\n             regularizer = getattr(v, \"regularizer\", None)\n@@ -860,6 +873,18 @@ class Layer(BackendLayer, Operation):\n         losses.extend(weight_regularization_losses)\n         return losses\n \n+    def _clear_losses(self):\n+        if backend.in_stateless_scope():\n+            scope = backend.get_stateless_scope()\n+            if scope.collect_losses:\n+                for x in scope.losses:\n+                    if id(x) in self._loss_ids:\n+                        scope.losses.remove(x)\n+        self._losses.clear()\n+        self._loss_ids.clear()\n+        for layer in self._layers:\n+            layer._clear_losses()\n+\n     def save_own_variables(self, store):\n         \"\"\"Saves the state of the layer.\n \n@@ -917,17 +942,6 @@ class Layer(BackendLayer, Operation):\n         for i, v in enumerate(all_vars):\n             v.assign(store[f\"{i}\"])\n \n-    def _clear_losses(self):\n-        if backend.in_stateless_scope():\n-            scope = backend.get_stateless_scope()\n-            if scope.collect_losses:\n-                for x in scope.losses:\n-                    if x in self._losses:\n-                        scope.losses.remove(x)\n-        self._losses.clear()\n-        for layer in self._layers:\n-            layer._clear_losses()\n-\n     def _track_variable(self, variable):\n         if variable.trainable:\n             self._tracker.add_to_store(\"trainable_variables\", variable)\n\n@@ -123,7 +123,9 @@ class Trainer:\n         for m in self.metrics:\n             m.reset_state()\n \n-    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n+    def compute_loss(\n+        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n+    ):\n         \"\"\"Compute the total loss, validate it, and return it.\n \n         Subclasses can optionally override this method to provide custom loss\n@@ -167,6 +169,9 @@ class Trainer:\n             y: Target data.\n             y_pred: Predictions returned by the model (output of `model(x)`)\n             sample_weight: Sample weights for weighting the loss function.\n+            allow_empty: If `False`, the method will error out if\n+                no loss has been computed by the model. If `True`, then\n+                if no loss is computed, the method returns 0.\n \n         Returns:\n             The total loss as a scalar tensor, or `None` if no loss results\n@@ -180,12 +185,14 @@ class Trainer:\n                 losses.append(loss)\n         for loss in self.losses:\n             losses.append(ops.cast(loss, dtype=backend.floatx()))\n-        if len(losses) == 0:\n+        if not allow_empty and len(losses) == 0:\n             raise ValueError(\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n             )\n         if len(losses) == 1:\n             total_loss = losses[0]\n+        elif len(losses) == 0:\n+            total_loss = ops.zeros(())\n         else:\n             total_loss = ops.sum(losses)\n         return total_loss\n\n@@ -8,6 +8,7 @@ from keras_core import initializers\n from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n+from keras_core import operations as ops\n from keras_core import optimizers\n from keras_core import testing\n from keras_core.callbacks.callback import Callback\n@@ -431,3 +432,21 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         )\n         model.evaluate(x_test, y_test, batch_size=4)\n         model.predict(x_test, batch_size=4)\n+\n+    def test_internal_only_loss(self):\n+        class LossLayer(layers.Layer):\n+            def call(self, x):\n+                self.add_loss(ops.sum(x))\n+                return x\n+\n+        model = keras_core.Sequential(\n+            [\n+                layers.Dense(2),\n+                LossLayer(),\n+                layers.Dense(1),\n+            ]\n+        )\n+        model.compile(optimizer=\"adam\")\n+        x = np.ones((16, 2))\n+        y = np.zeros((16, 1))\n+        model.fit(x, y, batch_size=4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
