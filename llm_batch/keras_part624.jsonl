{"custom_id": "keras#16c9a0af224e0bec4e14c89963d86251216a89a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 767 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 12 | Complexity Δ (Sum/Max): 14/9 | Churn Δ: 767 | Churn Cumulative: 767 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,131 @@\n+\"\"\"\n+Title: Character-level text generation with LSTM\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2015/06/15\n+Last modified: 2020/04/30\n+Description: Generate text from Nietzsche's writings with a character-level LSTM.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example demonstrates how to use a LSTM model to generate\n+text character-by-character.\n+\n+At least 20 epochs are required before the generated text\n+starts sounding locally coherent.\n+\n+It is recommended to run this script on GPU, as recurrent\n+networks are quite computationally intensive.\n+\n+If you try this script on new data, make sure your corpus\n+has at least ~100k characters. ~1M is better.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+import keras_core as keras\n+from keras_core import layers\n+\n+import numpy as np\n+import random\n+import io\n+\n+\"\"\"\n+## Prepare the data\n+\"\"\"\n+\n+path = keras.utils.get_file(\n+    \"nietzsche.txt\",\n+    origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\",\n+)\n+with io.open(path, encoding=\"utf-8\") as f:\n+    text = f.read().lower()\n+text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n+print(\"Corpus length:\", len(text))\n+\n+chars = sorted(list(set(text)))\n+print(\"Total chars:\", len(chars))\n+char_indices = dict((c, i) for i, c in enumerate(chars))\n+indices_char = dict((i, c) for i, c in enumerate(chars))\n+\n+# cut the text in semi-redundant sequences of maxlen characters\n+maxlen = 40\n+step = 3\n+sentences = []\n+next_chars = []\n+for i in range(0, len(text) - maxlen, step):\n+    sentences.append(text[i : i + maxlen])\n+    next_chars.append(text[i + maxlen])\n+print(\"Number of sequences:\", len(sentences))\n+\n+x = np.zeros((len(sentences), maxlen, len(chars)), dtype=\"bool\")\n+y = np.zeros((len(sentences), len(chars)), dtype=\"bool\")\n+for i, sentence in enumerate(sentences):\n+    for t, char in enumerate(sentence):\n+        x[i, t, char_indices[char]] = 1\n+    y[i, char_indices[next_chars[i]]] = 1\n+\n+\n+\"\"\"\n+## Build the model: a single LSTM layer\n+\"\"\"\n+\n+model = keras.Sequential(\n+    [\n+        keras.Input(shape=(maxlen, len(chars))),\n+        layers.LSTM(128),\n+        layers.Dense(len(chars), activation=\"softmax\"),\n+    ]\n+)\n+optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n+model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n+\n+\"\"\"\n+## Prepare the text sampling function\n+\"\"\"\n+\n+\n+def sample(preds, temperature=1.0):\n+    # helper function to sample an index from a probability array\n+    preds = np.asarray(preds).astype(\"float64\")\n+    preds = np.log(preds) / temperature\n+    exp_preds = np.exp(preds)\n+    preds = exp_preds / np.sum(exp_preds)\n+    probas = np.random.multinomial(1, preds, 1)\n+    return np.argmax(probas)\n+\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+epochs = 40\n+batch_size = 128\n+\n+for epoch in range(epochs):\n+    model.fit(x, y, batch_size=batch_size, epochs=1)\n+    print()\n+    print(\"Generating text after epoch: %d\" % epoch)\n+\n+    start_index = random.randint(0, len(text) - maxlen - 1)\n+    for diversity in [0.2, 0.5, 1.0, 1.2]:\n+        print(\"...Diversity:\", diversity)\n+\n+        generated = \"\"\n+        sentence = text[start_index : start_index + maxlen]\n+        print('...Generating with seed: \"' + sentence + '\"')\n+\n+        for i in range(400):\n+            x_pred = np.zeros((1, maxlen, len(chars)))\n+            for t, char in enumerate(sentence):\n+                x_pred[0, t, char_indices[char]] = 1.0\n+            preds = model.predict(x_pred, verbose=0)[0]\n+            next_index = sample(preds, diversity)\n+            next_char = indices_char[next_index]\n+            sentence = sentence[1:] + next_char\n+            generated += next_char\n+\n+        print(\"...Generated: \", generated)\n+        print(\"-\")\n\n@@ -0,0 +1,285 @@\n+\"\"\"\n+Title: Neural style transfer\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2016/01/11\n+Last modified: 2020/05/02\n+Description: Transferring the style of a reference image to target image using gradient descent.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+Style transfer consists in generating an image\n+with the same \"content\" as a base image, but with the\n+\"style\" of a different picture (typically artistic).\n+This is achieved through the optimization of a loss function\n+that has 3 components: \"style loss\", \"content loss\",\n+and \"total variation loss\":\n+\n+- The total variation loss imposes local spatial continuity between\n+the pixels of the combination image, giving it visual coherence.\n+- The style loss is where the deep learning keeps in --that one is defined\n+using a deep convolutional neural network. Precisely, it consists in a sum of\n+L2 distances between the Gram matrices of the representations of\n+the base image and the style reference image, extracted from\n+different layers of a convnet (trained on ImageNet). The general idea\n+is to capture color/texture information at different spatial\n+scales (fairly large scales --defined by the depth of the layer considered).\n+- The content loss is a L2 distance between the features of the base\n+image (extracted from a deep layer) and the features of the combination image,\n+keeping the generated image close enough to the original one.\n+\n+**Reference:** [A Neural Algorithm of Artistic Style](\n+  http://arxiv.org/abs/1508.06576)\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core.applications import vgg19\n+\n+base_image_path = keras.utils.get_file(\n+    \"paris.jpg\", \"https://i.imgur.com/F28w3Ac.jpg\"\n+)\n+style_reference_image_path = keras.utils.get_file(\n+    \"starry_night.jpg\", \"https://i.imgur.com/9ooB60I.jpg\"\n+)\n+result_prefix = \"paris_generated\"\n+\n+# Weights of the different loss components\n+total_variation_weight = 1e-6\n+style_weight = 1e-6\n+content_weight = 2.5e-8\n+\n+# Dimensions of the generated picture.\n+width, height = keras.utils.load_img(base_image_path).size\n+img_nrows = 400\n+img_ncols = int(width * img_nrows / height)\n+\n+\"\"\"\n+## Let's take a look at our base (content) image and our style reference image\n+\"\"\"\n+\n+from IPython.display import Image, display\n+\n+display(Image(base_image_path))\n+display(Image(style_reference_image_path))\n+\n+\"\"\"\n+## Image preprocessing / deprocessing utilities\n+\"\"\"\n+\n+\n+def preprocess_image(image_path):\n+    # Util function to open, resize and format pictures into appropriate tensors\n+    img = keras.utils.load_img(image_path, target_size=(img_nrows, img_ncols))\n+    img = keras.utils.img_to_array(img)\n+    img = np.expand_dims(img, axis=0)\n+    img = vgg19.preprocess_input(img)\n+    return tf.convert_to_tensor(img)\n+\n+\n+def deprocess_image(x):\n+    # Util function to convert a tensor into a valid image\n+    x = x.reshape((img_nrows, img_ncols, 3))\n+    # Remove zero-center by mean pixel\n+    x[:, :, 0] += 103.939\n+    x[:, :, 1] += 116.779\n+    x[:, :, 2] += 123.68\n+    # 'BGR'->'RGB'\n+    x = x[:, :, ::-1]\n+    x = np.clip(x, 0, 255).astype(\"uint8\")\n+    return x\n+\n+\n+\"\"\"\n+## Compute the style transfer loss\n+\n+First, we need to define 4 utility functions:\n+\n+- `gram_matrix` (used to compute the style loss)\n+- The `style_loss` function, which keeps the generated image close to the local textures\n+of the style reference image\n+- The `content_loss` function, which keeps the high-level representation of the\n+generated image close to that of the base image\n+- The `total_variation_loss` function, a regularization loss which keeps the generated\n+image locally-coherent\n+\"\"\"\n+\n+# The gram matrix of an image tensor (feature-wise outer product)\n+\n+\n+def gram_matrix(x):\n+    x = tf.transpose(x, (2, 0, 1))\n+    features = tf.reshape(x, (tf.shape(x)[0], -1))\n+    gram = tf.matmul(features, tf.transpose(features))\n+    return gram\n+\n+\n+# The \"style loss\" is designed to maintain\n+# the style of the reference image in the generated image.\n+# It is based on the gram matrices (which capture style) of\n+# feature maps from the style reference image\n+# and from the generated image\n+\n+\n+def style_loss(style, combination):\n+    S = gram_matrix(style)\n+    C = gram_matrix(combination)\n+    channels = 3\n+    size = img_nrows * img_ncols\n+    return tf.reduce_sum(tf.square(S - C)) / (\n+        4.0 * (channels**2) * (size**2)\n+    )\n+\n+\n+# An auxiliary loss function\n+# designed to maintain the \"content\" of the\n+# base image in the generated image\n+\n+\n+def content_loss(base, combination):\n+    return tf.reduce_sum(tf.square(combination - base))\n+\n+\n+# The 3rd loss function, total variation loss,\n+# designed to keep the generated image locally coherent\n+\n+\n+def total_variation_loss(x):\n+    a = tf.square(\n+        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n+    )\n+    b = tf.square(\n+        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n+    )\n+    return tf.reduce_sum(tf.pow(a + b, 1.25))\n+\n+\n+\"\"\"\n+Next, let's create a feature extraction model that retrieves the intermediate activations\n+of VGG19 (as a dict, by name).\n+\"\"\"\n+\n+# Build a VGG19 model loaded with pre-trained ImageNet weights\n+model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n+\n+# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n+outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n+\n+# Set up a model that returns the activation values for every layer in\n+# VGG19 (as a dict).\n+feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n+\n+\"\"\"\n+Finally, here's the code that computes the style transfer loss.\n+\"\"\"\n+\n+# List of layers to use for the style loss.\n+style_layer_names = [\n+    \"block1_conv1\",\n+    \"block2_conv1\",\n+    \"block3_conv1\",\n+    \"block4_conv1\",\n+    \"block5_conv1\",\n+]\n+# The layer to use for the content loss.\n+content_layer_name = \"block5_conv2\"\n+\n+\n+def compute_loss(combination_image, base_image, style_reference_image):\n+    input_tensor = tf.concat(\n+        [base_image, style_reference_image, combination_image], axis=0\n+    )\n+    features = feature_extractor(input_tensor)\n+\n+    # Initialize the loss\n+    loss = tf.zeros(shape=())\n+\n+    # Add content loss\n+    layer_features = features[content_layer_name]\n+    base_image_features = layer_features[0, :, :, :]\n+    combination_features = layer_features[2, :, :, :]\n+    loss = loss + content_weight * content_loss(\n+        base_image_features, combination_features\n+    )\n+    # Add style loss\n+    for layer_name in style_layer_names:\n+        layer_features = features[layer_name]\n+        style_reference_features = layer_features[1, :, :, :]\n+        combination_features = layer_features[2, :, :, :]\n+        sl = style_loss(style_reference_features, combination_features)\n+        loss += (style_weight / len(style_layer_names)) * sl\n+\n+    # Add total variation loss\n+    loss += total_variation_weight * total_variation_loss(combination_image)\n+    return loss\n+\n+\n+\"\"\"\n+## Add a tf.function decorator to loss & gradient computation\n+\n+To compile it, and thus make it fast.\n+\"\"\"\n+\n+\n+@tf.function\n+def compute_loss_and_grads(\n+    combination_image, base_image, style_reference_image\n+):\n+    with tf.GradientTape() as tape:\n+        loss = compute_loss(\n+            combination_image, base_image, style_reference_image\n+        )\n+    grads = tape.gradient(loss, combination_image)\n+    return loss, grads\n+\n+\n+\"\"\"\n+## The training loop\n+\n+Repeatedly run vanilla gradient descent steps to minimize the loss, and save the\n+resulting image every 100 iterations.\n+\n+We decay the learning rate by 0.96 every 100 steps.\n+\"\"\"\n+\n+optimizer = keras.optimizers.SGD(\n+    keras.optimizers.schedules.ExponentialDecay(\n+        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n+    )\n+)\n+\n+base_image = preprocess_image(base_image_path)\n+style_reference_image = preprocess_image(style_reference_image_path)\n+combination_image = tf.Variable(preprocess_image(base_image_path))\n+\n+iterations = 4000\n+for i in range(1, iterations + 1):\n+    loss, grads = compute_loss_and_grads(\n+        combination_image, base_image, style_reference_image\n+    )\n+    optimizer.apply_gradients([(grads, combination_image)])\n+    if i % 100 == 0:\n+        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n+        img = deprocess_image(combination_image.numpy())\n+        fname = result_prefix + \"_at_iteration_%d.png\" % i\n+        keras.utils.save_img(fname, img)\n+\n+\"\"\"\n+After 4000 iterations, you get the following result:\n+\"\"\"\n+\n+display(Image(result_prefix + \"_at_iteration_4000.png\"))\n+\n+\"\"\"\n+**Example available on HuggingFace**\n+Trained Model | Demo \n+--- | --- \n+[![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Neural%20style%20transfer-black.svg)](https://huggingface.co/keras-io/VGG19) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Neural%20style%20transfer-black.svg)](https://huggingface.co/spaces/keras-io/neural-style-transfer)\n+\"\"\"\n\n@@ -0,0 +1,351 @@\n+\"\"\"\n+Title: Structured data classification from scratch\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2020/06/09\n+Last modified: 2020/06/09\n+Description: Binary classification of structured data including numerical and categorical features.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example demonstrates how to do structured data classification, starting from a raw\n+CSV file. Our data includes both numerical and categorical features. We will use Keras\n+preprocessing layers to normalize the numerical features and vectorize the categorical\n+ones.\n+\n+Note that this example should be run with TensorFlow 2.5 or higher.\n+\n+### The dataset\n+\n+[Our dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) is provided by the\n+Cleveland Clinic Foundation for Heart Disease.\n+It's a CSV file with 303 rows. Each row contains information about a patient (a\n+**sample**), and each column describes an attribute of the patient (a **feature**). We\n+use the features to predict whether a patient has a heart disease (**binary\n+classification**).\n+\n+Here's the description of each feature:\n+\n+Column| Description| Feature Type\n+------------|--------------------|----------------------\n+Age | Age in years | Numerical\n+Sex | (1 = male; 0 = female) | Categorical\n+CP | Chest pain type (0, 1, 2, 3, 4) | Categorical\n+Trestbpd | Resting blood pressure (in mm Hg on admission) | Numerical\n+Chol | Serum cholesterol in mg/dl | Numerical\n+FBS | fasting blood sugar in 120 mg/dl (1 = true; 0 = false) | Categorical\n+RestECG | Resting electrocardiogram results (0, 1, 2) | Categorical\n+Thalach | Maximum heart rate achieved | Numerical\n+Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical\n+Oldpeak | ST depression induced by exercise relative to rest | Numerical\n+Slope | Slope of the peak exercise ST segment | Numerical\n+CA | Number of major vessels (0-3) colored by fluoroscopy | Both numerical & categorical\n+Thal | 3 = normal; 6 = fixed defect; 7 = reversible defect | Categorical\n+Target | Diagnosis of heart disease (1 = true; 0 = false) | Target\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import tensorflow as tf\n+import pandas as pd\n+import keras_core as keras\n+from keras_core import layers\n+\n+\"\"\"\n+## Preparing the data\n+\n+Let's download the data and load it into a Pandas dataframe:\n+\"\"\"\n+\n+file_url = (\n+    \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n+)\n+dataframe = pd.read_csv(file_url)\n+\n+\"\"\"\n+The dataset includes 303 samples with 14 columns per sample (13 features, plus the target\n+label):\n+\"\"\"\n+\n+dataframe.shape\n+\n+\"\"\"\n+Here's a preview of a few samples:\n+\"\"\"\n+\n+dataframe.head()\n+\n+\"\"\"\n+The last column, \"target\", indicates whether the patient has a heart disease (1) or not\n+(0).\n+\n+Let's split the data into a training and validation set:\n+\"\"\"\n+\n+val_dataframe = dataframe.sample(frac=0.2, random_state=1337)\n+train_dataframe = dataframe.drop(val_dataframe.index)\n+\n+print(\n+    f\"Using {len(train_dataframe)} samples for training \"\n+    f\"and {len(val_dataframe)} for validation\"\n+)\n+\n+\"\"\"\n+Let's generate `tf.data.Dataset` objects for each dataframe:\n+\"\"\"\n+\n+\n+def dataframe_to_dataset(dataframe):\n+    dataframe = dataframe.copy()\n+    labels = dataframe.pop(\"target\")\n+    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n+    ds = ds.shuffle(buffer_size=len(dataframe))\n+    return ds\n+\n+\n+train_ds = dataframe_to_dataset(train_dataframe)\n+val_ds = dataframe_to_dataset(val_dataframe)\n+\n+\"\"\"\n+Each `Dataset` yields a tuple `(input, target)` where `input` is a dictionary of features\n+and `target` is the value `0` or `1`:\n+\"\"\"\n+\n+for x, y in train_ds.take(1):\n+    print(\"Input:\", x)\n+    print(\"Target:\", y)\n+\n+\"\"\"\n+Let's batch the datasets:\n+\"\"\"\n+\n+train_ds = train_ds.batch(32)\n+val_ds = val_ds.batch(32)\n+\n+\"\"\"\n+## Feature preprocessing with Keras layers\n+\n+\n+The following features are categorical features encoded as integers:\n+\n+- `sex`\n+- `cp`\n+- `fbs`\n+- `restecg`\n+- `exang`\n+- `ca`\n+\n+We will encode these features using **one-hot encoding**. We have two options\n+here:\n+\n+ - Use `CategoryEncoding()`, which requires knowing the range of input values\n+ and will error on input outside the range.\n+ - Use `IntegerLookup()` which will build a lookup table for inputs and reserve\n+ an output index for unkown input values.\n+\n+For this example, we want a simple solution that will handle out of range inputs\n+at inference, so we will use `IntegerLookup()`.\n+\n+We also have a categorical feature encoded as a string: `thal`. We will create an\n+index of all possible features and encode output using the `StringLookup()` layer.\n+\n+Finally, the following feature are continuous numerical features:\n+\n+- `age`\n+- `trestbps`\n+- `chol`\n+- `thalach`\n+- `oldpeak`\n+- `slope`\n+\n+For each of these features, we will use a `Normalization()` layer to make sure the mean\n+of each feature is 0 and its standard deviation is 1.\n+\n+Below, we define 3 utility functions to do the operations:\n+\n+- `encode_numerical_feature` to apply featurewise normalization to numerical features.\n+- `encode_string_categorical_feature` to first turn string inputs into integer indices,\n+then one-hot encode these integer indices.\n+- `encode_integer_categorical_feature` to one-hot encode integer categorical features.\n+\"\"\"\n+\n+from keras_core.layers import IntegerLookup\n+from keras_core.layers import Normalization\n+from keras_core.layers import StringLookup\n+\n+\n+def encode_numerical_feature(feature, name, dataset):\n+    # Create a Normalization layer for our feature\n+    normalizer = Normalization()\n+\n+    # Prepare a Dataset that only yields our feature\n+    feature_ds = dataset.map(lambda x, y: x[name])\n+    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n+\n+    # Learn the statistics of the data\n+    normalizer.adapt(feature_ds)\n+\n+    # Normalize the input feature\n+    encoded_feature = normalizer(feature)\n+    return encoded_feature\n+\n+\n+def encode_categorical_feature(feature, name, dataset, is_string):\n+    lookup_class = StringLookup if is_string else IntegerLookup\n+    # Create a lookup layer which will turn strings into integer indices\n+    lookup = lookup_class(output_mode=\"binary\")\n+\n+    # Prepare a Dataset that only yields our feature\n+    feature_ds = dataset.map(lambda x, y: x[name])\n+    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n+\n+    # Learn the set of possible string values and assign them a fixed integer index\n+    lookup.adapt(feature_ds)\n+\n+    # Turn the string input into integer indices\n+    encoded_feature = lookup(feature)\n+    return encoded_feature\n+\n+\n+\"\"\"\n+## Build a model\n+\n+With this done, we can create our end-to-end model:\n+\"\"\"\n+\n+# Categorical features encoded as integers\n+sex = keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\")\n+cp = keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\")\n+fbs = keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\")\n+restecg = keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\")\n+exang = keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\")\n+ca = keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")\n+\n+# Categorical feature encoded as string\n+thal = keras.Input(shape=(1,), name=\"thal\", dtype=\"string\")\n+\n+# Numerical features\n+age = keras.Input(shape=(1,), name=\"age\")\n+trestbps = keras.Input(shape=(1,), name=\"trestbps\")\n+chol = keras.Input(shape=(1,), name=\"chol\")\n+thalach = keras.Input(shape=(1,), name=\"thalach\")\n+oldpeak = keras.Input(shape=(1,), name=\"oldpeak\")\n+slope = keras.Input(shape=(1,), name=\"slope\")\n+\n+all_inputs = [\n+    sex,\n+    cp,\n+    fbs,\n+    restecg,\n+    exang,\n+    ca,\n+    thal,\n+    age,\n+    trestbps,\n+    chol,\n+    thalach,\n+    oldpeak,\n+    slope,\n+]\n+\n+# Integer categorical features\n+sex_encoded = encode_categorical_feature(sex, \"sex\", train_ds, False)\n+cp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False)\n+fbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False)\n+restecg_encoded = encode_categorical_feature(\n+    restecg, \"restecg\", train_ds, False\n+)\n+exang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False)\n+ca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)\n+\n+# String categorical features\n+thal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, True)\n+\n+# Numerical features\n+age_encoded = encode_numerical_feature(age, \"age\", train_ds)\n+trestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds)\n+chol_encoded = encode_numerical_feature(chol, \"chol\", train_ds)\n+thalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds)\n+oldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds)\n+slope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)\n+\n+all_features = layers.concatenate(\n+    [\n+        sex_encoded,\n+        cp_encoded,\n+        fbs_encoded,\n+        restecg_encoded,\n+        exang_encoded,\n+        slope_encoded,\n+        ca_encoded,\n+        thal_encoded,\n+        age_encoded,\n+        trestbps_encoded,\n+        chol_encoded,\n+        thalach_encoded,\n+        oldpeak_encoded,\n+    ]\n+)\n+x = layers.Dense(32, activation=\"relu\")(all_features)\n+x = layers.Dropout(0.5)(x)\n+output = layers.Dense(1, activation=\"sigmoid\")(x)\n+model = keras.Model(all_inputs, output)\n+model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n+\n+\"\"\"\n+Let's visualize our connectivity graph:\n+\"\"\"\n+\n+# `rankdir='LR'` is to make the graph horizontal.\n+keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+model.fit(train_ds, epochs=50, validation_data=val_ds)\n+\n+\"\"\"\n+We quickly get to 80% validation accuracy.\n+\"\"\"\n+\n+\"\"\"\n+## Inference on new data\n+\n+To get a prediction for a new sample, you can simply call `model.predict()`. There are\n+just two things you need to do:\n+\n+1. wrap scalars into a list so as to have a batch dimension (models only process batches\n+of data, not single samples)\n+2. Call `convert_to_tensor` on each feature\n+\"\"\"\n+\n+sample = {\n+    \"age\": 60,\n+    \"sex\": 1,\n+    \"cp\": 1,\n+    \"trestbps\": 145,\n+    \"chol\": 233,\n+    \"fbs\": 1,\n+    \"restecg\": 2,\n+    \"thalach\": 150,\n+    \"exang\": 0,\n+    \"oldpeak\": 2.3,\n+    \"slope\": 3,\n+    \"ca\": 0,\n+    \"thal\": \"fixed\",\n+}\n+\n+input_dict = {\n+    name: tf.convert_to_tensor([value]) for name, value in sample.items()\n+}\n+predictions = model.predict(input_dict)\n+\n+print(\n+    f\"This particular patient had a {100 * predictions[0][0]:.1f} \"\n+    \"percent probability of having a heart disease, \"\n+    \"as evaluated by our model.\"\n+)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ddf134ecdb5a1601289b12d14557dcc70b9c484e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 99/99 | Churn Δ: 15 | Churn Cumulative: 4489 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -52,18 +52,11 @@ class Sequential(functional.Functional):\n     Examples:\n \n     ```python\n-    # Optionally, the first layer can receive an `input_shape` argument:\n-    model = tf.keras.Sequential()\n-    model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\n-    # Afterwards, we do automatic shape inference:\n-    model.add(tf.keras.layers.Dense(4))\n-\n-    # This is identical to the following:\n     model = tf.keras.Sequential()\n     model.add(tf.keras.Input(shape=(16,)))\n     model.add(tf.keras.layers.Dense(8))\n \n-    # Note that you can also omit the `input_shape` argument.\n+    # Note that you can also omit the initial `Input`.\n     # In that case the model doesn't have any weights until the first call\n     # to a training/evaluation method (since it isn't yet built):\n     model = tf.keras.Sequential()\n@@ -71,13 +64,13 @@ class Sequential(functional.Functional):\n     model.add(tf.keras.layers.Dense(4))\n     # model.weights not created yet\n \n-    # Whereas if you specify the input shape, the model gets built\n+    # Whereas if you specify an `Input`, the model gets built\n     # continuously as you are adding layers:\n     model = tf.keras.Sequential()\n-    model.add(tf.keras.layers.Dense(8, input_shape=(16,)))\n+    model.add(tf.keras.Input(shape=(16,)))\n     model.add(tf.keras.layers.Dense(4))\n     len(model.weights)\n-    # Returns \"4\"\n+    # Returns \"2\"\n \n     # When using the delayed-build pattern (no input shape specified), you can\n     # choose to manually build your model by calling\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b14eaaf5da6fbb8749b39653266bb3114ac5d7d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 15/11 | Churn Δ: 87 | Churn Cumulative: 212 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,71 @@\n+from keras_core.backend.torch.core import convert_to_tensor\n+\n+RESIZE_METHODS = {}  # populated after torchvision import\n+\n+UNSUPPORTED_METHODS = (\n+    \"lanczos3\",\n+    \"lanczos5\",\n+)\n+\n+\n+def resize(\n+    image, size, method=\"bilinear\", antialias=False, data_format=\"channels_last\"\n+):\n+    try:\n+        import torchvision\n+        from torchvision.transforms import InterpolationMode as im\n+\n+        RESIZE_METHODS.update(\n+            {\n+                \"bilinear\": im.BILINEAR,\n+                \"nearest\": im.NEAREST_EXACT,\n+                \"bicubic\": im.BICUBIC,\n+            }\n+        )\n+    except:\n+        raise ImportError(\n+            \"The torchvision package is necessary to use `resize` with the \"\n+            \"torch backend. Please install torchvision.\"\n+        )\n+    if method in UNSUPPORTED_METHODS:\n+        raise ValueError(\n+            \"Resizing with Lanczos interpolation is \"\n+            \"not supported by the PyTorch backend. \"\n+            f\"Received: method={method}.\"\n+        )\n+    if method not in RESIZE_METHODS:\n+        raise ValueError(\n+            \"Invalid value for argument `method`. Expected of one \"\n+            f\"{RESIZE_METHODS}. Received: method={method}\"\n+        )\n+    if not len(size) == 2:\n+        raise ValueError(\n+            \"Argument `size` must be a tuple of two elements \"\n+            f\"(height, width). Received: size={size}\"\n+        )\n+    size = tuple(size)\n+    image = convert_to_tensor(image)\n+    if data_format == \"channels_last\":\n+        if image.ndim == 4:\n+            image = image.permute((0, 3, 1, 2))\n+        elif image.ndim == 3:\n+            image = image.permute((2, 0, 1))\n+        else:\n+            raise ValueError(\n+                \"Invalid input rank: expected rank 3 (single image) \"\n+                \"or rank 4 (batch of images). Received input with shape: \"\n+                f\"image.shape={image.shape}\"\n+            )\n+\n+    resized = torchvision.transforms.functional.resize(\n+        img=image,\n+        size=size,\n+        interpolation=RESIZE_METHODS[method],\n+        antialias=antialias,\n+    )\n+    if data_format == \"channels_last\":\n+        if len(image.shape) == 4:\n+            resized = resized.permute((0, 2, 3, 1))\n+        elif len(image.shape) == 3:\n+            resized = resized.permute((1, 2, 0))\n+    return resized\n\n@@ -2,6 +2,7 @@ import numpy as np\n import tensorflow as tf\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import testing\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.operations import image as kimage\n@@ -42,6 +43,21 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ]\n     )\n     def test_resize(self, method, antialias, data_format):\n+        if backend.backend() == \"torch\":\n+            if \"lanczos\" in method:\n+                self.skipTest(\n+                    \"Resizing with Lanczos interpolation is \"\n+                    \"not supported by the PyTorch backend. \"\n+                    f\"Received: method={method}.\"\n+                )\n+            if method == \"bicubic\" and antialias is False:\n+                self.skipTest(\n+                    \"Resizing with Bicubic interpolation in \"\n+                    \"PyTorch backend produces noise. Please \"\n+                    \"turn on anti-aliasing. \"\n+                    f\"Received: method={method}, \"\n+                    f\"antialias={antialias}.\"\n+                )\n         # Unbatched case\n         if data_format == \"channels_first\":\n             x = np.random.random((3, 50, 50)) * 255\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a33eb25ca94c0cf5ef682e71e7b7142fc5033dcb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1555 | Contributors (this commit): 9 | Commits (past 90d): 77 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ elif backend.backend() == \"torch\":\n     from keras_core.backend.torch.layer import TorchLayer as BackendLayer\n else:\n     raise RuntimeError(\n-        f\"Backend '{backend.backend()}' must implement the LayerMixin class.\"\n+        f\"Backend '{backend.backend()}' must implement a layer mixin class.\"\n     )\n \n \n\n@@ -5,8 +5,8 @@ from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n \n @keras_core_export(\n     [\n-        \"keras_core.layers.GlobalMaxPooling1D\",\n-        \"keras_core.layers.GlobalMaxPool1D\",\n+        \"keras_core.layers.GlobalMaxPooling2D\",\n+        \"keras_core.layers.GlobalMaxPool2D\",\n     ]\n )\n class GlobalMaxPooling2D(BaseGlobalPooling):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ad6fd768e4dd8b0c7fcc39165922342696a86100", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 593 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,10 +3,10 @@\n import os\n \n import numpy as np\n-from keras import backend\n-from keras.datasets.cifar import load_batch\n \n+from keras_core import backend\n from keras_core.api_export import keras_core_export\n+from keras_core.datasets.cifar import load_batch\n from keras_core.utils.file_utils import get_file\n \n \n\n@@ -119,10 +119,11 @@ class Nadam(optimizer.Optimizer):\n             self._u_product_counter += 1\n             return u_product_t\n \n-        if self._u_product_counter == (self.iterations + 2):\n-            u_product_t = get_cached_u_product()\n-        else:\n-            u_product_t = compute_new_u_product()\n+        u_product_t = ops.cond(\n+            ops.equal(self._u_product_counter, (self.iterations + 2)),\n+            get_cached_u_product,\n+            compute_new_u_product,\n+        )\n \n         u_product_t_1 = u_product_t * u_t_1\n         beta_2_power = ops.power(beta_2, local_step)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#304bb3d9ab137dd26263381977890f00aac62e75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 52 | Churn Cumulative: 21365 | Contributors (this commit): 55 | Commits (past 90d): 30 | Contributors (cumulative): 73 | DMM Complexity: None\n\nDIFF:\n@@ -84,22 +84,7 @@ def softmax(x, axis=-1):\n     >>> layer = tf.keras.layers.Dense(32,\n     ...                               activation=tf.keras.activations.softmax)\n     \"\"\"\n-    if x.shape.rank <= 1:\n-        raise ValueError(\n-            f\"Cannot apply softmax to a tensor that is 1D. Received input: {x}\"\n-        )\n-\n-    if isinstance(axis, int):\n-        output = tf.nn.softmax(x, axis=axis)\n-    else:\n-        # nn.softmax does not support tuple axis.\n-        numerator = tf.exp(x - tf.reduce_max(x, axis=axis, keepdims=True))\n-        denominator = tf.reduce_sum(numerator, axis=axis, keepdims=True)\n-        output = numerator / denominator\n-\n-    # Cache the logits to use for crossentropy loss.\n-    output._keras_logits = x\n-    return output\n+    return backend.softmax(x, axis)\n \n \n @keras_export(\"keras.activations.elu\")\n@@ -412,10 +397,7 @@ def sigmoid(x):\n     Returns:\n         Tensor with the sigmoid activation: `1 / (1 + exp(-x))`.\n     \"\"\"\n-    output = tf.sigmoid(x)\n-    # Cache the logits to use for crossentropy loss.\n-    output._keras_logits = x\n-    return output\n+    return backend.sigmoid(x)\n \n \n @keras_export(\"keras.activations.exponential\")\n\n@@ -5441,7 +5441,22 @@ def softmax(x, axis=-1):\n     Returns:\n         A tensor.\n     \"\"\"\n-    return tf.nn.softmax(x, axis=axis)\n+    if x.shape.rank <= 1:\n+        raise ValueError(\n+            f\"Cannot apply softmax to a tensor that is 1D. Received input: {x}\"\n+        )\n+\n+    if isinstance(axis, int):\n+        output = tf.nn.softmax(x, axis=axis)\n+    else:\n+        # nn.softmax does not support tuple axis.\n+        numerator = tf.exp(x - tf.reduce_max(x, axis=axis, keepdims=True))\n+        denominator = tf.reduce_sum(numerator, axis=axis, keepdims=True)\n+        output = numerator / denominator\n+\n+    # Cache the logits to use for crossentropy loss.\n+    output._keras_logits = x\n+    return output\n \n \n @keras_export(\"keras.backend.softplus\")\n@@ -5899,7 +5914,10 @@ def sigmoid(x):\n     Returns:\n         A tensor.\n     \"\"\"\n-    return tf.math.sigmoid(x)\n+    output = tf.sigmoid(x)\n+    # Cache the logits to use for crossentropy loss.\n+    output._keras_logits = x\n+    return output\n \n \n @keras_export(\"keras.backend.hard_sigmoid\")\n\n@@ -51,13 +51,13 @@ class Softmax(Layer):\n \n     Example without mask:\n \n-    >>> inp = np.asarray([1., 2., 1.])\n+    >>> inp = np.asarray([[1., 2., 1.]])\n     >>> layer = tf.keras.layers.Softmax()\n     >>> layer(inp).numpy()\n-    array([0.21194157, 0.5761169 , 0.21194157], dtype=float32)\n-    >>> mask = np.asarray([True, False, True], dtype=bool)\n+    array([[0.21194157, 0.5761169 , 0.21194157]], dtype=float32)\n+    >>> mask = np.asarray([[True, False, True]], dtype=bool)\n     >>> layer(inp, mask).numpy()\n-    array([0.5, 0. , 0.5], dtype=float32)\n+    array([[0.5, 0. , 0.5]], dtype=float32)\n \n     Input shape:\n         Arbitrary. Use the keyword argument `input_shape`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b18cc3bb4c5112c1298f45075bf153b098eb13e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 134 | Lines Deleted: 63 | Files Changed: 15 | Hunks: 33 | Methods Changed: 19 | Complexity Δ (Sum/Max): 5/1 | Churn Δ: 197 | Churn Cumulative: 2621 | Contributors (this commit): 10 | Commits (past 90d): 145 | Contributors (cumulative): 43 | DMM Complexity: 0.8627450980392157\n\nDIFF:\n@@ -91,9 +91,11 @@ def get(identifier):\n     if identifier is None:\n         return linear\n     if isinstance(identifier, (str, dict)):\n-        identifier = deserialize(identifier)\n-    if callable(identifier):\n-        return identifier\n-    raise TypeError(\n+        obj = deserialize(identifier)\n+    else:\n+        obj = identifier\n+    if callable(obj):\n+        return obj\n+    raise ValueError(\n         f\"Could not interpret activation function identifier: {identifier}\"\n     )\n\n@@ -172,3 +172,13 @@ class ActivationsTest(testing.TestCase):\n     def test_linear(self):\n         x = np.random.random((10, 5))\n         self.assertAllClose(x, activations.linear(x))\n+\n+    def test_get_method(self):\n+        obj = activations.get(\"relu\")\n+        self.assertEqual(obj, activations.relu)\n+\n+        obj = activations.get(None)\n+        self.assertEqual(obj, activations.linear)\n+\n+        with self.assertRaises(ValueError):\n+            activations.get(\"typo\")\n\n@@ -44,15 +44,18 @@ def get(identifier):\n     if identifier is None:\n         return None\n     if isinstance(identifier, dict):\n-        return deserialize(identifier)\n+        obj = deserialize(identifier)\n     elif isinstance(identifier, str):\n         config = {\"class_name\": str(identifier), \"config\": {}}\n-        return deserialize(config)\n-    elif callable(identifier):\n-        if inspect.isclass(identifier):\n-            identifier = identifier()\n-        return identifier\n+        obj = deserialize(config)\n+    else:\n+        obj = identifier\n+\n+    if callable(obj):\n+        if inspect.isclass(obj):\n+            obj = obj()\n+        return obj\n     else:\n         raise ValueError(\n-            f\"Could not interpret constraint object identifier: {identifier}\"\n+            f\"Could not interpret constraint identifier: {identifier}\"\n         )\n\n@@ -44,3 +44,13 @@ class ConstraintsTest(testing.TestCase):\n         l2 = np.sqrt(np.sum(np.square(output), axis=0))\n         self.assertFalse(l2[l2 < 0.2])\n         self.assertFalse(l2[l2 > 0.5 + 1e-6])\n+\n+    def test_get_method(self):\n+        obj = constraints.get(\"unit_norm\")\n+        self.assertTrue(obj, constraints.UnitNorm)\n+\n+        obj = constraints.get(None)\n+        self.assertEqual(obj, None)\n+\n+        with self.assertRaises(ValueError):\n+            constraints.get(\"typo\")\n\n@@ -96,19 +96,21 @@ def get(identifier):\n     Returns:\n         Initializer instance base on the input identifier.\n     \"\"\"\n-\n     if identifier is None:\n         return None\n     if isinstance(identifier, dict):\n-        identifier = deserialize(identifier)\n+        obj = deserialize(identifier)\n     elif isinstance(identifier, str):\n         config = {\"class_name\": str(identifier), \"config\": {}}\n-        identifier = deserialize(config)\n+        obj = deserialize(config)\n+    else:\n+        obj = identifier\n \n-    if callable(identifier):\n-        if inspect.isclass(identifier):\n-            identifier = identifier()\n-        return identifier\n+    if callable(obj):\n+        if inspect.isclass(obj):\n+            obj = obj()\n+        return obj\n+    else:\n         raise ValueError(\n-        f\"Could not interpret initializer object identifier: {identifier}\"\n+            f\"Could not interpret initializer identifier: {identifier}\"\n         )\n\n@@ -105,3 +105,13 @@ class InitializersTest(testing.TestCase):\n         # TODO: test correctness\n \n         self.run_class_serialization_test(initializer)\n+\n+    def test_get_method(self):\n+        obj = initializers.get(\"glorot_normal\")\n+        self.assertTrue(obj, initializers.GlorotNormal)\n+\n+        obj = initializers.get(None)\n+        self.assertEqual(obj, None)\n+\n+        with self.assertRaises(ValueError):\n+            initializers.get(\"typo\")\n\n@@ -152,13 +152,14 @@ def get(identifier):\n     \"\"\"\n     if identifier is None:\n         return None\n-    if isinstance(identifier, str):\n-        identifier = str(identifier)\n-        return deserialize(identifier)\n     if isinstance(identifier, dict):\n-        return deserialize(identifier)\n-    if callable(identifier):\n-        return identifier\n-    raise ValueError(\n-        f\"Could not interpret loss function identifier: {identifier}\"\n-    )\n+        obj = deserialize(identifier)\n+    elif isinstance(identifier, str):\n+        obj = deserialize(identifier)\n+    else:\n+        obj = identifier\n+\n+    if callable(obj):\n+        return obj\n+    else:\n+        raise ValueError(f\"Could not interpret loss identifier: {identifier}\")\n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+from keras_core import losses as losses_module\n from keras_core import operations as ops\n from keras_core import testing\n from keras_core.losses.loss import Loss\n@@ -157,6 +158,12 @@ class LossTest(testing.TestCase):\n             loss,\n         )\n \n-    def test_serialization(self):\n-        # TODO\n-        pass\n+    def test_get_method(self):\n+        loss = losses_module.get(\"mse\")\n+        self.assertEqual(loss, losses_module.mean_squared_error)\n+\n+        loss = losses_module.get(None)\n+        self.assertEqual(loss, None)\n+\n+        with self.assertRaises(ValueError):\n+            losses_module.get(\"typo\")\n\n@@ -174,11 +174,15 @@ def get(identifier):\n     Returns:\n         A Keras metric as a `function`/ `Metric` class instance.\n     \"\"\"\n+    if identifier is None:\n+        return None\n     if isinstance(identifier, dict):\n-        return deserialize(identifier)\n+        obj = deserialize(identifier)\n     elif isinstance(identifier, str):\n-        return deserialize(identifier)\n-    elif callable(identifier):\n-        return identifier\n+        obj = deserialize(identifier)\n+    else:\n+        obj = identifier\n+    if callable(obj):\n+        return obj\n     else:\n         raise ValueError(f\"Could not interpret metric identifier: {identifier}\")\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras_core import backend\n from keras_core import initializers\n+from keras_core import metrics as metrics_module\n from keras_core import operations as ops\n from keras_core import testing\n from keras_core.metrics.metric import Metric\n@@ -106,3 +107,13 @@ class MetricTest(testing.TestCase):\n             ExampleMetric(name=\"mse\"),\n             custom_objects={\"ExampleMetric\": ExampleMetric},\n         )\n+\n+    def test_get_method(self):\n+        metric = metrics_module.get(\"mse\")\n+        self.assertTrue(isinstance(metric, metrics_module.MeanSquaredError))\n+\n+        metric = metrics_module.get(None)\n+        self.assertEqual(metric, None)\n+\n+        with self.assertRaises(ValueError):\n+            metrics_module.get(\"typo\")\n\n@@ -78,14 +78,16 @@ def get(identifier):\n     Returns:\n         A Keras Optimizer instance.\n     \"\"\"\n-    if isinstance(identifier, Optimizer):\n-        return identifier\n+    if identifier is None:\n+        return None\n     elif isinstance(identifier, dict):\n-        opt = deserialize(identifier)\n+        obj = deserialize(identifier)\n     elif isinstance(identifier, str):\n         config = {\"class_name\": identifier, \"config\": {}}\n-        opt = deserialize(config)\n+        obj = deserialize(config)\n+    else:\n+        obj = identifier\n \n-    if isinstance(opt, Optimizer):\n-        return opt\n+    if isinstance(obj, Optimizer):\n+        return obj\n     raise ValueError(f\"Could not interpret optimizer identifier: {identifier}\")\n\n@@ -80,9 +80,6 @@ class Nadam(optimizer.Optimizer):\n         self._momentums = []\n         self._velocities = []\n         self._u_product = backend.Variable(1.0, dtype=var_list[0].dtype)\n-        # Keep a counter on how many times of _u_product has been computed to\n-        # avoid duplicated computations.\n-        self._u_product_counter = 1\n \n         for var in var_list:\n             self._momentums.append(\n@@ -110,20 +107,8 @@ class Nadam(optimizer.Optimizer):\n         u_t = beta_1 * (1.0 - 0.5 * (ops.power(decay, local_step)))\n         u_t_1 = beta_1 * (1.0 - 0.5 * (ops.power(decay, next_step)))\n \n-        def get_cached_u_product():\n-            return self._u_product\n-\n-        def compute_new_u_product():\n         u_product_t = self._u_product * u_t\n         self._u_product.assign(u_product_t)\n-            self._u_product_counter += 1\n-            return u_product_t\n-\n-        u_product_t = ops.cond(\n-            ops.equal(self._u_product_counter, (self.iterations + 2)),\n-            get_cached_u_product,\n-            compute_new_u_product,\n-        )\n \n         u_product_t_1 = u_product_t * u_t_1\n         beta_2_power = ops.power(beta_2, local_step)\n\n@@ -14,3 +14,15 @@ class OptimizerTest(testing.TestCase):\n         grad = backend.numpy.zeros((2, 2))\n         optimizer.apply_gradients([(grad, v)])\n         self.assertAlmostEqual(np.min(v), 0.0)\n+\n+    def test_get_method(self):\n+        obj = optimizers.get(\"sgd\")\n+        self.assertTrue(isinstance(obj, optimizers.SGD))\n+        obj = optimizers.get(\"adamw\")\n+        self.assertTrue(isinstance(obj, optimizers.AdamW))\n+\n+        obj = optimizers.get(None)\n+        self.assertEqual(obj, None)\n+\n+        with self.assertRaises(ValueError):\n+            optimizers.get(\"typo\")\n\n@@ -41,18 +41,20 @@ def deserialize(config, custom_objects=None):\n @keras_core_export(\"keras_core.regularizers.get\")\n def get(identifier):\n     \"\"\"Retrieve a Keras regularizer object via an identifier.\"\"\"\n-\n     if identifier is None:\n         return None\n     if isinstance(identifier, dict):\n-        return deserialize(identifier)\n+        obj = deserialize(identifier)\n     elif isinstance(identifier, str):\n         config = {\"class_name\": str(identifier), \"config\": {}}\n-        return deserialize(config)\n-    elif callable(identifier):\n-        if inspect.isclass(identifier):\n-            identifier = identifier()\n-        return identifier\n+        obj = deserialize(config)\n+    else:\n+        obj = identifier\n+\n+    if callable(obj):\n+        if inspect.isclass(obj):\n+            obj = obj()\n+        return obj\n     else:\n         raise ValueError(\n             f\"Could not interpret regularizer identifier: {identifier}\"\n\n@@ -32,3 +32,13 @@ class RegularizersTest(testing.TestCase):\n         x = backend.Variable(value)\n         regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n         # TODO\n+\n+    def test_get_method(self):\n+        obj = regularizers.get(\"l1l2\")\n+        self.assertTrue(obj, regularizers.L1L2)\n+\n+        obj = regularizers.get(None)\n+        self.assertEqual(obj, None)\n+\n+        with self.assertRaises(ValueError):\n+            regularizers.get(\"typo\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9586906d068740016b076d854ed2c574b2b54e60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 142 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,6 +74,7 @@ class Embedding(Layer):\n         self.embeddings_constraint = constraints.get(embeddings_constraint)\n         self.mask_zero = mask_zero\n         self.supports_masking = mask_zero\n+        self.autocast = False\n \n     def build(self, input_shape=None):\n         self.embeddings = self.add_weight(\n@@ -83,7 +84,6 @@ class Embedding(Layer):\n             regularizer=self.embeddings_regularizer,\n             constraint=self.embeddings_constraint,\n             trainable=True,\n-            # autocast=False,  # TODO\n         )\n         self.built = True\n \n@@ -91,7 +91,8 @@ class Embedding(Layer):\n         if inputs.dtype != \"int32\" and inputs.dtype != \"int64\":\n             inputs = ops.cast(inputs, \"int32\")\n         one_hot_data = ops.one_hot(inputs, num_classes=self.input_dim)\n-        return ops.matmul(one_hot_data, self.embeddings)\n+        out = ops.matmul(one_hot_data, self.embeddings)\n+        return ops.cast(out, dtype=self.compute_dtype)\n \n     def compute_mask(self, inputs, mask=None):\n         if not self.mask_zero:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
