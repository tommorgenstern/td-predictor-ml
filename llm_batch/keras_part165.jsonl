{"custom_id": "keras#70e5cd1620119afdf892c656b761fb482b478842", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 28 | Files Changed: 3 | Hunks: 31 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 106 | Churn Cumulative: 4295 | Contributors (this commit): 46 | Commits (past 90d): 38 | Contributors (cumulative): 48 | DMM Complexity: None\n\nDIFF:\n@@ -143,7 +143,7 @@ def _standardize_sample_or_class_weights(x_weight, output_names, weight_type):\n         A list of `sample_weight` or `class_weight` where there are exactly\n             one element per model output.\n \n-    # ValueError\n+    # Raises\n         ValueError: In case of invalid user-provided argument.\n     \"\"\"\n     if x_weight is None or len(x_weight) == 0:\n@@ -404,6 +404,17 @@ def _weighted_masked_objective(fn):\n         A function with signature `fn(y_true, y_pred, weights, mask)`.\n     \"\"\"\n     def weighted(y_true, y_pred, weights, mask=None):\n+        \"\"\"Wrapper function.\n+\n+        # Arguments\n+            y_true: `y_true` argument of `fn`.\n+            y_pred: `y_pred` argument of `fn`.\n+            weights: Weights tensor.\n+            mask: Mask tensor.\n+\n+        # Returns\n+            Scalar tensor.\n+        \"\"\"\n         # score_array has ndim >= 2\n         score_array = fn(y_true, y_pred)\n         if mask is not None:\n@@ -429,11 +440,30 @@ def _weighted_masked_objective(fn):\n \n \n def _masked_objective(fn):\n-    \"\"\"Transforms an objective function `fn(y_true, y_pred)`\n+    \"\"\"Adds support for masking to an objective function.\n+\n+    It transforms an objective function `fn(y_true, y_pred)`\n     into a cost-masked objective function\n     `fn(y_true, y_pred, mask)`.\n+\n+    # Arguments\n+        fn: The objective function to wrap,\n+            with signature `fn(y_true, y_pred)`.\n+\n+    # Returns\n+        A function with signature `fn(y_true, y_pred, mask)`.\n     \"\"\"\n     def masked(y_true, y_pred, mask=None):\n+        \"\"\"Wrapper function.\n+\n+        # Arguments\n+            y_true: `y_true` argument of `fn`.\n+            y_pred: `y_pred` argument of `fn`.\n+            mask: Mask tensor.\n+\n+        # Returns\n+            Scalar tensor.\n+        \"\"\"\n         # score_array has ndim >= 2\n         score_array = fn(y_true, y_pred)\n         if mask is not None:\n@@ -456,7 +486,20 @@ def _standardize_weights(y, sample_weight=None, class_weight=None,\n     Everything gets normalized to a single sample-wise (or timestep-wise)\n     weight array.\n \n-    # \n+    # Arguments\n+        y: Numpy array of model targets to be weighted.\n+        sample_weight: User-provided `sample_weight` argument.\n+        class_weight: User-provided `class_weight` argument.\n+        sample_weight_mode: One of `None` or `\"temporal\"`.\n+            `\"temporal\"` indicated that we expect 2D weight data\n+            that will be applied to the last 2 dimensions of\n+            the targets (i.e. we are weighting timesteps, not samples).\n+\n+    # Returns\n+        A numpy array of target weights, one entry per sample to weight.\n+\n+    # Raises\n+        ValueError: In case of invalid user-provided arguments.\n     \"\"\"\n     if sample_weight_mode is not None:\n         if sample_weight_mode != 'temporal':\n@@ -512,6 +555,7 @@ def _standardize_weights(y, sample_weight=None, class_weight=None,\n \n class GeneratorEnqueuer(object):\n     \"\"\"Builds a queue out of a data generator.\n+\n     Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n \n     # Arguments\n@@ -556,7 +600,7 @@ class GeneratorEnqueuer(object):\n                 self.queue = queue.Queue()\n                 self._stop_event = threading.Event()\n \n-            for i in range(workers):\n+            for _ in range(workers):\n                 if self._pickle_safe:\n                     # Reset random seed else all children processes\n                     # share the same seed\n@@ -576,6 +620,7 @@ class GeneratorEnqueuer(object):\n \n     def stop(self, timeout=None):\n         \"\"\"Stop running threads and wait for them to exit, if necessary.\n+\n         Should be called by the same thread which called start().\n \n         # Arguments\n@@ -601,6 +646,8 @@ class GeneratorEnqueuer(object):\n \n \n class Model(Container):\n+    \"\"\"The `Model` class adds training & evaluation routines to a `Container`.\n+    \"\"\"\n \n     def compile(self, optimizer, loss, metrics=None, loss_weights=None,\n                 sample_weight_mode=None, **kwargs):\n@@ -620,20 +667,24 @@ class Model(Container):\n                 multi-output model, you could also pass a dictionary,\n                 such as `metrics={'output_a': 'accuracy'}`.\n             sample_weight_mode: if you need to do timestep-wise\n-                sample weighting (2D weights), set this to \"temporal\".\n-                \"None\" defaults to sample-wise weights (1D).\n+                sample weighting (2D weights), set this to `\"temporal\"`.\n+                `None` defaults to sample-wise weights (1D).\n                 If the model has multiple outputs, you can use a different\n                 `sample_weight_mode` on each output by passing a\n                 dictionary or a list of modes.\n-            kwargs: when using the Theano backend, these arguments\n+            **kwargs: when using the Theano backend, these arguments\n                 are passed into K.function. Ignored for Tensorflow backend.\n+\n+        # Raises\n+            ValueError: In case of invalid arguments for\n+                `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n         \"\"\"\n         self.optimizer = optimizers.get(optimizer)\n         self.sample_weight_mode = sample_weight_mode\n         self.loss = loss\n         self.loss_weights = loss_weights\n \n-        # prepare loss weights\n+        # Prepare loss weights.\n         if loss_weights is None:\n             loss_weights_list = [1. for _ in range(len(self.outputs))]\n         elif isinstance(loss_weights, dict):\n@@ -659,7 +710,7 @@ class Model(Container):\n                             str(loss_weights) +\n                             ' - expected a list of dicts.')\n \n-        # prepare loss functions\n+        # Prepare loss functions.\n         if isinstance(loss, dict):\n             for name in loss:\n                 if name not in self.output_names:\n@@ -687,14 +738,14 @@ class Model(Container):\n         self.loss_functions = loss_functions\n         weighted_losses = [_weighted_masked_objective(fn) for fn in loss_functions]\n \n-        # prepare output masks\n+        # Prepare output masks.\n         masks = self.compute_mask(self.inputs, mask=None)\n         if masks is None:\n             masks = [None for _ in self.outputs]\n         if not isinstance(masks, list):\n             masks = [masks]\n \n-        # prepare sample weights\n+        # Prepare sample weights.\n         if isinstance(sample_weight_mode, dict):\n             for name in sample_weight_mode:\n                 if name not in self.output_names:\n@@ -753,7 +804,7 @@ class Model(Container):\n                 sample_weight_modes = [None for name in self.output_names]\n         self.sample_weight_modes = sample_weight_modes\n \n-        # prepare targets of model\n+        # Prepare targets of model.\n         self.targets = []\n         for i in range(len(self.outputs)):\n             shape = self.internal_output_shapes[i]\n@@ -762,12 +813,12 @@ class Model(Container):\n                                               name=name + '_target',\n                                               sparse=K.is_sparse(self.outputs[i]),\n                                               dtype=K.dtype(self.outputs[i])))\n-        # prepare metrics\n+        # Prepare metrics.\n         self.metrics = metrics\n         self.metrics_names = ['loss']\n         self.metrics_tensors = []\n \n-        # compute total loss\n+        # Compute total loss.\n         total_loss = None\n         for i in range(len(self.outputs)):\n             y_true = self.targets[i]\n@@ -786,13 +837,13 @@ class Model(Container):\n             else:\n                 total_loss += loss_weight * output_loss\n \n-        # add regularization penalties\n-        # and other layer-specific losses\n+        # Add regularization penalties\n+        # and other layer-specific losses.\n         for loss_tensor in self.losses:\n             total_loss += loss_tensor\n \n-        # list of same size as output_names.\n-        # contains tuples (metrics for output, names of metrics)\n+        # List of same size as output_names.\n+        # contains tuples (metrics for output, names of metrics).\n         nested_metrics = _collect_metrics(metrics, self.output_names)\n \n         def append_metric(layer_num, metric_name, metric_tensor):\n@@ -834,11 +885,11 @@ class Model(Container):\n                     for name, tensor in six.iteritems(metric_result):\n                         append_metric(i, name, tensor)\n \n-        # prepare gradient updates and state updates\n+        # Prepare gradient updates and state updates.\n         self.total_loss = total_loss\n         self.sample_weights = sample_weights\n \n-        # functions for train, test and predict will\n+        # Functions for train, test and predict will\n         # be compiled lazily when required.\n         # This saves time when the user is not using all functions.\n         self._function_kwargs = kwargs\n@@ -847,9 +898,9 @@ class Model(Container):\n         self.test_function = None\n         self.predict_function = None\n \n-        # collected trainable weights and sort them deterministically.\n+        # Collected trainable weights and sort them deterministically.\n         trainable_weights = self.trainable_weights\n-        # Sort weights by name\n+        # Sort weights by name.\n         if trainable_weights:\n             if K.backend() == 'theano':\n                 trainable_weights.sort(key=lambda x: x.name if x.name else x.auto_name)\n@@ -871,7 +922,7 @@ class Model(Container):\n                                                           self.total_loss)\n             updates = self.updates + training_updates\n \n-            # returns loss and metrics. Updates weights at each call.\n+            # Returns loss and metrics. Updates weights at each call.\n             self.train_function = K.function(inputs,\n                                              [self.total_loss] + self.metrics_tensors,\n                                              updates=updates,\n@@ -885,7 +936,7 @@ class Model(Container):\n                 inputs = self.inputs + self.targets + self.sample_weights + [K.learning_phase()]\n             else:\n                 inputs = self.inputs + self.targets + self.sample_weights\n-            # return loss and metrics, no gradient updates.\n+            # Return loss and metrics, no gradient updates.\n             # Does update the network states.\n             self.test_function = K.function(inputs,\n                                             [self.total_loss] + self.metrics_tensors,\n@@ -900,7 +951,7 @@ class Model(Container):\n                 inputs = self.inputs + [K.learning_phase()]\n             else:\n                 inputs = self.inputs\n-            # returns network outputs. Does not update weights.\n+            # Returns network outputs. Does not update weights.\n             # Does update the network states.\n             kwargs = getattr(self, '_function_kwargs', {})\n             self.predict_function = K.function(inputs,\n\n@@ -135,8 +135,8 @@ def deserialize_keras_object(identifier, module_objects=None,\n             if 'custom_objects' in arg_spec.args:\n                 custom_objects = custom_objects or {}\n                 return cls.from_config(config['config'],\n-                                       custom_objects=dict(_GLOBAL_CUSTOM_OBJECTS.items() +\n-                                                           custom_objects.items()))\n+                                       custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n+                                                           list(custom_objects.items())))\n             return cls.from_config(config['config'])\n         else:\n             # Then `cls` may be a function returning a class.\n\n@@ -113,6 +113,5 @@ def test_merge_dot():\n     assert_allclose(out, expected, atol=1e-4)\n \n \n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c15d8697754a74a63a1185ba31ccf310b3c5d0ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 10951 | Contributors (this commit): 56 | Commits (past 90d): 65 | Contributors (cumulative): 61 | DMM Complexity: None\n\nDIFF:\n@@ -2304,6 +2304,9 @@ class Container(Layer):\n \n             # Arguments\n                 layer_data: layer config dict.\n+\n+            # Raises\n+                ValueError: In case of improperly formatted `layer_data` dict.\n             \"\"\"\n             layer_name = layer_data['name']\n \n\n@@ -922,7 +922,7 @@ class Model(Container):\n                                                           self.total_loss)\n             updates = self.updates + training_updates\n \n-            # Returns loss and metrics. Updates weights at each call.\n+            # Gets loss and metrics. Updates weights at each call.\n             self.train_function = K.function(inputs,\n                                              [self.total_loss] + self.metrics_tensors,\n                                              updates=updates,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb567d27eabd78b19145f3f6a018413aabcf3396", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 3574 | Contributors (this commit): 34 | Commits (past 90d): 25 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -951,7 +951,7 @@ class Model(Container):\n                 inputs = self.inputs + [K.learning_phase()]\n             else:\n                 inputs = self.inputs\n-            # Returns network outputs. Does not update weights.\n+            # Gets network outputs. Does not update weights.\n             # Does update the network states.\n             kwargs = getattr(self, '_function_kwargs', {})\n             self.predict_function = K.function(inputs,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e25cb7d13ff6788c569051c7cad6d7ad85ab821", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 14 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 44 | Churn Cumulative: 3618 | Contributors (this commit): 34 | Commits (past 90d): 26 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -912,16 +912,15 @@ class Model(Container):\n         if not hasattr(self, 'train_function'):\n             raise RuntimeError('You must compile your model before using it.')\n         if self.train_function is None:\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n-                inputs = self.inputs + self.targets + self.sample_weights + [K.learning_phase()]\n-            else:\n             inputs = self.inputs + self.targets + self.sample_weights\n+            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+                inputs += [K.learning_phase()]\n \n-            training_updates = self.optimizer.get_updates(self._collected_trainable_weights,\n+            training_updates = self.optimizer.get_updates(\n+                self._collected_trainable_weights,\n                 self.constraints,\n                 self.total_loss)\n             updates = self.updates + training_updates\n-\n             # Gets loss and metrics. Updates weights at each call.\n             self.train_function = K.function(inputs,\n                                              [self.total_loss] + self.metrics_tensors,\n@@ -932,10 +931,9 @@ class Model(Container):\n         if not hasattr(self, 'test_function'):\n             raise RuntimeError('You must compile your model before using it.')\n         if self.test_function is None:\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n-                inputs = self.inputs + self.targets + self.sample_weights + [K.learning_phase()]\n-            else:\n             inputs = self.inputs + self.targets + self.sample_weights\n+            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+                inputs += [K.learning_phase()]\n             # Return loss and metrics, no gradient updates.\n             # Does update the network states.\n             self.test_function = K.function(inputs,\n@@ -963,7 +961,8 @@ class Model(Container):\n                   epochs=100, verbose=1, callbacks=None,\n                   val_f=None, val_ins=None, shuffle=True,\n                   callback_metrics=None, initial_epoch=0):\n-        \"\"\"Abstract fit function for f(ins).\n+        \"\"\"Abstract fit function for `f(ins)`.\n+\n         Assume that f returns a list, labeled by out_labels.\n \n         # Arguments\n@@ -1164,7 +1163,7 @@ class Model(Container):\n \n             if verbose == 1:\n                 progbar.update(batch_end)\n-        for i, out in enumerate(outs):\n+        for i in range(len(outs)):\n             outs[i] /= samples\n         if len(outs) == 1:\n             return outs[0]\n@@ -1202,7 +1201,9 @@ class Model(Container):\n                           for (ref, sw, cw, mode)\n                           in zip(y, sample_weights, class_weights, self.sample_weight_modes)]\n         _check_array_lengths(x, y, sample_weights)\n-        _check_loss_and_target_compatibility(y, self.loss_functions, self.internal_output_shapes)\n+        _check_loss_and_target_compatibility(y,\n+                                             self.loss_functions,\n+                                             self.internal_output_shapes)\n         if self.stateful and batch_size:\n             if x[0].shape[0] % batch_size != 0:\n                 raise ValueError('In a stateful network, '\n@@ -1358,8 +1359,9 @@ class Model(Container):\n                               initial_epoch=initial_epoch)\n \n     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\n-        \"\"\"Returns the loss value and metrics values for the model\n-        in test mode. Computation is done in batches.\n+        \"\"\"Returns the loss value & metrics values for the model in test mode.\n+\n+        Computation is done in batches.\n \n         # Arguments\n             x: Numpy array of test data,\n@@ -1398,8 +1400,9 @@ class Model(Container):\n                                verbose=verbose)\n \n     def predict(self, x, batch_size=32, verbose=0):\n-        \"\"\"Generates output predictions for the input samples,\n-        processing the samples in a batched way.\n+        \"\"\"Generates output predictions for the input samples.\n+\n+        Computation is done in batches.\n \n         # Arguments\n             x: the input data, as a Numpy array\n@@ -1547,8 +1550,8 @@ class Model(Container):\n                       class_weight=None,\n                       max_q_size=10, workers=1, pickle_safe=False,\n                       initial_epoch=0):\n-        \"\"\"Fits the model on data generated batch-by-batch by\n-        a Python generator.\n+        \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\n+\n         The generator is run in parallel to the model, for efficiency.\n         For instance, this allows you to do real-time data augmentation\n         on images on CPU in parallel to training your model on GPU.\n@@ -1771,8 +1774,10 @@ class Model(Container):\n \n     def evaluate_generator(self, generator, val_samples,\n                            max_q_size=10, workers=1, pickle_safe=False):\n-        \"\"\"Evaluates the model on a data generator. The generator should\n-        return the same kind of data as accepted by `test_on_batch`.\n+        \"\"\"Evaluates the model on a data generator.\n+\n+        The generator should return the same kind of data\n+        as accepted by `test_on_batch`.\n \n         Arguments:\n             generator:\n@@ -1864,6 +1869,7 @@ class Model(Container):\n     def predict_generator(self, generator, val_samples,\n                           max_q_size=10, workers=1, pickle_safe=False):\n         \"\"\"Generates predictions for the input samples from a data generator.\n+\n         The generator should return the same kind of data as accepted by\n         `predict_on_batch`.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#db22fdf9afe2c92d604a52e9e9953fff203aea73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 24743 | Contributors (this commit): 116 | Commits (past 90d): 82 | Contributors (cumulative): 147 | DMM Complexity: None\n\nDIFF:\n@@ -666,6 +666,12 @@ class Model(Container):\n                 To specify different metrics for different outputs of a\n                 multi-output model, you could also pass a dictionary,\n                 such as `metrics={'output_a': 'accuracy'}`.\n+            loss_weights: Optional list or dictionary specifying scalar\n+                coefficients (Python floats) to weight the loss contributions\n+                of different model outputs.\n+                If a list, it is expected to have a 1:1 mapping\n+                to the model's outputs. If a tensor, it is expected to map\n+                output names (strings) to scalar coefficients.\n             sample_weight_mode: if you need to do timestep-wise\n                 sample weighting (2D weights), set this to `\"temporal\"`.\n                 `None` defaults to sample-wise weights (1D).\n@@ -1269,6 +1275,10 @@ class Model(Container):\n         # Returns\n             A `History` instance. Its `history` attribute contains\n             all information collected during training.\n+\n+        # Raises\n+            ValueError: In case of mismatch between the provided input data\n+                and what the model expects.\n         \"\"\"\n         # validate user data\n         x, y, sample_weights = self._standardize_user_data(\n@@ -1375,6 +1385,9 @@ class Model(Container):\n                 you can also pass a dictionary\n                 mapping output names to Numpy arrays.\n             batch_size: integer. Number of samples per gradient update.\n+            verbose: verbosity mode, 0 or 1.\n+            sample_weight: Array of weights to weight the contribution\n+                of different samples to the loss and metrics.\n \n         # Returns\n             Scalar test loss (if the model has a single output and no metrics)\n@@ -1411,7 +1424,13 @@ class Model(Container):\n             verbose: verbosity mode, 0 or 1.\n \n         # Returns\n-            A Numpy array of predictions.\n+            Numpy array(s) of predictions.\n+\n+        # Raises\n+            ValueError: In case of mismatch between the provided\n+                input data and the model's expectations,\n+                or in case a stateful model receives a number of samples\n+                that is not a multiple of the batch size.\n         \"\"\"\n         # validate user data\n         x = _standardize_input_data(x, self.input_names,\n@@ -1531,6 +1550,12 @@ class Model(Container):\n \n     def predict_on_batch(self, x):\n         \"\"\"Returns predictions for a single batch of samples.\n+\n+        # Arguments\n+            x: Input samples, as a Numpy array.\n+\n+        # Returns\n+            Numpy array(s) of predictions.\n         \"\"\"\n         x = _standardize_input_data(x, self.input_names,\n                                     self.internal_input_shapes)\n\n@@ -688,7 +688,7 @@ class Lambda(Layer):\n     def from_config(cls, config, custom_objects=None):\n         globs = globals()\n         if custom_objects:\n-            globs = dict(globs.items() + custom_objects.items())\n+            globs = dict(list(globs.items()) + list(custom_objects.items()))\n         function_type = config.pop('function_type')\n         if function_type == 'function':\n             # Simple lookup in custom objects\n\n@@ -187,6 +187,11 @@ def load_model(filepath, custom_objects=None):\n \n         # Arguments\n             obj: object, dict, or list.\n+\n+        # Returns\n+            The same structure, where occurences\n+                of a custom object name have been replaced\n+                with the custom object.\n         \"\"\"\n         if isinstance(obj, list):\n             deserialized = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0599ade6dae88be98ea1ec1d150d7f525335b0e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 3675 | Contributors (this commit): 34 | Commits (past 90d): 28 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -1636,6 +1636,10 @@ class Model(Container):\n             model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n                                 samples_per_epoch=10000, epochs=10)\n         ```\n+\n+        # Raises\n+            ValueError: In case the generator yields\n+                data in an invalid format.\n         \"\"\"\n         wait_time = 0.01  # in seconds\n         epoch = initial_epoch\n@@ -1805,11 +1809,9 @@ class Model(Container):\n         as accepted by `test_on_batch`.\n \n         Arguments:\n-            generator:\n-                generator yielding tuples (inputs, targets)\n+            generator: Generator yielding tuples (inputs, targets)\n                 or (inputs, targets, sample_weights)\n-            val_samples:\n-                total number of samples to generate from `generator`\n+            val_samples: Total number of samples to generate from `generator`\n                 before returning.\n             max_q_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n@@ -1827,6 +1829,10 @@ class Model(Container):\n             or list of scalars (if the model has multiple outputs\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n+\n+        # Raises\n+            ValueError: In case the generator yields\n+                data in an invalid format.\n         \"\"\"\n         self._make_test_function()\n \n@@ -1915,6 +1921,10 @@ class Model(Container):\n \n         # Returns\n             Numpy array(s) of predictions.\n+\n+        # Raises\n+            ValueError: In case the generator yields\n+                data in an invalid format.\n         \"\"\"\n         self._make_predict_function()\n \n@@ -1954,23 +1964,23 @@ class Model(Container):\n                 outs = self.predict_on_batch(x)\n \n                 if isinstance(x, list):\n-                    sampless = len(x[0])\n+                    samples = len(x[0])\n                 elif isinstance(x, dict):\n-                    sampless = len(list(x.values())[0])\n+                    samples = len(list(x.values())[0])\n                 else:\n-                    sampless = len(x)\n+                    samples = len(x)\n \n                 if not isinstance(outs, list):\n                     outs = [outs]\n \n-                if len(all_outs) == 0:\n+                if not all_outs:\n                     for out in outs:\n                         shape = (val_samples,) + out.shape[1:]\n                         all_outs.append(np.zeros(shape, dtype=K.floatx()))\n \n                 for i, out in enumerate(outs):\n-                    all_outs[i][processed_samples:(processed_samples + sampless)] = out\n-                processed_samples += sampless\n+                    all_outs[i][processed_samples:(processed_samples + samples)] = out\n+                processed_samples += samples\n \n         finally:\n             if enqueuer is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aeb22266c91af9d17bee7a4c266c7396a6167cbf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 47 | Files Changed: 3 | Hunks: 29 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 143 | Churn Cumulative: 20701 | Contributors (this commit): 94 | Commits (past 90d): 132 | Contributors (cumulative): 148 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2208,13 +2208,13 @@ def rnn(step_function, inputs, initial_states,\n \n \n def switch(condition, then_expression, else_expression):\n-    \"\"\"Switches between two operations\n-    depending on a scalar value (`int` or `bool`).\n+    \"\"\"Switches between two operations depending on a scalar value.\n+\n     Note that both `then_expression` and `else_expression`\n     should be symbolic tensors of the *same shape*.\n \n     # Arguments\n-        condition: scalar tensor.\n+        condition: scalar tensor (`int` or `bool`).\n         then_expression: either a tensor, or a callable that returns a tensor.\n         else_expression: either a tensor, or a callable that returns a tensor.\n \n@@ -2244,6 +2244,15 @@ def in_train_phase(x, alt, training=None):\n \n     Note that `alt` should have the *same shape* as `x`.\n \n+    # Arguments\n+        x: What to return in train phase\n+            (tensor or callable that returns a tensor).\n+        alt: What to return otherwise\n+            (tensor or callable that returns a tensor).\n+        training: Optional scalar tensor\n+            (or Python boolean, or Python integer)\n+            specifing the learning phase.\n+\n     # Returns\n         Either `x` or `alt` based on the `training` flag.\n         the `training` flag defaults to `K.learning_phase()`.\n@@ -2275,8 +2284,18 @@ def in_train_phase(x, alt, training=None):\n \n def in_test_phase(x, alt, training=None):\n     \"\"\"Selects `x` in test phase, and `alt` otherwise.\n+\n     Note that `alt` should have the *same shape* as `x`.\n \n+    # Arguments\n+        x: What to return in test phase\n+            (tensor or callable that returns a tensor).\n+        alt: What to return otherwise\n+            (tensor or callable that returns a tensor).\n+        training: Optional scalar tensor\n+            (or Python boolean, or Python integer)\n+            specifing the learning phase.\n+\n     # Returns\n         Either `x` or `alt` based on `K.learning_phase`.\n     \"\"\"\n@@ -2287,6 +2306,7 @@ def in_test_phase(x, alt, training=None):\n \n def relu(x, alpha=0., max_value=None):\n     \"\"\"Rectified linear unit.\n+\n     With default values, it returns element-wise `max(x, 0)`.\n \n     # Arguments\n@@ -2364,9 +2384,18 @@ def softsign(x):\n \n \n def categorical_crossentropy(output, target, from_logits=False):\n-    \"\"\"Categorical crossentropy between an output tensor\n-    and a target tensor, where the target is a tensor of the same\n-    shape as the output.\n+    \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n+\n+    # Arguments\n+        output: A tensor resulting from a softmax\n+            (unless `from_logits` is True, in which\n+            case `output` is expected to be the logits).\n+        target: A tensor of the same shape as `output`.\n+        from_logits: Boolean, whether `output` is the\n+            result of a softmax, or is a tensor of logits.\n+\n+    # Returns\n+        Output tensor.\n     \"\"\"\n     # Note: tf.nn.softmax_cross_entropy_with_logits\n     # expects logits, Keras expects probabilities.\n@@ -2389,8 +2418,18 @@ def categorical_crossentropy(output, target, from_logits=False):\n \n \n def sparse_categorical_crossentropy(output, target, from_logits=False):\n-    \"\"\"Categorical crossentropy between an output tensor\n-    and a target tensor, where the target is an integer tensor.\n+    \"\"\"Categorical crossentropy with integer targets.\n+\n+    # Arguments\n+        output: A tensor resulting from a softmax\n+            (unless `from_logits` is True, in which\n+            case `output` is expected to be the logits).\n+        target: An integer tensor.\n+        from_logits: Boolean, whether `output` is the\n+            result of a softmax, or is a tensor of logits.\n+\n+    # Returns\n+        Output tensor.\n     \"\"\"\n     # Note: tf.nn.softmax_cross_entropy_with_logits\n     # expects logits, Keras expects probabilities.\n@@ -2457,6 +2496,7 @@ def sigmoid(x):\n \n def hard_sigmoid(x):\n     \"\"\"Segment-wise linear approximation of sigmoid.\n+\n     Faster than sigmoid.\n     Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n     In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n@@ -2524,7 +2564,7 @@ def l2_normalize(x, axis):\n \n \n def in_top_k(predictions, targets, k):\n-    \"\"\"Returns whether the `targets` are in the top `k` `predictions`\n+    \"\"\"Returns whether the `targets` are in the top `k` `predictions`.\n \n     # Arguments\n         predictions: A tensor of shape `batch_size` x classes and type `float32`.\n@@ -2566,10 +2606,6 @@ def _preprocess_conv3d_input(x, data_format):\n     if dtype(x) == 'float64':\n         x = tf.cast(x, 'float32')\n     if data_format == 'channels_first':\n-        # TF uses the last dimension as channel dimension,\n-        # instead of the 2nd one.\n-        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n-        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3, input_depth)\n         x = tf.transpose(x, (0, 2, 3, 4, 1))\n     return x\n \n@@ -2578,10 +2614,6 @@ def _preprocess_conv2d_kernel(kernel, data_format):\n     if dtype(kernel) == 'float64':\n         kernel = tf.cast(kernel, 'float32')\n     if data_format == 'channels_first':\n-        # TF uses the last dimension as channel dimension,\n-        # instead of the 2nd one.\n-        # TH kernel shape: (depth, input_depth, rows, cols)\n-        # TF kernel shape: (rows, cols, input_depth, depth)\n         kernel = tf.transpose(kernel, (2, 3, 1, 0))\n     return kernel\n \n@@ -2590,10 +2622,6 @@ def _preprocess_conv3d_kernel(kernel, data_format):\n     if dtype(kernel) == 'float64':\n         kernel = tf.cast(kernel, 'float32')\n     if data_format == 'channels_first':\n-        # TF uses the last dimension as channel dimension,\n-        # instead of the 2nd one.\n-        # TH kernel shape: (out_depth, input_depth, kernel_dim1, kernel_dim2, kernel_dim3)\n-        # TF kernel shape: (kernel_dim1, kernel_dim2, kernel_dim3, input_depth, out_depth)\n         kernel = tf.transpose(kernel, (2, 3, 4, 1, 0))\n     return kernel\n \n@@ -2626,7 +2654,7 @@ def _postprocess_conv3d_output(x, data_format):\n     return x\n \n \n-def conv1d(x, kernel, stride=1, padding='valid',\n+def conv1d(x, kernel, strides=1, padding='valid',\n            data_format=None, dilation_rate=1):\n     \"\"\"1D convolution.\n \n@@ -2651,7 +2679,7 @@ def conv1d(x, kernel, stride=1, padding='valid',\n         input=x,\n         filter=kernel,\n         dilation_rate=(dilation_rate,),\n-        strides=(stride,),\n+        strides=(strides,),\n         padding=padding,\n         data_format=tf_data_format)\n     return x\n@@ -2868,7 +2896,8 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n         A tensor, result of 3D pooling.\n \n     # Raises\n-        ValueError: if `data_format` is neither `channels_last` or `channels_first`.\n+        ValueError: if `data_format` is neither\n+            `channels_last` or `channels_first`.\n         ValueError: if `pool_mode` is neither `max` or `avg`.\n     \"\"\"\n     if data_format is None:\n@@ -2893,6 +2922,20 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n \n \n def bias_add(x, bias, data_format=None):\n+    \"\"\"Adds a bias vector to a tensor.\n+\n+    # Arguments\n+        x: Input tensor.\n+        bias: Bias tensor to add.\n+        data_format: Data format for 3D, 4D or 5D tensors:\n+            one of \"channels_first\", \"channels_last\".\n+\n+    # Returns\n+        Output tensor.\n+\n+    # Raises\n+        ValueError: In case of invalid `data_format` argument.\n+    \"\"\"\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n@@ -3018,14 +3061,14 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def ctc_label_dense_to_sparse(labels, label_lengths):\n-    # undocumented feature soon to be made public\n     from tensorflow.python.ops import functional_ops\n     label_shape = tf.shape(labels)\n     num_batches_tns = tf.stack([label_shape[0]])\n     max_num_labels_tns = tf.stack([label_shape[1]])\n \n-    def range_less_than(previous_state, current_input):\n-        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)\n+    def range_less_than(current_input):\n+        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(\n+            max_num_labels_tns, current_input)\n \n     init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)\n     dense_mask = functional_ops.scan(range_less_than, label_lengths,\n@@ -3050,9 +3093,10 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     \"\"\"Runs CTC loss algorithm on each batch element.\n \n     # Arguments\n-        y_true: tensor `(samples, max_string_length)` containing the truth labels.\n-        y_pred: tensor `(samples, time_steps, num_categories)` containing the prediction,\n-                or output of the softmax.\n+        y_true: tensor `(samples, max_string_length)`\n+            containing the truth labels.\n+        y_pred: tensor `(samples, time_steps, num_categories)`\n+            containing the prediction, or output of the softmax.\n         input_length: tensor `(samples, 1)` containing the sequence length for\n             each batch item in `y_pred`.\n         label_length: tensor `(samples, 1)` containing the sequence length for\n@@ -3060,7 +3104,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n \n     # Returns\n         Tensor with shape (samples,1) containing the\n-            CTC loss of each element\n+            CTC loss of each element.\n     \"\"\"\n     label_length = tf.to_int32(tf.squeeze(label_length))\n     input_length = tf.to_int32(tf.squeeze(input_length))\n@@ -3075,27 +3119,32 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n \n def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n                top_paths=1):\n-    \"\"\"Decodes the output of a softmax using either\n-       greedy (also known as best path) or a constrained dictionary\n-       search.\n+    \"\"\"Decodes the output of a softmax.\n+\n+    Can use either greedy search (also known as best path)\n+    or a constrained dictionary search.\n \n     # Arguments\n-        y_pred: tensor `(samples, time_steps, num_categories)` containing the prediction,\n-                or output of the softmax.\n+        y_pred: tensor `(samples, time_steps, num_categories)`\n+            containing the prediction, or output of the softmax.\n         input_length: tensor `(samples, )` containing the sequence length for\n             each batch item in `y_pred`.\n-        greedy: perform much faster best-path search if `true`. This does\n-                not use a dictionary\n+        greedy: perform much faster best-path search if `true`.\n+            This does not use a dictionary.\n         beam_width: if `greedy` is `false`: a beam search decoder will be used\n-                with a beam of this width\n-        top_paths: if `greedy` is `false`: how many of the most probable paths will be returned\n+            with a beam of this width.\n+        top_paths: if `greedy` is `false`,\n+            how many of the most probable paths will be returned.\n \n     # Returns\n         Tuple:\n-            List: if `greedy` is `true`, returns a list of one element that contains\n-                the decoded sequence. If `false`, returns the `top_paths` most probable\n-                decoded sequences. Important: blank labels are returned as `-1`.\n-            Tensor `(top_paths, )` that contains the log probability of each decoded sequence\n+            List: if `greedy` is `true`, returns a list of one element that\n+                contains the decoded sequence.\n+                If `false`, returns the `top_paths` most probable\n+                decoded sequences.\n+                Important: blank labels are returned as `-1`.\n+            Tensor `(top_paths, )` that contains\n+                the log probability of each decoded sequence.\n     \"\"\"\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + 1e-8)\n     input_length = tf.to_int32(input_length)\n\n@@ -1558,7 +1558,7 @@ def _postprocess_conv3d_output(conv_out, x,\n     return conv_out\n \n \n-def conv1d(x, kernel, stride=1, padding='valid',\n+def conv1d(x, kernel, strides=1, padding='valid',\n            data_format=None, dilation_rate=1):\n     \"\"\"1D convolution.\n \n@@ -1593,7 +1593,7 @@ def conv1d(x, kernel, stride=1, padding='valid',\n             x._keras_shape = (shape[0], shape[1], shape[2], 1)\n     # update dilation rate, strides\n     dilation_rate = (dilation_rate, 1)\n-    strides = (stride, 1)\n+    strides = (strides, 1)\n     # add dim to kernel (always same format independently of data_format)\n     # i.e. (rows, 1, input_depth, depth)\n     kernel = expand_dims(kernel, 1)\n\n@@ -146,7 +146,7 @@ class _Conv(Layer):\n             outputs = K.conv1d(\n                 inputs,\n                 self.kernel,\n-                stride=self.strides[0],\n+                strides=self.strides[0],\n                 padding=self.padding,\n                 data_format=self.data_format,\n                 dilation_rate=self.dilation_rate[0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c81f9447c7b008e075f1ef89e5bc0dd546e4fa79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 146 | Lines Deleted: 53 | Files Changed: 1 | Hunks: 51 | Methods Changed: 41 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 199 | Churn Cumulative: 6875 | Contributors (this commit): 61 | Commits (past 90d): 55 | Contributors (cumulative): 61 | DMM Complexity: 1.0\n\nDIFF:\n@@ -968,8 +968,7 @@ def transpose(x):\n \n \n def gather(reference, indices):\n-    \"\"\"Retrieves the elements of indices `indices`\n-    in the tensor `reference`.\n+    \"\"\"Retrieves the elements of indices `indices` in the tensor `reference`.\n \n     # Arguments\n         reference: A tensor.\n@@ -1169,7 +1168,6 @@ def argmax(x, axis=-1):\n     # Arguments\n         x: input tensor.\n         axis: axis along which to perform the reduction.\n-        keepdims: whether the drop or broadcast the reduction axes.\n \n     # Returns\n         A tensor.\n@@ -1185,7 +1183,6 @@ def argmin(x, axis=-1):\n     # Arguments\n         x: input tensor.\n         axis: axis along which to perform the reduction.\n-        keepdims: whether the drop or broadcast the reduction axes.\n \n     # Returns\n         A tensor.\n@@ -1289,6 +1286,7 @@ def pow(x, a):\n \n     # Arguments\n         x: input tensor.\n+        a: Python integer.\n \n     # Returns\n         A tensor.\n@@ -1299,6 +1297,11 @@ def pow(x, a):\n def clip(x, min_value, max_value):\n     \"\"\"Element-wise value clipping.\n \n+    # Arguments\n+        x: input tensor.\n+        min_value: Python float or integer.\n+        max_value: Python float or integer.\n+\n     # Returns\n         A tensor.\n     \"\"\"\n@@ -1314,6 +1317,10 @@ def clip(x, min_value, max_value):\n def equal(x, y):\n     \"\"\"Element-wise equality between two tensors.\n \n+    # Arguments\n+        x: input tensor.\n+        y: input tensor.\n+\n     # Returns\n         A bool tensor.\n     \"\"\"\n@@ -1332,6 +1339,10 @@ def not_equal(x, y):\n def greater(x, y):\n     \"\"\"Element-wise truth value of (x > y).\n \n+    # Arguments\n+        x: input tensor.\n+        y: input tensor.\n+\n     # Returns\n         A bool tensor.\n     \"\"\"\n@@ -1350,6 +1361,10 @@ def greater_equal(x, y):\n def lesser(x, y):\n     \"\"\"Element-wise truth value of (x < y).\n \n+    # Arguments\n+        x: input tensor.\n+        y: input tensor.\n+\n     # Returns\n         A bool tensor.\n     \"\"\"\n@@ -1359,6 +1374,10 @@ def lesser(x, y):\n def lesser_equal(x, y):\n     \"\"\"Element-wise truth value of (x <= y).\n \n+    # Arguments\n+        x: input tensor.\n+        y: input tensor.\n+\n     # Returns\n         A bool tensor.\n     \"\"\"\n@@ -1368,6 +1387,10 @@ def lesser_equal(x, y):\n def maximum(x, y):\n     \"\"\"Element-wise maximum of two tensors.\n \n+    # Arguments\n+        x: input tensor.\n+        y: input tensor.\n+\n     # Returns\n         A tensor.\n     \"\"\"\n@@ -1377,6 +1400,10 @@ def maximum(x, y):\n def minimum(x, y):\n     \"\"\"Element-wise minimum of two tensors.\n \n+    # Arguments\n+        x: input tensor.\n+        y: input tensor.\n+\n     # Returns\n         A tensor.\n     \"\"\"\n@@ -1386,6 +1413,9 @@ def minimum(x, y):\n def sin(x):\n     \"\"\"Computes sin of x element-wise.\n \n+    # Arguments\n+        x: input tensor.\n+\n     # Returns\n         A tensor.\n     \"\"\"\n@@ -1408,6 +1438,8 @@ def normalize_batch_in_training(x, gamma, beta,\n                                 reduction_axes, epsilon=1e-3):\n     \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n \n+    # Arguments\n+\n     # Returns\n         A tuple length of 3, `(normalized_tensor, mean, variance)`.\n     \"\"\"\n@@ -1440,7 +1472,10 @@ def normalize_batch_in_training(x, gamma, beta,\n def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n     \"\"\"Applies batch normalization on x given mean, var, beta and gamma:\n \n-    output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta\n+    `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n+\n+    # Arguments\n+\n \n     # Returns\n         A tensor.\n@@ -1453,6 +1488,8 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n def concatenate(tensors, axis=-1):\n     \"\"\"Concatenates a list of tensors alongside the specified axis.\n \n+    # Arguments\n+\n     # Returns\n         A tensor.\n     \"\"\"\n@@ -1478,6 +1515,8 @@ def concatenate(tensors, axis=-1):\n def reshape(x, shape):\n     \"\"\"Reshapes a tensor to the specified shape.\n \n+    # Arguments\n+\n     # Returns\n         A tensor.\n     \"\"\"\n@@ -1488,8 +1527,9 @@ def permute_dimensions(x, pattern):\n     \"\"\"Permutes axes in a tensor.\n \n     # Arguments\n-        pattern: should be a tuple of\n-            dimension indices, e.g. (0, 2, 1).\n+        x: input tensor.\n+        pattern: A tuple of\n+            dimension indices, e.g. `(0, 2, 1)`.\n \n     # Returns\n         A tensor.\n@@ -1498,17 +1538,16 @@ def permute_dimensions(x, pattern):\n \n \n def resize_images(X, height_factor, width_factor, data_format):\n-    \"\"\"Resizes the images contained in a 4D tensor of shape\n-    - `[batch, channels, height, width]` (for 'channels_first' data_format)\n-    - `[batch, height, width, channels]` (for 'channels_last' data_format)\n-    by a factor of `(height_factor, width_factor)`. Both factors should be\n-    positive integers.\n+    \"\"\"Resizes the images contained in a 4D tensor.\n+\n+    # Arguments\n \n     # Returns\n         A tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `channels_last` or `channels_first`.\n+        ValueError: if `data_format` is neither\n+            `channels_last` or `channels_first`.\n     \"\"\"\n     if data_format == 'channels_first':\n         original_shape = int_shape(X)\n@@ -1532,26 +1571,25 @@ def resize_images(X, height_factor, width_factor, data_format):\n         raise ValueError('Invalid data_format:', data_format)\n \n \n-def resize_volumes(X, depth_factor, height_factor, width_factor, data_format):\n-    \"\"\"Resizes the volume contained in a 5D tensor of shape\n-    - `[batch, channels, depth, height, width]` (for 'channels_first' data_format)\n-    - `[batch, depth, height, width, channels]` (for 'channels_last' data_format)\n-    by a factor of `(depth_factor, height_factor, width_factor)`.\n-    All three factors should be positive integers.\n+def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n+    \"\"\"Resizes the volume contained in a 5D tensor.\n+\n+    # Arguments\n \n     # Returns\n         A tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `channels_last` or `channels_first`.\n+        ValueError: if `data_format` is neither\n+            `channels_last` or `channels_first`.\n     \"\"\"\n     if data_format == 'channels_first':\n-        output = repeat_elements(X, depth_factor, axis=2)\n+        output = repeat_elements(x, depth_factor, axis=2)\n         output = repeat_elements(output, height_factor, axis=3)\n         output = repeat_elements(output, width_factor, axis=4)\n         return output\n     elif data_format == 'channels_last':\n-        output = repeat_elements(X, depth_factor, axis=1)\n+        output = repeat_elements(x, depth_factor, axis=1)\n         output = repeat_elements(output, height_factor, axis=2)\n         output = repeat_elements(output, width_factor, axis=3)\n         return output\n@@ -1565,6 +1603,8 @@ def repeat_elements(x, rep, axis):\n     If `x` has shape `(s1, s2, s3)` and `axis` is `1`, the output\n     will have shape `(s1, s2 * rep, s3)`.\n \n+    # Arguments\n+\n     # Returns\n         A tensor.\n     \"\"\"\n@@ -1591,6 +1631,8 @@ def repeat(x, n):\n     if `x` has shape (samples, dim) and `n` is `2`,\n     the output will have shape `(samples, 2, dim)`.\n \n+    # Arguments\n+\n     # Returns\n         A tensor.\n     \"\"\"\n@@ -1609,6 +1651,11 @@ def arange(start, stop=None, step=1, dtype='int32'):\n \n     The default type of the returned tensor is `'int32'` to\n     match TensorFlow's default.\n+\n+    # Arguments\n+\n+    # Returns\n+\n     \"\"\"\n     # Match the behavior of numpy and Theano by returning an empty seqence.\n     if stop is None and start < 0:\n@@ -1638,6 +1685,8 @@ def tile(x, n):\n def flatten(x):\n     \"\"\"Flatten a tensor.\n \n+    # Arguments\n+\n     # Returns\n         A tensor, reshaped into 1-D\n     \"\"\"\n@@ -1645,11 +1694,12 @@ def flatten(x):\n \n \n def batch_flatten(x):\n-    \"\"\"Turn a n-D tensor into a 2D tensor where\n-    the first dimension is conserved.\n+    \"\"\"Turn a nD tensor into a 2D tensor with same 0th dimension.\n \n     In other words, it flattens each data samples of a batch.\n \n+    # Arguments\n+\n     # Returns\n         A tensor.\n     \"\"\"\n@@ -1660,6 +1710,8 @@ def batch_flatten(x):\n def expand_dims(x, dim=-1):\n     \"\"\"Adds a 1-sized dimension at index \"dim\".\n \n+    # Arguments\n+\n     # Returns\n         A tensor with expended dimensions.\n     \"\"\"\n@@ -1669,6 +1721,8 @@ def expand_dims(x, dim=-1):\n def squeeze(x, axis):\n     \"\"\"Removes a 1-dimension from the tensor at index \"axis\".\n \n+    # Arguments\n+\n     # Returns\n         A tensor with the same data as `x` but reduced dimensions.\n     \"\"\"\n@@ -1676,8 +1730,9 @@ def squeeze(x, axis):\n \n \n def temporal_padding(x, padding=1):\n-    \"\"\"Pads the middle dimension of a 3D tensor\n-    with \"padding\" zeros left and right.\n+    \"\"\"Pads the middle dimension of a 3D tensor.\n+\n+    # Arguments\n \n     # Returns\n         A padded 3D tensor.\n@@ -1687,8 +1742,9 @@ def temporal_padding(x, padding=1):\n \n \n def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n-    \"\"\"Pad the middle dimension of a 3D tensor\n-    with \"left_pad\" zeros left and \"right_pad\" right.\n+    \"\"\"Pad the middle dimension of a 3D tensor.\n+\n+    # Arguments\n \n     # Returns\n         A padded 3D tensor.\n@@ -1698,14 +1754,19 @@ def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n \n \n def spatial_2d_padding(x, padding=(1, 1), data_format=None):\n-    \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor\n-    with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n+    \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor.\n+\n+    Pads these dimensions with\n+    \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n+\n+    # Arguments\n \n     # Returns\n         A padded 4D tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `channels_last` or `channels_first`.\n+        ValueError: if `data_format` is neither\n+            `channels_last` or `channels_first`.\n     \"\"\"\n     if data_format is None:\n         data_format = image_data_format()\n@@ -1725,10 +1786,14 @@ def spatial_2d_padding(x, padding=(1, 1), data_format=None):\n def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n                                   left_pad=1, right_pad=1,\n                                   data_format=None):\n-    \"\"\"Pad the rows and columns of a 4D tensor\n-    with \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\" (resp.) zeros\n+    \"\"\"Pad the rows and columns of a 4D tensor.\n+\n+    Pads these dimensions with\n+    \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\" (resp.) zeros\n     rows on top, bottom; cols on left, right.\n \n+    # Arguments\n+\n     # Returns\n         A padded 4D tensor.\n \n@@ -1754,17 +1819,24 @@ def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n \n \n def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n-    \"\"\"Pads 5D tensor with zeros for the depth, height, width dimension with\n-    \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right\n+    \"\"\"Pads 5D tensor with zeros along the depth, height, width dimensions.\n \n-    For 'channels_last' data_format, the 2nd, 3rd and 4th dimension will be padded.\n-    For 'channels_first' data_format, the 3rd, 4th and 5th dimension will be padded.\n+    Pads these dimensions with respectively\n+    \"padding[0]\", \"padding[1]\" and \"padding[2]\" zeros left and right.\n+\n+    For 'channels_last' data_format,\n+    the 2nd, 3rd and 4th dimension will be padded.\n+    For 'channels_first' data_format,\n+    the 3rd, 4th and 5th dimension will be padded.\n+\n+    # Arguments\n \n     # Returns\n         A padded 5D tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `channels_last` or `channels_first`.\n+        ValueError: if `data_format` is neither\n+            `channels_last` or `channels_first`.\n \n     \"\"\"\n     if data_format is None:\n@@ -1804,8 +1876,15 @@ def stack(x):\n \n \n def one_hot(indices, num_classes):\n-    \"\"\"Input: nD integer tensor of shape `(batch_size, dim1, dim2, ... dim(n-1))`\n-    Output: (n + 1)D one hot representation of the input\n+    \"\"\"Computes the one-hot representation of an integer tensor.\n+\n+    # Arguments\n+        indices: nD integer tensor of shape\n+            `(batch_size, dim1, dim2, ... dim(n-1))`\n+        num_classes: Integer, number of classes to consider.\n+\n+    # Returns\n+        (n + 1)D one hot representation of the input\n         with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n \n     # Returns\n@@ -1815,7 +1894,12 @@ def one_hot(indices, num_classes):\n \n \n def reverse(x, axes):\n-    \"\"\"Reverse a tensor along the the specified axes\n+    \"\"\"Reverse a tensor along the the specified axes.\n+\n+    # Arguments\n+        x: Tensor to reverse.\n+        axes: Integer or iterable of integers.\n+            Axes to reverse.\n \n     # Returns\n         A tensor.\n@@ -1845,24 +1929,28 @@ def get_value(x):\n     return x.eval(session=get_session())\n \n \n-def batch_get_value(xs):\n+def batch_get_value(ops):\n     \"\"\"Returns the value of more than one tensor variable.\n \n     # Arguments\n-        x: list of variables.\n+        ops: list of ops to run.\n \n     # Returns\n         A list of Numpy arrays.\n     \"\"\"\n-    if xs:\n-        return get_session().run(xs)\n+    if ops:\n+        return get_session().run(ops)\n     else:\n         return []\n \n \n def set_value(x, value):\n-    \"\"\"Sets the value of a variable,\n-    from a Numpy array. It returns `None`.\n+    \"\"\"Sets the value of a variable, from a Numpy array.\n+\n+    # Arguments\n+        x: Tensor to set to a new value.\n+        value: Value to set the tensor to, as a Numpy array\n+            (of the same shape).\n     \"\"\"\n     value = np.asarray(value)\n     tf_dtype = _convert_string_dtype(x.dtype.name.split('_')[0])\n@@ -1879,7 +1967,6 @@ def set_value(x, value):\n \n def batch_set_value(tuples):\n     \"\"\"Sets the values of many tensor variables at once.\n-    It returns `None`.\n \n     # Arguments\n         tuples: a list of tuples `(tensor, value)`.\n@@ -1906,10 +1993,10 @@ def batch_set_value(tuples):\n \n \n def get_variable_shape(x):\n-    \"\"\"Returns shape of a variable.\n+    \"\"\"Returns the shape of a variable.\n \n     # Arguments\n-        A variable.\n+        x: A variable.\n \n     # Returns\n         A tuple of integers.\n@@ -1918,8 +2005,14 @@ def get_variable_shape(x):\n \n \n def print_tensor(x, message=''):\n-    \"\"\"Print the message and the tensor when evaluated and return the same\n-    tensor.\n+    \"\"\"Prints `message` and the tensor value when evaluated.\n+\n+    # Arguments\n+        x: Tensor to print.\n+        message: Message to print jointly with the tensor.\n+\n+    # Returns\n+        The same tensor `x`, unchanged.\n     \"\"\"\n     return tf.Print(x, [x], message)\n \n@@ -2559,7 +2652,7 @@ def l2_normalize(x, axis):\n         A tensor.\n     \"\"\"\n     if axis < 0:\n-        axis = axis % len(x.get_shape())\n+        axis %= len(x.get_shape())\n     return tf.nn.l2_normalize(x, dim=axis)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
