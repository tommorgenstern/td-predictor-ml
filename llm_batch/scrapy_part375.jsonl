{"custom_id": "scrapy#ab5ea32ffd9cbea22d0fb10ece5258cea207dd61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 2603 | Contributors (this commit): 30 | Commits (past 90d): 3 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -987,10 +987,14 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"The value of FOO is 42\", log)\n \n \n-@skipIf(platform.system() != \"Windows\", \"Windows required for .pyw files\")\n class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n     spider_filename = \"myspider.pyw\"\n \n+    def setUp(self):\n+        # https://github.com/scrapy/scrapy/issues/6286\n+        if platform.system() != \"Windows\":\n+            raise unittest.SkipTest(\"Windows required for .pyw files\")\n+\n     def test_start_requests_errors(self):\n         log = self.get_log(self.badspider, name=\"badspider.pyw\")\n         self.assertIn(\"start_requests\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#188d9a8bb363ab3ff37dbb6020354afbc72ec02d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2604 | Contributors (this commit): 30 | Commits (past 90d): 4 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -991,7 +991,6 @@ class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n     spider_filename = \"myspider.pyw\"\n \n     def setUp(self):\n-        # https://github.com/scrapy/scrapy/issues/6286\n         if platform.system() != \"Windows\":\n             raise unittest.SkipTest(\"Windows required for .pyw files\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e72de11f55dc5f37d449385f20d2ce4c504914d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2605 | Contributors (this commit): 30 | Commits (past 90d): 5 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -993,6 +993,7 @@ class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n     def setUp(self):\n         if platform.system() != \"Windows\":\n             raise unittest.SkipTest(\"Windows required for .pyw files\")\n+        return super().setUp()\n \n     def test_start_requests_errors(self):\n         log = self.get_log(self.badspider, name=\"badspider.pyw\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7581c6b41e97fc09c011b089cf34ddd62f41876", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 16 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 72 | Churn Cumulative: 1746 | Contributors (this commit): 19 | Commits (past 90d): 9 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@ import random\n from collections import deque\n from datetime import datetime\n from time import time\n-from typing import TYPE_CHECKING, Any, Deque, Dict, Set, Tuple, cast\n+from typing import TYPE_CHECKING, Any, Deque, Dict, Optional, Set, Tuple, cast\n \n from twisted.internet import task\n from twisted.internet.defer import Deferred\n@@ -24,10 +24,18 @@ if TYPE_CHECKING:\n class Slot:\n     \"\"\"Downloader slot\"\"\"\n \n-    def __init__(self, concurrency: int, delay: float, randomize_delay: bool):\n+    def __init__(\n+        self,\n+        concurrency: int,\n+        delay: float,\n+        randomize_delay: bool,\n+        *,\n+        throttle: Optional[bool] = None,\n+    ):\n         self.concurrency: int = concurrency\n         self.delay: float = delay\n         self.randomize_delay: bool = randomize_delay\n+        self.throttle = throttle\n \n         self.active: Set[Request] = set()\n         self.queue: Deque[Tuple[Request, Deferred]] = deque()\n@@ -52,13 +60,15 @@ class Slot:\n         return (\n             f\"{cls_name}(concurrency={self.concurrency!r}, \"\n             f\"delay={self.delay:.2f}, \"\n-            f\"randomize_delay={self.randomize_delay!r})\"\n+            f\"randomize_delay={self.randomize_delay!r}, \"\n+            f\"throttle={self.throttle!r})\"\n         )\n \n     def __str__(self) -> str:\n         return (\n             f\"<downloader.Slot concurrency={self.concurrency!r} \"\n             f\"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} \"\n+            f\"throttle={self.throttle!r} \"\n             f\"len(active)={len(self.active)} len(queue)={len(self.queue)} \"\n             f\"len(transferring)={len(self.transferring)} \"\n             f\"lastseen={datetime.fromtimestamp(self.lastseen).isoformat()}>\"\n@@ -127,7 +137,8 @@ class Downloader:\n                 slot_settings.get(\"delay\", delay),\n             )\n             randomize_delay = slot_settings.get(\"randomize_delay\", self.randomize_delay)\n-            new_slot = Slot(conc, delay, randomize_delay)\n+            throttle = slot_settings.get(\"throttle\", None)\n+            new_slot = Slot(conc, delay, randomize_delay, throttle=throttle)\n             self.slots[key] = new_slot\n \n         return key, self.slots[key]\n\n@@ -62,7 +62,7 @@ class AutoThrottle:\n     ) -> None:\n         key, slot = self._get_slot(request, spider)\n         latency = request.meta.get(\"download_latency\")\n-        if latency is None or slot is None:\n+        if latency is None or slot is None or slot.throttle is False:\n             return\n \n         olddelay = slot.delay\n\n@@ -7,5 +7,6 @@ class SlotTest(unittest.TestCase):\n     def test_repr(self):\n         slot = Slot(concurrency=8, delay=0.1, randomize_delay=True)\n         self.assertEqual(\n-            repr(slot), \"Slot(concurrency=8, delay=0.10, randomize_delay=True)\"\n+            repr(slot),\n+            \"Slot(concurrency=8, delay=0.10, randomize_delay=True, throttle=None)\",\n         )\n\n@@ -3,8 +3,10 @@ import time\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n+from scrapy import Request\n+from scrapy.core.downloader import Downloader, Slot\n from scrapy.crawler import CrawlerRunner\n-from scrapy.http import Request\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n from tests.spiders import MetaSpider\n \n@@ -20,6 +22,7 @@ class DownloaderSlotsSettingsTestSpider(MetaSpider):\n                 \"concurrency\": 1,\n                 \"delay\": 2,\n                 \"randomize_delay\": False,\n+                \"throttle\": False,\n             },\n             \"books.toscrape.com\": {\"delay\": 3, \"randomize_delay\": False},\n         },\n@@ -70,3 +73,27 @@ class CrawlTestCase(TestCase):\n         }\n \n         self.assertTrue(max(list(error_delta.values())) < tolerance)\n+\n+\n+def test_params():\n+    params = {\n+        \"concurrency\": 1,\n+        \"delay\": 2,\n+        \"randomize_delay\": False,\n+        \"throttle\": False,\n+    }\n+    settings = {\n+        \"DOWNLOAD_SLOTS\": {\n+            \"example.com\": params,\n+        },\n+    }\n+    crawler = get_crawler(settings_dict=settings)\n+    downloader = Downloader(crawler)\n+    downloader._slot_gc_loop.stop()  # Prevent an unclean reactor.\n+    request = Request(\"https://example.com\")\n+    _, actual = downloader._get_slot(request, spider=None)\n+    expected = Slot(**params)\n+    for param in params:\n+        assert getattr(expected, param) == getattr(\n+            actual, param\n+        ), f\"Slot.{param}: {getattr(expected, param)!r} != {getattr(actual, param)!r}\"\n\n@@ -157,16 +157,17 @@ def test_startdelay_definition(min_spider, min_setting, start_setting, expected)\n \n \n @pytest.mark.parametrize(\n-    (\"meta\", \"slot\"),\n+    (\"meta\", \"slot\", \"throttle\"),\n     (\n-        ({}, None),\n-        ({\"download_latency\": 1.0}, None),\n-        ({\"download_slot\": \"foo\"}, None),\n-        ({\"download_slot\": \"foo\"}, \"foo\"),\n-        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, None),\n+        ({}, None, None),\n+        ({\"download_latency\": 1.0}, None, None),\n+        ({\"download_slot\": \"foo\"}, None, None),\n+        ({\"download_slot\": \"foo\"}, \"foo\", None),\n+        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, None, None),\n+        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, \"foo\", False),\n     ),\n )\n-def test_skipped(meta, slot):\n+def test_skipped(meta, slot, throttle):\n     crawler = get_crawler()\n     at = build_from_crawler(AutoThrottle, crawler)\n     spider = TestSpider()\n@@ -177,7 +178,9 @@ def test_skipped(meta, slot):\n     crawler.engine.downloader = Mock()\n     crawler.engine.downloader.slots = {}\n     if slot is not None:\n-        crawler.engine.downloader.slots[slot] = object()\n+        _slot = Mock()\n+        _slot.throttle = throttle\n+        crawler.engine.downloader.slots[slot] = _slot\n     at._adjust_delay = None  # Raise exception if called.\n \n     at._response_downloaded(None, request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4460d3ed9631f8409c24f78f1abb36345967b5d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 18 | Churn Cumulative: 300 | Contributors (this commit): 18 | Commits (past 90d): 1 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,6 @@\n-import platform\n-import sys\n from pathlib import Path\n \n import pytest\n-from twisted import version as twisted_version\n-from twisted.python.versions import Version\n from twisted.web.http import H2_ENABLED\n \n from scrapy.utils.reactor import install_reactor\n@@ -85,14 +81,12 @@ def only_not_asyncio(request, reactor_pytest):\n def requires_uvloop(request):\n     if not request.node.get_closest_marker(\"requires_uvloop\"):\n         return\n-    if sys.implementation.name == \"pypy\":\n-        pytest.skip(\"uvloop does not support pypy properly\")\n-    if platform.system() == \"Windows\":\n-        pytest.skip(\"uvloop does not support Windows\")\n-    if twisted_version == Version(\"twisted\", 21, 2, 0):\n-        pytest.skip(\"https://twistedmatrix.com/trac/ticket/10106\")\n-    if sys.version_info >= (3, 12):\n-        pytest.skip(\"uvloop doesn't support Python 3.12 yet\")\n+    try:\n+        import uvloop\n+\n+        del uvloop\n+    except ImportError:\n+        pytest.skip(\"uvloop is not installed\")\n \n \n def pytest_configure(config):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d917c0b55cbeebd284f7446ccc0f90af60729ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 269 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n import sys\n from io import StringIO\n-from unittest.mock import Mock, call, PropertyMock, patch\n+from unittest.mock import Mock, PropertyMock, call, patch\n \n from scrapy.commands.check import Command, TextTestResult\n from tests.test_commands import CommandTest\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f7bf3f726e3f19bf68b5e7e460f116850896eb42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1147 | Contributors (this commit): 15 | Commits (past 90d): 1 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -80,10 +80,10 @@ class SelectorTestCase(unittest.TestCase):\n         meta = (\n             '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">'\n         )\n-        head = \"<head>\" + meta + \"</head>\"\n+        head = f\"<head>{meta}</head>\"\n         body_content = '<span id=\"blank\">\\xa3</span>'\n-        body = \"<body>\" + body_content + \"</body>\"\n-        html = \"<html>\" + head + body + \"</html>\"\n+        body = f\"<body>{body_content}</body>\"\n+        html = f\"<html>{head}{body}</html>\"\n         encoding = \"utf-8\"\n         html_utf8 = html.encode(encoding)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b37dcd80d3783d3a21ff524e572019ff2b9e0ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 23 | Churn Cumulative: 555 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,7 +23,7 @@ def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n         if to_native_str_type:\n             robotstxt_body = to_unicode(robotstxt_body)\n         else:\n-            robotstxt_body = robotstxt_body.decode(\"utf-8\")\n+            robotstxt_body = robotstxt_body.decode(\"utf-8\", errors=\"ignore\")\n     except UnicodeDecodeError:\n         # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n         # Switch to 'allow all' state.\n\n@@ -1,5 +1,7 @@\n from twisted.trial import unittest\n \n+from scrapy.robotstxt import decode_robotstxt\n+\n \n def reppy_available():\n     # check if reppy parser is installed\n@@ -141,6 +143,25 @@ class BaseRobotParserTest:\n         )\n \n \n+class DecodeRobotsTxtTest(unittest.TestCase):\n+    def test_native_string_conversion(self):\n+        robotstxt_body = \"User-agent: *\\nDisallow: /\\n\".encode(\"utf-8\")\n+        decoded_content = decode_robotstxt(\n+            robotstxt_body, spider=None, to_native_str_type=True\n+        )\n+        self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n+\n+    def test_decode_utf8(self):\n+        robotstxt_body = \"User-agent: *\\nDisallow: /\\n\".encode(\"utf-8\")\n+        decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n+        self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n+\n+    def test_decode_non_utf8(self):\n+        robotstxt_body = b\"User-agent: *\\n\\xFFDisallow: /\\n\"\n+        decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n+        self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n+\n+\n class PythonRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n     def setUp(self):\n         from scrapy.robotstxt import PythonRobotParser\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#48c5a8c98f545e35708a580e724d0b8e1ada5e6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 340 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -170,7 +170,8 @@ class WrappedRequest:\n         return name in self.request.headers\n \n     def get_header(self, name, default=None):\n-        return to_unicode(self.request.headers.get(name, default), errors=\"replace\")\n+        value = self.request.headers.get(name, default)\n+        return to_unicode(value, errors=\"replace\") if value is not None else None\n \n     def header_items(self):\n         return [\n\n@@ -43,6 +43,13 @@ class WrappedRequestTest(TestCase):\n     def test_get_header(self):\n         self.assertEqual(self.wrapped.get_header(\"content-type\"), \"text/html\")\n         self.assertEqual(self.wrapped.get_header(\"xxxxx\", \"def\"), \"def\")\n+        self.assertEqual(self.wrapped.get_header(\"xxxxx\"), None)\n+        wrapped = WrappedRequest(\n+            Request(\n+                \"http://www.example.com/page.html\", headers={\"empty-binary-header\": b\"\"}\n+            )\n+        )\n+        self.assertEqual(wrapped.get_header(\"empty-binary-header\"), \"\")\n \n     def test_header_items(self):\n         self.assertEqual(self.wrapped.header_items(), [(\"Content-Type\", [\"text/html\"])])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b1fe97dc6c8509d58b29c61cf7801eeee1b409a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 711 | Contributors (this commit): 22 | Commits (past 90d): 2 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -239,8 +239,11 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n \n         if platform.python_implementation() == \"CPython\":\n-            # doesn't work on CPython: https://bugs.python.org/issue42785\n-            self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n+            # This didn't work on older versions of CPython: https://github.com/python/cpython/issues/86951\n+            self.assertIn(\n+                get_func_args(operator.itemgetter(2), stripself=True),\n+                [[], [\"args\", \"kwargs\"]],\n+            )\n         elif platform.python_implementation() == \"PyPy\":\n             self.assertEqual(\n                 get_func_args(operator.itemgetter(2), stripself=True), [\"obj\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a166e9739962ec7cca8a655e5f20a18a1bce7d14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 426 | Contributors (this commit): 24 | Commits (past 90d): 2 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,6 @@\n # serve to show the default.\n \n import sys\n-from datetime import datetime\n from pathlib import Path\n \n # If your extensions are in another directory, add it here. If the directory\n@@ -48,7 +47,7 @@ master_doc = \"index\"\n \n # General information about the project.\n project = \"Scrapy\"\n-copyright = f\"2008–{datetime.now().year}, Scrapy developers\"\n+copyright = \"Scrapy developers\"\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#57acad3c38602f4399c307c2c002f9eddde97cbc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 321 | Lines Deleted: 173 | Files Changed: 4 | Hunks: 134 | Methods Changed: 103 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 494 | Churn Cumulative: 3857 | Contributors (this commit): 63 | Commits (past 90d): 11 | Contributors (cumulative): 92 | DMM Complexity: 0.9323308270676691\n\nDIFF:\n@@ -4,6 +4,8 @@ Feed Exports extension\n See documentation in docs/topics/feed-exports.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n import re\n import sys\n@@ -11,18 +13,36 @@ import warnings\n from datetime import datetime, timezone\n from pathlib import Path, PureWindowsPath\n from tempfile import NamedTemporaryFile\n-from typing import IO, Any, Callable, Dict, List, Optional, Tuple, Type, Union\n+from typing import (\n+    IO,\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Protocol,\n+    Tuple,\n+    Type,\n+    TypeVar,\n+    Union,\n+    cast,\n+)\n from urllib.parse import unquote, urlparse\n \n-from twisted.internet import defer, threads\n-from twisted.internet.defer import DeferredList\n+from twisted.internet import threads\n+from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n+from twisted.python.failure import Failure\n from w3lib.url import file_uri_to_path\n from zope.interface import Interface, implementer\n \n from scrapy import Spider, signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.exporters import BaseItemExporter\n from scrapy.extensions.postprocessing import PostProcessingManager\n+from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.defer import maybe_deferred_to_future\n@@ -32,6 +52,12 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import without_none_values\n \n+if TYPE_CHECKING:\n+    from _typeshed import OpenBinaryMode\n+\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n logger = logging.getLogger(__name__)\n \n try:\n@@ -41,8 +67,19 @@ try:\n except ImportError:\n     IS_BOTO3_AVAILABLE = False\n \n+UriParamsCallableT = Callable[[Dict[str, Any], Spider], Optional[Dict[str, Any]]]\n \n-def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n+_StorageT = TypeVar(\"_StorageT\", bound=\"FeedStorageProtocol\")\n+\n+\n+def build_storage(\n+    builder: Callable[..., _StorageT],\n+    uri: str,\n+    *args: Any,\n+    feed_options: Optional[Dict[str, Any]] = None,\n+    preargs: Iterable[Any] = (),\n+    **kwargs: Any,\n+) -> _StorageT:\n     kwargs[\"feed_options\"] = feed_options\n     return builder(*preargs, uri, *args, **kwargs)\n \n@@ -56,10 +93,10 @@ class ItemFilter:\n     :type feed_options: dict\n     \"\"\"\n \n-    feed_options: Optional[dict]\n-    item_classes: Tuple\n+    feed_options: Optional[Dict[str, Any]]\n+    item_classes: Tuple[type, ...]\n \n-    def __init__(self, feed_options: Optional[dict]) -> None:\n+    def __init__(self, feed_options: Optional[Dict[str, Any]]) -> None:\n         self.feed_options = feed_options\n         if feed_options is not None:\n             self.item_classes = tuple(\n@@ -98,28 +135,49 @@ class IFeedStorage(Interface):\n         \"\"\"Store the given file stream\"\"\"\n \n \n+class FeedStorageProtocol(Protocol):\n+    \"\"\"Reimplementation of ``IFeedStorage`` that can be used in type hints.\"\"\"\n+\n+    def __init__(self, uri: str, *, feed_options: Optional[Dict[str, Any]] = None):\n+        \"\"\"Initialize the storage with the parameters given in the URI and the\n+        feed-specific options (see :setting:`FEEDS`)\"\"\"\n+\n+    def open(self, spider: Spider) -> IO[bytes]:\n+        \"\"\"Open the storage for the given spider. It must return a file-like\n+        object that will be used for the exporters\"\"\"\n+\n+    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n+        \"\"\"Store the given file stream\"\"\"\n+\n+\n @implementer(IFeedStorage)\n class BlockingFeedStorage:\n-    def open(self, spider):\n+    def open(self, spider: Spider) -> IO[bytes]:\n         path = spider.crawler.settings[\"FEED_TEMPDIR\"]\n         if path and not Path(path).is_dir():\n             raise OSError(\"Not a Directory: \" + str(path))\n \n         return NamedTemporaryFile(prefix=\"feed-\", dir=path)\n \n-    def store(self, file):\n+    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n         return threads.deferToThread(self._store_in_thread, file)\n \n-    def _store_in_thread(self, file):\n+    def _store_in_thread(self, file: IO[bytes]) -> None:\n         raise NotImplementedError\n \n \n @implementer(IFeedStorage)\n class StdoutFeedStorage:\n-    def __init__(self, uri, _stdout=None, *, feed_options=None):\n+    def __init__(\n+        self,\n+        uri: str,\n+        _stdout: Optional[IO[bytes]] = None,\n+        *,\n+        feed_options: Optional[Dict[str, Any]] = None,\n+    ):\n         if not _stdout:\n             _stdout = sys.stdout.buffer\n-        self._stdout = _stdout\n+        self._stdout: IO[bytes] = _stdout\n         if feed_options and feed_options.get(\"overwrite\", False) is True:\n             logger.warning(\n                 \"Standard output (stdout) storage does not support \"\n@@ -128,54 +186,58 @@ class StdoutFeedStorage:\n                 \"it to False.\"\n             )\n \n-    def open(self, spider):\n+    def open(self, spider: Spider) -> IO[bytes]:\n         return self._stdout\n \n-    def store(self, file):\n+    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n         pass\n \n \n @implementer(IFeedStorage)\n class FileFeedStorage:\n-    def __init__(self, uri, *, feed_options=None):\n-        self.path = file_uri_to_path(uri)\n+    def __init__(self, uri: str, *, feed_options: Optional[Dict[str, Any]] = None):\n+        self.path: str = file_uri_to_path(uri)\n         feed_options = feed_options or {}\n-        self.write_mode = \"wb\" if feed_options.get(\"overwrite\", False) else \"ab\"\n+        self.write_mode: OpenBinaryMode = (\n+            \"wb\" if feed_options.get(\"overwrite\", False) else \"ab\"\n+        )\n \n-    def open(self, spider) -> IO[Any]:\n+    def open(self, spider: Spider) -> IO[bytes]:\n         dirname = Path(self.path).parent\n         if dirname and not dirname.exists():\n             dirname.mkdir(parents=True)\n         return Path(self.path).open(self.write_mode)\n \n-    def store(self, file):\n+    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n         file.close()\n+        return None\n \n \n class S3FeedStorage(BlockingFeedStorage):\n     def __init__(\n         self,\n-        uri,\n-        access_key=None,\n-        secret_key=None,\n-        acl=None,\n-        endpoint_url=None,\n+        uri: str,\n+        access_key: Optional[str] = None,\n+        secret_key: Optional[str] = None,\n+        acl: Optional[str] = None,\n+        endpoint_url: Optional[str] = None,\n         *,\n-        feed_options=None,\n-        session_token=None,\n-        region_name=None,\n+        feed_options: Optional[Dict[str, Any]] = None,\n+        session_token: Optional[str] = None,\n+        region_name: Optional[str] = None,\n     ):\n         if not is_botocore_available():\n             raise NotConfigured(\"missing botocore library\")\n         u = urlparse(uri)\n-        self.bucketname = u.hostname\n-        self.access_key = u.username or access_key\n-        self.secret_key = u.password or secret_key\n-        self.session_token = session_token\n-        self.keyname = u.path[1:]  # remove first \"/\"\n-        self.acl = acl\n-        self.endpoint_url = endpoint_url\n-        self.region_name = region_name\n+        assert u.hostname\n+        self.bucketname: str = u.hostname\n+        self.access_key: Optional[str] = u.username or access_key\n+        self.secret_key: Optional[str] = u.password or secret_key\n+        self.session_token: Optional[str] = session_token\n+        self.keyname: str = u.path[1:]  # remove first \"/\"\n+        self.acl: Optional[str] = acl\n+        self.endpoint_url: Optional[str] = endpoint_url\n+        self.region_name: Optional[str] = region_name\n \n         if IS_BOTO3_AVAILABLE:\n             import boto3.session\n@@ -218,7 +280,13 @@ class S3FeedStorage(BlockingFeedStorage):\n             )\n \n     @classmethod\n-    def from_crawler(cls, crawler, uri, *, feed_options=None):\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        uri: str,\n+        *,\n+        feed_options: Optional[Dict[str, Any]] = None,\n+    ) -> Self:\n         return build_storage(\n             cls,\n             uri,\n@@ -231,8 +299,9 @@ class S3FeedStorage(BlockingFeedStorage):\n             feed_options=feed_options,\n         )\n \n-    def _store_in_thread(self, file):\n+    def _store_in_thread(self, file: IO[bytes]) -> None:\n         file.seek(0)\n+        kwargs: Dict[str, Any]\n         if IS_BOTO3_AVAILABLE:\n             kwargs = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n             self.s3_client.upload_fileobj(\n@@ -247,22 +316,23 @@ class S3FeedStorage(BlockingFeedStorage):\n \n \n class GCSFeedStorage(BlockingFeedStorage):\n-    def __init__(self, uri, project_id, acl):\n-        self.project_id = project_id\n-        self.acl = acl\n+    def __init__(self, uri: str, project_id: Optional[str], acl: Optional[str]):\n+        self.project_id: Optional[str] = project_id\n+        self.acl: Optional[str] = acl\n         u = urlparse(uri)\n-        self.bucket_name = u.hostname\n-        self.blob_name = u.path[1:]  # remove first \"/\"\n+        assert u.hostname\n+        self.bucket_name: str = u.hostname\n+        self.blob_name: str = u.path[1:]  # remove first \"/\"\n \n     @classmethod\n-    def from_crawler(cls, crawler, uri):\n+    def from_crawler(cls, crawler: Crawler, uri: str) -> Self:\n         return cls(\n             uri,\n             crawler.settings[\"GCS_PROJECT_ID\"],\n             crawler.settings[\"FEED_STORAGE_GCS_ACL\"] or None,\n         )\n \n-    def _store_in_thread(self, file):\n+    def _store_in_thread(self, file: IO[bytes]) -> None:\n         file.seek(0)\n         from google.cloud.storage import Client\n \n@@ -292,7 +362,13 @@ class FTPFeedStorage(BlockingFeedStorage):\n         self.overwrite: bool = not feed_options or feed_options.get(\"overwrite\", True)\n \n     @classmethod\n-    def from_crawler(cls, crawler, uri, *, feed_options=None):\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        uri: str,\n+        *,\n+        feed_options: Optional[Dict[str, Any]] = None,\n+    ) -> Self:\n         return build_storage(\n             cls,\n             uri,\n@@ -300,7 +376,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n             feed_options=feed_options,\n         )\n \n-    def _store_in_thread(self, file):\n+    def _store_in_thread(self, file: IO[bytes]) -> None:\n         ftp_store_file(\n             path=self.path,\n             file=file,\n@@ -316,46 +392,51 @@ class FTPFeedStorage(BlockingFeedStorage):\n class FeedSlot:\n     def __init__(\n         self,\n-        storage,\n-        uri,\n-        format,\n-        store_empty,\n-        batch_id,\n-        uri_template,\n-        filter,\n-        feed_options,\n-        spider,\n+        storage: FeedStorageProtocol,\n+        uri: str,\n+        format: str,\n+        store_empty: bool,\n+        batch_id: int,\n+        uri_template: str,\n+        filter: ItemFilter,\n+        feed_options: Dict[str, Any],\n+        spider: Spider,\n         exporters: Dict[str, Type[BaseItemExporter]],\n-        settings,\n-        crawler,\n+        settings: BaseSettings,\n+        crawler: Crawler,\n     ):\n-        self.file = None\n+        self.file: Optional[IO[bytes]] = None\n         self.exporter: Optional[BaseItemExporter] = None\n-        self.storage = storage\n+        self.storage: FeedStorageProtocol = storage\n         # feed params\n-        self.batch_id = batch_id\n-        self.format = format\n-        self.store_empty = store_empty\n-        self.uri_template = uri_template\n-        self.uri = uri\n-        self.filter = filter\n+        self.batch_id: int = batch_id\n+        self.format: str = format\n+        self.store_empty: bool = store_empty\n+        self.uri_template: str = uri_template\n+        self.uri: str = uri\n+        self.filter: ItemFilter = filter\n         # exporter params\n-        self.feed_options = feed_options\n-        self.spider = spider\n+        self.feed_options: Dict[str, Any] = feed_options\n+        self.spider: Spider = spider\n         self.exporters: Dict[str, Type[BaseItemExporter]] = exporters\n-        self.settings = settings\n-        self.crawler = crawler\n+        self.settings: BaseSettings = settings\n+        self.crawler: Crawler = crawler\n         # flags\n-        self.itemcount = 0\n-        self._exporting = False\n-        self._fileloaded = False\n+        self.itemcount: int = 0\n+        self._exporting: bool = False\n+        self._fileloaded: bool = False\n \n-    def start_exporting(self):\n+    def start_exporting(self) -> None:\n         if not self._fileloaded:\n             self.file = self.storage.open(self.spider)\n             if \"postprocessing\" in self.feed_options:\n-                self.file = PostProcessingManager(\n-                    self.feed_options[\"postprocessing\"], self.file, self.feed_options\n+                self.file = cast(\n+                    IO[bytes],\n+                    PostProcessingManager(\n+                        self.feed_options[\"postprocessing\"],\n+                        self.file,\n+                        self.feed_options,\n+                    ),\n                 )\n             self.exporter = self._get_exporter(\n                 file=self.file,\n@@ -368,17 +449,23 @@ class FeedSlot:\n             self._fileloaded = True\n \n         if not self._exporting:\n+            assert self.exporter\n             self.exporter.start_exporting()\n             self._exporting = True\n \n-    def _get_instance(self, objcls, *args, **kwargs):\n+    def _get_instance(\n+        self, objcls: Type[BaseItemExporter], *args: Any, **kwargs: Any\n+    ) -> BaseItemExporter:\n         return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n \n-    def _get_exporter(self, file, format, *args, **kwargs) -> BaseItemExporter:\n+    def _get_exporter(\n+        self, file: IO[bytes], format: str, *args: Any, **kwargs: Any\n+    ) -> BaseItemExporter:\n         return self._get_instance(self.exporters[format], file, *args, **kwargs)\n \n-    def finish_exporting(self):\n+    def finish_exporting(self) -> None:\n         if self._exporting:\n+            assert self.exporter\n             self.exporter.finish_exporting()\n             self._exporting = False\n \n@@ -390,22 +477,22 @@ _FeedSlot = create_deprecated_class(\n \n \n class FeedExporter:\n-    _pending_deferreds: List[defer.Deferred] = []\n+    _pending_deferreds: List[Deferred] = []\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         exporter = cls(crawler)\n         crawler.signals.connect(exporter.open_spider, signals.spider_opened)\n         crawler.signals.connect(exporter.close_spider, signals.spider_closed)\n         crawler.signals.connect(exporter.item_scraped, signals.item_scraped)\n         return exporter\n \n-    def __init__(self, crawler):\n-        self.crawler = crawler\n-        self.settings = crawler.settings\n+    def __init__(self, crawler: Crawler):\n+        self.crawler: Crawler = crawler\n+        self.settings: Settings = crawler.settings\n         self.feeds = {}\n-        self.slots = []\n-        self.filters = {}\n+        self.slots: List[FeedSlot] = []\n+        self.filters: Dict[str, ItemFilter] = {}\n \n         if not self.settings[\"FEEDS\"] and not self.settings[\"FEED_URI\"]:\n             raise NotConfigured\n@@ -437,8 +524,12 @@ class FeedExporter:\n             )\n             self.filters[uri] = self._load_filter(feed_options)\n \n-        self.storages = self._load_components(\"FEED_STORAGES\")\n-        self.exporters = self._load_components(\"FEED_EXPORTERS\")\n+        self.storages: Dict[str, Type[FeedStorageProtocol]] = self._load_components(\n+            \"FEED_STORAGES\"\n+        )\n+        self.exporters: Dict[str, Type[BaseItemExporter]] = self._load_components(\n+            \"FEED_EXPORTERS\"\n+        )\n         for uri, feed_options in self.feeds.items():\n             if not self._storage_supported(uri, feed_options):\n                 raise NotConfigured\n@@ -447,7 +538,7 @@ class FeedExporter:\n             if not self._exporter_supported(feed_options[\"format\"]):\n                 raise NotConfigured\n \n-    def open_spider(self, spider):\n+    def open_spider(self, spider: Spider) -> None:\n         for uri, feed_options in self.feeds.items():\n             uri_params = self._get_uri_params(spider, feed_options[\"uri_params\"])\n             self.slots.append(\n@@ -460,7 +551,7 @@ class FeedExporter:\n                 )\n             )\n \n-    async def close_spider(self, spider):\n+    async def close_spider(self, spider: Spider) -> None:\n         for slot in self.slots:\n             self._close_slot(slot, spider)\n \n@@ -473,8 +564,9 @@ class FeedExporter:\n             self.crawler.signals.send_catch_log_deferred(signals.feed_exporter_closed)\n         )\n \n-    def _close_slot(self, slot, spider):\n-        def get_file(slot_):\n+    def _close_slot(self, slot: FeedSlot, spider: Spider) -> Optional[Deferred]:\n+        def get_file(slot_: FeedSlot) -> IO[bytes]:\n+            assert slot_.file\n             if isinstance(slot_.file, PostProcessingManager):\n                 slot_.file.close()\n                 return slot_.file.file\n@@ -492,7 +584,7 @@ class FeedExporter:\n             return None\n \n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n-        d = defer.maybeDeferred(slot.storage.store, get_file(slot))\n+        d: Deferred = maybeDeferred(slot.storage.store, get_file(slot))\n \n         d.addCallback(\n             self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n@@ -510,20 +602,33 @@ class FeedExporter:\n \n         return d\n \n-    def _handle_store_error(self, f, logmsg, spider, slot_type):\n+    def _handle_store_error(\n+        self, f: Failure, logmsg: str, spider: Spider, slot_type: str\n+    ) -> None:\n         logger.error(\n             \"Error storing %s\",\n             logmsg,\n             exc_info=failure_to_exc_info(f),\n             extra={\"spider\": spider},\n         )\n+        assert self.crawler.stats\n         self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")\n \n-    def _handle_store_success(self, f, logmsg, spider, slot_type):\n+    def _handle_store_success(\n+        self, f: Failure, logmsg: str, spider: Spider, slot_type: str\n+    ) -> None:\n         logger.info(\"Stored %s\", logmsg, extra={\"spider\": spider})\n+        assert self.crawler.stats\n         self.crawler.stats.inc_value(f\"feedexport/success_count/{slot_type}\")\n \n-    def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n+    def _start_new_batch(\n+        self,\n+        batch_id: int,\n+        uri: str,\n+        feed_options: Dict[str, Any],\n+        spider: Spider,\n+        uri_template: str,\n+    ) -> FeedSlot:\n         \"\"\"\n         Redirect the output data stream to a new file.\n         Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified\n@@ -546,11 +651,11 @@ class FeedExporter:\n             spider=spider,\n             exporters=self.exporters,\n             settings=self.settings,\n-            crawler=getattr(self, \"crawler\", None),\n+            crawler=self.crawler,\n         )\n         return slot\n \n-    def item_scraped(self, item, spider):\n+    def item_scraped(self, item: Any, spider: Spider) -> None:\n         slots = []\n         for slot in self.slots:\n             if not slot.filter.accepts(item):\n@@ -560,6 +665,7 @@ class FeedExporter:\n                 continue\n \n             slot.start_exporting()\n+            assert slot.exporter\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n             # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one\n@@ -584,7 +690,7 @@ class FeedExporter:\n                 slots.append(slot)\n         self.slots = slots\n \n-    def _load_components(self, setting_prefix):\n+    def _load_components(self, setting_prefix: str) -> Dict[str, Any]:\n         conf = without_none_values(self.settings.getwithbase(setting_prefix))\n         d = {}\n         for k, v in conf.items():\n@@ -594,12 +700,13 @@ class FeedExporter:\n                 pass\n         return d\n \n-    def _exporter_supported(self, format):\n+    def _exporter_supported(self, format: str) -> bool:\n         if format in self.exporters:\n             return True\n         logger.error(\"Unknown feed format: %(format)s\", {\"format\": format})\n+        return False\n \n-    def _settings_are_valid(self):\n+    def _settings_are_valid(self) -> bool:\n         \"\"\"\n         If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain\n         %(batch_time)s or %(batch_id)d to distinguish different files of partial output\n@@ -617,7 +724,7 @@ class FeedExporter:\n                 return False\n         return True\n \n-    def _storage_supported(self, uri, feed_options):\n+    def _storage_supported(self, uri: str, feed_options: Dict[str, Any]) -> bool:\n         scheme = urlparse(uri).scheme\n         if scheme in self.storages or PureWindowsPath(uri).drive:\n             try:\n@@ -630,8 +737,11 @@ class FeedExporter:\n                 )\n         else:\n             logger.error(\"Unknown feed storage scheme: %(scheme)s\", {\"scheme\": scheme})\n+        return False\n \n-    def _get_storage(self, uri, feed_options):\n+    def _get_storage(\n+        self, uri: str, feed_options: Dict[str, Any]\n+    ) -> FeedStorageProtocol:\n         \"\"\"Fork of create_instance specific to feed storage classes\n \n         It supports not passing the *feed_options* parameters to classes that\n@@ -640,11 +750,14 @@ class FeedExporter:\n         feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n         crawler = getattr(self, \"crawler\", None)\n \n-        def build_instance(builder, *preargs):\n+        def build_instance(\n+            builder: Type[FeedStorageProtocol], *preargs: Any\n+        ) -> FeedStorageProtocol:\n             return build_storage(\n                 builder, uri, feed_options=feed_options, preargs=preargs\n             )\n \n+        instance: FeedStorageProtocol\n         if crawler and hasattr(feedcls, \"from_crawler\"):\n             instance = build_instance(feedcls.from_crawler, crawler)\n             method_name = \"from_crawler\"\n@@ -661,9 +774,9 @@ class FeedExporter:\n     def _get_uri_params(\n         self,\n         spider: Spider,\n-        uri_params_function: Optional[Union[str, Callable[[dict, Spider], dict]]],\n+        uri_params_function: Union[str, UriParamsCallableT, None],\n         slot: Optional[FeedSlot] = None,\n-    ) -> dict:\n+    ) -> Dict[str, Any]:\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n@@ -671,7 +784,7 @@ class FeedExporter:\n         params[\"time\"] = utc_now.replace(microsecond=0).isoformat().replace(\":\", \"-\")\n         params[\"batch_time\"] = utc_now.isoformat().replace(\":\", \"-\")\n         params[\"batch_id\"] = slot.batch_id + 1 if slot is not None else 1\n-        uripar_function = (\n+        uripar_function: UriParamsCallableT = (\n             load_object(uri_params_function)\n             if uri_params_function\n             else lambda params, _: params\n@@ -679,7 +792,9 @@ class FeedExporter:\n         new_params = uripar_function(params, spider)\n         return new_params if new_params is not None else params\n \n-    def _load_filter(self, feed_options):\n+    def _load_filter(self, feed_options: Dict[str, Any]) -> ItemFilter:\n         # load the item filter if declared else load the default filter class\n-        item_filter_class = load_object(feed_options.get(\"item_filter\", ItemFilter))\n+        item_filter_class: Type[ItemFilter] = load_object(\n+            feed_options.get(\"item_filter\", ItemFilter)\n+        )\n         return item_filter_class(feed_options)\n\n@@ -1,10 +1,13 @@\n import gzip\n import logging\n+import os\n import pickle  # nosec\n from email.utils import mktime_tz, parsedate_tz\n from importlib import import_module\n from pathlib import Path\n from time import time\n+from types import ModuleType\n+from typing import IO, TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union, cast\n from weakref import WeakKeyDictionary\n \n from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n@@ -12,49 +15,65 @@ from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n from scrapy.http import Headers, Response\n from scrapy.http.request import Request\n from scrapy.responsetypes import responsetypes\n+from scrapy.settings import BaseSettings\n from scrapy.spiders import Spider\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.project import data_path\n from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.request import RequestFingerprinter\n+\n+if TYPE_CHECKING:\n+    # typing.Concatenate requires Python 3.10\n+    from typing_extensions import Concatenate\n+\n \n logger = logging.getLogger(__name__)\n \n \n class DummyPolicy:\n-    def __init__(self, settings):\n-        self.ignore_schemes = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n-        self.ignore_http_codes = [\n+    def __init__(self, settings: BaseSettings):\n+        self.ignore_schemes: List[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n+        self.ignore_http_codes: List[int] = [\n             int(x) for x in settings.getlist(\"HTTPCACHE_IGNORE_HTTP_CODES\")\n         ]\n \n-    def should_cache_request(self, request):\n+    def should_cache_request(self, request: Request) -> bool:\n         return urlparse_cached(request).scheme not in self.ignore_schemes\n \n-    def should_cache_response(self, response, request):\n+    def should_cache_response(self, response: Response, request: Request) -> bool:\n         return response.status not in self.ignore_http_codes\n \n-    def is_cached_response_fresh(self, cachedresponse, request):\n+    def is_cached_response_fresh(\n+        self, cachedresponse: Response, request: Request\n+    ) -> bool:\n         return True\n \n-    def is_cached_response_valid(self, cachedresponse, response, request):\n+    def is_cached_response_valid(\n+        self, cachedresponse: Response, response: Response, request: Request\n+    ) -> bool:\n         return True\n \n \n class RFC2616Policy:\n     MAXAGE = 3600 * 24 * 365  # one year\n \n-    def __init__(self, settings):\n-        self.always_store = settings.getbool(\"HTTPCACHE_ALWAYS_STORE\")\n-        self.ignore_schemes = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n-        self._cc_parsed = WeakKeyDictionary()\n-        self.ignore_response_cache_controls = [\n+    def __init__(self, settings: BaseSettings):\n+        self.always_store: bool = settings.getbool(\"HTTPCACHE_ALWAYS_STORE\")\n+        self.ignore_schemes: List[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n+        self._cc_parsed: WeakKeyDictionary[\n+            Union[Request, Response], Dict[bytes, Optional[bytes]]\n+        ] = WeakKeyDictionary()\n+        self.ignore_response_cache_controls: List[bytes] = [\n             to_bytes(cc)\n             for cc in settings.getlist(\"HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\")\n         ]\n \n-    def _parse_cachecontrol(self, r):\n+    def _parse_cachecontrol(\n+        self, r: Union[Request, Response]\n+    ) -> Dict[bytes, Optional[bytes]]:\n         if r not in self._cc_parsed:\n             cch = r.headers.get(b\"Cache-Control\", b\"\")\n+            assert cch is not None\n             parsed = parse_cachecontrol(cch)\n             if isinstance(r, Response):\n                 for key in self.ignore_response_cache_controls:\n@@ -62,7 +81,7 @@ class RFC2616Policy:\n             self._cc_parsed[r] = parsed\n         return self._cc_parsed[r]\n \n-    def should_cache_request(self, request):\n+    def should_cache_request(self, request: Request) -> bool:\n         if urlparse_cached(request).scheme in self.ignore_schemes:\n             return False\n         cc = self._parse_cachecontrol(request)\n@@ -72,7 +91,7 @@ class RFC2616Policy:\n         # Any other is eligible for caching\n         return True\n \n-    def should_cache_response(self, response, request):\n+    def should_cache_response(self, response: Response, request: Request) -> bool:\n         # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\n         # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n         # Status code 206 is not included because cache can not deal with partial contents\n@@ -100,7 +119,9 @@ class RFC2616Policy:\n         # info and can not be revalidated\n         return False\n \n-    def is_cached_response_fresh(self, cachedresponse, request):\n+    def is_cached_response_fresh(\n+        self, cachedresponse: Response, request: Request\n+    ) -> bool:\n         cc = self._parse_cachecontrol(cachedresponse)\n         ccreq = self._parse_cachecontrol(request)\n         if b\"no-cache\" in cc or b\"no-cache\" in ccreq:\n@@ -141,7 +162,9 @@ class RFC2616Policy:\n         self._set_conditional_validators(request, cachedresponse)\n         return False\n \n-    def is_cached_response_valid(self, cachedresponse, response, request):\n+    def is_cached_response_valid(\n+        self, cachedresponse: Response, response: Response, request: Request\n+    ) -> bool:\n         # Use the cached response if the new response is a server error,\n         # as long as the old response didn't specify must-revalidate.\n         if response.status >= 500:\n@@ -152,7 +175,9 @@ class RFC2616Policy:\n         # Use the cached response if the server says it hasn't changed.\n         return response.status == 304\n \n-    def _set_conditional_validators(self, request, cachedresponse):\n+    def _set_conditional_validators(\n+        self, request: Request, cachedresponse: Response\n+    ) -> None:\n         if b\"Last-Modified\" in cachedresponse.headers:\n             request.headers[b\"If-Modified-Since\"] = cachedresponse.headers[\n                 b\"Last-Modified\"\n@@ -161,13 +186,15 @@ class RFC2616Policy:\n         if b\"ETag\" in cachedresponse.headers:\n             request.headers[b\"If-None-Match\"] = cachedresponse.headers[b\"ETag\"]\n \n-    def _get_max_age(self, cc):\n+    def _get_max_age(self, cc: Dict[bytes, Optional[bytes]]) -> Optional[int]:\n         try:\n-            return max(0, int(cc[b\"max-age\"]))\n+            return max(0, int(cc[b\"max-age\"]))  # type: ignore[arg-type]\n         except (KeyError, ValueError):\n             return None\n \n-    def _compute_freshness_lifetime(self, response, request, now):\n+    def _compute_freshness_lifetime(\n+        self, response: Response, request: Request, now: float\n+    ) -> float:\n         # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n         # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706\n         cc = self._parse_cachecontrol(response)\n@@ -198,10 +225,12 @@ class RFC2616Policy:\n         # Insufficient information to compute freshness lifetime\n         return 0\n \n-    def _compute_current_age(self, response, request, now):\n+    def _compute_current_age(\n+        self, response: Response, request: Request, now: float\n+    ) -> float:\n         # Reference nsHttpResponseHead::ComputeCurrentAge\n         # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658\n-        currentage = 0\n+        currentage: float = 0\n         # If Date header is not set we assume it is a fast connection, and\n         # clock is in sync with the server\n         date = rfc1123_to_epoch(response.headers.get(b\"Date\")) or now\n@@ -210,7 +239,7 @@ class RFC2616Policy:\n \n         if b\"Age\" in response.headers:\n             try:\n-                age = int(response.headers[b\"Age\"])\n+                age = int(response.headers[b\"Age\"])  # type: ignore[arg-type]\n                 currentage = max(currentage, age)\n             except ValueError:\n                 pass\n@@ -219,13 +248,13 @@ class RFC2616Policy:\n \n \n class DbmCacheStorage:\n-    def __init__(self, settings):\n-        self.cachedir = data_path(settings[\"HTTPCACHE_DIR\"], createdir=True)\n-        self.expiration_secs = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n-        self.dbmodule = import_module(settings[\"HTTPCACHE_DBM_MODULE\"])\n-        self.db = None\n+    def __init__(self, settings: BaseSettings):\n+        self.cachedir: str = data_path(settings[\"HTTPCACHE_DIR\"], createdir=True)\n+        self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n+        self.dbmodule: ModuleType = import_module(settings[\"HTTPCACHE_DBM_MODULE\"])\n+        self.db: Any = None  # the real type is private\n \n-    def open_spider(self, spider: Spider):\n+    def open_spider(self, spider: Spider) -> None:\n         dbpath = Path(self.cachedir, f\"{spider.name}.db\")\n         self.db = self.dbmodule.open(str(dbpath), \"c\")\n \n@@ -235,15 +264,16 @@ class DbmCacheStorage:\n             extra={\"spider\": spider},\n         )\n \n-        self._fingerprinter = spider.crawler.request_fingerprinter\n+        assert spider.crawler.request_fingerprinter\n+        self._fingerprinter: RequestFingerprinter = spider.crawler.request_fingerprinter\n \n-    def close_spider(self, spider):\n+    def close_spider(self, spider: Spider) -> None:\n         self.db.close()\n \n-    def retrieve_response(self, spider, request):\n+    def retrieve_response(self, spider: Spider, request: Request) -> Optional[Response]:\n         data = self._read_data(spider, request)\n         if data is None:\n-            return  # not cached\n+            return None  # not cached\n         url = data[\"url\"]\n         status = data[\"status\"]\n         headers = Headers(data[\"headers\"])\n@@ -252,7 +282,9 @@ class DbmCacheStorage:\n         response = respcls(url=url, headers=headers, status=status, body=body)\n         return response\n \n-    def store_response(self, spider, request, response):\n+    def store_response(\n+        self, spider: Spider, request: Request, response: Response\n+    ) -> None:\n         key = self._fingerprinter.fingerprint(request).hex()\n         data = {\n             \"status\": response.status,\n@@ -263,28 +295,31 @@ class DbmCacheStorage:\n         self.db[f\"{key}_data\"] = pickle.dumps(data, protocol=4)\n         self.db[f\"{key}_time\"] = str(time())\n \n-    def _read_data(self, spider, request):\n+    def _read_data(self, spider: Spider, request: Request) -> Optional[Dict[str, Any]]:\n         key = self._fingerprinter.fingerprint(request).hex()\n         db = self.db\n         tkey = f\"{key}_time\"\n         if tkey not in db:\n-            return  # not found\n+            return None  # not found\n \n         ts = db[tkey]\n         if 0 < self.expiration_secs < time() - float(ts):\n-            return  # expired\n+            return None  # expired\n \n-        return pickle.loads(db[f\"{key}_data\"])  # nosec\n+        return cast(Dict[str, Any], pickle.loads(db[f\"{key}_data\"]))  # nosec\n \n \n class FilesystemCacheStorage:\n-    def __init__(self, settings):\n-        self.cachedir = data_path(settings[\"HTTPCACHE_DIR\"])\n-        self.expiration_secs = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n-        self.use_gzip = settings.getbool(\"HTTPCACHE_GZIP\")\n-        self._open = gzip.open if self.use_gzip else open\n+    def __init__(self, settings: BaseSettings):\n+        self.cachedir: str = data_path(settings[\"HTTPCACHE_DIR\"])\n+        self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n+        self.use_gzip: bool = settings.getbool(\"HTTPCACHE_GZIP\")\n+        # https://github.com/python/mypy/issues/10740\n+        self._open: Callable[Concatenate[Union[str, os.PathLike], str, ...], IO] = (\n+            gzip.open if self.use_gzip else open  # type: ignore[assignment]\n+        )\n \n-    def open_spider(self, spider: Spider):\n+    def open_spider(self, spider: Spider) -> None:\n         logger.debug(\n             \"Using filesystem cache storage in %(cachedir)s\",\n             {\"cachedir\": self.cachedir},\n@@ -294,27 +329,29 @@ class FilesystemCacheStorage:\n         assert spider.crawler.request_fingerprinter\n         self._fingerprinter = spider.crawler.request_fingerprinter\n \n-    def close_spider(self, spider):\n+    def close_spider(self, spider: Spider) -> None:\n         pass\n \n-    def retrieve_response(self, spider: Spider, request: Request):\n+    def retrieve_response(self, spider: Spider, request: Request) -> Optional[Response]:\n         \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n         metadata = self._read_meta(spider, request)\n         if metadata is None:\n-            return  # not cached\n+            return None  # not cached\n         rpath = Path(self._get_request_path(spider, request))\n         with self._open(rpath / \"response_body\", \"rb\") as f:\n             body = f.read()\n         with self._open(rpath / \"response_headers\", \"rb\") as f:\n             rawheaders = f.read()\n-        url = metadata.get(\"response_url\")\n+        url = metadata[\"response_url\"]\n         status = metadata[\"status\"]\n         headers = Headers(headers_raw_to_dict(rawheaders))\n         respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n         response = respcls(url=url, headers=headers, status=status, body=body)\n         return response\n \n-    def store_response(self, spider: Spider, request: Request, response):\n+    def store_response(\n+        self, spider: Spider, request: Request, response: Response\n+    ) -> None:\n         \"\"\"Store the given response in the cache.\"\"\"\n         rpath = Path(self._get_request_path(spider, request))\n         if not rpath.exists():\n@@ -343,19 +380,19 @@ class FilesystemCacheStorage:\n         key = self._fingerprinter.fingerprint(request).hex()\n         return str(Path(self.cachedir, spider.name, key[0:2], key))\n \n-    def _read_meta(self, spider: Spider, request: Request):\n+    def _read_meta(self, spider: Spider, request: Request) -> Optional[Dict[str, Any]]:\n         rpath = Path(self._get_request_path(spider, request))\n         metapath = rpath / \"pickled_meta\"\n         if not metapath.exists():\n-            return  # not found\n+            return None  # not found\n         mtime = metapath.stat().st_mtime\n         if 0 < self.expiration_secs < time() - mtime:\n-            return  # expired\n+            return None  # expired\n         with self._open(metapath, \"rb\") as f:\n-            return pickle.load(f)  # nosec\n+            return cast(Dict[str, Any], pickle.load(f))  # nosec\n \n \n-def parse_cachecontrol(header):\n+def parse_cachecontrol(header: bytes) -> Dict[bytes, Optional[bytes]]:\n     \"\"\"Parse Cache-Control header\n \n     https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n@@ -375,9 +412,9 @@ def parse_cachecontrol(header):\n     return directives\n \n \n-def rfc1123_to_epoch(date_str):\n+def rfc1123_to_epoch(date_str: Union[str, bytes, None]) -> Optional[int]:\n     try:\n-        date_str = to_unicode(date_str, encoding=\"ascii\")\n-        return mktime_tz(parsedate_tz(date_str))\n+        date_str = to_unicode(date_str, encoding=\"ascii\")  # type: ignore[arg-type]\n+        return mktime_tz(parsedate_tz(date_str))  # type: ignore[arg-type]\n     except Exception:\n         return None\n\n@@ -6,7 +6,7 @@ from bz2 import BZ2File\n from gzip import GzipFile\n from io import IOBase\n from lzma import LZMAFile\n-from typing import Any, BinaryIO, Dict, List, cast\n+from typing import IO, Any, BinaryIO, Dict, List, cast\n \n from scrapy.utils.misc import load_object\n \n@@ -126,7 +126,7 @@ class PostProcessingManager(IOBase):\n     \"\"\"\n \n     def __init__(\n-        self, plugins: List[Any], file: BinaryIO, feed_options: Dict[str, Any]\n+        self, plugins: List[Any], file: IO[bytes], feed_options: Dict[str, Any]\n     ) -> None:\n         self.plugins = self._load_plugins(plugins)\n         self.file = file\n\n@@ -3,7 +3,6 @@ import logging\n import re\n from io import StringIO\n from typing import (\n-    TYPE_CHECKING,\n     Any,\n     Callable,\n     Dict,\n@@ -25,9 +24,6 @@ from scrapy.http import Response, TextResponse\n from scrapy.selector import Selector\n from scrapy.utils.python import re_rsearch, to_unicode\n \n-if TYPE_CHECKING:\n-    from lxml._types import SupportsReadClose  # nosec\n-\n logger = logging.getLogger(__name__)\n \n \n@@ -98,7 +94,7 @@ def xmliter_lxml(\n     reader = _StreamReader(obj)\n     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n     iterable = etree.iterparse(\n-        cast(\"SupportsReadClose[bytes]\", reader),\n+        reader,\n         encoding=reader.encoding,\n         events=(\"end\", \"start-ns\"),\n         resolve_entities=False,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
