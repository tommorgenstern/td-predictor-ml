{"custom_id": "keras#6f789cc4ba300d261dbe68c12f81074dd239c790", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 19044 | Contributors (this commit): 77 | Commits (past 90d): 13 | Contributors (cumulative): 77 | DMM Complexity: 0.0\n\nDIFF:\n@@ -225,10 +225,6 @@ class Conv(Layer):\n     else:\n       tf_padding = self.padding\n \n-    tf_op_name = self.__class__.__name__\n-    if tf_op_name == 'Conv1D':\n-      tf_op_name = 'conv1d'  # Backwards compat.\n-\n     return tf.nn.convolution(\n         inputs,\n         kernel,\n@@ -236,7 +232,7 @@ class Conv(Layer):\n         padding=tf_padding,\n         dilations=list(self.dilation_rate),\n         data_format=self._tf_data_format,\n-        name=tf_op_name)\n+        name=self.__class__.__name__)\n \n   def call(self, inputs):\n     input_shape = inputs.shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#20c8480a3fdbfc6f6c090f4f37aeed75f82c0496", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 124 | Lines Deleted: 111 | Files Changed: 8 | Hunks: 42 | Methods Changed: 23 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 235 | Churn Cumulative: 5105 | Contributors (this commit): 9 | Commits (past 90d): 25 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -76,7 +76,8 @@ def save_model_to_hdf5(model, filepath, overwrite=True, include_optimizer=True):\n   \"\"\"\n \n   if h5py is None:\n-    raise ImportError('`save_model` requires h5py.')\n+    raise ImportError('`save_model()` using h5 format requires h5py. Could not '\n+                      'import h5py.')\n \n   # TODO(psv) Add warning when we save models that contain non-serializable\n   # entities like metrics added using `add_metric` and losses added using\n@@ -156,7 +157,8 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):  # pylint\n       ValueError: In case of an invalid savefile.\n   \"\"\"\n   if h5py is None:\n-    raise ImportError('`load_model` requires h5py.')\n+    raise ImportError('`load_model()` using h5 format requires h5py. Could not '\n+                      'import h5py.')\n \n   if not custom_objects:\n     custom_objects = {}\n@@ -172,7 +174,7 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):  # pylint\n     # instantiate model\n     model_config = f.attrs.get('model_config')\n     if model_config is None:\n-      raise ValueError('No model found in config file.')\n+      raise ValueError(f'No model config found in the file at {filepath}.')\n     if hasattr(model_config, 'decode'):\n       model_config = model_config.decode('utf-8')\n     model_config = json_utils.decode(model_config)\n@@ -714,10 +716,10 @@ def load_weights_from_hdf5_group(f, model):\n       filtered_layer_names.append(name)\n   layer_names = filtered_layer_names\n   if len(layer_names) != len(filtered_layers):\n-    raise ValueError('You are trying to load a weight file '\n-                     'containing ' + str(len(layer_names)) +\n-                     ' layers into a model with ' + str(len(filtered_layers)) +\n-                     ' layers.')\n+    raise ValueError(\n+        f'Layer count mismatch when loading weights from file. '\n+        f'Model expected {len(filtered_layers)} layers, found '\n+        f'{len(layer_names)} saved layers.')\n \n   # We batch weight value assignments in a single backend call\n   # which provides a speedup in TensorFlow.\n@@ -731,13 +733,11 @@ def load_weights_from_hdf5_group(f, model):\n                                                    original_keras_version,\n                                                    original_backend)\n     if len(weight_values) != len(symbolic_weights):\n-      raise ValueError('Layer #' + str(k) + ' (named \"' + layer.name +\n-                       '\" in the current model) was found to '\n-                       'correspond to layer ' + name + ' in the save file. '\n-                       'However the new layer ' + layer.name + ' expects ' +\n-                       str(len(symbolic_weights)) +\n-                       ' weights, but the saved weights have ' +\n-                       str(len(weight_values)) + ' elements.')\n+      raise ValueError(\n+          f'Weight count mismatch for layer #{k} (named {layer.name} in the '\n+          f'current model, {name} in the save file). '\n+          f'Layer expects {len(symbolic_weights)} weight(s). Received '\n+          f'{len(weight_values)} saved weight(s)')\n     weight_value_tuples += zip(symbolic_weights, weight_values)\n \n   if 'top_level_model_weights' in f:\n@@ -746,10 +746,10 @@ def load_weights_from_hdf5_group(f, model):\n         f['top_level_model_weights'])\n     if len(weight_values) != len(symbolic_weights):\n       raise ValueError(\n-          'The number of top-level weights of the model and the number '\n-          'of top-level weights in the saved file do not match. Expected '\n-          f'{len(symbolic_weights)} top-level weights, but found '\n-          f'{len(symbolic_weights)} top-level weights in file.')\n+          f'Weight count mismatch for top-level weights when loading weights '\n+          f'from file. '\n+          f'Model expects {len(symbolic_weights)} top-level weight(s). '\n+          f'Received {len(weight_values)} saved top-level weight(s)')\n     weight_value_tuples += zip(symbolic_weights, weight_values)\n   backend.batch_set_value(weight_value_tuples)\n \n@@ -804,32 +804,34 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n           layer, weight_values, original_keras_version, original_backend)\n       if len(weight_values) != len(symbolic_weights):\n         if skip_mismatch:\n-          logging.warning('Skipping loading of weights for '\n-                          'layer {}'.format(layer.name) + ' due to mismatch '\n-                          'in number of weights ({} vs {}).'.format(\n-                              len(symbolic_weights), len(weight_values)))\n+          logging.warning(\n+              f'Skipping loading of weights for layer #{k} (named '\n+              f'{layer.name}) due to mismatch in number of weights. '\n+              f'Layer expects {len(symbolic_weights)} weight(s). Received '\n+              f'{len(weight_values)} saved weight(s)')\n           continue\n-        raise ValueError('Layer #' + str(k) + ' (named \"' + layer.name +\n-                         '\") expects ' + str(len(symbolic_weights)) +\n-                         ' weight(s), but the saved weights' + ' have ' +\n-                         str(len(weight_values)) + ' element(s).')\n+        raise ValueError(\n+            f'Weight count mismatch for layer #{k} (named {layer.name}). '\n+            f'Layer expects {len(symbolic_weights)} weight(s). Received '\n+            f'{len(weight_values)} saved weight(s)')\n       # Set values.\n       for i in range(len(weight_values)):\n-        if backend.int_shape(symbolic_weights[i]) != weight_values[i].shape:\n+        expected_shape = backend.int_shape(symbolic_weights[i])\n+        received_shape = weight_values[i].shape\n+        if expected_shape != received_shape:\n           if skip_mismatch:\n-            logging.warning('Skipping loading of weights for '\n-                            'layer {}'.format(layer.name) + ' due to '\n-                            'mismatch in shape ({} vs {}).'.format(\n-                                symbolic_weights[i].shape,\n-                                weight_values[i].shape))\n+            logging.warning(\n+                f'Skipping loading weights for layer #{k} (named '\n+                f'{layer.name}) due to mismatch in shape for weight '\n+                f'{symbolic_weights[i].name}. '\n+                f'Weight expects shape {expected_shape}. Received saved weight '\n+                f'with shape {received_shape}')\n             continue\n-          raise ValueError('Layer #' + str(k) +' (named \"' + layer.name +\n-                           '\"), weight ' + str(symbolic_weights[i]) +\n-                           ' has shape {}'.format(backend.int_shape(\n-                               symbolic_weights[i])) +\n-                           ', but the saved weight has shape ' +\n-                           str(weight_values[i].shape) + '.')\n-\n+          raise ValueError(\n+              f'Shape mismatch in layer #{k} (named {layer.name}) for weight '\n+              f'{symbolic_weights[i].name}. '\n+              f'Weight expects shape {expected_shape}. Received saved weight '\n+              f'with shape {received_shape}')\n         else:\n           weight_value_tuples.append((symbolic_weights[i], weight_values[i]))\n \n@@ -840,29 +842,33 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n \n     if len(weight_values) != len(symbolic_weights):\n       if skip_mismatch:\n-        logging.warning('Skipping loading of top-level weights for '\n-                        f'model {model.name} due to mismatch '\n-                        f'in number of weights ({len(symbolic_weights)} '\n-                        f'vs {len(weight_values)}).')\n+        logging.warning(\n+            f'Skipping loading top-level weights for model due to mismatch '\n+            f'in number of weights. '\n+            f'Model expects {len(symbolic_weights)} top-level weight(s). '\n+            f'Received {len(weight_values)} saved top-level weight(s)')\n       else:\n-        raise ValueError(f'Expected {len(symbolic_weights)} top-level weights, '\n-                         f'but the saved weights have {len(weight_values)} '\n-                         'element(s).')\n+        raise ValueError(\n+            f'Weight count mismatch for top-level weights of model. '\n+            f'Model expects {len(symbolic_weights)} top-level weight(s). '\n+            f'Received {len(weight_values)} saved top-level weight(s)')\n     else:\n       for i in range(len(weight_values)):\n-        if backend.int_shape(symbolic_weights[i]) != weight_values[i].shape:\n-          ref_shape = backend.int_shape(symbolic_weights[i])\n-          actual_shape = weight_values[i].shape\n+        expected_shape = backend.int_shape(symbolic_weights[i])\n+        received_shape = weight_values[i].shape\n+        if expected_shape != received_shape:\n           if skip_mismatch:\n-            logging.warning('Skipping loading of top-level weight '\n-                            f'{symbolic_weights[i].name} due to shape mismatch.'\n-                            f' Expected shape: {ref_shape}, found in '\n-                            f'file: {actual_shape}')\n+            logging.warning(\n+                f'Skipping loading top-level weight for model due to '\n+                f'mismatch in shape for weight {symbolic_weights[i].name}. '\n+                f'Weight expects shape {expected_shape}. Received saved weight '\n+                f'with shape {received_shape}')\n           else:\n-            raise ValueError('Expected top-level weight '\n-                             f'{symbolic_weights[i].name} to '\n-                             f'have shape {ref_shape}, but weight found in '\n-                             f'file had shape {actual_shape}')\n+            raise ValueError(\n+                f'Shape mismatch in model for top-level weight '\n+                f'{symbolic_weights[i].name}. '\n+                f'Weight expects shape {expected_shape}. Received saved weight '\n+                f'with shape {received_shape}')\n         else:\n           weight_value_tuples.append((symbolic_weights[i], weight_values[i]))\n \n@@ -890,9 +896,9 @@ def save_attributes_to_hdf5_group(group, name, data):\n \n   # Expecting this to never be true.\n   if bad_attributes:\n-    raise RuntimeError('The following attributes cannot be saved to HDF5 '\n-                       'file because they are larger than %d bytes: %s' %\n-                       (HDF5_OBJECT_HEADER_LIMIT, ', '.join(bad_attributes)))\n+    raise RuntimeError(\n+        'The following attributes cannot be saved to HDF5 file because they '\n+        f'are larger than {HDF5_OBJECT_HEADER_LIMIT} bytes: {bad_attributes}')\n \n   data_npy = np.asarray(data)\n \n@@ -963,7 +969,7 @@ def _legacy_weights(layer):\n   weights = layer.trainable_weights + layer.non_trainable_weights\n   if any(not isinstance(w, tf.Variable) for w in weights):\n     raise NotImplementedError(\n-        'Save or restore weights that is not an instance of `tf.Variable` is '\n-        'not supported in h5, use `save_format=\\'tf\\'` instead. Got a model '\n-        'or layer {} with weights {}'.format(layer.__class__.__name__, weights))\n+        f'Save or restore weights that is not an instance of `tf.Variable` is '\n+        f'not supported in h5, use `save_format=\\'tf\\'` instead. Received a '\n+        f'model or layer {layer.__class__.__name__} with weights {weights}')\n   return weights\n\n@@ -46,7 +46,7 @@ def model_from_config(config, custom_objects=None):\n   \"\"\"\n   if isinstance(config, list):\n     raise TypeError('`model_from_config` expects a dictionary, not a list. '\n-                    'Maybe you meant to use '\n+                    f'Received: config={config}. Did you meant to use '\n                     '`Sequential.from_config(config)`?')\n   from keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n   return deserialize(config, custom_objects=custom_objects)\n\n@@ -209,7 +209,7 @@ def load_model(filepath, custom_objects=None, compile=True, options=None):  # py\n \n   raise IOError(\n       'Unable to load model. Filepath is not an hdf5 file (or h5py is not '\n-      'available) or SavedModel.')\n+      f'available) or SavedModel. Received: filepath={filepath}')\n \n # Inject the load_model function to keras_deps to remove the dependency\n # from TFLite to Keras.\n\n@@ -294,10 +294,10 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n       model.compile(loss=keras.losses.MSE,\n                     optimizer='rmsprop',\n                     metrics=[keras.metrics.categorical_accuracy])\n-      with self.assertRaisesRegex(\n-          ValueError, r'Layer #0 \\(named \\\"d1\\\"\\) expects 1 '\n-          r'weight\\(s\\), but the saved weights have 2 '\n-          r'element\\(s\\)\\.'):\n+      with self.assertRaises(\n+          ValueError,\n+          msg='Weight count mismatch for layer #0 (named d1). '\n+          'Layer expects 1 weight(s). Received 2 saved weight(s)'):\n         hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model)\n \n       hdf5_format.load_weights_from_hdf5_group_by_name(\n@@ -335,12 +335,11 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n       model.compile(loss=keras.losses.MSE,\n                     optimizer=optimizer_v1.RMSprop(lr=0.0001),\n                     metrics=[keras.metrics.categorical_accuracy])\n-      with self.assertRaisesRegex(\n-          ValueError, r'Layer #0 \\(named \"d1\"\\), weight '\n-          r'<tf\\.Variable \\'d1_1\\/kernel:0\\' '\n-          r'shape=\\(3, 10\\) dtype=float32> has '\n-          r'shape \\(3, 10\\), but the saved weight has '\n-          r'shape \\(3, 5\\)\\.'):\n+      with self.assertRaises(\n+          ValueError,\n+          msg='Shape mismatch in layer #0 (named d1) for weight d1_1/kernel:0. '\n+          'Weight expects shape (3, 10). '\n+          'Received saved weight with shape (3, 5)'):\n         hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model)\n \n       hdf5_format.load_weights_from_hdf5_group_by_name(\n\n@@ -132,11 +132,12 @@ def get_json_type(obj):\n       return {'class_name': 'TypeSpec', 'type_spec': type_spec_name,\n               'serialized': obj._serialize()}  # pylint: disable=protected-access\n     except ValueError:\n-      raise ValueError('Unable to serialize {} to JSON, because the TypeSpec '\n-                       'class {} has not been registered.'\n-                       .format(obj, type(obj)))\n+      raise ValueError(\n+          f'Unable to serialize {obj} to JSON, because the TypeSpec '\n+          f'class {type(obj)} has not been registered.')\n \n   if isinstance(obj, enum.Enum):\n     return obj.value\n \n-  raise TypeError('Not JSON Serializable:', obj)\n+  raise TypeError(\n+      f'Unable to serialize {obj} to JSON. Unrecognized type {type(obj)}.')\n\n@@ -115,8 +115,9 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n         file_content = f.read()\n       metadata.ParseFromString(file_content)\n     except message.DecodeError as e:\n-      raise IOError('Cannot parse keras metadata {}: {}.'\n-                    .format(path_to_metadata_pb, str(e)))\n+      raise IOError(\n+          f'Cannot parse keras metadata at path {path_to_metadata_pb}: '\n+          f'Received error: {e}')\n   else:\n     logging.warning('SavedModel saved prior to TF 2.5 detected when loading '\n                     'Keras model. Please ensure that you are saving the model '\n@@ -404,12 +405,12 @@ class KerasObjectLoader:\n         self.loaded_nodes[node_metadata.node_id] = self._load_layer(\n             node_metadata.node_id, node_metadata.identifier,\n             node_metadata.metadata)\n-      except ValueError:\n+      except ValueError as e:\n         # Metrics are only needed when the model is compiled later. We ignore\n         # errors when trying to load custom metrics when `compile=False` until\n         # custom metrics are serialized properly (b/135550038).\n         if compile:\n-          raise\n+          raise e\n         logging.warning('Unable to restore custom metric. Please ensure that '\n                         'the layer implements `get_config` and `from_config` '\n                         'when saving. In addition, please use the '\n@@ -525,14 +526,13 @@ class KerasObjectLoader:\n     except ValueError:\n       if must_restore_from_config:\n         raise RuntimeError(\n-            'Unable to restore a layer of class {cls}. Layers of '\n-            'class {cls} require that the class be provided to '\n+            f'Unable to restore a layer of class {class_name}. Layers of '\n+            f'class {class_name} require that the class be provided to '\n             'the model loading code, either by registering the '\n-            'class using @keras.utils.register_keras_serializable '\n+            'class using `@keras.utils.register_keras_serializable` '\n             'on the class def and including that file in your '\n             'program, or by passing the class in a '\n-            'keras.utils.CustomObjectScope that wraps this load '\n-            'call.'.format(cls=class_name))\n+            '`keras.utils.CustomObjectScope` that wraps this load call.')\n       else:\n         return None\n \n@@ -674,9 +674,9 @@ class KerasObjectLoader:\n       uninitialized_model_names = [\n           self.model_layer_dependencies[model_id][0].name\n           for model_id in uninitialized_model_ids]\n-      raise ValueError('Error when loading from SavedModel -- the following '\n-                       'models could not be initialized: {}'\n-                       .format(uninitialized_model_names))\n+      raise ValueError(f'Error loading model(s) in the SavedModel format. '\n+                       f'The following model(s) could not be initialized: '\n+                       f'{uninitialized_model_names}')\n \n   def _reconstruct_model(self, model_id, model, layers):\n     \"\"\"Reconstructs the network structure.\"\"\"\n@@ -886,8 +886,8 @@ def _unable_to_call_layer_due_to_serialization_issue(\n   \"\"\"\n \n   raise ValueError(\n-      'Cannot call custom layer {} of type {}, because the call function was '\n-      'not serialized to the SavedModel.'\n+      f'Cannot call custom layer {layer.name} of type {type(layer)}, because '\n+      'the call function was not serialized to the SavedModel.'\n       'Please try one of the following methods to fix this issue:'\n       '\\n\\n(1) Implement `get_config` and `from_config` in the layer/model '\n       'class, and pass the object to the `custom_objects` argument when '\n@@ -897,7 +897,7 @@ def _unable_to_call_layer_due_to_serialization_issue(\n       'and not `__call__`. The input shape and dtype will be automatically '\n       'recorded when the object is called, and used when saving. To manually '\n       'specify the input shape/dtype, decorate the call function with '\n-      '`@tf.function(input_signature=...)`.'.format(layer.name, type(layer)))\n+      '`@tf.function(input_signature=...)`.')\n \n \n def _finalize_config_layers(layers):\n@@ -990,11 +990,11 @@ def revive_custom_object(identifier, metadata):\n         tf.compat.as_str(metadata['class_name']), parent_classes, {})\n     return revived_cls._init_from_metadata(metadata)  # pylint: disable=protected-access\n   else:\n-    raise ValueError('Unable to restore custom object of type {} currently. '\n-                     'Please make sure that the layer implements `get_config`'\n-                     'and `from_config` when saving. In addition, please use '\n-                     'the `custom_objects` arg when calling `load_model()`.'\n-                     .format(identifier))\n+    raise ValueError(\n+        f'Unable to restore custom object of type {identifier}. '\n+        f'Please make sure that any custom layers are included in the '\n+        f'`custom_objects` arg when calling `load_model()` and make sure that '\n+        f'all layers implement `get_config` and `from_config`.')\n \n \n def _restore_layer_metrics(layer):\n@@ -1118,11 +1118,13 @@ def recursively_deserialize_keras_object(config, module_objects=None):\n       return {key: recursively_deserialize_keras_object(config[key],\n                                                         module_objects)\n               for key in config}\n-  if isinstance(config, (tuple, list)):\n+  elif isinstance(config, (tuple, list)):\n     return [recursively_deserialize_keras_object(x, module_objects)\n             for x in config]\n   else:\n-    raise ValueError('Unable to decode config: {}'.format(config))\n+    raise ValueError(\n+        f'Unable to decode Keras layer config. Config should be a dictionary, '\n+        f'tuple or list. Received: config={config}')\n \n \n def get_common_shape(x, y):\n@@ -1134,9 +1136,11 @@ def get_common_shape(x, y):\n   if x is None:\n     return None  # The associated input was not a Tensor, no shape generated.\n   if not isinstance(x, tf.TensorShape):\n-    raise TypeError('Expected x to be a TensorShape but saw %s' % (x,))\n+    raise TypeError(\n+        f'Expected x to be a TensorShape but received {x} with type {type(x)}')\n   if not isinstance(y, tf.TensorShape):\n-    raise TypeError('Expected y to be a TensorShape but saw %s' % (y,))\n+    raise TypeError(\n+        f'Expected y to be a TensorShape but received {y} with type {type(y)}')\n   if x.rank != y.rank or x.rank is None:\n     return tf.TensorShape(None)\n   dims = []\n\n@@ -244,7 +244,8 @@ def _export_mode(\n   compile_clone = (mode != mode_keys.ModeKeys.PREDICT)\n   if compile_clone and not model.optimizer:\n     raise ValueError(\n-        'Model does not have an optimizer. Cannot export mode %s' % mode)\n+        f'Model {model.name} does not have an optimizer. '\n+        f'Cannot export mode {mode}.')\n \n   model_graph = tf.compat.v1.get_default_graph()\n   with tf.Graph().as_default() as g, backend.learning_phase_scope(\n\n@@ -82,10 +82,10 @@ def model_call_inputs(model, keep_original_batch_size=False):\n \n def raise_model_input_error(model):\n   raise ValueError(\n-      'Model {} cannot be saved because the input shapes have not been '\n-      'set. Usually, input shapes are automatically determined from calling'\n-      ' `.fit()` or `.predict()`. To manually set the shapes, call '\n-      '`model.build(input_shape)`.'.format(model))\n+      f'Model {model} cannot be saved because the input shapes have not '\n+      f'been set. Usually, input shapes are automatically determined when '\n+      f'calling `.fit()` or `.predict()`. To manually set the shapes, call '\n+      f'`model.build(input_shape)')\n \n \n def trace_model_call(model, input_signature=None):\n@@ -169,8 +169,8 @@ def model_metadata(model, include_optimizer=True, require_config=True):\n       metadata['training_config'] = _serialize_nested_config(training_config)\n       if isinstance(model.optimizer, optimizer_v2.RestoredOptimizer):\n         raise NotImplementedError(\n-            'As of now, Optimizers loaded from SavedModel cannot be saved. '\n-            'If you\\'re calling `model.save` or `tf.keras.models.save_model`,'\n+            'Optimizers loaded from a SavedModel cannot be saved. '\n+            'If you are calling `model.save` or `tf.keras.models.save_model`, '\n             'please set the `include_optimizer` option to `False`. For '\n             '`tf.saved_model.save`, delete the optimizer from the model.')\n       else:\n@@ -255,7 +255,9 @@ def _deserialize_nested_config(deserialize_fn, config):\n   elif isinstance(config, (tuple, list)):\n     return [_deserialize_nested_config(deserialize_fn, obj) for obj in config]\n \n-  raise ValueError('Saved configuration not understood.')\n+  raise ValueError(\n+      'Saved configuration not understood. Configuration should be a '\n+      f'dictionary, string, tuple or list. Received: config={config}.')\n \n \n def _serialize_nested_config(config):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#366349617152fb3dbd9e10cbca93c927ab91bbca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 476 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -22,14 +22,10 @@ from keras.engine import base_layer\n from keras.engine import input_spec\n from keras.engine import training\n from keras.layers import core\n-from tensorflow.python.util import deprecation  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\n-    'keras.experimental.LinearModel',\n-    v1=['keras.experimental.LinearModel', 'keras.models.LinearModel'])\n-@deprecation.deprecated_endpoints('keras.experimental.LinearModel')\n+@keras_export('keras.experimental.LinearModel')\n class LinearModel(training.Model):\n   r\"\"\"Linear Model for regression and classification problems.\n \n\n@@ -22,14 +22,10 @@ from keras.engine import base_layer\n from keras.engine import data_adapter\n from keras.engine import training as keras_training\n from keras.utils import generic_utils\n-from tensorflow.python.util import deprecation  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\n-    'keras.experimental.WideDeepModel',\n-    v1=['keras.experimental.WideDeepModel', 'keras.models.WideDeepModel'])\n-@deprecation.deprecated_endpoints('keras.experimental.WideDeepModel')\n+@keras_export('keras.experimental.WideDeepModel')\n class WideDeepModel(keras_training.Model):\n   r\"\"\"Wide & Deep Model for regression and classification problems.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d0708f93aa6458345d31647a76b53ff4abf267f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 947 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -116,11 +116,13 @@ def VGG19(\n     raise ValueError('The `weights` argument should be either '\n                      '`None` (random initialization), `imagenet` '\n                      '(pre-training on ImageNet), '\n-                     'or the path to the weights file to be loaded.')\n+                     'or the path to the weights file to be loaded.  '\n+                     f'Received: `weights={weights}.`')\n \n   if weights == 'imagenet' and include_top and classes != 1000:\n     raise ValueError('If using `weights` as `\"imagenet\"` with `include_top` '\n-                     ' as true, `classes` should be 1000')\n+                     'as true, `classes` should be 1000.  '\n+                     f'Received: `classes={classes}.`')\n   # Determine proper input shape\n   input_shape = imagenet_utils.obtain_input_shape(\n       input_shape,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4aeb3755594d0c0796c23eb60d9536760c4ce24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 43 | Files Changed: 2 | Hunks: 28 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 101 | Churn Cumulative: 20318 | Contributors (this commit): 78 | Commits (past 90d): 15 | Contributors (cumulative): 81 | DMM Complexity: 1.0\n\nDIFF:\n@@ -133,7 +133,7 @@ class Conv(Layer):\n       filters = int(filters)\n     if filters is not None and filters < 0:\n       raise ValueError(f'Received a negative value for `filters`.'\n-                       f'Was expecting a positive value, got {filters}.')\n+                       f'Was expecting a positive value. Received {filters}.')\n     self.filters = filters\n     self.groups = groups or 1\n     self.kernel_size = conv_utils.normalize_tuple(\n@@ -357,8 +357,10 @@ class Conv(Layer):\n   def _get_input_channel(self, input_shape):\n     channel_axis = self._get_channel_axis()\n     if input_shape.dims[channel_axis].value is None:\n-      raise ValueError('The channel dimension of the inputs '\n-                       'should be defined. Found `None`.')\n+      raise ValueError('The channel dimension of the inputs should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n     return int(input_shape[channel_axis])\n \n   def _get_padding_op(self):\n@@ -954,19 +956,22 @@ class Conv1DTranspose(Conv1D):\n           self.output_padding, 1, 'output_padding')\n       for stride, out_pad in zip(self.strides, self.output_padding):\n         if out_pad >= stride:\n-          raise ValueError('Stride ' + str(self.strides) + ' must be '\n-                           'greater than output padding ' +\n-                           str(self.output_padding))\n+          raise ValueError('Strides must be greater than output padding. '\n+                           f'Received strides={self.strides}, '\n+                           f'output_padding={self.output_padding}.')\n \n   def build(self, input_shape):\n     input_shape = tf.TensorShape(input_shape)\n     if len(input_shape) != 3:\n-      raise ValueError('Inputs should have rank 3. Received input shape: ' +\n-                       str(input_shape))\n+      raise ValueError('Inputs should have rank 3. '\n+                       f'Received input_shape={input_shape}.')\n     channel_axis = self._get_channel_axis()\n     if input_shape.dims[channel_axis].value is None:\n       raise ValueError('The channel dimension of the inputs '\n-                       'should be defined. Found `None`.')\n+                       'to `Conv1DTranspose` should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n     input_dim = int(input_shape[channel_axis])\n     self.input_spec = InputSpec(ndim=3, axes={channel_axis: input_dim})\n     kernel_shape = self.kernel_size + (self.filters, input_dim)\n@@ -1225,19 +1230,22 @@ class Conv2DTranspose(Conv2D):\n           self.output_padding, 2, 'output_padding')\n       for stride, out_pad in zip(self.strides, self.output_padding):\n         if out_pad >= stride:\n-          raise ValueError('Stride ' + str(self.strides) + ' must be '\n-                           'greater than output padding ' +\n-                           str(self.output_padding))\n+          raise ValueError('Strides must be greater than output padding. '\n+                           f'Received strides={self.strides}, '\n+                           f'output_padding={self.output_padding}.')\n \n   def build(self, input_shape):\n     input_shape = tf.TensorShape(input_shape)\n     if len(input_shape) != 4:\n-      raise ValueError('Inputs should have rank 4. Received input '\n-                       'shape: ' + str(input_shape))\n+      raise ValueError('Inputs should have rank 4. '\n+                       f'Received input_shape={input_shape}.')\n     channel_axis = self._get_channel_axis()\n     if input_shape.dims[channel_axis].value is None:\n       raise ValueError('The channel dimension of the inputs '\n-                       'should be defined. Found `None`.')\n+                       'to `Conv2DTranspose` should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n     input_dim = int(input_shape[channel_axis])\n     self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n     kernel_shape = self.kernel_size + (self.filters, input_dim)\n@@ -1535,19 +1543,22 @@ class Conv3DTranspose(Conv3D):\n           self.output_padding, 3, 'output_padding')\n       for stride, out_pad in zip(self.strides, self.output_padding):\n         if out_pad >= stride:\n-          raise ValueError('Stride ' + str(self.strides) + ' must be '\n-                           'greater than output padding ' +\n-                           str(self.output_padding))\n+          raise ValueError('Strides must be greater than output padding. '\n+                           f'Received strides={self.strides}, '\n+                           f'output_padding={self.output_padding}.')\n \n   def build(self, input_shape):\n     input_shape = tf.TensorShape(input_shape)\n     if len(input_shape) != 5:\n-      raise ValueError('Inputs should have rank 5, received input shape:',\n-                       str(input_shape))\n+      raise ValueError('Inputs should have rank 5. '\n+                       f'Received input_shape={input_shape}.')\n     channel_axis = self._get_channel_axis()\n     if input_shape.dims[channel_axis].value is None:\n       raise ValueError('The channel dimension of the inputs '\n-                       'should be defined, found None: ' + str(input_shape))\n+                       'to `Conv3DTranspose` should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n     input_dim = int(input_shape[channel_axis])\n     kernel_shape = self.kernel_size + (self.filters, input_dim)\n     self.input_spec = InputSpec(ndim=5, axes={channel_axis: input_dim})\n@@ -1810,8 +1821,10 @@ class SeparableConv(Conv):\n     input_shape = tf.TensorShape(input_shape)\n     channel_axis = self._get_channel_axis()\n     if input_shape.dims[channel_axis].value is None:\n-      raise ValueError('The channel dimension of the inputs '\n-                       'should be defined. Found `None`.')\n+      raise ValueError('The channel dimension of the inputs should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n     input_dim = int(input_shape[channel_axis])\n     self.input_spec = InputSpec(ndim=self.rank + 2,\n                                 axes={channel_axis: input_dim})\n@@ -2383,15 +2396,17 @@ class DepthwiseConv(Conv):\n \n   def build(self, input_shape):\n     if len(input_shape) != self.rank + 2:\n-      raise ValueError('Inputs to `DepthwiseConv` should have rank ',\n-                       str(self.rank + 2), '. ', 'Received input shape:',\n-                       str(input_shape))\n+      raise ValueError('Inputs to `DepthwiseConv` should have '\n+                       f'rank {self.rank + 2}. '\n+                       f'Received input_shape={input_shape}.')\n     input_shape = tf.TensorShape(input_shape)\n     channel_axis = self._get_channel_axis()\n     if input_shape.dims[channel_axis].value is None:\n-      raise ValueError('The channel dimension of the inputs to '\n-                       '`DepthwiseConv` '\n-                       'should be defined. Found `None`.')\n+      raise ValueError('The channel dimension of the inputs to `DepthwiseConv` '\n+                       'should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n     input_dim = int(input_shape[channel_axis])\n     depthwise_kernel_shape = self.kernel_size + (input_dim,\n                                                  self.depth_multiplier)\n@@ -2910,7 +2925,7 @@ class UpSampling2D(Layer):\n     self.size = conv_utils.normalize_tuple(size, 2, 'size')\n     if interpolation not in {'nearest', 'bilinear'}:\n       raise ValueError('`interpolation` argument should be one of `\"nearest\"` '\n-                       'or `\"bilinear\"`.')\n+                       f'or `\"bilinear\"`. Received: \"{interpolation}\".')\n     self.interpolation = interpolation\n     self.input_spec = InputSpec(ndim=4)\n \n@@ -3168,7 +3183,7 @@ class ZeroPadding2D(Layer):\n     elif hasattr(padding, '__len__'):\n       if len(padding) != 2:\n         raise ValueError('`padding` should have two elements. '\n-                         'Found: ' + str(padding))\n+                         f'Received: {padding}.')\n       height_padding = conv_utils.normalize_tuple(padding[0], 2,\n                                                   '1st entry of padding')\n       width_padding = conv_utils.normalize_tuple(padding[1], 2,\n@@ -3180,7 +3195,7 @@ class ZeroPadding2D(Layer):\n                        '(symmetric_height_pad, symmetric_width_pad), '\n                        'or a tuple of 2 tuples of 2 ints '\n                        '((top_pad, bottom_pad), (left_pad, right_pad)). '\n-                       'Found: ' + str(padding))\n+                       f'Received: {padding}.')\n     self.input_spec = InputSpec(ndim=4)\n \n   def compute_output_shape(self, input_shape):\n@@ -3281,7 +3296,7 @@ class ZeroPadding3D(Layer):\n     elif hasattr(padding, '__len__'):\n       if len(padding) != 3:\n         raise ValueError('`padding` should have 3 elements. '\n-                         'Found: ' + str(padding))\n+                         f'Received: {padding}.')\n       dim1_padding = conv_utils.normalize_tuple(padding[0], 2,\n                                                 '1st entry of padding')\n       dim2_padding = conv_utils.normalize_tuple(padding[1], 2,\n@@ -3298,7 +3313,7 @@ class ZeroPadding3D(Layer):\n           '((left_dim1_pad, right_dim1_pad),'\n           ' (left_dim2_pad, right_dim2_pad),'\n           ' (left_dim3_pad, right_dim2_pad)). '\n-          'Found: ' + str(padding))\n+          f'Received: {padding}.')\n     self.input_spec = InputSpec(ndim=5)\n \n   def compute_output_shape(self, input_shape):\n@@ -3464,7 +3479,7 @@ class Cropping2D(Layer):\n     elif hasattr(cropping, '__len__'):\n       if len(cropping) != 2:\n         raise ValueError('`cropping` should have two elements. '\n-                         'Found: ' + str(cropping))\n+                         f'Received: {cropping}.')\n       height_cropping = conv_utils.normalize_tuple(cropping[0], 2,\n                                                    '1st entry of cropping')\n       width_cropping = conv_utils.normalize_tuple(cropping[1], 2,\n@@ -3476,7 +3491,7 @@ class Cropping2D(Layer):\n                        '(symmetric_height_crop, symmetric_width_crop), '\n                        'or a tuple of 2 tuples of 2 ints '\n                        '((top_crop, bottom_crop), (left_crop, right_crop)). '\n-                       'Found: ' + str(cropping))\n+                       f'Received: {cropping}.')\n     self.input_spec = InputSpec(ndim=4)\n \n   def compute_output_shape(self, input_shape):\n@@ -3596,7 +3611,7 @@ class Cropping3D(Layer):\n     elif hasattr(cropping, '__len__'):\n       if len(cropping) != 3:\n         raise ValueError('`cropping` should have 3 elements. '\n-                         'Found: ' + str(cropping))\n+                         f'Received: {cropping}.')\n       dim1_cropping = conv_utils.normalize_tuple(cropping[0], 2,\n                                                  '1st entry of cropping')\n       dim2_cropping = conv_utils.normalize_tuple(cropping[1], 2,\n@@ -3613,7 +3628,7 @@ class Cropping3D(Layer):\n           '((left_dim1_crop, right_dim1_crop),'\n           ' (left_dim2_crop, right_dim2_crop),'\n           ' (left_dim3_crop, right_dim2_crop)). '\n-          'Found: ' + str(cropping))\n+          f'Received: {cropping}.')\n     self.input_spec = InputSpec(ndim=5)\n \n   def compute_output_shape(self, input_shape):\n\n@@ -96,14 +96,14 @@ class ConvTest(tf.test.TestCase):\n       layer = conv_layers.Conv2D(32, [3, 3], activation=tf.nn.relu)\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n-          'should be defined. Found `None`.'):\n+          'should be defined. The input_shape received is'):\n         _ = layer.apply(images)\n \n       images = tf.compat.v1.placeholder(tf.float32, (5, None, 7, 9))\n       layer = conv_layers.Conv2D(32, [3, 3], data_format='channels_first')\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n-          'should be defined. Found `None`.'):\n+          'should be defined. The input_shape received is'):\n         _ = layer.apply(images)\n \n   def testConv2DPaddingSame(self):\n@@ -167,14 +167,14 @@ class ConvTest(tf.test.TestCase):\n       layer = conv_layers.Conv1D(32, 3, activation=tf.nn.relu)\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n-          'should be defined. Found `None`.'):\n+          'should be defined. The input_shape received is'):\n         _ = layer.apply(data)\n \n       data = tf.compat.v1.placeholder(tf.float32, (5, None, 4))\n       layer = conv_layers.Conv1D(32, 3, data_format='channels_first')\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n-          'should be defined. Found `None`.'):\n+          'should be defined. The input_shape received is'):\n         _ = layer.apply(data)\n \n   def testCreateConv3D(self):\n@@ -195,7 +195,7 @@ class ConvTest(tf.test.TestCase):\n       layer = conv_layers.Conv3D(32, [3, 3, 3], activation=tf.nn.relu)\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n-          'should be defined. Found `None`.'):\n+          'should be defined. The input_shape received is'):\n         _ = layer.apply(volumes)\n \n   def testConv2DKernelRegularizer(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d295efc4adc6bfd12aa02b988a82cbdd2f5e94a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 488 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -22,10 +22,14 @@ from keras.engine import base_layer\n from keras.engine import input_spec\n from keras.engine import training\n from keras.layers import core\n+from tensorflow.python.util import deprecation  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export('keras.experimental.LinearModel')\n+@keras_export(\n+    'keras.experimental.LinearModel',\n+    v1=['keras.experimental.LinearModel', 'keras.models.LinearModel'])\n+@deprecation.deprecated_endpoints('keras.experimental.LinearModel')\n class LinearModel(training.Model):\n   r\"\"\"Linear Model for regression and classification problems.\n \n\n@@ -22,10 +22,14 @@ from keras.engine import base_layer\n from keras.engine import data_adapter\n from keras.engine import training as keras_training\n from keras.utils import generic_utils\n+from tensorflow.python.util import deprecation  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export('keras.experimental.WideDeepModel')\n+@keras_export(\n+    'keras.experimental.WideDeepModel',\n+    v1=['keras.experimental.WideDeepModel', 'keras.models.WideDeepModel'])\n+@deprecation.deprecated_endpoints('keras.experimental.WideDeepModel')\n class WideDeepModel(keras_training.Model):\n   r\"\"\"Wide & Deep Model for regression and classification problems.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aca519b93eb607c0bde2dfc726eae758031b543c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 19907 | Contributors (this commit): 108 | Commits (past 90d): 45 | Contributors (cumulative): 114 | DMM Complexity: 0.0\n\nDIFF:\n@@ -782,9 +782,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           y, y_pred, sample_weight, regularization_losses=self.losses)\n     if self.loss and y is None:\n       raise TypeError(\n-          'Target data is missing. Your model was compiled with a `loss` '\n-          'argument and therefore expects target data to be passed in `fit()`.'\n-      )\n+          f'Target data is missing. Your model has `loss`: {self.loss}, '\n+          'and therefore expects target data to be passed in `fit()`.')\n     # Run backwards pass.\n     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n@@ -1191,7 +1190,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n         logs = tf_utils.sync_to_numpy_or_python_type(logs)\n         if logs is None:\n-          raise ValueError('Expect x to be a non-empty array or dataset.')\n+          raise ValueError('Unexpected result of `train_function` '\n+                           '(Empty logs). Please use '\n+                           '`Model.compile(..., run_eagerly=True)`, or '\n+                           '`tf.config.run_functions_eagerly(True)` for more '\n+                           'information of where went wrong, or file a '\n+                           'issue/bug to `tf.keras`.')\n         epoch_logs = copy.copy(logs)\n \n         # Run validation.\n@@ -1448,7 +1452,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     _disallow_inside_tf_function('evaluate')\n     use_cached_eval_dataset = kwargs.pop('_use_cached_eval_dataset', False)\n     if kwargs:\n-      raise TypeError('Invalid keyword arguments: %s' % (kwargs,))\n+      raise TypeError(f'Invalid keyword arguments: {(kwargs,)}')\n \n     if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n       self._cluster_coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n@@ -1761,7 +1765,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             end_step = step + data_handler.step_increment\n             callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})\n       if batch_outputs is None:\n-        raise ValueError('Expect x to be a non-empty array or dataset.')\n+        raise ValueError('Unexpected result of `predict_function` '\n+                         '(Empty batch_outputs). Please use '\n+                         '`Model.compile(..., run_eagerly=True)`, or '\n+                         '`tf.config.run_functions_eagerly(True)` for more '\n+                         'information of where went wrong, or file a '\n+                         'issue/bug to `tf.keras`.')\n       callbacks.on_predict_end()\n     all_outputs = tf.__internal__.nest.map_structure_up_to(batch_outputs, concat, outputs)\n \n@@ -2221,18 +2230,18 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         save_format = 'h5'\n       else:\n         raise ValueError(\n-            'Unknown format \"%s\". Was expecting one of {\"tf\", \"h5\"}.' % (\n-                save_format,))\n+            f'Unknown format. Received: `save_format`={save_format}. Was '\n+            'expecting one of {\"tf\", \"h5\"}.')\n     if save_format == 'tf' and filepath_is_h5:\n       raise ValueError(\n-          ('save_weights got save_format=\"tf\"/\"tensorflow\", but the '\n-           'filepath (\"%s\") looks like an HDF5 file. Omit the \".h5\"/\".keras\" '\n-           'when saving in TensorFlow format.')\n-          % filepath)\n+          'save_weights got save_format=\"tf\"/\"tensorflow\", but the '\n+          f'filepath ({filepath}) looks like an HDF5 file. '\n+          'Omit the \".h5\"/\".keras\" when saving in TensorFlow format.')\n \n     if save_format == 'h5' and h5py is None:\n       raise ImportError(\n-          '`save_weights` requires h5py when saving in hdf5.')\n+          '`save_weights` requires h5py when saving in hdf5, but h5py is not '\n+          'available. Try installing h5py package.')\n     if save_format == 'tf':\n       check_filepath = filepath + '.index'\n     else:\n@@ -2315,8 +2324,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     if backend.is_tpu_strategy(self._distribution_strategy):\n       if (self._distribution_strategy.extended.steps_per_run > 1 and\n           (not saving_utils.is_hdf5_filepath(filepath))):\n-        raise ValueError('Load weights is not yet supported with TPUStrategy '\n-                         'with steps_per_run greater than 1.')\n+        spr = self._distribution_strategy.extended.steps_per_run\n+        raise ValueError('Load weights is not implemented with TPUStrategy '\n+                         'with `steps_per_run` greater than 1. The '\n+                         f'`steps_per_run` is {spr}')\n     if skip_mismatch and not by_name:\n       raise ValueError(\n           'When calling model.load_weights, skip_mismatch can only be set to '\n@@ -2341,7 +2352,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       status = None\n       if h5py is None:\n         raise ImportError(\n-            '`load_weights` requires h5py when loading weights from HDF5.')\n+            '`load_weights` requires h5py package when loading weights from '\n+            'HDF5. Try installing h5py.')\n       if not self._is_graph_network and not self.built:\n         raise ValueError(\n             'Unable to load weights saved in HDF5 format into a subclassed '\n\n@@ -88,7 +88,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model = sequential.Sequential([layers_module.Dense(1)])\n     model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n     with self.assertRaisesRegex(ValueError,\n-                                'Expect x to be a non-empty array or dataset.'):\n+                                'Unexpected result of `train_function`.*'):\n       model.fit(x=np.array([]), y=np.array([]))\n \n   @keras_parameterized.run_all_keras_modes\n@@ -1765,7 +1765,7 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n     model.compile(loss='mse')\n \n     with self.assertRaisesRegex(ValueError,\n-                                'Expect x to be a non-empty array or dataset.'):\n+                                'Unexpected result of `predict_function`.*'):\n       model.predict(np.array([]))\n \n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
