{"custom_id": "scrapy#cf09af787eafa6770bc5ab00bb0ee9759c75df23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 18 | Files Changed: 14 | Hunks: 14 | Methods Changed: 0 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 4482 | Contributors (this commit): 47 | Commits (past 90d): 55 | Contributors (cumulative): 138 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,3 @@\n-# -*- coding: utf-8 -*-\n-#\n # Scrapy documentation build configuration file, created by\n # sphinx-quickstart on Mon Nov 24 12:02:52 2008.\n #\n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import re\n import logging\n \n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import traceback\n import warnings\n from collections import defaultdict\n\n@@ -1,5 +1,3 @@\n-# -*- coding: utf-8 -*-\n-\n import logging\n import sys\n import warnings\n\n@@ -1,5 +1,3 @@\n-# -*- coding: utf-8 -*-\n-\n import OpenSSL\n import OpenSSL._util as pyOpenSSLutil\n \n\n@@ -1,5 +1,3 @@\n-# -*- coding: utf-8 -*-\n-\n import unittest\n \n from scrapy.downloadermiddlewares.redirect import RedirectMiddleware, MetaRefreshMiddleware\n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n from unittest import mock\n \n from twisted.internet import reactor, error\n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import unittest\n \n from w3lib.encoding import resolve_encoding\n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import os\n import shutil\n \n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import unittest\n from scrapy.responsetypes import responsetypes\n \n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import inspect\n import unittest\n from unittest import mock\n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import os\n \n from twisted.trial import unittest\n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import sys\n import logging\n import unittest\n\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n import unittest\n \n from scrapy.spiders import Spider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b852fff6f82e24c535c0dd9b4ef7fa78e7946497", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 50 | Churn Cumulative: 185 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,8 +28,9 @@ def _nons(tag):\n \n \n class LxmlParserLinkExtractor:\n-    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False,\n-                 strip=True, canonicalized=False):\n+    def __init__(\n+        self, tag=\"a\", attr=\"href\", process=None, unique=False, strip=True, canonicalized=False\n+    ):\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n         self.process_attr = process if callable(process) else lambda v: v\n@@ -93,10 +94,23 @@ class LxmlParserLinkExtractor:\n \n class LxmlLinkExtractor(FilteringLinkExtractor):\n \n-    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n-                 tags=('a', 'area'), attrs=('href',), canonicalize=False,\n-                 unique=True, process_value=None, deny_extensions=None, restrict_css=(),\n-                 strip=True, restrict_text=None):\n+    def __init__(\n+        self,\n+        allow=(),\n+        deny=(),\n+        allow_domains=(),\n+        deny_domains=(),\n+        restrict_xpaths=(),\n+        tags=('a', 'area'),\n+        attrs=('href',),\n+        canonicalize=False,\n+        unique=True,\n+        process_value=None,\n+        deny_extensions=None,\n+        restrict_css=(),\n+        strip=True,\n+        restrict_text=None,\n+    ):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         lx = LxmlParserLinkExtractor(\n             tag=lambda x: x in tags,\n@@ -106,12 +120,18 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n             strip=strip,\n             canonicalized=canonicalize\n         )\n-\n-        super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n-                                                allow_domains=allow_domains, deny_domains=deny_domains,\n-                                                restrict_xpaths=restrict_xpaths, restrict_css=restrict_css,\n-                                                canonicalize=canonicalize, deny_extensions=deny_extensions,\n-                                                restrict_text=restrict_text)\n+        super(LxmlLinkExtractor, self).__init__(\n+            link_extractor=lx,\n+            allow=allow,\n+            deny=deny,\n+            allow_domains=allow_domains,\n+            deny_domains=deny_domains,\n+            restrict_xpaths=restrict_xpaths,\n+            restrict_css=restrict_css,\n+            canonicalize=canonicalize,\n+            deny_extensions=deny_extensions,\n+            restrict_text=restrict_text,\n+        )\n \n     def extract_links(self, response):\n         \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n@@ -124,9 +144,11 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n         \"\"\"\n         base_url = get_base_url(response)\n         if self.restrict_xpaths:\n-            docs = [subdoc\n+            docs = [\n+                subdoc\n                 for x in self.restrict_xpaths\n-                    for subdoc in response.xpath(x)]\n+                for subdoc in response.xpath(x)\n+            ]\n         else:\n             docs = [response.selector]\n         all_links = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ebf2a0d82b0ae5b9e701972ba2a9e1420c6d2c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 33 | Churn Cumulative: 818 | Contributors (this commit): 11 | Commits (past 90d): 10 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,8 @@\n \"\"\"\n Link extractor based on lxml.html\n \"\"\"\n+import operator\n+from functools import partial\n from urllib.parse import urljoin\n \n import lxml.etree as etree\n@@ -8,10 +10,10 @@ from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url, safe_url_string\n \n from scrapy.link import Link\n+from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list\n from scrapy.utils.response import get_base_url\n-from scrapy.linkextractors import FilteringLinkExtractor\n \n \n # from lxml/src/lxml/html/__init__.py\n@@ -27,20 +29,24 @@ def _nons(tag):\n     return tag\n \n \n+def _identity(x):\n+    return x\n+\n+\n+def _canonicalize_link_url(link):\n+    return canonicalize_url(link.url, keep_fragments=True)\n+\n+\n class LxmlParserLinkExtractor:\n     def __init__(\n         self, tag=\"a\", attr=\"href\", process=None, unique=False, strip=True, canonicalized=False\n     ):\n-        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n-        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n-        self.process_attr = process if callable(process) else lambda v: v\n+        self.scan_tag = tag if callable(tag) else partial(operator.eq, tag)\n+        self.scan_attr = attr if callable(attr) else partial(operator.eq, attr)\n+        self.process_attr = process if callable(process) else _identity\n         self.unique = unique\n         self.strip = strip\n-        if canonicalized:\n-            self.link_key = lambda link: link.url\n-        else:\n-            self.link_key = lambda link: canonicalize_url(link.url,\n-                                                          keep_fragments=True)\n+        self.link_key = operator.attrgetter(\"url\") if canonicalized else _canonicalize_link_url\n \n     def _iter_links(self, document):\n         for el in document.iter(etree.Element):\n@@ -113,8 +119,8 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n     ):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         lx = LxmlParserLinkExtractor(\n-            tag=lambda x: x in tags,\n-            attr=lambda x: x in attrs,\n+            tag=partial(operator.contains, tags),\n+            attr=partial(operator.contains, attrs),\n             unique=unique,\n             process=process_value,\n             strip=strip,\n\n@@ -1,3 +1,4 @@\n+import pickle\n import re\n import unittest\n from warnings import catch_warnings\n@@ -462,6 +463,10 @@ class Base:\n                 Link(url='ftp://www.external.com/', text=u'An Item', fragment='', nofollow=False),\n             ])\n \n+        def test_pickle_extractor(self):\n+            lx = self.extractor_cls()\n+            self.assertIsInstance(pickle.loads(pickle.dumps(lx)), self.extractor_cls)\n+\n \n class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n     extractor_cls = LxmlLinkExtractor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81d0b2f61ac119efab4d5970bd235dbc288496ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 662 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -65,7 +65,7 @@ class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n \n     def __init__(self, response=None, text=None, type=None, root=None, **kwargs):\n-        if not(response is None or text is None):\n+        if response is not None and text is not None:\n             raise ValueError('%s.__init__() received both response and text'\n                              % self.__class__.__name__)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a157f2e26274455405b7e272bab5aede0fa59fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 615 | Contributors (this commit): 20 | Commits (past 90d): 6 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -83,8 +83,7 @@ class S3FilesStore:\n     AWS_USE_SSL = None\n     AWS_VERIFY = None\n \n-    POLICY = 'private'  # Overriden from settings.FILES_STORE_S3_ACL in\n-                        # FilesPipeline.from_settings.\n+    POLICY = 'private'  # Overriden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings\n     HEADERS = {\n         'Cache-Control': 'max-age=172800',\n     }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f689e917bf2caa2c48def7399317dd019264663d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 390 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -79,8 +79,7 @@ class DictItem(MutableMapping, BaseItem):\n \n     def __new__(cls, *args, **kwargs):\n         if issubclass(cls, DictItem) and not issubclass(cls, Item):\n-            warn('scrapy.item.DictItem is deprecated, please use '\n-                 'scrapy.item.Item instead',\n+            warn('scrapy.item.DictItem is deprecated, please use scrapy.item.Item instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n         return super(DictItem, cls).__new__(cls, *args, **kwargs)\n \n@@ -147,6 +146,11 @@ class Item(DictItem, metaclass=ItemMeta):\n     If you need instances of a custom class to be considered items by Scrapy,\n     you must inherit from either :class:`Item` or :class:`dict`.\n \n+    Items offer the ability to declare :class:`Field` attributes, which can be\n+    used to define metadata and control the way data is processed internally.\n+    Please refer to the :ref:`documentation about fields <topics-items-fields>`\n+    for additional information.\n+\n     Unlike instances of :class:`dict`, instances of :class:`Item` may be\n     :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92792cc3f7c66a81a9e7a0576be0848266950c7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 36 | Files Changed: 4 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/4 | Churn Δ: 77 | Churn Cumulative: 624 | Contributors (this commit): 20 | Commits (past 90d): 10 | Contributors (cumulative): 35 | DMM Complexity: 0.5\n\nDIFF:\n@@ -5,7 +5,7 @@ import os\n from optparse import OptionGroup\n from twisted.python import failure\n \n-from scrapy.utils.conf import arglist_to_dict\n+from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n from scrapy.exceptions import UsageError\n \n \n@@ -70,7 +70,8 @@ class ScrapyCommand:\n                          help=\"write process ID to FILE\")\n         group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                          help=\"set/override setting (may be repeated)\")\n-        group.add_option(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n+        group.add_option(\"--pdb\", action=\"store_true\",\n+                         help=\"enable pdb on failure\")\n \n         parser.add_option_group(group)\n \n@@ -79,7 +80,8 @@ class ScrapyCommand:\n             self.settings.setdict(arglist_to_dict(opts.set),\n                                   priority='cmdline')\n         except ValueError:\n-            raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n+            raise UsageError(\n+                \"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n \n         if opts.logfile:\n             self.settings.set('LOG_ENABLED', True, priority='cmdline')\n@@ -104,3 +106,35 @@ class ScrapyCommand:\n         Entry point for running commands\n         \"\"\"\n         raise NotImplementedError\n+\n+\n+''''\n+The BaseRunSpiderCommands class inherits the ScrapyCommand class and it Used for\n+performing common functionality between crawl.py and runspider.py\n+'''\n+\n+\n+class BaseRunSpiderCommands(ScrapyCommand):\n+\n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[],\n+                          metavar=\"NAME=VALUE\",\n+                          help=\"set spider argument (may be repeated)\")\n+        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n+                          help=\"dump scraped items into FILE\"\n+                          + \"(use - for stdout)\")\n+        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n+                          help=\"format to use for dumping items with -o\")\n+\n+    def process_options(self, args, opts):\n+        ScrapyCommand.process_options(self, args, opts)\n+        try:\n+            opts.spargs = arglist_to_dict(opts.spargs)\n+        except ValueError:\n+            raise UsageError(\n+                \"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n+        if opts.output:\n+            feeds = feed_process_params_from_cli(\n+                self.settings, opts.output, opts.output_format)\n+            self.settings.set('FEEDS', feeds, priority='cmdline')\n\n@@ -1,29 +0,0 @@\n-from scrapy.commands import ScrapyCommand\n-from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n-from scrapy.exceptions import UsageError\n-\n-\n-class CommonCommands(ScrapyCommand):\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[],\n-                          metavar=\"NAME=VALUE\",\n-                          help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n-                          help=\"dump scraped items into FILE\"\n-                          + \"(use - for stdout)\")\n-        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n-                          help=\"format to use for dumping items with -o\")\n-\n-    def process_options(self, args, opts):\n-        ScrapyCommand.process_options(self, args, opts)\n-        try:\n-            opts.spargs = arglist_to_dict(opts.spargs)\n-        except ValueError:\n-            raise UsageError(\n-                \"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n-        if opts.output:\n-            feeds = feed_process_params_from_cli(\n-                self.settings, opts.output, opts.output_format)\n-            self.settings.set('FEEDS', feeds, priority='cmdline')\n\n@@ -1,8 +1,8 @@\n from scrapy.exceptions import UsageError\n-from scrapy.commands.common_commands import CommonCommands\n+from scrapy.commands import BaseRunSpiderCommands\n \n \n-class Command(CommonCommands):\n+class Command(BaseRunSpiderCommands):\n \n     requires_project = True\n \n\n@@ -4,7 +4,7 @@ from importlib import import_module\n \n from scrapy.utils.spider import iter_spider_classes\n from scrapy.exceptions import UsageError\n-from scrapy.commands.common_commands import CommonCommands\n+from scrapy.commands import BaseRunSpiderCommands\n \n \n def _import_file(filepath):\n@@ -23,7 +23,7 @@ def _import_file(filepath):\n     return module\n \n \n-class Command(CommonCommands):\n+class Command(BaseRunSpiderCommands):\n \n     requires_project = False\n     default_settings = {'SPIDER_LOADER_WARN_ONLY': True}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ed4f4f84082f7023734d1200d88e8f1aec57904c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 352 | Contributors (this commit): 16 | Commits (past 90d): 8 | Contributors (cumulative): 19 | DMM Complexity: 0.4\n\nDIFF:\n@@ -80,8 +80,7 @@ class ScrapyCommand:\n             self.settings.setdict(arglist_to_dict(opts.set),\n                                   priority='cmdline')\n         except ValueError:\n-            raise UsageError(\n-                \"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n+            raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n \n         if opts.logfile:\n             self.settings.set('LOG_ENABLED', True, priority='cmdline')\n@@ -108,18 +107,15 @@ class ScrapyCommand:\n         raise NotImplementedError\n \n \n+class BaseRunSpiderCommands(ScrapyCommand):\n     ''''\n     The BaseRunSpiderCommands class inherits the ScrapyCommand class and it Used for\n     performing common functionality between crawl.py and runspider.py\n     '''\n \n-\n-class BaseRunSpiderCommands(ScrapyCommand):\n-\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[],\n-                          metavar=\"NAME=VALUE\",\n+        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[],metavar=\"NAME=VALUE\",\n                           help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n                           help=\"dump scraped items into FILE\"\n@@ -132,9 +128,7 @@ class BaseRunSpiderCommands(ScrapyCommand):\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n-            raise UsageError(\n-                \"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n+            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n         if opts.output:\n-            feeds = feed_process_params_from_cli(\n-                self.settings, opts.output, opts.output_format)\n+            feeds = feed_process_params_from_cli(self.settings, opts.output, opts.output_format)\n             self.settings.set('FEEDS', feeds, priority='cmdline')\n\n@@ -1,5 +1,5 @@\n-from scrapy.exceptions import UsageError\n from scrapy.commands import BaseRunSpiderCommands\n+from scrapy.exceptions import UsageError\n \n \n class Command(BaseRunSpiderCommands):\n@@ -16,8 +16,7 @@ class Command(BaseRunSpiderCommands):\n         if len(args) < 1:\n             raise UsageError()\n         elif len(args) > 1:\n-            raise UsageError(\n-                \"running 'scrapy crawl' with more than one spider is no longer supported\")\n+            raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n         spname = args[0]\n \n         crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1bfbcc618090d4e71e06471b78aaac3071a51956", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 241 | Contributors (this commit): 17 | Commits (past 90d): 8 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -300,3 +300,4 @@ hoverxref_role_types = {\n     \"mod\": \"tooltip\",\n     \"ref\": \"tooltip\",\n }\n+hoverxref_roles = ['command', 'reqmeta', 'setting', 'signal']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b5684909d1cb01ad138a389caa750485b51f79cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 967 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,6 +83,7 @@ class BaseSettings(MutableMapping):\n     def __init__(self, values=None, priority='project'):\n         self.frozen = False\n         self.attributes = {}\n+        if values is not None:\n             self.update(values, priority)\n \n     def __getitem__(self, opt_name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#abfdc1b5425997f1aa69b29465c6ab2324f37fd0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 404 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -138,18 +138,20 @@ class Item(DictItem, metaclass=ItemMeta):\n     \"\"\"\n     Base class for scraped items.\n \n-    In Scrapy, an object is considered an *item* if it is an instance of either\n-    :class:`Item` or :class:`dict`. For example, when the output of a\n+    In Scrapy, an object is considered an ``item`` if it is an instance of either\n+    :class:`Item` or :class:`dict`, or any subclass. For example, when the output of a\n     spider callback is evaluated, only instances of :class:`Item` or\n     :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n \n     If you need instances of a custom class to be considered items by Scrapy,\n     you must inherit from either :class:`Item` or :class:`dict`.\n \n-    Items offer the ability to declare :class:`Field` attributes, which can be\n-    used to define metadata and control the way data is processed internally.\n-    Please refer to the :ref:`documentation about fields <topics-items-fields>`\n-    for additional information.\n+    Items must declare :class:`Field` attributes, which are processed and stored\n+    in the ``fields`` attribute. This restricts the set of allowed field names\n+    and prevents typos, raising ``KeyError`` when referring to undefined fields.\n+    Additionally, fields can be used to define metadata and control the way\n+    data is processed internally. Please refer to the :ref:`documentation\n+    about fields <topics-items-fields>` for additional information.\n \n     Unlike instances of :class:`dict`, instances of :class:`Item` may be\n     :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f8758624c8d3df7472948ce9805601f6037548a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 20 | Files Changed: 11 | Hunks: 19 | Methods Changed: 16 | Complexity Δ (Sum/Max): -68/0 | Churn Δ: 36 | Churn Cumulative: 5804 | Contributors (this commit): 36 | Commits (past 90d): 50 | Contributors (cumulative): 101 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,7 +6,7 @@ Some pipelines used for testing\n class ZeroDivisionErrorPipeline:\n \n     def open_spider(self, spider):\n-        a = 1 / 0\n+        1 / 0\n \n     def process_item(self, item, spider):\n         return item\n\n@@ -87,7 +87,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         class MySpider(scrapy.Spider):\n             name = 'spider'\n \n-        crawler = Crawler(MySpider, {})\n+        Crawler(MySpider, {})\n         assert get_scrapy_root_handler() is None\n \n     def test_spider_custom_settings_log_level(self):\n@@ -240,13 +240,13 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n     def test_crawler_runner_asyncio_enabled_true(self):\n         if self.reactor_pytest == 'asyncio':\n-            runner = CrawlerRunner(settings={\n+            CrawlerRunner(settings={\n                 \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n             })\n         else:\n             msg = r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\"\n             with self.assertRaisesRegex(Exception, msg):\n-                runner = CrawlerRunner(settings={\n+                CrawlerRunner(settings={\n                     \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n                 })\n \n\n@@ -6,7 +6,7 @@ class ScrapyUtilsTest(unittest.TestCase):\n     def test_required_openssl_version(self):\n         try:\n             module = import_module('OpenSSL')\n-        except ImportError as ex:\n+        except ImportError:\n             raise unittest.SkipTest(\"OpenSSL is not available\")\n \n         if hasattr(module, '__version__'):\n\n@@ -11,8 +11,6 @@ class TelnetExtensionTest(unittest.TestCase):\n     def _get_console_and_portal(self, settings=None):\n         crawler = get_crawler(settings_dict=settings)\n         console = TelnetConsole(crawler)\n-        username = console.username\n-        password = console.password\n \n         # This function has some side effects we don't need for this test\n         console._get_telnet_vars = lambda: {}\n\n@@ -715,7 +715,6 @@ class FeedExportTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_export_encoding(self):\n         items = [dict({'foo': u'Test\\xd6'})]\n-        header = ['foo']\n \n         formats = {\n             'json': u'[{\"foo\": \"Test\\\\u00d6\"}]'.encode('utf-8'),\n\n@@ -264,12 +264,12 @@ class ItemTest(unittest.TestCase):\n         \"\"\"Make sure the DictItem deprecation warning is not issued for\n         Item\"\"\"\n         with catch_warnings(record=True) as warnings:\n-            item = Item()\n+            Item()\n             self.assertEqual(len(warnings), 0)\n \n             class SubclassedItem(Item):\n                 pass\n-            subclassed_item = SubclassedItem()\n+            SubclassedItem()\n             self.assertEqual(len(warnings), 0)\n \n \n@@ -321,13 +321,13 @@ class DictItemTest(unittest.TestCase):\n \n     def test_deprecation_warning(self):\n         with catch_warnings(record=True) as warnings:\n-            dict_item = DictItem()\n+            DictItem()\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n         with catch_warnings(record=True) as warnings:\n             class SubclassedDictItem(DictItem):\n                 pass\n-            subclassed_dict_item = SubclassedDictItem()\n+            SubclassedDictItem()\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n \n\n@@ -15,7 +15,7 @@ from scrapy.utils.python import to_bytes\n skip = False\n try:\n     from PIL import Image\n-except ImportError as e:\n+except ImportError:\n     skip = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n else:\n     encoders = set(('jpeg_encoder', 'jpeg_decoder'))\n\n@@ -459,7 +459,6 @@ class TestRequestMetaSettingFallback(TestCase):\n         target = 'http://www.example.com'\n \n         for settings, response_headers, request_meta, policy_class, check_warning in self.params[3:]:\n-            spider = Spider('foo')\n             mw = RefererMiddleware(Settings(settings))\n \n             response = Response(origin, headers=response_headers)\n@@ -511,7 +510,7 @@ class TestSettingsPolicyByName(TestCase):\n     def test_invalid_name(self):\n         settings = Settings({'REFERRER_POLICY': 'some-custom-unknown-policy'})\n         with self.assertRaises(RuntimeError):\n-            mw = RefererMiddleware(settings)\n+            RefererMiddleware(settings)\n \n \n class TestPolicyHeaderPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n\n@@ -64,7 +64,7 @@ class DeferUtilsTest(unittest.TestCase):\n         gotexc = False\n         try:\n             yield process_chain([cb1, cb_fail, cb3], 'res', 'v1', 'v2')\n-        except TypeError as e:\n+        except TypeError:\n             gotexc = True\n         self.assertTrue(gotexc)\n \n@@ -104,7 +104,7 @@ class IterErrbackTest(unittest.TestCase):\n         def iterbad():\n             for x in range(10):\n                 if x == 5:\n-                    a = 1 / 0\n+                    1 / 0\n                 yield x\n \n         errors = []\n\n@@ -25,7 +25,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n     def test_no_warning_on_definition(self):\n         with warnings.catch_warnings(record=True) as w:\n-            Deprecated = create_deprecated_class('Deprecated', NewName)\n+            create_deprecated_class('Deprecated', NewName)\n \n         w = self._mywarnings(w)\n         self.assertEqual(w, [])\n@@ -217,7 +217,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n     def test_deprecate_a_class_with_custom_metaclass(self):\n         Meta1 = type('Meta1', (type,), {})\n         New = Meta1('New', (), {})\n-        Deprecated = create_deprecated_class('Deprecated', New)\n+        create_deprecated_class('Deprecated', New)\n \n     def test_deprecate_subclass_of_deprecated_class(self):\n         with warnings.catch_warnings(record=True) as w:\n\n@@ -44,7 +44,7 @@ class SendCatchLogTest(unittest.TestCase):\n \n     def error_handler(self, arg, handlers_called):\n         handlers_called.add(self.error_handler)\n-        a = 1 / 0\n+        1 / 0\n \n     def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cb8140a42a27ede87b0880372024f2f1804618b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 1610 | Contributors (this commit): 18 | Commits (past 90d): 10 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ discovering (through HTTP headers) to base Response class.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+import warnings\n from contextlib import suppress\n from typing import Generator\n from urllib.parse import urljoin\n@@ -14,6 +15,7 @@ from w3lib.encoding import (html_body_declared_encoding, html_to_unicode,\n                             http_content_type_encoding, resolve_encoding)\n from w3lib.html import strip_html5_whitespace\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs, to_unicode\n@@ -61,6 +63,9 @@ class TextResponse(Response):\n \n     def body_as_unicode(self):\n         \"\"\"Return body as unicode\"\"\"\n+        warnings.warn('Response.body_as_unicode() is deprecated, '\n+                      'please use Response.text instead.',\n+                      ScrapyDeprecationWarning)\n         return self.text\n \n     @property\n\n@@ -1,7 +1,9 @@\n import unittest\n+from warnings import catch_warnings\n \n from w3lib.encoding import resolve_encoding\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n                          XmlResponse, Headers)\n from scrapy.selector import Selector\n@@ -660,6 +662,13 @@ class TextResponseTest(BaseResponseTest):\n         with self.assertRaises(ValueError):\n             response.follow_all(css='a[href*=\"example.com\"]', xpath='//a[contains(@href, \"example.com\")]')\n \n+    def test_body_as_unicode_deprecation_warning(self):\n+        with catch_warnings(record=True) as warnings:\n+            r1 = self.response_class(\"http://www.example.com\", body=u'Hello', encoding='utf-8')\n+            self.assertEqual(r1.body_as_unicode(), u'Hello')\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+\n \n class HtmlResponseTest(TextResponseTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf9be5344a89dd8e14f8241ec69de9c984ec1e05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 29 | Churn Cumulative: 744 | Contributors (this commit): 26 | Commits (past 90d): 4 | Contributors (cumulative): 31 | DMM Complexity: 0.5384615384615384\n\nDIFF:\n@@ -137,17 +137,26 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n     ``*args`` and ``**kwargs`` are forwarded to the constructors.\n \n     Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n+\n+    Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n+    extension has not been implemented correctly).\n     \"\"\"\n     if settings is None:\n         if crawler is None:\n             raise ValueError(\"Specify at least one of settings and crawler.\")\n         settings = crawler.settings\n     if crawler and hasattr(objcls, 'from_crawler'):\n-        return objcls.from_crawler(crawler, *args, **kwargs)\n+        instance = objcls.from_crawler(crawler, *args, **kwargs)\n+        method_name = 'from_crawler'\n     elif hasattr(objcls, 'from_settings'):\n-        return objcls.from_settings(settings, *args, **kwargs)\n+        instance = objcls.from_settings(settings, *args, **kwargs)\n+        method_name = 'from_settings'\n     else:\n-        return objcls(*args, **kwargs)\n+        instance = objcls(*args, **kwargs)\n+        method_name = '__new__'\n+    if instance is None:\n+        raise TypeError(\"%s.%s returned None\" % (objcls.__qualname__, method_name))\n+    return instance\n \n \n @contextmanager\n\n@@ -114,8 +114,12 @@ class UtilsMiscTestCase(unittest.TestCase):\n         #   2. with from_settings() constructor\n         #   3. with from_crawler() constructor\n         #   4. with from_settings() and from_crawler() constructor\n-        spec_sets = ([], ['from_settings'], ['from_crawler'],\n-                     ['from_settings', 'from_crawler'])\n+        spec_sets = (\n+            ['__qualname__'],\n+            ['__qualname__', 'from_settings'],\n+            ['__qualname__', 'from_crawler'],\n+            ['__qualname__', 'from_settings', 'from_crawler'],\n+        )\n         for specs in spec_sets:\n             m = mock.MagicMock(spec_set=specs)\n             _test_with_settings(m, settings)\n@@ -123,7 +127,7 @@ class UtilsMiscTestCase(unittest.TestCase):\n             _test_with_crawler(m, settings, crawler)\n \n         # Check adoption of crawler settings\n-        m = mock.MagicMock(spec_set=['from_settings'])\n+        m = mock.MagicMock(spec_set=['__qualname__', 'from_settings'])\n         create_instance(m, None, crawler, *args, **kwargs)\n         m.from_settings.assert_called_once_with(crawler.settings, *args,\n                                                 **kwargs)\n@@ -131,6 +135,10 @@ class UtilsMiscTestCase(unittest.TestCase):\n         with self.assertRaises(ValueError):\n             create_instance(m, None, None)\n \n+        m.from_settings.return_value = None\n+        with self.assertRaises(TypeError):\n+            create_instance(m, settings, None)\n+\n     def test_set_environ(self):\n         assert os.environ.get('some_test_environ') is None\n         with set_environ(some_test_environ='test_value'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e01c30f0d54ba838aa3212634cddf3630f77b2d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 69 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -108,10 +108,10 @@ class ScrapyCommand:\n \n \n class BaseRunSpiderCommands(ScrapyCommand):\n-    ''''\n+    \"\"\"\n     The BaseRunSpiderCommands class inherits the ScrapyCommand class and it Used for\n     performing common functionality between crawl.py and runspider.py\n-    '''\n+    \"\"\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#28d223dd87fb655743f67087be0009b094ddad10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 72 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -118,8 +118,7 @@ class BaseRunSpiderCommands(ScrapyCommand):\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                           help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n-                          help=\"dump scraped items into FILE\"\n-                          + \"(use - for stdout)\")\n+                          help=\"dump scraped items into FILE (use - for stdout)\")\n         parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n                           help=\"format to use for dumping items with -o\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#07e125f4c916fcd2d4d978a89c7d7e508cc63620", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 73 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -112,7 +112,6 @@ class BaseRunSpiderCommands(ScrapyCommand):\n     The BaseRunSpiderCommands class inherits the ScrapyCommand class and it Used for\n     performing common functionality between crawl.py and runspider.py\n     \"\"\"\n-\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97532a9144cc011e5a6ede84fa7d317c3af60131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 237 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -137,6 +137,11 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             msg = str(w[0].message)\n             self.assertIn(\"several spiders with the same name\", msg)\n             self.assertIn(\"'spider3'\", msg)\n+            self.assertTrue(msg.count(\"'spider3'\") == 2)\n+\n+            self.assertNotIn(\"'spider1'\", msg)\n+            self.assertNotIn(\"'spider2'\", msg)\n+            self.assertNotIn(\"'spider4'\", msg)\n \n             spiders = set(spider_loader.list())\n             self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n@@ -156,7 +161,13 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             msg = str(w[0].message)\n             self.assertIn(\"several spiders with the same name\", msg)\n             self.assertIn(\"'spider1'\", msg)\n+            self.assertTrue(msg.count(\"'spider1'\") == 2)\n+\n             self.assertIn(\"'spider2'\", msg)\n+            self.assertTrue(msg.count(\"'spider2'\") == 2)\n+\n+            self.assertNotIn(\"'spider3'\", msg)\n+            self.assertNotIn(\"'spider4'\", msg)\n \n             spiders = set(spider_loader.list())\n             self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33ab0a36635fbd45debbc44584002bd7a4ef7fed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 969 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -83,7 +83,7 @@ class BaseSettings(MutableMapping):\n     def __init__(self, values=None, priority='project'):\n         self.frozen = False\n         self.attributes = {}\n-        if values is not None:\n+        if values:\n             self.update(values, priority)\n \n     def __getitem__(self, opt_name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8971878c1d4d95aff10a1e910688502de892662b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1490 | Contributors (this commit): 26 | Commits (past 90d): 6 | Contributors (cumulative): 38 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,7 @@ def _isiterable(possible_iterator):\n \n \n def _fname(f):\n-    return \"%s.%s\".format(\n+    return \"{}.{}\".format(\n         f.__self__.__class__.__name__,\n         f.__func__.__name__\n     )\n\n@@ -28,6 +28,7 @@ def _embed_ipython_shell(namespace={}, banner=''):\n def _embed_bpython_shell(namespace={}, banner=''):\n     \"\"\"Start a bpython shell\"\"\"\n     import bpython\n+\n     @wraps(_embed_bpython_shell)\n     def wrapper(namespace=namespace, banner=''):\n         bpython.embed(locals_=namespace, banner=banner)\n@@ -37,6 +38,7 @@ def _embed_bpython_shell(namespace={}, banner=''):\n def _embed_ptpython_shell(namespace={}, banner=''):\n     \"\"\"Start a ptpython shell\"\"\"\n     import ptpython.repl\n+\n     @wraps(_embed_ptpython_shell)\n     def wrapper(namespace=namespace, banner=''):\n         print(banner)\n\n@@ -152,6 +152,7 @@ def memoizemethod_noargs(method):\n     weak reference to its object\n     \"\"\"\n     cache = weakref.WeakKeyDictionary()\n+\n     @wraps(method)\n     def new_method(self, *args, **kwargs):\n         if self not in cache:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2327ecead085a41d1a71a70a12eb988bbf982268", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 20 | Methods Changed: 17 | Complexity Δ (Sum/Max): 65/65 | Churn Δ: 40 | Churn Cumulative: 3600 | Contributors (this commit): 59 | Commits (past 90d): 27 | Contributors (cumulative): 95 | DMM Complexity: None\n\nDIFF:\n@@ -242,7 +242,7 @@ class FeedExporter:\n \n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n-        self.storage_batch_size = self.settings.get('FEED_STORAGE_BATCH_SIZE', None)\n+        self.storage_batch_size = self.settings.get('FEED_STORAGE_BATCH_ITEM_COUNT', None)\n         for uri, feed in self.feeds.items():\n             if not self._storage_supported(uri):\n                 raise NotConfigured\n@@ -290,7 +290,7 @@ class FeedExporter:\n     def _start_new_batch(self, batch_id, uri, feed, spider, template_uri):\n         \"\"\"\n         Redirect the output data stream to a new file.\n-        Execute multiple times if 'FEED_STORAGE_BATCH' setting is specified.\n+        Execute multiple times if 'FEED_STORAGE_BATCH_ITEM_COUNT' setting is specified.\n         :param batch_id: sequence number of current batch\n         :param uri: uri of the new batch to start\n         :param feed: dict with parameters of feed\n@@ -326,7 +326,7 @@ class FeedExporter:\n             slot.start_exporting()\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n-            # create new slot for each slot with itemcount == FEED_STORAGE_BATCH_SIZE and close the old one\n+            # create new slot for each slot with itemcount == FEED_STORAGE_BATCH_ITEM_COUNT and close the old one\n             if self.storage_batch_size and slot.itemcount == self.storage_batch_size:\n                 uri_params = self._get_uri_params(spider, self.feeds[slot.template_uri]['uri_params'], slot)\n                 self._close_slot(slot, spider)\n@@ -358,12 +358,12 @@ class FeedExporter:\n \n     def _batch_deliveries_supported(self, uri):\n         \"\"\"\n-        If FEED_STORAGE_BATCH_SIZE setting is specified uri has to contain %(batch_time)s or %(batch_id)s\n+        If FEED_STORAGE_BATCH_ITEM_COUNT setting is specified uri has to contain %(batch_time)s or %(batch_id)s\n         to distinguish different files of partial output\n         \"\"\"\n         if self.storage_batch_size is None or '%(batch_time)s' in uri or '%(batch_id)s' in uri:\n             return True\n-        logger.warning('%(batch_time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_SIZE setting is specified')\n+        logger.warning('%(batch_time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_ITEM_COUNT setting is specified')\n         return False\n \n     def _storage_supported(self, uri):\n\n@@ -146,7 +146,7 @@ FEED_STORAGES_BASE = {\n     's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n     'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n }\n-FEED_STORAGE_BATCH_SIZE = None\n+FEED_STORAGE_BATCH_ITEM_COUNT = None\n FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n     'json': 'scrapy.exporters.JsonItemExporter',\n\n@@ -1025,7 +1025,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'jl', self._file_mark): {'format': 'jl'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         for batch in data['jl']:\n@@ -1041,7 +1041,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {'format': 'csv'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n         data = yield self.exported_data(items, settings)\n         for batch in data['csv']:\n             got_batch = csv.DictReader(to_unicode(batch).splitlines())\n@@ -1057,7 +1057,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         for batch in data['xml']:\n@@ -1075,7 +1075,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'json', self._file_mark): {'format': 'json'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         # XML\n@@ -1100,7 +1100,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'pickle', self._file_mark): {'format': 'pickle'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import pickle\n@@ -1117,7 +1117,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'marshal', self._file_mark): {'format': 'marshal'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import marshal\n@@ -1140,7 +1140,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n             {'foo': 'bar3', 'baz': 'quux3', 'egg': ''}\n         ]\n         settings = {\n-            'FEED_STORAGE_BATCH_SIZE': 2\n+            'FEED_STORAGE_BATCH_ITEM_COUNT': 2\n         }\n         header = self.MyItem.fields.keys()\n         yield self.assertExported(items, header, rows, settings=settings)\n@@ -1151,7 +1151,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'xml'},\n             },\n-            'FEED_STORAGE_BATCH_SIZE': 1\n+            'FEED_STORAGE_BATCH_ITEM_COUNT': 1\n         }\n         crawler = get_crawler(settings_dict=settings)\n         self.assertRaises(NotConfigured, FeedExporter, crawler)\n@@ -1163,7 +1163,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 'FEEDS': {\n                     os.path.join(self._random_temp_filename(), fmt, self._file_mark): {'format': fmt},\n                 },\n-                'FEED_STORAGE_BATCH_SIZE': 1\n+                'FEED_STORAGE_BATCH_ITEM_COUNT': 1\n             }\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n@@ -1185,7 +1185,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                 },\n                 'FEED_STORE_EMPTY': True,\n                 'FEED_EXPORT_INDENT': None,\n-                'FEED_STORAGE_BATCH_SIZE': 1,\n+                'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n             }\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n@@ -1225,7 +1225,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                     'encoding': 'utf-8',\n                 },\n             },\n-            'FEED_STORAGE_BATCH_SIZE': 1,\n+            'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n         }\n         data = yield self.exported_data(items, settings)\n         for fmt, expected in formats.items():\n@@ -1249,7 +1249,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                     'format': 'json',\n                 },\n             },\n-            'FEED_STORAGE_BATCH_SIZE': 1,\n+            'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n         }\n         data = yield self.exported_data(items, settings)\n         self.assertEqual(len(items) + 1, len(data['json']))\n@@ -1291,7 +1291,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n                     'format': 'json',\n                 },\n             },\n-            'FEED_STORAGE_BATCH_SIZE': 1,\n+            'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n         }\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n@@ -1309,7 +1309,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n \n         s3 = boto3.resource('s3')\n         my_bucket = s3.Bucket(s3_test_bucket_name)\n-        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n \n         with MockServer() as s:\n             runner = CrawlerRunner(Settings(settings))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
