{"custom_id": "scrapy#acf69dac444ee6d6714f74628e40ae831850d6ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 564 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,8 +54,7 @@ class Request(object_ref):\n             if self.encoding is None:\n                 raise TypeError('Cannot convert unicode url - %s has no encoding' %\n                     type(self).__name__)\n-            unicode_url = url if isinstance(url, unicode) else url.decode(self.encoding)\n-            self._url = safe_url_string(unicode_url, self.encoding)\n+            self._set_url(url.encode(self.encoding))\n         else:\n             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n         if ':' not in self._url:\n\n@@ -108,8 +108,12 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r4.body, \"Price: \\xa3100\")\n \n     def test_ajax_url(self):\n+        # ascii url\n         r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n         self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key=value\")\n+        # unicode url\n+        r = self.request_class(url=u\"http://www.example.com/ajax.html#!key=value\")\n+        self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key=value\")\n \n     def test_copy(self):\n         \"\"\"Test Request copy\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#900bf08fb652183ac9ea33d87735f3a0b4343b21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 32 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import struct\n from cStringIO import StringIO\n from gzip import GzipFile\n \n@@ -13,8 +14,9 @@ def gunzip(data):\n         try:\n             chunk = f.read(8196)\n             output += chunk\n-        except IOError:\n+        except (IOError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n+            # see issue 87 about catching struct.error\n             if output:\n                 output += f.extrabuf\n                 break\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe2ce938ee10cb7d06592798d43ba026237076a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 70 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ Extensions for debugging Scrapy\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n-import os\n import sys\n import signal\n import traceback\n@@ -12,6 +11,7 @@ import threading\n from pdb import Pdb\n \n from scrapy.utils.engine import format_engine_status\n+from scrapy.utils.trackref import format_live_refs\n from scrapy import log\n \n \n@@ -19,6 +19,7 @@ class StackTraceDump(object):\n \n     def __init__(self, crawler=None):\n         self.crawler = crawler\n+        self.dumprefs = crawler.settings.getbool('TRACK_REFS')\n         try:\n             signal.signal(signal.SIGUSR2, self.dump_stacktrace)\n             signal.signal(signal.SIGQUIT, self.dump_stacktrace)\n@@ -26,7 +27,6 @@ class StackTraceDump(object):\n             # win32 platforms don't support SIGUSR signals\n             pass\n \n-\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler)\n@@ -34,8 +34,9 @@ class StackTraceDump(object):\n     def dump_stacktrace(self, signum, frame):\n         stackdumps = self._thread_stacks()\n         enginestatus = format_engine_status(self.crawler.engine)\n+        liverefs = format_live_refs() if self.dumprefs else \"\"\n         msg = \"Dumping stack trace and engine status\" \\\n-            \"\\n{0}\\n{1}\".format(enginestatus, stackdumps)\n+            \"\\n{0}\\n{1}\\n{2}\".format(enginestatus, liverefs, stackdumps)\n         log.msg(msg)\n \n     def _thread_stacks(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b094cd4c57cbc25a6dd43005ebd8eeee109a75b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 265 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -87,7 +87,7 @@ class AutoThrottle(object):\n         spider.max_concurrent_requests = 1\n \n     def response_received(self, response, spider):\n-        slot = self._get_slot(response.request)\n+        key, slot = self._get_slot(response.request)\n         latency = response.meta.get('download_latency')\n         \n         if not latency or not slot:\n@@ -97,8 +97,8 @@ class AutoThrottle(object):\n         self._check_concurrency(slot, latency)\n \n         if self.DEBUG:\n-            spider.log(\"conc:%2d | delay:%5d ms | latency:%5d ms | size:%6d bytes\" % \\\n-                (slot.concurrency, slot.delay*1000, \\\n+            spider.log(\"slot: %s | conc:%2d | delay:%5d ms | latency:%5d ms | size:%6d bytes\" % \\\n+                (key, slot.concurrency, slot.delay*1000, \\\n                 latency*1000, len(response.body)))\n \n     def _get_slot(self, request):\n@@ -106,7 +106,7 @@ class AutoThrottle(object):\n         key = urlparse_cached(request).hostname or ''\n         if downloader.ip_concurrency:\n             key = dnscache.get(key, key)\n-        return downloader.slots.get(key)\n+        return key, downloader.slots.get(key)\n \n     def _check_concurrency(self, slot, latency):\n         latencies = self.last_latencies\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#093910687294a23a6e466cfd20c12f680ac32fda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 175 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -100,6 +100,6 @@ class MemoryUsage(object):\n \n         s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n         s += \"\\r\\n\"\n-        s += pformat(get_engine_status())\n+        s += pformat(get_engine_status(self.crawler.engine))\n         s += \"\\r\\n\"\n         self.mail.send(rcpts, subject, s)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7fe7c3f3b1f35dac692e61ef92783beb689462fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 306 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,6 +76,11 @@ class MemoryUsage(object):\n                         (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n                 stats.set_value('memusage/limit_notified', 1)\n+            open_spiders = self.crawler.engine.open_spiders\n+            if open_spiders:\n+                for spider in open_spiders:\n+                    self.crawler.engine.close_spider(spider, 'memusage_exceeded')\n+            else:\n                 self.crawler.stop()\n \n     def _check_warning(self):\n\n@@ -203,7 +203,7 @@ MAIL_USER = None\n MEMDEBUG_ENABLED = False        # enable memory debugging\n MEMDEBUG_NOTIFY = []            # send memory debugging report by mail at engine shutdown\n \n-MEMUSAGE_ENABLED = 1\n+MEMUSAGE_ENABLED = False\n MEMUSAGE_LIMIT_MB = 0\n MEMUSAGE_NOTIFY_MAIL = []\n MEMUSAGE_REPORT = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e312a88582e62874969b33e3bd09c8e76a51fd30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 182 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,7 @@ class MemoryUsage(object):\n         return cls(crawler)\n \n     def get_virtual_size(self):\n-        return get_vmvalue_from_procfs('VmSize')\n+        return get_vmvalue_from_procfs('VmRSS')\n \n     def engine_started(self):\n         stats.set_value('memusage/startup', self.get_virtual_size())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c73a0b1c1e4af1b2d2d0835c4790cad93b51d1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 404 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from collections import defaultdict\n from twisted.internet.defer import Deferred, DeferredList\n+from twisted.python.failure import Failure\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n@@ -88,6 +89,11 @@ class MediaPipeline(object):\n         return dfd\n \n     def _cache_result_and_execute_waiters(self, result, fp, info):\n+        if isinstance(result, Failure):\n+            # minimize cached information for failure\n+            result.cleanFailure()\n+            result.frames = []\n+            result.stack = None\n         info.downloading.remove(fp)\n         info.downloaded[fp] = result # cache result\n         for wad in info.waiting.pop(fp):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3286ce4f42c7a663f24adee2a6c57ebba9c68ba0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 642 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -249,13 +249,12 @@ class ImagesPipeline(MediaPipeline):\n         return dfd\n \n     def image_downloaded(self, response, request, info):\n-        first_buf = None\n+        checksum = None\n         for key, image, buf in self.get_images(response, request, info):\n+            if checksum is None:\n+                checksum = md5sum(buf)\n             self.store.persist_image(key, image, buf, info)\n-            if first_buf is None:\n-                first_buf = buf\n-        first_buf.seek(0)\n-        return md5sum(first_buf)\n+        return checksum\n \n     def get_images(self, response, request, info):\n         key = self.image_key(request.url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2dbf2a38a2d59ce0cdad2fa9c942bb536e8546b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 643 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -252,6 +252,7 @@ class ImagesPipeline(MediaPipeline):\n         checksum = None\n         for key, image, buf in self.get_images(response, request, info):\n             if checksum is None:\n+                buf.seek(0)\n                 checksum = md5sum(buf)\n             self.store.persist_image(key, image, buf, info)\n         return checksum\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b1f011d7408343aabdf89c2c25fcde9b3c2b38a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 703 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,10 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.co.uk']))\n         self.assertFalse(url_is_from_any_domain(url, ['art.co.uk']))\n \n+        url = 'http://192.169.0.15:8080/mypage.html'\n+        self.assertTrue(url_is_from_any_domain(url, ['192.169.0.15:8080']))\n+        self.assertFalse(url_is_from_any_domain(url, ['192.169.0.15']))\n+\n         url = 'javascript:%20document.orderform_2581_1190810811.mode.value=%27add%27;%20javascript:%20document.orderform_2581_1190810811.submit%28%29'\n         self.assertFalse(url_is_from_any_domain(url, ['testdomain.com']))\n         self.assertFalse(url_is_from_any_domain(url+'.testdomain.com', ['testdomain.com']))\n\n@@ -15,7 +15,7 @@ from scrapy.utils.python import unicode_to_str\n \n def url_is_from_any_domain(url, domains):\n     \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n-    host = parse_url(url).hostname\n+    host = parse_url(url).netloc\n \n     if host:\n         return any(((host == d) or (host.endswith('.%s' % d)) for d in domains))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#049f315ff453445c1237209f50e02b372f02d179", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 13 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ class ResponseTypes(object):\n         'application/rdf+xml': 'scrapy.http.XmlResponse',\n         'application/rss+xml': 'scrapy.http.XmlResponse',\n         'application/xhtml+xml': 'scrapy.http.HtmlResponse',\n+        'application/vnd.wap.xhtml+xml': 'scrapy.http.HtmlResponse',\n         'application/xml': 'scrapy.http.XmlResponse',\n         'application/json': 'scrapy.http.TextResponse',\n         'application/javascript': 'scrapy.http.TextResponse',\n\n@@ -32,6 +32,7 @@ class ResponseTypesTest(unittest.TestCase):\n             ('text/html; charset=UTF-8', HtmlResponse),\n             ('text/xml; charset=UTF-8', XmlResponse),\n             ('application/xhtml+xml; charset=UTF-8', HtmlResponse),\n+            ('application/vnd.wap.xhtml+xml; charset=utf-8', HtmlResponse),\n             ('application/xml; charset=UTF-8', XmlResponse),\n             ('application/octet-stream', Response),\n         ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c093ac5ec6e808003c4a5c402c4bd36c70f1b6ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 35 | Churn Cumulative: 122 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,6 @@\n import os\n from collections import defaultdict\n-from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy import signals\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n@@ -18,13 +16,13 @@ class CookiesMiddleware(object):\n         if not settings.getbool('COOKIES_ENABLED'):\n             raise NotConfigured\n         self.jars = defaultdict(CookieJar)\n-        dispatcher.connect(self.spider_closed, signals.spider_closed)\n \n     def process_request(self, request, spider):\n         if 'dont_merge_cookies' in request.meta:\n             return\n \n-        jar = self.jars[spider]\n+        cookiejarkey = request.meta.get(\"cookiejar\")\n+        jar = self.jars[cookiejarkey]\n         cookies = self._get_request_cookies(jar, request)\n         for cookie in cookies:\n             jar.set_cookie_if_ok(cookie, request)\n@@ -39,15 +37,13 @@ class CookiesMiddleware(object):\n             return response\n \n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n-        jar = self.jars[spider]\n+        cookiejarkey = request.meta.get(\"cookiejar\")\n+        jar = self.jars[cookiejarkey]\n         jar.extract_cookies(response, request)\n         self._debug_set_cookie(response, spider)\n \n         return response\n \n-    def spider_closed(self, spider):\n-        self.jars.pop(spider, None)\n-\n     def _debug_cookie(self, request, spider):\n         if self.debug:\n             cl = request.headers.getlist('Cookie')\n\n@@ -14,7 +14,6 @@ class CookiesMiddlewareTest(TestCase):\n         self.mw = CookiesMiddleware()\n \n     def tearDown(self):\n-        self.mw.spider_closed(self.spider)\n         del self.mw\n \n     def test_basic(self):\n@@ -65,6 +64,28 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(req2, self.spider) is None\n         self.assertEquals(req2.headers.get('Cookie'), \"C1=value1; galleta=salada\")\n \n+    def test_cookiejar_key(self):\n+        req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'}, meta={'cookiejar': \"store1\"})\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEquals(req.headers.get('Cookie'), 'galleta=salada')\n+\n+        headers = {'Set-Cookie': 'C1=value1; path=/'}\n+        res = Response('http://scrapytest.org/', headers=headers, request=req)\n+        assert self.mw.process_response(req, res, self.spider) is res\n+\n+        req2 = Request('http://scrapytest.org/', meta=res.meta)\n+        assert self.mw.process_request(req2, self.spider) is None\n+        self.assertEquals(req2.headers.get('Cookie'), 'C1=value1; galleta=salada')\n \n \n+        req3 = Request('http://scrapytest.org/', cookies={'galleta': 'dulce'}, meta={'cookiejar': \"store2\"})\n+        assert self.mw.process_request(req3, self.spider) is None\n+        self.assertEquals(req3.headers.get('Cookie'), 'galleta=dulce')\n \n+        headers = {'Set-Cookie': 'C2=value2; path=/'}\n+        res2 = Response('http://scrapytest.org/', headers=headers, request=req3)\n+        assert self.mw.process_response(req3, res2, self.spider) is res2\n+\n+        req4 = Request('http://scrapytest.org/', meta=res2.meta)\n+        assert self.mw.process_request(req4, self.spider) is None\n+        self.assertEquals(req4.headers.get('Cookie'), 'C2=value2; galleta=dulce')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#798169805ad13a01c057cda0b66a18605a59a529", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 49 | Churn Cumulative: 690 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,10 +6,9 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n-    html_body_declared_encoding, http_content_type_encoding\n+    html_body_declared_encoding, http_content_type_encoding, to_unicode\n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs\n-from scrapy.utils.encoding import encoding_exists\n from scrapy.conf import settings\n \n \n@@ -48,17 +47,7 @@ class TextResponse(Response):\n \n     @property\n     def encoding(self):\n-        return self._get_encoding(infer=True)\n-\n-    def _get_encoding(self, infer=False):\n-        enc = self._declared_encoding()\n-        if enc and not encoding_exists(enc):\n-            enc = None\n-        if not enc and infer:\n-            enc = self._body_inferred_encoding()\n-        if not enc:\n-            enc = self._DEFAULT_ENCODING\n-        return resolve_encoding(enc)\n+        return self._declared_encoding() or self._body_inferred_encoding()\n \n     def _declared_encoding(self):\n         return self._encoding or self._headers_encoding() \\\n@@ -67,7 +56,7 @@ class TextResponse(Response):\n     def body_as_unicode(self):\n         \"\"\"Return body as unicode\"\"\"\n         if self._cached_ubody is None:\n-            self._cached_ubody = self.body.decode(self.encoding, 'scrapy_replace')\n+            self._cached_ubody = to_unicode(self.body, self.encoding)\n         return self._cached_ubody\n \n     @memoizemethod_noargs\n@@ -78,14 +67,21 @@ class TextResponse(Response):\n     def _body_inferred_encoding(self):\n         if self._cached_benc is None:\n             content_type = self.headers.get('Content-Type')\n-            benc, _ = html_to_unicode(content_type, self.body, default_encoding=self._DEFAULT_ENCODING)\n+            benc, ubody = html_to_unicode(content_type, self.body, \\\n+                    auto_detect_fun=self._auto_detect_fun, \\\n+                    default_encoding=self._DEFAULT_ENCODING)\n             self._cached_benc = benc\n-            # XXX: is this needed?\n-            # UnicodeDammit is buggy decoding utf-16\n-            #if self._cached_ubody is None and benc != 'utf-16':\n-            #    self._cached_ubody = dammit.unicode\n+            self._cached_ubody = ubody\n         return self._cached_benc\n \n+    def _auto_detect_fun(self, text):\n+        for enc in (self._DEFAULT_ENCODING, 'utf-8', 'cp1252'):\n+            try:\n+                text.decode(enc)\n+            except UnicodeError:\n+                continue\n+            return resolve_encoding(enc)\n+\n     @memoizemethod_noargs\n     def _body_declared_encoding(self):\n         return html_body_declared_encoding(self.body)\n\n@@ -171,8 +171,8 @@ class TextResponseTest(BaseResponseTest):\n         self.assertEqual(r2._headers_encoding(), None)\n         self.assertEqual(r2._declared_encoding(), 'utf-8')\n         self._assert_response_encoding(r2, 'utf-8')\n-        self.assertEqual(r3._headers_encoding(), \"iso-8859-1\")\n-        self.assertEqual(r3._declared_encoding(), \"iso-8859-1\")\n+        self.assertEqual(r3._headers_encoding(), \"cp1252\")\n+        self.assertEqual(r3._declared_encoding(), \"cp1252\")\n         self.assertEqual(r4._headers_encoding(), None)\n         self.assertEqual(r5._headers_encoding(), None)\n         self._assert_response_encoding(r5, \"utf-8\")\n\n@@ -1,11 +0,0 @@\n-import codecs\n-\n-from w3lib.encoding import resolve_encoding\n-\n-def encoding_exists(encoding):\n-    \"\"\"Returns ``True`` if encoding is valid, otherwise returns ``False``\"\"\"\n-    try:\n-        codecs.lookup(resolve_encoding(encoding))\n-    except LookupError:\n-        return False\n-    return True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5afe4f50c12348a8c31d4fda7bb833489043ef2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 61 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ from .config import Config\n def application(config):\n     app = Application(\"Scrapyd\")\n     http_port = config.getint('http_port', 6800)\n+    bind_address = config.get('bind_address', '0.0.0.0')\n \n     poller = QueuePoller(config)\n     eggstorage = FilesystemEggStorage(config)\n@@ -28,8 +29,8 @@ def application(config):\n \n     launcher = Launcher(config, app)\n     timer = TimerService(5, poller.poll)\n-    webservice = TCPServer(http_port, server.Site(Root(config, app)))\n-    log.msg(\"Scrapyd web console available at http://localhost:%s/\" % http_port)\n+    webservice = TCPServer(http_port, server.Site(Root(config, app)), interface=bind_address)\n+    log.msg(\"Scrapyd web console available at http://%s:%s/\" % (bind_address, http_port))\n \n     launcher.setServiceParent(app)\n     timer.setServiceParent(app)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de3a3b68dc20ccb003adbc2ba99ac6aa7382bf47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 183 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -123,7 +123,7 @@ try:\n except ImportError:\n     from distutils.core import setup\n else:\n-    setup_args['install_requires'] = ['Twisted>=2.5', 'w3lib', 'pyOpenSSL']\n+    setup_args['install_requires'] = ['Twisted>=2.5', 'w3lib>=1.1', 'pyOpenSSL']\n     if sys.version_info < (2, 6):\n         setup_args['install_requires'] += ['simplejson']\n     try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8eb0b11f8a6a20d4beb335da83a54ef778a82db0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 59 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@ from __future__ import with_statement\n import os, cPickle as pickle\n \n from scrapy import signals\n-from scrapy.exceptions import NotConfigured\n from scrapy.xlib.pydispatch import dispatcher\n \n class SpiderState(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e521da2e2f551aea6a5f2f58dcae5f1d7885e52c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 201 | Files Changed: 37 | Hunks: 61 | Methods Changed: 5 | Complexity Δ (Sum/Max): -30/0 | Churn Δ: 224 | Churn Cumulative: 6287 | Contributors (this commit): 10 | Commits (past 90d): 56 | Contributors (cumulative): 65 | DMM Complexity: 0.8904109589041096\n\nDIFF:\n@@ -11,11 +11,10 @@ command).\n \n \"\"\"\n \n-import sys, optparse, urllib\n+import sys, optparse, urllib, json\n from urlparse import urljoin\n \n from scrapy.utils.jsonrpc import jsonrpc_client_call, JsonRpcError\n-from scrapy.utils.py26 import json\n \n def get_commands():\n     return {\n\n@@ -7,8 +7,8 @@ __version__ = \"0.15.1\"\n \n import sys, os, warnings\n \n-if sys.version_info < (2,5):\n-    print \"Scrapy %s requires Python 2.5 or above\" % __version__\n+if sys.version_info < (2,6):\n+    print \"Scrapy %s requires Python 2.6 or above\" % __version__\n     sys.exit(1)\n \n # ignore noisy twisted deprecation warnings\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import sys\n import os\n import optparse\n\n@@ -2,8 +2,6 @@\n Base class for Scrapy commands\n \"\"\"\n \n-from __future__ import with_statement\n-\n import os\n from optparse import OptionGroup\n \n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import sys\n import os\n import glob\n@@ -8,6 +6,7 @@ import shutil\n import time\n import urllib2\n import netrc\n+import json\n from urlparse import urlparse, urljoin\n from subprocess import Popen, PIPE, check_call\n \n@@ -15,7 +14,6 @@ from w3lib.form import encode_multipart\n \n from scrapy.command import ScrapyCommand\n from scrapy.exceptions import UsageError\n-from scrapy.utils.py26 import json\n from scrapy.utils.http import basic_auth_header\n from scrapy.utils.conf import get_config, closest_scrapy_cfg\n \n\n@@ -3,11 +3,11 @@ import string\n import re\n import shutil\n from os.path import join, exists\n+from shutil import copytree, ignore_patterns\n \n import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.utils.template import render_templatefile, string_camelcase\n-from scrapy.utils.py26 import ignore_patterns, copytree\n from scrapy.exceptions import UsageError\n \n TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'project')\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import os\n from os.path import join, exists\n from time import time\n\n@@ -5,11 +5,10 @@ Item Exporters are used to export/serialize items into different formats.\n import csv\n import pprint\n import marshal\n+import json\n import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n \n-from scrapy.utils.py26 import json\n-\n \n __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter', \\\n     'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter', \\\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import os\n from time import time\n import cPickle as pickle\n\n@@ -4,7 +4,6 @@ Images Pipeline\n See documentation in topics/images.rst\n \"\"\"\n \n-from __future__ import with_statement\n import os\n import time\n import hashlib\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import os, cPickle as pickle\n \n from scrapy import signals\n\n@@ -1,13 +1,11 @@\n-from __future__ import with_statement\n-\n import os\n+import json\n from os.path import join, exists\n \n from scrapy.utils.pqueue import PriorityQueue\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n from scrapy.utils.misc import load_object\n from scrapy.utils.job import job_dir\n-from scrapy.utils.py26 import json\n from scrapy.stats import stats\n from scrapy import log\n \n\n@@ -4,14 +4,13 @@ based on different criterias.\n \n \"\"\"\n \n-from os.path import abspath, dirname, join\n from mimetypes import MimeTypes\n+from pkgutil import get_data\n from cStringIO import StringIO\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import isbinarytext\n-from scrapy.utils.py26 import get_data\n from scrapy.conf import settings\n \n class ResponseTypes(object):\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import sys\n import os\n import subprocess\n\n@@ -1,9 +1,8 @@\n-import unittest, cPickle as pickle\n+import unittest, json, cPickle as pickle\n from cStringIO import StringIO\n \n from scrapy.item import Item, Field\n from scrapy.utils.python import str_to_unicode\n-from scrapy.utils.py26 import json\n from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \\\n     PickleItemExporter, CsvItemExporter, XmlItemExporter, JsonLinesItemExporter, \\\n     JsonItemExporter\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n from unittest import TestCase\n \n from scrapy.http import Response, Request\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n from unittest import TestCase\n from os.path import join, abspath, dirname\n from cStringIO import StringIO\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import gzip, warnings\n from cStringIO import StringIO\n \n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import unittest\n from os.path import join\n \n\n@@ -1,11 +1,10 @@\n-import unittest\n+import unittest, json\n from cStringIO import StringIO\n \n from scrapy.utils.jsonrpc import jsonrpc_client_call, jsonrpc_server_call, \\\n     JsonRpcError, jsonrpc_errors\n from scrapy.utils.serialize import ScrapyJSONDecoder\n from scrapy.tests.test_utils_serialize import CrawlerMock\n-from scrapy.utils.py26 import json\n \n class urllib_mock(object):\n     def __init__(self, result=None, error=None):\n\n@@ -1,11 +1,11 @@\n import unittest\n import datetime\n+import json\n from decimal import Decimal\n \n from twisted.internet import defer\n \n from scrapy.utils.serialize import SpiderReferencer, ScrapyJSONEncoder, ScrapyJSONDecoder\n-from scrapy.utils.py26 import json\n from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n \n\n@@ -4,9 +4,9 @@ http://groups.google.com/group/json-rpc/web/json-rpc-2-0\n \"\"\"\n \n import urllib\n+import json\n import traceback\n \n-from scrapy.utils.py26 import json\n from scrapy.utils.serialize import ScrapyJSONDecoder\n \n # JSON-RPC 2.0 errors, as defined in:\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import os\n import sys\n import struct\n\n@@ -1,129 +0,0 @@\n-\"\"\"\n-This module provides functions added in Python 2.6, which weren't yet available\n-in Python 2.5. The Python 2.6 function is used when available.\n-\"\"\"\n-\n-import sys\n-import os\n-import fnmatch\n-import pkgutil\n-from shutil import copy2, copystat\n-\n-__all__ = ['cpu_count', 'copytree', 'ignore_patterns']\n-\n-try:\n-    import multiprocessing\n-    cpu_count = multiprocessing.cpu_count\n-except ImportError:\n-    def cpu_count():\n-        '''\n-        Returns the number of CPUs in the system\n-        '''\n-        if sys.platform == 'win32':\n-            try:\n-                num = int(os.environ['NUMBER_OF_PROCESSORS'])\n-            except (ValueError, KeyError):\n-                num = 0\n-        elif 'bsd' in sys.platform or sys.platform == 'darwin':\n-            try:\n-                num = int(os.popen('sysctl -n hw.ncpu').read())\n-            except ValueError:\n-                num = 0\n-        else:\n-            try:\n-                num = os.sysconf('SC_NPROCESSORS_ONLN')\n-            except (ValueError, OSError, AttributeError):\n-                num = 0\n-\n-        if num >= 1:\n-            return num\n-        else:\n-            raise NotImplementedError('cannot determine number of cpus')\n-\n-if sys.version_info >= (2, 6):\n-    from shutil import copytree, ignore_patterns\n-else:\n-    try:\n-        WindowsError\n-    except NameError:\n-        WindowsError = None\n-\n-    class Error(EnvironmentError):\n-        pass\n-\n-    def ignore_patterns(*patterns):\n-        def _ignore_patterns(path, names):\n-            ignored_names = []\n-            for pattern in patterns:\n-                ignored_names.extend(fnmatch.filter(names, pattern))\n-            return set(ignored_names)\n-        return _ignore_patterns\n-\n-    def copytree(src, dst, symlinks=False, ignore=None):\n-        names = os.listdir(src)\n-        if ignore is not None:\n-            ignored_names = ignore(src, names)\n-        else:\n-            ignored_names = set()\n-\n-        os.makedirs(dst)\n-        errors = []\n-        for name in names:\n-            if name in ignored_names:\n-                continue\n-            srcname = os.path.join(src, name)\n-            dstname = os.path.join(dst, name)\n-            try:\n-                if symlinks and os.path.islink(srcname):\n-                    linkto = os.readlink(srcname)\n-                    os.symlink(linkto, dstname)\n-                elif os.path.isdir(srcname):\n-                    copytree(srcname, dstname, symlinks, ignore)\n-                else:\n-                    copy2(srcname, dstname)\n-                # XXX What about devices, sockets etc.?\n-            except (IOError, os.error), why:\n-                errors.append((srcname, dstname, str(why)))\n-            # catch the Error from the recursive copytree so that we can\n-            # continue with other files\n-            except Error, err:\n-                errors.extend(err.args[0])\n-        try:\n-            copystat(src, dst)\n-        except OSError, why:\n-            if WindowsError is not None and isinstance(why, WindowsError):\n-                # Copying file access times may fail on Windows\n-                pass\n-            else:\n-                errors.extend((src, dst, str(why)))\n-        if errors:\n-            raise Error, errors\n-\n-try:\n-    import json\n-except ImportError:\n-    import simplejson as json\n-\n-\n-def _get_data(package, resource):\n-    loader = pkgutil.get_loader(package)\n-    if loader is None or not hasattr(loader, 'get_data'):\n-        return None\n-    mod = sys.modules.get(package) or loader.load_module(package)\n-    if mod is None or not hasattr(mod, '__file__'):\n-        return None\n-\n-    # Modify the resource name to be compatible with the loader.get_data\n-    # signature - an os.path format \"filename\" starting with the dirname of\n-    # the package's __file__\n-    parts = resource.split('/')\n-    parts.insert(0, os.path.dirname(mod.__file__))\n-    resource_name = os.path.join(*parts)\n-    return loader.get_data(resource_name)\n-\n-# pkgutil.get_data() not available in python 2.5\n-# see http://docs.python.org/release/2.5/lib/module-pkgutil.html\n-try:\n-    get_data = pkgutil.get_data\n-except AttributeError:\n-    get_data = _get_data\n\n@@ -1,5 +1,6 @@\n \"\"\"\n-Similar to scrapy.utils.py26, but for Python 2.7\n+This module provides functions added in Python 2.7, which weren't yet available\n+in Python 2.6. The Python 2.7 function is used when available.\n \"\"\"\n \n __all__ = ['OrderedDict']\n\n@@ -2,7 +2,7 @@\n This module contains essential stuff that should've come with Python itself ;)\n \n It also contains functions (or functionality) which is in Python versions\n-higher than 2.5 which is the lowest version supported by Scrapy.\n+higher than 2.5 which used to be the lowest version supported by Scrapy.\n \n \"\"\"\n import os\n\n@@ -1,12 +1,9 @@\n-from __future__ import with_statement\n-\n import os\n import struct\n import glob\n+import json\n from collections import deque\n \n-from scrapy.utils.py26 import json\n-\n \n class FifoMemoryQueue(object):\n     \"\"\"Memory FIFO queue.\"\"\"\n\n@@ -1,12 +1,12 @@\n import re\n import datetime\n import decimal\n+import json\n \n from twisted.internet import defer\n \n from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n-from scrapy.utils.py26 import json\n \n \n class SpiderReferencer(object):\n\n@@ -1,5 +1,4 @@\n \"\"\"Helper functions for working with templates\"\"\"\n-from __future__ import with_statement\n \n import os\n import re\n\n@@ -1,6 +1,6 @@\n-from twisted.web import resource\n+import json\n \n-from scrapy.utils.py26 import json\n+from twisted.web import resource\n \n class JsonResource(resource.Resource):\n \n\n@@ -1,11 +1,9 @@\n import glob\n from cStringIO import StringIO\n+from pkgutil import get_data\n from ConfigParser import SafeConfigParser, NoSectionError, NoOptionError\n \n from scrapy.utils.conf import closest_scrapy_cfg\n-from scrapy.utils.py26 import get_data\n-\n-__package__ = 'scrapyd' # required for compatibility with python 2.5\n \n class Config(object):\n     \"\"\"A ConfigParser wrapper to support defaults when calling instance\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n from glob import glob\n from os import path, makedirs, remove\n from shutil import copyfileobj, rmtree\n\n@@ -1,11 +1,11 @@\n import sys\n from datetime import datetime\n+from multiprocessing import cpu_count\n \n from twisted.internet import reactor, defer, protocol, error\n from twisted.application.service import Service\n from twisted.python import log\n \n-from scrapy.utils.py26 import cpu_count\n from scrapy.utils.python import stringify_dict\n from scrapyd.utils import get_crawl_args\n from .interfaces import IPoller, IEnvironment\n\n@@ -1,5 +1,3 @@\n-from __future__ import with_statement\n-\n import sys\n import os\n import shutil\n\n@@ -1,9 +1,8 @@\n import sqlite3\n import cPickle\n+import json\n from UserDict import DictMixin\n \n-from scrapy.utils.py26 import json\n-\n \n class SqliteDict(DictMixin):\n     \"\"\"SQLite-backed dictionary\"\"\"\n\n@@ -1,17 +1,13 @@\n-from __future__ import with_statement\n-\n import os\n+from pkgutil import get_data\n from cStringIO import StringIO\n \n from twisted.trial import unittest\n \n-from scrapy.utils.py26 import get_data\n from scrapyd.interfaces import IEggStorage\n from scrapyd.utils import get_crawl_args, get_spider_list\n from scrapyd import get_application\n \n-__package__ = 'scrapyd.tests' # required for compatibility with python 2.5\n-\n class UtilsTest(unittest.TestCase):\n \n     def test_get_crawl_args(self):\n\n@@ -3,8 +3,6 @@\n # It doesn't depend on setuptools, but if setuptools is available it'll use\n # some of its features, like package dependencies.\n \n-from __future__ import with_statement\n-\n from distutils.command.install_data import install_data\n from distutils.command.install import INSTALL_SCHEMES\n from subprocess import Popen, PIPE\n@@ -104,7 +102,6 @@ setup_args = {\n     'scripts': scripts,\n     'classifiers': [\n         'Programming Language :: Python',\n-        'Programming Language :: Python :: 2.5',\n         'Programming Language :: Python :: 2.6',\n         'Programming Language :: Python :: 2.7',\n         'License :: OSI Approved :: BSD License',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc7fc33833318bdafed640d3c1c6635bd3bacb88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 35 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -50,10 +50,8 @@ class BaseSpider(object_ref):\n         return self._settings\n \n     def start_requests(self):\n-        reqs = []\n         for url in self.start_urls:\n-            reqs.extend(arg_to_iter(self.make_requests_from_url(url)))\n-        return reqs\n+            yield self.make_requests_from_url(url)\n \n     def make_requests_from_url(self, url):\n         return Request(url, dont_filter=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f6179a927e553ca362365e23ed8b8878f773ba32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 115 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ class SitemapSpider(BaseSpider):\n         self._follow = [regex(x) for x in self.sitemap_follow]\n \n     def start_requests(self):\n-        return [Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls]\n+        return (Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls)\n \n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45685ea6cdc1860a5a0bf901cd989bd52bd8ac4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 280 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,10 @@\n+import json\n+from shutil import copytree, ignore_patterns\n+from multiprocessing import cpu_count\n+from pkgutil import get_data\n+\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n+import warnings\n+warnings.warn(\"Module `scrapy.utils.py26` is deprecated and will be removed in Scrapy 0.17\",\n+    ScrapyDeprecationWarning, stacklevel=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2909a60e95e1350415358bca8efe526c9a3f0c72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 227 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import gzip, warnings\n+import gzip, warnings, inspect\n from cStringIO import StringIO\n \n from twisted.trial import unittest\n@@ -24,6 +24,12 @@ class BaseSpiderTest(unittest.TestCase):\n         self.assertEqual(spider.name, 'example.com')\n         self.assertEqual(spider.start_urls, [])\n \n+    def test_start_requests(self):\n+        spider = self.spider_class('example.com')\n+        start_requests = spider.start_requests()\n+        self.assertTrue(inspect.isgenerator(start_requests))\n+        self.assertEqual(list(start_requests), [])\n+\n     def test_spider_args(self):\n         \"\"\"Constructor arguments are assigned to spider attributes\"\"\"\n         spider = self.spider_class('example.com', foo='bar')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b83177655e7c090e7a006590a943c0878bb3295", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 127 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -22,6 +22,7 @@ BOT_VERSION = '1.0'\n CLOSESPIDER_TIMEOUT = 0\n CLOSESPIDER_PAGECOUNT = 0\n CLOSESPIDER_ITEMCOUNT = 0\n+CLOSESPIDER_ERRORCOUNT = 0\n \n COMMANDS_MODULE = ''\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a508d4638b1f80bda73490b65bd791051d02ddb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 101 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,10 +21,6 @@ class CloseSpider(object):\n         self.crawler = crawler\n         self.timeout = settings.getint('CLOSESPIDER_TIMEOUT')\n         self.itemcount = settings.getint('CLOSESPIDER_ITEMCOUNT')\n-        # XXX: legacy support - remove for future releases\n-        if settings.getint('CLOSESPIDER_ITEMPASSED'):\n-            warnings.warn(\"CLOSESPIDER_ITEMPASSED setting is deprecated, use CLOSESPIDER_ITEMCOUNT instead\", ScrapyDeprecationWarning)\n-            self.pagecount = settings.getint('CLOSESPIDER_ITEMPASSED')\n         self.pagecount = settings.getint('CLOSESPIDER_PAGECOUNT')\n         self.errorcount = settings.getint('CLOSESPIDER_ERRORCOUNT')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
