{"custom_id": "keras#097673fe6fad06fcdcdcd8b9e9dec323ab315506", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 52 | Churn Cumulative: 88 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1961,28 +1961,57 @@ class Dice(LossFunctionWrapper):\n \n     Returns:\n         Dice loss value.\n+\n+    Example:\n+\n+    >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]],\n+    ...           [[[1.0], [1.0]], [[0.0], [0.0]]]]\n+    >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]],\n+    ...           [[[0.4], [0.0]], [[0.0], [0.9]]]]\n+    >>> axis = (1, 2, 3)\n+    >>> loss = keras.losses.dice(y_true, y_pred, axis=axis)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.5, 0.75757575], shape=(2,), dtype=float32)\n+\n+    >>> loss = keras.losses.dice(y_true, y_pred)\n+    >>> assert loss.shape == ()\n+    >>> loss\n+    array(0.6164384, shape=(), dtype=float32)\n+\n+    >>> y_true = np.array(y_true)\n+    >>> y_pred = np.array(y_pred)\n+    >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.5, 0.75757575], shape=(2,), dtype=float32)\n+\n     \"\"\"\n \n     def __init__(\n         self,\n         reduction=\"sum_over_batch_size\",\n         name=\"dice\",\n+        axis=None,\n     ):\n         super().__init__(\n             dice,\n             name=name,\n             reduction=reduction,\n+            axis=axis,\n         )\n+        self.axis = axis\n \n     def get_config(self):\n         return {\n             \"name\": self.name,\n             \"reduction\": self.reduction,\n+            \"axis\": self.axis,\n         }\n \n \n @keras_export(\"keras.losses.dice\")\n-def dice(y_true, y_pred):\n+def dice(y_true, y_pred, axis=None):\n     \"\"\"Computes the Dice loss value between `y_true` and `y_pred`.\n \n     Formula:\n@@ -1993,6 +2022,7 @@ def dice(y_true, y_pred):\n     Args:\n         y_true: tensor of true targets.\n         y_pred: tensor of predicted targets.\n+        axis: tuple for which dimensions the loss is calculated\n \n     Returns:\n         Dice loss value.\n@@ -2000,13 +2030,15 @@ def dice(y_true, y_pred):\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n \n-    inputs = ops.reshape(y_true, [-1])\n-    targets = ops.reshape(y_pred, [-1])\n+    inputs = y_true\n+    targets = y_pred\n \n-    intersection = ops.sum(inputs * targets)\n+    intersection = ops.sum(inputs * targets, axis=axis)\n     dice = ops.divide(\n         2.0 * intersection,\n-        ops.sum(y_true) + ops.sum(y_pred) + backend.epsilon(),\n+        ops.sum(y_true, axis=axis)\n+        + ops.sum(y_pred, axis=axis)\n+        + backend.epsilon(),\n     )\n \n     return 1 - dice\n\n@@ -1410,6 +1410,16 @@ class DiceTest(testing.TestCase):\n         output = losses.Dice()(y_true, y_pred)\n         self.assertAllClose(output, 0.77777773)\n \n+    def test_binary_segmentation_with_axis(self):\n+        y_true = np.array(\n+            [[[[1.0], [1.0]], [[0.0], [0.0]]], [[[1.0], [1.0]], [[0.0], [0.0]]]]\n+        )\n+        y_pred = np.array(\n+            [[[[0.0], [1.0]], [[0.0], [1.0]]], [[[0.4], [0.0]], [[0.0], [0.9]]]]\n+        )\n+        output = losses.Dice(axis=(1, 2, 3), reduction=None)(y_true, y_pred)\n+        self.assertAllClose(output, [0.5, 0.75757575])\n+\n \n class TverskyTest(testing.TestCase):\n     def test_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6e40533541566d518a14a30cbdae09d73b3df0f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 27 | Churn Cumulative: 145 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -60,7 +60,7 @@ def get(identifier):\n     The `identifier` may be the string name of a `DTypePolicy` class.\n \n     >>> policy = dtype_policies.get(\"mixed_bfloat16\")\n-    >>> type(loss)\n+    >>> type(policy)\n     <class '...FloatDTypePolicy'>\n \n     You can also specify `config` of the dtype policy to this function by\n@@ -70,7 +70,7 @@ def get(identifier):\n     >>> identifier = {\"class_name\": \"FloatDTypePolicy\",\n     ...               \"config\": {\"name\": \"float32\"}}\n     >>> policy = dtype_policies.get(identifier)\n-    >>> type(loss)\n+    >>> type(policy)\n     <class '...FloatDTypePolicy'>\n \n     Args:\n\n@@ -98,10 +98,16 @@ class Operation:\n         out of the box in most cases without forcing the user\n         to manually implement `get_config()`.\n         \"\"\"\n+        instance = super(Operation, cls).__new__(cls)\n+\n         # Generate a config to be returned by default by `get_config()`.\n         arg_names = inspect.getfullargspec(cls.__init__).args\n         kwargs.update(dict(zip(arg_names[1 : len(args) + 1], args)))\n-        instance = super(Operation, cls).__new__(cls)\n+        if \"dtype\" in kwargs and isinstance(\n+            kwargs[\"dtype\"], dtype_policies.DTypePolicy\n+        ):\n+            kwargs[\"dtype\"] = kwargs[\"dtype\"].get_config()\n+\n         # For safety, we only rely on auto-configs for a small set of\n         # serializable types.\n         supported_types = (str, int, float, bool, type(None))\n@@ -187,20 +193,19 @@ class Operation:\n \n     @classmethod\n     def from_config(cls, config):\n-        \"\"\"Creates a layer from its config.\n+        \"\"\"Creates an operation from its config.\n \n-        This method is the reverse of `get_config`,\n-        capable of instantiating the same layer from the config\n-        dictionary. It does not handle layer connectivity\n-        (handled by Network), nor weights (handled by `set_weights`).\n+        This method is the reverse of `get_config`, capable of instantiating the\n+        same operation from the config dictionary.\n \n         Args:\n-            config: A Python dictionary, typically the\n-                output of get_config.\n+            config: A Python dictionary, typically the output of `get_config`.\n \n         Returns:\n-            A layer instance.\n+            An operation instance.\n         \"\"\"\n+        if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n+            config[\"dtype\"] = dtype_policies.deserialize(config[\"dtype\"])\n         try:\n             return cls(**config)\n         except Exception as e:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#20bc26771ca64fe6874a6b856b812b71b1ed7f59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 43 | Churn Cumulative: 170 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -621,18 +621,17 @@ def export_model(model, filepath):\n             input_signature = [input_signature]\n         export_archive.add_endpoint(\"serve\", model.__call__, input_signature)\n     else:\n-        save_spec = _get_save_spec(model)\n-        if not save_spec or not model._called:\n+        input_signature = _get_input_signature(model)\n+        if not input_signature or not model._called:\n             raise ValueError(\n                 \"The model provided has never called. \"\n                 \"It must be called at least once before export.\"\n             )\n-        input_signature = [save_spec]\n         export_archive.add_endpoint(\"serve\", model.__call__, input_signature)\n     export_archive.write_out(filepath)\n \n \n-def _get_save_spec(model):\n+def _get_input_signature(model):\n     shapes_dict = getattr(model, \"_build_shapes_dict\", None)\n     if not shapes_dict:\n         return None\n@@ -654,16 +653,7 @@ def _get_save_spec(model):\n                 f\"Unsupported type {type(structure)} for {structure}\"\n             )\n \n-    if len(shapes_dict) == 1:\n-        value = list(shapes_dict.values())[0]\n-        return make_tensor_spec(value)\n-\n-    specs = {}\n-    for key, value in shapes_dict.items():\n-        key = key.rstrip(\"_shape\")\n-        specs[key] = make_tensor_spec(value)\n-\n-    return specs\n+    return [make_tensor_spec(value) for value in shapes_dict.values()]\n \n \n @keras_export(\"keras.layers.TFSMLayer\")\n\n@@ -145,6 +145,31 @@ class ExportArchiveTest(testing.TestCase, parameterized.TestCase):\n         )\n         revived_model.serve(bigger_input)\n \n+    def test_model_with_multiple_inputs(self):\n+\n+        class TwoInputsModel(models.Model):\n+            def call(self, x, y):\n+                return x + y\n+\n+            def build(self, y_shape, x_shape):\n+                self.built = True\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        model = TwoInputsModel()\n+        ref_input_x = tf.random.normal((3, 10))\n+        ref_input_y = tf.random.normal((3, 10))\n+        ref_output = model(ref_input_x, ref_input_y)\n+\n+        export_lib.export_model(model, temp_filepath)\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertAllClose(\n+            ref_output, revived_model.serve(ref_input_x, ref_input_y)\n+        )\n+        # Test with a different batch size\n+        revived_model.serve(\n+            tf.random.normal((6, 10)), tf.random.normal((6, 10))\n+        )\n+\n     @parameterized.named_parameters(\n         named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a05ac12efad3309462853605e67944550b9b2f0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 36 | Churn Cumulative: 749 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -87,7 +87,9 @@ def get(identifier):\n \n     if identifier is None:\n         return dtype_policy.dtype_policy()\n-    if isinstance(identifier, (FloatDTypePolicy, QuantizedDTypePolicy)):\n+    if isinstance(\n+        identifier, (DTypePolicy, FloatDTypePolicy, QuantizedDTypePolicy)\n+    ):\n         return identifier\n     if isinstance(identifier, dict):\n         return deserialize(identifier)\n\n@@ -570,6 +570,12 @@ class DTypePolicyGlobalFunctionsTest(test_case.TestCase):\n         policy = get(\"mixed_float16\")\n         self.assertEqual(policy.name, \"mixed_float16\")\n \n+        policy = get(DTypePolicy(\"bfloat16\"))\n+        self.assertEqual(policy.name, \"bfloat16\")\n+\n+        policy = get(FloatDTypePolicy(\"mixed_float16\"))\n+        self.assertEqual(policy.name, \"mixed_float16\")\n+\n     def test_get_valid_policy_quantized(self):\n         policy = get(\"int8_from_mixed_bfloat16\")\n         self.assertEqual(policy.name, \"int8_from_mixed_bfloat16\")\n@@ -577,6 +583,12 @@ class DTypePolicyGlobalFunctionsTest(test_case.TestCase):\n         policy = get(\"float8_from_float32\")\n         self.assertEqual(policy.name, \"float8_from_float32\")\n \n+        policy = get(QuantizedDTypePolicy(\"int8\", \"mixed_bfloat16\"))\n+        self.assertEqual(policy.name, \"int8_from_mixed_bfloat16\")\n+\n+        policy = get(QuantizedFloat8DTypePolicy(\"float8\", \"mixed_float16\"))\n+        self.assertEqual(policy.name, \"float8_from_mixed_float16\")\n+\n     def test_get_invalid_policy(self):\n         with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n             get(\"mixed_bfloat15\")\n\n@@ -1,6 +1,7 @@\n import numpy as np\n \n from keras.src import backend\n+from keras.src import dtype_policies\n from keras.src import testing\n from keras.src.backend.common import keras_tensor\n from keras.src.ops import numpy as knp\n@@ -43,6 +44,17 @@ class OpWithCustomConstructor(operation.Operation):\n         return keras_tensor.KerasTensor(x.shape, x.dtype)\n \n \n+class OpWithCustomDtype(operation.Operation):\n+    def __init__(self, dtype):\n+        super().__init__(dtype=dtype)\n+\n+    def call(self, x):\n+        return x\n+\n+    def compute_output_spec(self, x):\n+        return keras_tensor.KerasTensor(x.shape, x.dtype)\n+\n+\n class OperationTest(testing.TestCase):\n     def test_symbolic_call(self):\n         x = keras_tensor.KerasTensor(shape=(2, 3), name=\"x\")\n@@ -160,3 +172,11 @@ class OperationTest(testing.TestCase):\n             ValueError, \"must be a string and cannot contain character `/`.\"\n         ):\n             OpWithMultipleOutputs(name=\"test/op\")\n+\n+    def test_dtype(self):\n+        op = OpWithCustomDtype(dtype=\"bfloat16\")\n+        self.assertEqual(op._dtype_policy.name, \"bfloat16\")\n+\n+        policy = dtype_policies.DTypePolicy(\"mixed_bfloat16\")\n+        op = OpWithCustomDtype(dtype=policy)\n+        self.assertEqual(op._dtype_policy.name, \"mixed_bfloat16\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#505d8462bada68916e4404e6476f2f9a9214dd52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 188 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -562,13 +562,14 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n     # Axis is the output axis. By default, PyTorch, outputs to last axis.\n     # If axis is not last, change output to axis and shift remaining elements.\n     x = convert_to_tensor(x, dtype=torch.long)\n+    zero = convert_to_tensor(0, dtype=torch.long)\n \n     # Torch one_hot does not natively handle negative values, so we add some\n     # manual handling for negatives in the input to one_hot by using max(x, 0).\n     # The output will have some invalid results, so we set them back to 0 using\n     # `where` afterwards.\n     output = tnn.one_hot(maximum(x, 0), num_classes)\n-    output = where(expand_dims(x, axis=-1) >= 0, output, 0)\n+    output = where(expand_dims(x, axis=-1) >= 0, output, zero)\n     output = convert_to_tensor(output, dtype=dtype)\n     dims = output.dim()\n     if axis != -1 and axis != dims:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ac76385dc0c3df42106515502829d5c9f9c632cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 9 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -148,6 +148,7 @@ class CategoryEncoding(TFDataLayer):\n         return outputs\n \n     def _encode(self, inputs, count_weights=None):\n+        inputs = self.backend.core.convert_to_tensor(inputs)\n         if self.output_mode == \"multi_hot\":\n             return self.backend.nn.multi_hot(\n                 inputs, self.num_tokens, dtype=self.dtype, sparse=self.sparse\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d56ebccd170619222fa6327d9022e5aecdf6410", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 30 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -433,9 +433,6 @@ class FeatureSpaceTest(testing.TestCase):\n         out = fs(data)\n         self.assertEqual(out.shape, (148,))\n \n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\", reason=\"TODO: debug it\"\n-    )\n     def test_manual_kpl(self):\n         data = {\n             \"text\": [\"1st string\", \"2nd string\", \"3rd string\"],\n@@ -454,7 +451,7 @@ class FeatureSpaceTest(testing.TestCase):\n         )\n         fs.adapt(tf_data.Dataset.from_tensor_slices(data))\n         out = fs(data)\n-        self.assertEqual(out.shape, [3, 5])\n+        self.assertEqual(list(out.shape), [3, 5])\n \n     def test_no_adapt(self):\n         data = {\n@@ -469,12 +466,7 @@ class FeatureSpaceTest(testing.TestCase):\n         out = fs(data)\n         self.assertEqual(tuple(out.shape), (10, 32))\n \n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\", reason=\"TODO: debug it\"\n-    )\n     def test_saving(self):\n-        # Torch GPU: `model.predict(ds.batch(4))` fails on device placement\n-        # JAX GPU: out[0] and ref_out don't match. May be concat feature order?\n         cls = feature_space.FeatureSpace\n         fs = feature_space.FeatureSpace(\n             features={\n\n@@ -1,3 +1,4 @@\n+from keras.src import backend\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -140,6 +141,9 @@ def get_tensor_spec(t, dynamic_batch=False, name=None):\n def ensure_tensor(inputs, dtype=None):\n     \"\"\"Ensures the input is a Tensor, SparseTensor or RaggedTensor.\"\"\"\n     if not isinstance(inputs, (tf.Tensor, tf.SparseTensor, tf.RaggedTensor)):\n+        if backend.backend() == \"torch\" and backend.is_tensor(inputs):\n+            # Plain `np.asarray()` conversion fails with PyTorch.\n+            inputs = backend.convert_to_numpy(inputs)\n         inputs = tf.convert_to_tensor(inputs, dtype)\n     if dtype is not None and inputs.dtype != dtype:\n         inputs = tf.cast(inputs, dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b36f03731954037ea3b8a0f334368ceedad364b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 25 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -466,6 +466,7 @@ class FeatureSpaceTest(testing.TestCase):\n         out = fs(data)\n         self.assertEqual(tuple(out.shape), (10, 32))\n \n+    @pytest.mark.skipif(backend.backend() == \"numpy\", reason=\"TODO: debug it\")\n     def test_saving(self):\n         cls = feature_space.FeatureSpace\n         fs = feature_space.FeatureSpace(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c09296b10e0646c9631301ec0f20526c85ab9dc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 49 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -205,6 +205,7 @@ class Operation:\n             An operation instance.\n         \"\"\"\n         if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n+            config = config.copy()\n             config[\"dtype\"] = dtype_policies.deserialize(config[\"dtype\"])\n         try:\n             return cls(**config)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#789c3c44065032dac57fd4d21f4b88cecd0f111c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 184 | Lines Deleted: 309 | Files Changed: 13 | Hunks: 68 | Methods Changed: 29 | Complexity Δ (Sum/Max): -25/3 | Churn Δ: 493 | Churn Cumulative: 878 | Contributors (this commit): 7 | Commits (past 90d): 36 | Contributors (cumulative): 34 | DMM Complexity: 0.5283018867924528\n\nDIFF:\n@@ -459,7 +459,8 @@ def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n         cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype, sparse=sparse\n     )\n     if sparse:\n-        # We don't use `tf.sparse.bincount`, it doesn't handle negative indices.\n+        # We don't use `tf.sparse.bincount`, it doesn't handle negative indices\n+        # and has a rank limitation.\n         return tf.sparse.reduce_max(\n             one_hot_outputs, axis=reduction_axis, output_is_sparse=True\n         )\n\n@@ -2,6 +2,7 @@ from keras.src.api_export import keras_export\n from keras.src.backend import KerasTensor\n from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras.src.utils import backend_utils\n+from keras.src.utils import numerical_utils\n \n \n @keras_export(\"keras.layers.CategoryEncoding\")\n@@ -113,62 +114,20 @@ class CategoryEncoding(TFDataLayer):\n         self._allow_non_tensor_positional_args = True\n         self._convert_input_args = False\n \n-    def _count(self, inputs, axis=-1, count_weights=None):\n-        # We don't use `ops.bincount` here because its output has a dynamic\n-        # shape (the last dimension is calculated from the highest value of\n-        # `inputs`). Instead, we reimplement a narrower use case where\n-        # `minlength` and `maxlength` (not supported by `ops.bincount`) are a\n-        # static value and the same value of `self.num_tokens`. Also, we don't\n-        # need to support indices that are negative or greater than\n-        # `self.num_tokens`.\n-        reduction_axis = 1 if len(inputs.shape) > 1 else 0\n-\n-        one_hot_encoding = self.backend.nn.one_hot(\n-            inputs,\n-            self.num_tokens,\n-            axis=axis,\n-            dtype=self.dtype,\n-            sparse=self.sparse,\n-        )\n-        if count_weights is not None:\n-            split_weights = self.backend.numpy.split(\n-                count_weights,\n-                count_weights.shape[reduction_axis],\n-                reduction_axis,\n-            )\n-            stacked_weights = self.backend.numpy.stack(\n-                split_weights, axis=reduction_axis\n-            )\n-            one_hot_encoding = one_hot_encoding * stacked_weights\n-\n-        outputs = self.backend.numpy.sum(\n-            one_hot_encoding,\n-            axis=reduction_axis,\n-        )\n-        return outputs\n-\n     def _encode(self, inputs, count_weights=None):\n         inputs = self.backend.core.convert_to_tensor(inputs)\n-        if self.output_mode == \"multi_hot\":\n-            return self.backend.nn.multi_hot(\n-                inputs, self.num_tokens, dtype=self.dtype, sparse=self.sparse\n+        return numerical_utils.encode_categorical_inputs(\n+            inputs,\n+            output_mode=self.output_mode,\n+            depth=self.num_tokens,\n+            dtype=self.dtype,\n+            sparse=self.sparse,\n+            count_weights=count_weights,\n+            backend_module=self.backend,\n         )\n-        elif self.output_mode == \"one_hot\":\n-            return self.backend.nn.one_hot(\n-                inputs, self.num_tokens, dtype=self.dtype, sparse=self.sparse\n-            )\n-        elif self.output_mode == \"count\":\n-            return self._count(inputs, count_weights=count_weights)\n \n     def compute_output_shape(self, input_shape):\n-        # Treat rank-0 inputs inconsistent with actual output .\n         if (input_shape is not None) & (len(input_shape) == 0):\n-            # If output_mode == \"multi_hot\" return same shape .\n-            if self.output_mode == \"multi_hot\":\n-                return input_shape\n-            # If output_mode == \"one_hot\" append num_tokens to shape .\n-            # This is inline with actual output .\n-            elif self.output_mode == \"one_hot\":\n             return (self.num_tokens,)\n         if self.output_mode == \"one_hot\":\n             if input_shape[-1] != 1:\n\n@@ -319,12 +319,12 @@ class CategoryEncodingTest(testing.TestCase, parameterized.TestCase):\n         layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"count\")\n         input_array = np.array([1, 2, 3, 1])\n         expected_output = np.array([0, 2, 1, 1])\n-        output = layer._count(input_array)\n+        output = layer(input_array)\n         self.assertAllClose(expected_output, output)\n \n     def test_count_batched_samples(self):\n         layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"count\")\n         input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n         expected_output = np.array([[0, 2, 1, 1], [2, 1, 0, 1]])\n-        output = layer._count(input_array)\n+        output = layer(input_array)\n         self.assertAllClose(expected_output, output)\n\n@@ -100,9 +100,9 @@ class Discretization(TFDataLayer):\n \n         super().__init__(name=name, dtype=dtype)\n \n-        if sparse and backend.backend() != \"tensorflow\":\n+        if sparse and not backend.SUPPORTS_SPARSE_TENSORS:\n             raise ValueError(\n-                \"`sparse=True` can only be used with the \" \"TensorFlow backend.\"\n+                f\"`sparse=True` cannot be used with backend {backend.backend()}\"\n             )\n         if sparse and output_mode == \"int\":\n             raise ValueError(\n@@ -226,16 +226,14 @@ class Discretization(TFDataLayer):\n \n     def call(self, inputs):\n         indices = self.backend.numpy.digitize(inputs, self.bin_boundaries)\n-        outputs = numerical_utils.encode_categorical_inputs(\n+        return numerical_utils.encode_categorical_inputs(\n             indices,\n             output_mode=self.output_mode,\n             depth=len(self.bin_boundaries) + 1,\n             dtype=self.compute_dtype,\n+            sparse=self.sparse,\n             backend_module=self.backend,\n         )\n-        if self.sparse:\n-            return tf.sparse.from_dense(outputs)\n-        return outputs\n \n     def get_config(self):\n         return {\n\n@@ -10,6 +10,7 @@ from keras.src import layers\n from keras.src import models\n from keras.src import testing\n from keras.src.saving import saving_api\n+from keras.src.testing.test_utils import named_product\n \n \n class DiscretizationTest(testing.TestCase, parameterized.TestCase):\n@@ -37,37 +38,74 @@ class DiscretizationTest(testing.TestCase, parameterized.TestCase):\n         output = layer(np.array([[0.0, 0.1, 0.3]]))\n         self.assertTrue(output.dtype, \"int32\")\n \n-    @parameterized.parameters(\n-        [\n-            (\"int\", [[-1.0, 0.0, 0.1, 0.8, 1.2]], [[0, 1, 1, 2, 3]]),\n-            (\"one_hot\", [0.1, 0.8], [[0, 1, 0, 0], [0, 0, 1, 0]]),\n-            (\"multi_hot\", [[0.1, 0.8]], [[0, 1, 1, 0]]),\n-            (\n-                \"one_hot\",\n-                [[[0.15, 0.75], [0.85, 0.45]]],\n+    @parameterized.named_parameters(\n+        named_product(\n             [\n+                {\n+                    \"testcase_name\": \"int\",\n+                    \"output_mode\": \"int\",\n+                    \"input_array\": [[-1.0, 0.0, 0.1, 0.8, 1.2]],\n+                    \"expected_output\": [[0, 1, 1, 2, 3]],\n+                },\n+                {\n+                    \"testcase_name\": \"one_hot_rank_1\",\n+                    \"output_mode\": \"one_hot\",\n+                    \"input_array\": [0.1, 0.8],\n+                    \"expected_output\": [[0, 1, 0, 0], [0, 0, 1, 0]],\n+                },\n+                {\n+                    \"testcase_name\": \"multi_hot_rank_2\",\n+                    \"output_mode\": \"multi_hot\",\n+                    \"input_array\": [[0.1, 0.8]],\n+                    \"expected_output\": [[0, 1, 1, 0]],\n+                },\n+                {\n+                    \"testcase_name\": \"one_hot_rank_3\",\n+                    \"output_mode\": \"one_hot\",\n+                    \"input_array\": [[[0.15, 0.75], [0.85, 0.45]]],\n+                    \"expected_output\": [\n                         [\n                             [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]],\n                             [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0]],\n                         ]\n                     ],\n+                },\n+                {\n+                    \"testcase_name\": \"multi_hot_rank_3\",\n+                    \"output_mode\": \"multi_hot\",\n+                    \"input_array\": [[[0.15, 0.75], [0.85, 0.45]]],\n+                    \"expected_output\": [\n+                        [[0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0]]\n+                    ],\n+                },\n+                {\n+                    \"testcase_name\": \"count\",\n+                    \"output_mode\": \"count\",\n+                    \"input_array\": [[0.1, 0.8, 0.9]],\n+                    \"expected_output\": [[0, 1, 2, 0]],\n+                },\n+            ],\n+            sparse=(\n+                [True, False] if backend.SUPPORTS_SPARSE_TENSORS else [False]\n             ),\n-            (\n-                \"multi_hot\",\n-                [[[0.15, 0.75], [0.85, 0.45]]],\n-                [[[0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0]]],\n-            ),\n-            (\"count\", [[0.1, 0.8, 0.9]], [[0, 1, 2, 0]]),\n-        ]\n         )\n-    def test_correctness(self, output_mode, input_array, expected_output):\n+    )\n+    def test_correctness(\n+        self, output_mode, input_array, expected_output, sparse\n+    ):\n+        if output_mode == \"int\" and sparse:\n+            pytest.skip(\"sparse=True cannot be combined with output_mode=int\")\n+\n         input_array = np.array(input_array)\n         expected_output = np.array(expected_output)\n \n         layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=output_mode\n+            bin_boundaries=[0.0, 0.5, 1.0],\n+            output_mode=output_mode,\n+            sparse=sparse,\n         )\n         output = layer(input_array)\n+        self.assertSparse(output, sparse)\n         self.assertTrue(backend.is_tensor(output))\n         self.assertAllClose(output, expected_output)\n \n@@ -125,56 +163,3 @@ class DiscretizationTest(testing.TestCase, parameterized.TestCase):\n         model.save(fpath)\n         model = saving_api.load_model(fpath)\n         self.assertAllClose(layer(ref_input), ref_output)\n-\n-    @parameterized.parameters(\n-        [\n-            (\n-                \"one_hot\",\n-                [[-1.0, 0.2, 0.7, 1.2]],\n-                [\n-                    [\n-                        [1.0, 0.0, 0.0, 0.0],\n-                        [0.0, 1.0, 0.0, 0.0],\n-                        [0.0, 0.0, 1.0, 0.0],\n-                        [0.0, 0.0, 0.0, 1.0],\n-                    ]\n-                ],\n-            ),\n-            (\n-                \"multi_hot\",\n-                [[[-1.0], [0.2], [0.7], [1.2]]],\n-                [\n-                    [\n-                        [1.0, 0.0, 0.0, 0.0],\n-                        [0.0, 1.0, 0.0, 0.0],\n-                        [0.0, 0.0, 1.0, 0.0],\n-                        [0.0, 0.0, 0.0, 1.0],\n-                    ]\n-                ],\n-            ),\n-            (\n-                \"count\",\n-                [[-1.0], [0.2], [0.7], [1.2]],\n-                [\n-                    [1.0, 0.0, 0.0, 0.0],\n-                    [0.0, 1.0, 0.0, 0.0],\n-                    [0.0, 0.0, 1.0, 0.0],\n-                    [0.0, 0.0, 0.0, 1.0],\n-                ],\n-            ),\n-        ]\n-    )\n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\",\n-        reason=\"Sparse tensor only works in TensorFlow\",\n-    )\n-    def test_sparse_output(self, output_mode, input_array, expected_output):\n-        from keras.src.utils.module_utils import tensorflow as tf\n-\n-        x = np.array(input_array)\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], sparse=True, output_mode=output_mode\n-        )\n-        output = layer(x)\n-        self.assertTrue(isinstance(output, tf.SparseTensor))\n-        self.assertAllClose(output, np.array(expected_output))\n\n@@ -3,7 +3,7 @@ from keras.src.api_export import keras_export\n from keras.src.layers.layer import Layer\n from keras.src.utils import argument_validation\n from keras.src.utils import backend_utils\n-from keras.src.utils import tf_utils\n+from keras.src.utils import numerical_utils\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -135,16 +135,18 @@ class HashedCrossing(Layer):\n         return tuple(input_shape[0])[:-1] + (self.num_bins,)\n \n     def call(self, inputs):\n+        from keras.src.backend import tensorflow as tf_backend\n+\n         self._check_at_least_two_inputs(inputs)\n-        inputs = [tf_utils.ensure_tensor(x) for x in inputs]\n+        inputs = [tf_backend.convert_to_tensor(x) for x in inputs]\n         self._check_input_shape_and_type(inputs)\n \n         # Uprank to rank 2 for the cross_hashed op.\n         rank = len(inputs[0].shape)\n         if rank < 2:\n-            inputs = [tf_utils.expand_dims(x, -1) for x in inputs]\n+            inputs = [tf_backend.numpy.expand_dims(x, -1) for x in inputs]\n         if rank < 1:\n-            inputs = [tf_utils.expand_dims(x, -1) for x in inputs]\n+            inputs = [tf_backend.numpy.expand_dims(x, -1) for x in inputs]\n \n         # Perform the cross and convert to dense\n         outputs = tf.sparse.cross_hashed(inputs, self.num_bins)\n@@ -162,12 +164,13 @@ class HashedCrossing(Layer):\n             outputs = tf.reshape(outputs, [])\n \n         # Encode outputs.\n-        outputs = tf_utils.encode_categorical_inputs(\n+        outputs = numerical_utils.encode_categorical_inputs(\n             outputs,\n             output_mode=self.output_mode,\n             depth=self.num_bins,\n             sparse=self.sparse,\n             dtype=self.compute_dtype,\n+            backend_module=tf_backend,\n         )\n         return backend_utils.convert_tf_tensor(outputs, dtype=self.dtype)\n \n\n@@ -1,13 +1,15 @@\n import numpy as np\n import pytest\n import tensorflow as tf\n+from absl.testing import parameterized\n \n from keras.src import backend\n from keras.src import layers\n from keras.src import testing\n+from keras.src.testing.test_utils import named_product\n \n \n-class HashedCrossingTest(testing.TestCase):\n+class HashedCrossingTest(testing.TestCase, parameterized.TestCase):\n     def test_basics(self):\n         self.run_layer_test(\n             layers.HashedCrossing,\n@@ -41,17 +43,27 @@ class HashedCrossingTest(testing.TestCase):\n             run_mixed_precision_check=False,\n         )\n \n-    def test_correctness(self):\n+    @parameterized.named_parameters(\n+        named_product(\n+            sparse=(\n+                [True, False] if backend.backend() == \"tensorflow\" else [False]\n+            )\n+        )\n+    )\n+    def test_correctness(self, sparse):\n         layer = layers.HashedCrossing(num_bins=5)\n         feat1 = np.array([\"A\", \"B\", \"A\", \"B\", \"A\"])\n         feat2 = np.array([101, 101, 101, 102, 102])\n         output = layer((feat1, feat2))\n         self.assertAllClose(tf.constant([1, 4, 1, 1, 3]), output)\n \n-        layer = layers.HashedCrossing(num_bins=5, output_mode=\"one_hot\")\n+        layer = layers.HashedCrossing(\n+            num_bins=5, output_mode=\"one_hot\", sparse=sparse\n+        )\n         feat1 = np.array([\"A\", \"B\", \"A\", \"B\", \"A\"])\n         feat2 = np.array([101, 101, 101, 102, 102])\n         output = layer((feat1, feat2))\n+        self.assertSparse(output, sparse)\n         self.assertAllClose(\n             np.array(\n                 [\n\n@@ -2,7 +2,7 @@ from keras.src import backend\n from keras.src.api_export import keras_export\n from keras.src.layers.layer import Layer\n from keras.src.utils import backend_utils\n-from keras.src.utils import tf_utils\n+from keras.src.utils import numerical_utils\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -209,10 +209,12 @@ class Hashing(Layer):\n         self.supports_jit = False\n \n     def call(self, inputs):\n-        if not isinstance(\n-            inputs, (tf.Tensor, tf.SparseTensor, tf.RaggedTensor)\n-        ):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        from keras.src.backend import tensorflow as tf_backend\n+\n+        inputs = tf_backend.convert_to_tensor(inputs)\n+        if self.output_mode == \"one_hot\" and inputs.shape[-1] == 1:\n+            # One hot only unpranks if the final dimension is not 1.\n+            inputs = tf_backend.numpy.squeeze(inputs, axis=-1)\n         if isinstance(inputs, tf.SparseTensor):\n             indices = tf.SparseTensor(\n                 indices=inputs.indices,\n@@ -221,12 +223,13 @@ class Hashing(Layer):\n             )\n         else:\n             indices = self._hash_values_to_bins(inputs)\n-        outputs = tf_utils.encode_categorical_inputs(\n+        outputs = numerical_utils.encode_categorical_inputs(\n             indices,\n             output_mode=self.output_mode,\n             depth=self.num_bins,\n             sparse=self.sparse,\n             dtype=self.dtype,\n+            backend_module=tf_backend,\n         )\n         return backend_utils.convert_tf_tensor(outputs)\n \n\n@@ -269,7 +269,7 @@ class HashingTest(testing.TestCase, parameterized.TestCase):\n \n         model = models.Model(inputs, outputs)\n         output_data = model(input_array)\n-        self.assertAllEqual(expected_output, output_data)\n+        self.assertAllClose(expected_output, output_data)\n \n     def test_multi_hot_output(self):\n         input_array = np.array([[0, 1, 2, 3, 4]])\n@@ -284,7 +284,7 @@ class HashingTest(testing.TestCase, parameterized.TestCase):\n \n         model = models.Model(inputs, outputs)\n         output_data = model(input_array)\n-        self.assertAllEqual(expected_output, output_data)\n+        self.assertAllClose(expected_output, output_data)\n \n     @parameterized.named_parameters(\n         (\n\n@@ -5,7 +5,7 @@ import numpy as np\n from keras.src import backend\n from keras.src.layers.layer import Layer\n from keras.src.utils import argument_validation\n-from keras.src.utils import tf_utils\n+from keras.src.utils import numerical_utils\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -565,6 +565,8 @@ class IndexLookup(Layer):\n         return backend.KerasTensor(output_shape, dtype=output_dtype)\n \n     def adapt(self, data, steps=None):\n+        from keras.src.backend import tensorflow as tf_backend\n+\n         self.reset_state()\n         if isinstance(data, tf.data.Dataset):\n             if steps is not None:\n@@ -572,7 +574,9 @@ class IndexLookup(Layer):\n             for batch in data:\n                 self.update_state(batch)\n         else:\n-            data = tf_utils.ensure_tensor(data, dtype=self.vocabulary_dtype)\n+            data = tf_backend.convert_to_tensor(\n+                data, dtype=self.vocabulary_dtype\n+            )\n             if data.shape.rank == 1:\n                 # A plain list of strings\n                 # is treated as as many documents\n@@ -581,6 +585,8 @@ class IndexLookup(Layer):\n         self.finalize_state()\n \n     def update_state(self, data):\n+        from keras.src.backend import tensorflow as tf_backend\n+\n         if self._has_input_vocabulary:\n             raise ValueError(\n                 f\"Cannot adapt layer '{self.name}' after setting a static \"\n@@ -588,7 +594,7 @@ class IndexLookup(Layer):\n                 \"`set_vocabulary()` method.\"\n             )\n \n-        data = tf_utils.ensure_tensor(data, dtype=self.vocabulary_dtype)\n+        data = tf_backend.convert_to_tensor(data, dtype=self.vocabulary_dtype)\n         if data.shape.rank == 0:\n             data = tf.expand_dims(data, 0)\n         if data.shape.rank == 1:\n@@ -698,9 +704,11 @@ class IndexLookup(Layer):\n             self.num_documents.assign(0)\n \n     def call(self, inputs):\n+        from keras.src.backend import tensorflow as tf_backend\n+\n         self._ensure_known_vocab_size()\n \n-        inputs = tf_utils.ensure_tensor(inputs, dtype=self._key_dtype)\n+        inputs = tf_backend.convert_to_tensor(inputs, dtype=self._key_dtype)\n         original_shape = inputs.shape\n         # Some ops will not handle scalar input, so uprank to rank 1.\n         if inputs.shape.rank == 0:\n@@ -731,14 +739,26 @@ class IndexLookup(Layer):\n         idf_weights = (\n             self.idf_weights_const if self.output_mode == \"tf_idf\" else None\n         )\n-        return tf_utils.encode_categorical_inputs(\n+        output = numerical_utils.encode_categorical_inputs(\n             lookups,\n-            output_mode=self.output_mode,\n+            output_mode=(\n+                \"count\" if self.output_mode == \"tf_idf\" else self.output_mode\n+            ),\n             depth=depth,\n             dtype=self._value_dtype,\n             sparse=self.sparse,\n-            idf_weights=idf_weights,\n+            backend_module=tf_backend,\n         )\n+        if self.output_mode == \"tf_idf\":\n+            if idf_weights is None:\n+                raise ValueError(\n+                    \"When `output_mode` is `'tf_idf'`, `idf_weights` must be \"\n+                    \"provided.\"\n+                )\n+            output = tf_backend.numpy.multiply(\n+                tf_backend.core.cast(output, idf_weights.dtype), idf_weights\n+            )\n+        return output\n \n     def _lookup_dense(self, inputs):\n         \"\"\"Lookup table values for a dense Tensor, handling masking and OOV.\"\"\"\n\n@@ -8,7 +8,6 @@ from keras.src.layers.preprocessing.string_lookup import StringLookup\n from keras.src.saving import serialization_lib\n from keras.src.utils import argument_validation\n from keras.src.utils import backend_utils\n-from keras.src.utils import tf_utils\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -413,6 +412,8 @@ class TextVectorization(Layer):\n                 repeating dataset, you must specify the `steps` argument. This\n                 argument is not supported with array inputs or list inputs.\n         \"\"\"\n+        from keras.src.backend import tensorflow as tf_backend\n+\n         self.reset_state()\n         if isinstance(data, tf.data.Dataset):\n             if steps is not None:\n@@ -420,7 +421,7 @@ class TextVectorization(Layer):\n             for batch in data:\n                 self.update_state(batch)\n         else:\n-            data = tf_utils.ensure_tensor(data, dtype=\"string\")\n+            data = tf_backend.convert_to_tensor(data, dtype=\"string\")\n             if data.shape.rank == 1:\n                 # A plain list of strings\n                 # is treated as as many documents\n@@ -519,7 +520,9 @@ class TextVectorization(Layer):\n         self._lookup_layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n \n     def _preprocess(self, inputs):\n-        inputs = tf_utils.ensure_tensor(inputs, dtype=tf.string)\n+        from keras.src.backend import tensorflow as tf_backend\n+\n+        inputs = tf_backend.convert_to_tensor(inputs, dtype=tf.string)\n         if self._standardize in (\"lower\", \"lower_and_strip_punctuation\"):\n             inputs = tf.strings.lower(inputs)\n         if self._standardize in (\n\n@@ -96,48 +96,62 @@ def encode_categorical_inputs(\n     inputs,\n     output_mode,\n     depth,\n-    dtype=\"float32\",\n+    dtype,\n+    sparse=False,\n+    count_weights=None,\n     backend_module=None,\n ):\n-    \"\"\"Encodes categorical inputs according to output_mode.\"\"\"\n+    \"\"\"Encodes categorical inputs according to output_mode.\n+\n+    Args:\n+        inputs: the inputs to encode.\n+        output_mode: one of `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or `\"count\"`.\n+        depth: number of classes, this will be the last dimension of the output.\n+        dtype: the dtype of the output, unless `count_weights` is not `None`.\n+        sparse: whether the output should be sparse for backends supporting it.\n+        count_weights: weights to apply if `output_mode` is `\"count\"`.\n+        backend_module: the backend to use instead of the curren one.\n+    Returns: the encoded inputs.\n+    \"\"\"\n     backend_module = backend_module or backend\n \n     if output_mode == \"int\":\n         return backend_module.cast(inputs, dtype=dtype)\n \n-    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n-    original_shape = backend_module.shape(inputs)\n-    rank_of_inputs = len(original_shape)\n+    rank_of_inputs = len(backend_module.shape(inputs))\n \n     # In all cases, we should uprank scalar input to a single sample.\n     if rank_of_inputs == 0:\n-        # We need to update `rank_of_inputs`\n-        # If necessary.\n+        # We need to update `rank_of_inputs` if necessary.\n         inputs = backend_module.numpy.expand_dims(inputs, -1)\n-    elif rank_of_inputs > 2:\n-        # The `count` mode does not support inputs with a rank greater than 2.\n-        if not binary_output:\n-            raise ValueError(\n-                \"When output_mode is anything other than \"\n-                \"`'multi_hot', 'one_hot', or 'int'`, \"\n-                \"the rank must be 2 or less. \"\n-                f\"Received output_mode: {output_mode} \"\n-                f\"and input shape: {original_shape}, \"\n-                f\"which would result in output rank {rank_of_inputs}.\"\n-            )\n \n-    if binary_output:\n-        if output_mode == \"one_hot\":\n-            bincounts = backend_module.nn.one_hot(inputs, depth)\n-        elif output_mode == \"multi_hot\":\n-            one_hot_input = backend_module.nn.one_hot(inputs, depth)\n-            bincounts = backend_module.numpy.where(\n-                backend_module.numpy.any(one_hot_input, axis=-2), 1, 0\n+    if output_mode == \"multi_hot\":\n+        return backend_module.nn.multi_hot(\n+            inputs, depth, dtype=dtype, sparse=sparse\n         )\n-    else:\n-        bincounts = backend_module.numpy.bincount(\n-            inputs,\n-            minlength=depth,\n+    elif output_mode == \"one_hot\":\n+        return backend_module.nn.one_hot(\n+            inputs, depth, dtype=dtype, sparse=sparse\n         )\n-    bincounts = backend_module.cast(bincounts, dtype)\n-    return bincounts\n+    elif output_mode == \"count\":\n+        # We don't use `ops.bincount` because its output has a dynamic shape\n+        # (last dimension is the highest value of `inputs`). We implement a\n+        # narrower use case where `minlength` and `maxlength` (not supported by\n+        # `ops.bincount`) are the same and static value: `depth`. We also don't\n+        # need to support indices that are negative or greater than `depth`.\n+        reduction_axis = 1 if len(inputs.shape) > 1 else 0\n+\n+        if count_weights is not None:\n+            dtype = count_weights.dtype\n+        one_hot_encoding = backend_module.nn.one_hot(\n+            inputs, depth, dtype=dtype, sparse=sparse\n+        )\n+        if count_weights is not None:\n+            count_weights = backend_module.numpy.expand_dims(count_weights, -1)\n+            one_hot_encoding = one_hot_encoding * count_weights\n+\n+        outputs = backend_module.numpy.sum(\n+            one_hot_encoding,\n+            axis=reduction_axis,\n+        )\n+        return outputs\n\n@@ -1,117 +1,6 @@\n-from keras.src import backend\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n-def expand_dims(inputs, axis):\n-    \"\"\"Expand dims on sparse, ragged, or dense tensors.\"\"\"\n-    if isinstance(inputs, tf.SparseTensor):\n-        return tf.sparse.expand_dims(inputs, axis)\n-    else:\n-        return tf.expand_dims(inputs, axis)\n-\n-\n-def sparse_bincount(inputs, depth, binary_output, dtype, count_weights=None):\n-    \"\"\"Apply binary or count encoding to an input and return a sparse tensor.\"\"\"\n-    result = tf.sparse.bincount(\n-        inputs,\n-        weights=count_weights,\n-        minlength=depth,\n-        maxlength=depth,\n-        axis=-1,\n-        binary_output=binary_output,\n-    )\n-    result = tf.cast(result, dtype)\n-    if inputs.shape.rank == 1:\n-        output_shape = (depth,)\n-    else:\n-        batch_size = tf.shape(result)[0]\n-        output_shape = (batch_size, depth)\n-    result = tf.SparseTensor(\n-        indices=result.indices, values=result.values, dense_shape=output_shape\n-    )\n-    return result\n-\n-\n-def dense_bincount(inputs, depth, binary_output, dtype, count_weights=None):\n-    \"\"\"Apply binary or count encoding to an input.\"\"\"\n-    result = tf.math.bincount(\n-        inputs,\n-        weights=count_weights,\n-        minlength=depth,\n-        maxlength=depth,\n-        dtype=dtype,\n-        axis=-1,\n-        binary_output=binary_output,\n-    )\n-    if inputs.shape.rank == 1:\n-        result.set_shape(tf.TensorShape((depth,)))\n-    else:\n-        batch_size = inputs.shape.as_list()[0]\n-        result.set_shape(tf.TensorShape((batch_size, depth)))\n-    return result\n-\n-\n-def encode_categorical_inputs(\n-    inputs,\n-    output_mode,\n-    depth,\n-    dtype=\"float32\",\n-    sparse=False,\n-    count_weights=None,\n-    idf_weights=None,\n-):\n-    \"\"\"Encodes categoical inputs according to output_mode.\"\"\"\n-    if output_mode == \"int\":\n-        return tf.identity(tf.cast(inputs, dtype))\n-\n-    original_shape = inputs.shape\n-    # In all cases, we should uprank scalar input to a single sample.\n-    if inputs.shape.rank == 0:\n-        inputs = expand_dims(inputs, -1)\n-    # One hot will unprank only if the final output dimension is not already 1.\n-    if output_mode == \"one_hot\":\n-        if inputs.shape[-1] != 1:\n-            inputs = expand_dims(inputs, -1)\n-\n-    # TODO(b/190445202): remove output rank restriction.\n-    if inputs.shape.rank > 2:\n-        raise ValueError(\n-            \"When output_mode is not `'int'`, maximum supported output rank \"\n-            f\"is 2. Received output_mode {output_mode} and input shape \"\n-            f\"{original_shape}, \"\n-            f\"which would result in output rank {inputs.shape.rank}.\"\n-        )\n-\n-    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n-    if sparse:\n-        bincounts = sparse_bincount(\n-            inputs, depth, binary_output, dtype, count_weights\n-        )\n-    else:\n-        bincounts = dense_bincount(\n-            inputs, depth, binary_output, dtype, count_weights\n-        )\n-\n-    if output_mode != \"tf_idf\":\n-        return bincounts\n-\n-    if idf_weights is None:\n-        raise ValueError(\n-            \"When output mode is `'tf_idf'`, idf_weights must be provided. \"\n-            f\"Received: output_mode={output_mode} and idf_weights={idf_weights}\"\n-        )\n-\n-    if sparse:\n-        value_weights = tf.gather(idf_weights, bincounts.indices[:, -1])\n-        return tf.SparseTensor(\n-            bincounts.indices,\n-            value_weights * bincounts.values,\n-            bincounts.dense_shape,\n-        )\n-    else:\n-        return tf.multiply(tf.cast(bincounts, idf_weights.dtype), idf_weights)\n-\n-\n def get_tensor_spec(t, dynamic_batch=False, name=None):\n     \"\"\"Returns a `TensorSpec` given a single `Tensor` or `TensorSpec`.\"\"\"\n     if isinstance(t, tf.TypeSpec):\n@@ -136,15 +25,3 @@ def get_tensor_spec(t, dynamic_batch=False, name=None):\n     shape = tf.TensorShape(shape_list)\n     spec._shape = shape\n     return spec\n-\n-\n-def ensure_tensor(inputs, dtype=None):\n-    \"\"\"Ensures the input is a Tensor, SparseTensor or RaggedTensor.\"\"\"\n-    if not isinstance(inputs, (tf.Tensor, tf.SparseTensor, tf.RaggedTensor)):\n-        if backend.backend() == \"torch\" and backend.is_tensor(inputs):\n-            # Plain `np.asarray()` conversion fails with PyTorch.\n-            inputs = backend.convert_to_numpy(inputs)\n-        inputs = tf.convert_to_tensor(inputs, dtype)\n-    if dtype is not None and inputs.dtype != dtype:\n-        inputs = tf.cast(inputs, dtype)\n-    return inputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
