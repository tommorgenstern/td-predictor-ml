{"custom_id": "keras#042c5cf5532aa76aea1f54e6c39b2d6895e38065", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 711 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -170,6 +170,12 @@ if flax is not None:\n         def from_config(cls, config):\n             return cls(**config)\n \n+    FLAX_MODEL_CLASSES = {\n+        \"FlaxTrainingIndependentModel\": FlaxTrainingIndependentModel,\n+        \"FlaxBatchNormModel\": FlaxBatchNormModel,\n+        \"FlaxDropoutModel\": FlaxDropoutModel,\n+    }\n+\n \n @pytest.mark.skipif(\n     backend.backend() != \"jax\",\n@@ -382,7 +388,7 @@ class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n     @parameterized.named_parameters(\n         {\n             \"testcase_name\": \"training_independent_bound_method\",\n-            \"flax_model_class\": FlaxTrainingIndependentModel,\n+            \"flax_model_class\": \"FlaxTrainingIndependentModel\",\n             \"flax_model_method\": \"forward\",\n             \"init_kwargs\": {},\n             \"trainable_weights\": 8,\n@@ -392,7 +398,7 @@ class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n         },\n         {\n             \"testcase_name\": \"training_rng_unbound_method\",\n-            \"flax_model_class\": FlaxDropoutModel,\n+            \"flax_model_class\": \"FlaxDropoutModel\",\n             \"flax_model_method\": None,\n             \"init_kwargs\": {\n                 \"method\": flax_dropout_wrapper,\n@@ -404,7 +410,7 @@ class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n         },\n         {\n             \"testcase_name\": \"training_rng_state_no_method\",\n-            \"flax_model_class\": FlaxBatchNormModel,\n+            \"flax_model_class\": \"FlaxBatchNormModel\",\n             \"flax_model_method\": None,\n             \"init_kwargs\": {},\n             \"trainable_weights\": 13,\n@@ -423,6 +429,8 @@ class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n         non_trainable_weights,\n         non_trainable_params,\n     ):\n+        flax_model_class = FLAX_MODEL_CLASSES.get(flax_model_class)\n+\n         def create_wrapper(**kwargs):\n             params = kwargs.pop(\"params\") if \"params\" in kwargs else None\n             state = kwargs.pop(\"state\") if \"state\" in kwargs else None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc398f7b072a8d177af698370eca9bd9d0e30056", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 720 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -170,10 +170,11 @@ if flax is not None:\n         def from_config(cls, config):\n             return cls(**config)\n \n-    FLAX_MODEL_CLASSES = {\n+    FLAX_OBJECTS = {\n         \"FlaxTrainingIndependentModel\": FlaxTrainingIndependentModel,\n         \"FlaxBatchNormModel\": FlaxBatchNormModel,\n         \"FlaxDropoutModel\": FlaxDropoutModel,\n+        \"flax_dropout_wrapper\": flax_dropout_wrapper,\n     }\n \n \n@@ -401,7 +402,7 @@ class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n             \"flax_model_class\": \"FlaxDropoutModel\",\n             \"flax_model_method\": None,\n             \"init_kwargs\": {\n-                \"method\": flax_dropout_wrapper,\n+                \"method\": \"flax_dropout_wrapper\",\n             },\n             \"trainable_weights\": 8,\n             \"trainable_params\": 648226,\n@@ -429,7 +430,9 @@ class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n         non_trainable_weights,\n         non_trainable_params,\n     ):\n-        flax_model_class = FLAX_MODEL_CLASSES.get(flax_model_class)\n+        flax_model_class = FLAX_OBJECTS.get(flax_model_class)\n+        if \"method\" in init_kwargs:\n+            init_kwargs[\"method\"] = FLAX_OBJECTS.get(init_kwargs[\"method\"])\n \n         def create_wrapper(**kwargs):\n             params = kwargs.pop(\"params\") if \"params\" in kwargs else None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08711c5cc48b569f4a4048a4a833217f13af0707", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 673 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,5 @@\n import inspect\n \n-import jax\n import numpy as np\n \n from keras import backend\n@@ -10,6 +9,11 @@ from keras.saving import serialization_lib\n from keras.utils import shape_utils\n from keras.utils import tracking\n \n+try:\n+    import jax\n+except ImportError:\n+    jax = None\n+\n \n @keras_export(\"keras.layers.JaxLayer\")\n class JaxLayer(Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2da19b8cfaae9c78b147d37abde0aa0928b48151", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 722 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -420,6 +420,7 @@ class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n             \"non_trainable_params\": 536,\n         },\n     )\n+    @pytest.mark.skipif(flax is None, reason=\"Flax library is not available.\")\n     def test_flax_layer(\n         self,\n         flax_model_class,\n@@ -575,6 +576,7 @@ class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n         test_output = model(test_inputs)\n         self.assertAllClose(test_output, np.ones((2, 60, 3)))\n \n+    @pytest.mark.skipif(flax is None, reason=\"Flax library is not available.\")\n     def test_with_flax_state_no_params(self):\n         class MyFlaxLayer(flax.linen.Module):\n             @flax.linen.compact\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d1cab147ba7a21e260577b301dd5481c8ce2d49e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 7 | Churn Cumulative: 680 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -8,11 +8,7 @@ from keras.layers.layer import Layer\n from keras.saving import serialization_lib\n from keras.utils import shape_utils\n from keras.utils import tracking\n-\n-try:\n-    import jax\n-except ImportError:\n-    jax = None\n+from keras.utils.module_utils import jax\n \n \n @keras_export(\"keras.layers.JaxLayer\")\n\n@@ -40,3 +40,4 @@ tensorflow = LazyModule(\"tensorflow\")\n gfile = LazyModule(\"tensorflow.io.gfile\", pip_name=\"tensorflow\")\n tensorflow_io = LazyModule(\"tensorflow_io\")\n scipy = LazyModule(\"scipy\")\n+jax = LazyModule(\"jax\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3255c9d8ec1910af0b399726114fd587a682310a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 81 | Churn Cumulative: 2648 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,7 +36,7 @@ _ASSETS_DIRNAME = \"assets\"\n \n \n def save_model(model, filepath, weights_format=\"h5\"):\n-    \"\"\"Save a zip-archive representing a Keras model to the given filepath.\n+    \"\"\"Save a zip-archive representing a Keras model to the given file or path.\n \n     The zip-based archive contains the following structure:\n \n@@ -59,12 +59,6 @@ def save_model(model, filepath, weights_format=\"h5\"):\n     container (list, tuple, or dict), and the container is referenced via a\n     layer attribute.\n     \"\"\"\n-    filepath = str(filepath)\n-    if not filepath.endswith(\".keras\"):\n-        raise ValueError(\n-            \"Invalid `filepath` argument: expected a `.keras` extension. \"\n-            f\"Received: filepath={filepath}\"\n-        )\n     if weights_format == \"h5\" and h5py is None:\n         raise ImportError(\"h5py must be installed in order to save a model.\")\n \n@@ -77,6 +71,28 @@ def save_model(model, filepath, weights_format=\"h5\"):\n             stacklevel=2,\n         )\n \n+    if isinstance(filepath, io.IOBase):\n+        _save_model_to_fileobj(model, filepath, weights_format)\n+        return\n+\n+    filepath = str(filepath)\n+    if not filepath.endswith(\".keras\"):\n+        raise ValueError(\n+            \"Invalid `filepath` argument: expected a `.keras` extension. \"\n+            f\"Received: filepath={filepath}\"\n+        )\n+    if file_utils.is_remote_path(filepath):\n+        # Remote path. Zip to local memory byte io and copy to remote\n+        zip_filepath = io.BytesIO()\n+        _save_model_to_fileobj(model, zip_filepath, weights_format)\n+        with file_utils.File(filepath, \"wb\") as f:\n+            f.write(zip_filepath.getvalue())\n+    else:\n+        with open(filepath, \"wb\") as f:\n+            _save_model_to_fileobj(model, f, weights_format)\n+\n+\n+def _save_model_to_fileobj(model, fileobj, weights_format):\n     with ObjectSharingScope():\n         serialized_model_dict = serialize_keras_object(model)\n     config_json = json.dumps(serialized_model_dict)\n@@ -86,13 +102,8 @@ def save_model(model, filepath, weights_format=\"h5\"):\n             \"date_saved\": datetime.datetime.now().strftime(\"%Y-%m-%d@%H:%M:%S\"),\n         }\n     )\n-    if file_utils.is_remote_path(filepath):\n-        # Remote path. Zip to local memory byte io and copy to remote\n-        zip_filepath = io.BytesIO()\n-    else:\n-        zip_filepath = filepath\n \n-    with zipfile.ZipFile(zip_filepath, \"w\") as zf:\n+    with zipfile.ZipFile(fileobj, \"w\") as zf:\n         with zf.open(_METADATA_FILENAME, \"w\") as f:\n             f.write(metadata_json.encode())\n         with zf.open(_CONFIG_FILENAME, \"w\") as f:\n@@ -123,24 +134,28 @@ def save_model(model, filepath, weights_format=\"h5\"):\n         weights_store.close()\n         asset_store.close()\n \n-    if file_utils.is_remote_path(filepath):\n-        with file_utils.File(filepath, \"wb\") as f:\n-            f.write(zip_filepath.getvalue())\n-\n \n def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     \"\"\"Load a zip archive representing a Keras model.\"\"\"\n-\n+    if isinstance(filepath, io.IOBase):\n+        return _load_model_from_fileobj(\n+            filepath, custom_objects, compile, safe_mode\n+        )\n+    else:\n         filepath = str(filepath)\n         if not filepath.endswith(\".keras\"):\n             raise ValueError(\n                 \"Invalid filename: expected a `.keras` extension. \"\n                 f\"Received: filepath={filepath}\"\n             )\n+        with open(filepath, \"rb\") as f:\n+            return _load_model_from_fileobj(\n+                f, custom_objects, compile, safe_mode\n+            )\n \n-    with file_utils.File(filepath, mode=\"r+b\") as gfile_handle, zipfile.ZipFile(\n-        gfile_handle, \"r\"\n-    ) as zf:\n+\n+def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n+    with zipfile.ZipFile(fileobj, \"r\") as zf:\n         with zf.open(_CONFIG_FILENAME, \"r\") as f:\n             config_json = f.read()\n \n\n@@ -4,6 +4,7 @@ import json\n import os\n import warnings\n import zipfile\n+from io import BytesIO\n from pathlib import Path\n from unittest import mock\n \n@@ -586,6 +587,29 @@ class SavingTest(testing.TestCase):\n             np.array(new_model.layers[2].kernel), new_layer_kernel_value\n         )\n \n+    def test_save_to_fileobj(self) -> None:\n+        model = keras.Sequential(\n+            [keras.layers.Dense(1, input_shape=(1,)), keras.layers.Dense(1)]\n+        )\n+        model.compile(optimizer=\"adam\", loss=\"mse\")\n+\n+        out = BytesIO()\n+        saving_lib.save_model(model, out)\n+        out.seek(0)\n+        model = saving_lib.load_model(out)\n+\n+        model.fit(np.array([1, 2]), np.array([1, 2]))\n+        pred1 = model.predict(np.array([1, 2]))\n+\n+        out = BytesIO()\n+        saving_lib.save_model(model, out)\n+        out.seek(0)\n+        new_model = saving_lib.load_model(out)\n+\n+        pred2 = new_model.predict(np.array([1, 2]))\n+\n+        self.assertAllClose(pred1, pred2, atol=1e-5)\n+\n \n @pytest.mark.requires_trainable_backend\n class SavingAPITest(testing.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7fa3990c188436f98db660c824cf952cf58450f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 279 | Lines Deleted: 152 | Files Changed: 4 | Hunks: 20 | Methods Changed: 14 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 431 | Churn Cumulative: 3901 | Contributors (this commit): 17 | Commits (past 90d): 27 | Contributors (cumulative): 33 | DMM Complexity: 0.39823008849557523\n\nDIFF:\n@@ -102,6 +102,11 @@ class Dense(Layer):\n \n     def build(self, input_shape):\n         input_dim = input_shape[-1]\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            self.quantized_build(\n+                input_shape, mode=self.dtype_policy.quantization_mode\n+            )\n+        else:\n             self._kernel = self.add_weight(\n                 name=\"kernel\",\n                 shape=(input_dim, self.units),\n@@ -124,7 +129,8 @@ class Dense(Layer):\n         if self.lora_rank:\n             self.enable_lora(self.lora_rank)\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n-            self.quantize(self.dtype_policy.quantization_mode)\n+            if self.bias is not None:\n+                self.bias.trainable = False\n \n     @property\n     def kernel(self):\n@@ -146,20 +152,6 @@ class Dense(Layer):\n             x = self.activation(x)\n         return x\n \n-    def quantized_call(self, inputs):\n-        if self.lora_enabled:\n-            raise ValueError(\"`quantized_call` doesn't support lora weights\")\n-        inputs, inputs_scale = self.inputs_quantizer(inputs)\n-        x = ops.matmul(inputs, self.kernel)\n-        # De-scale outputs\n-        x = ops.cast(x, self.compute_dtype)\n-        x = ops.divide(x, ops.multiply(inputs_scale, self.kernel_scale))\n-        if self.bias is not None:\n-            x = ops.add(x, self.bias)\n-        if self.activation is not None:\n-            x = self.activation(x)\n-        return x\n-\n     def compute_output_shape(self, input_shape):\n         output_shape = list(input_shape)\n         output_shape[-1] = self.units\n@@ -201,67 +193,6 @@ class Dense(Layer):\n         self.lora_enabled = True\n         self.lora_rank = rank\n \n-    def quantize(self, mode):\n-        self._check_quantize_args(mode, self.compute_dtype)\n-        if mode == \"int8\":\n-            if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n-                raise ValueError(\"`quantize` can only be done once per layer.\")\n-            # Merge lora-related parameters to make use of fully int8 kernel\n-            self._merge_lora_into_kernel()\n-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n-            # Quantize `self._kernel` to int8 and compute corresponding scale\n-            kernel_value, kernel_scale = quantizers.abs_max_quantize(\n-                self._kernel, axis=0\n-            )\n-            kernel_scale = ops.cast(kernel_scale, self.compute_dtype)\n-            self._tracker.unlock()\n-            self._untrack_variable(self._kernel)\n-            self._kernel = self.add_weight(\n-                name=\"kernel\",\n-                shape=self._kernel.shape,\n-                # Prevent adding a large constant to the computation graph\n-                initializer=lambda shape, dtype: kernel_value,\n-                dtype=\"int8\",\n-                trainable=False,\n-            )\n-            self.kernel_scale = self.add_weight(\n-                name=\"kernel_scale\",\n-                shape=kernel_scale.shape,\n-                # Prevent adding a large constant to the computation graph\n-                initializer=lambda shape, dtype: kernel_scale,\n-                dtype=self.compute_dtype,\n-                trainable=False,\n-            )\n-            if self.bias is not None:\n-                self.bias.trainable = False\n-            self._tracker.lock()\n-        else:\n-            NotImplementedError(\n-                \"Invalid quantization mode. Expected 'int8'. \"\n-                f\"Received: mode={mode}\"\n-            )\n-\n-        # Set new dtype policy\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n-        ):\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            self.dtype_policy = dtype_policies.get(quantized_dtype)\n-\n-    def _merge_lora_into_kernel(self, untrack=False):\n-        if not self.lora_enabled:\n-            return\n-        # Merge lora-enabled kernel into kernel\n-        self._kernel.assign(self.kernel)\n-        self.lora_enabled = False\n-        if untrack:\n-            self._tracker.unlock()\n-            self.lora_kernel_a = self._untrack_variable(self.lora_kernel_a)\n-            self.lora_kernel_b = self._untrack_variable(self.lora_kernel_b)\n-            self._tracker.lock()\n-            self.lora_rank = None\n-\n     def save_own_variables(self, store):\n         if not self.lora_enabled:\n             return super().save_own_variables(store)\n@@ -300,3 +231,99 @@ class Dense(Layer):\n         if self.lora_rank:\n             config[\"lora_rank\"] = self.lora_rank\n         return {**base_config, **config}\n+\n+    \"\"\"Quantization-related methods\"\"\"\n+\n+    def quantized_build(self, input_shape, mode):\n+        input_dim = input_shape[-1]\n+        if mode == \"int8\":\n+            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n+            self._kernel = self.add_weight(\n+                name=\"kernel\",\n+                shape=(input_dim, self.units),\n+                initializer=\"zeros\",\n+                dtype=\"int8\",\n+                trainable=False,\n+            )\n+            self.kernel_scale = self.add_weight(\n+                name=\"kernel_scale\",\n+                shape=(self.units,),\n+                initializer=\"ones\",\n+                trainable=False,\n+            )\n+\n+    def quantized_call(self, inputs):\n+        if self.lora_enabled:\n+            raise ValueError(\"`quantized_call` doesn't support lora weights\")\n+        inputs, inputs_scale = self.inputs_quantizer(inputs)\n+        x = ops.matmul(inputs, self.kernel)\n+        # De-scale outputs\n+        x = ops.cast(x, self.compute_dtype)\n+        x = ops.divide(x, ops.multiply(inputs_scale, self.kernel_scale))\n+        if self.bias is not None:\n+            x = ops.add(x, self.bias)\n+        if self.activation is not None:\n+            x = self.activation(x)\n+        return x\n+\n+    def quantize(self, mode):\n+        self._check_quantize_args(mode, self.compute_dtype)\n+        if mode == \"int8\":\n+            if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n+                raise ValueError(\"`quantize` can only be done once per layer.\")\n+            # Merge lora-related parameters to make use of fully int8 kernel\n+            self._merge_lora_into_kernel()\n+            # Configure `self.inputs_quantizer`\n+            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n+            # Quantize `self._kernel` to int8 and compute corresponding scale\n+            kernel_value, kernel_scale = quantizers.abs_max_quantize(\n+                self._kernel, axis=0\n+            )\n+            kernel_scale = ops.cast(\n+                ops.squeeze(kernel_scale, axis=0), self.compute_dtype\n+            )\n+            self._tracker.unlock()\n+            self._untrack_variable(self._kernel)\n+            self._kernel = self.add_weight(\n+                name=\"kernel\",\n+                shape=self._kernel.shape,\n+                # Prevent adding a large constant to the computation graph\n+                initializer=lambda shape, dtype: kernel_value,\n+                dtype=\"int8\",\n+                trainable=False,\n+            )\n+            self.kernel_scale = self.add_weight(\n+                name=\"kernel_scale\",\n+                shape=(self.units,),\n+                # Prevent adding a large constant to the computation graph\n+                initializer=lambda shape, dtype: kernel_scale,\n+                trainable=False,\n+            )\n+            if self.bias is not None:\n+                self.bias.trainable = False\n+            self._tracker.lock()\n+        else:\n+            NotImplementedError(\n+                \"Invalid quantization mode. Expected 'int8'. \"\n+                f\"Received: mode={mode}\"\n+            )\n+\n+        # Set new dtype policy\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        ):\n+            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n+            self.dtype_policy = dtype_policies.get(quantized_dtype)\n+\n+    def _merge_lora_into_kernel(self, untrack=False):\n+        if not self.lora_enabled:\n+            return\n+        # Merge lora-enabled kernel into kernel\n+        self._kernel.assign(self.kernel)\n+        self.lora_enabled = False\n+        if untrack:\n+            self._tracker.unlock()\n+            self.lora_kernel_a = self._untrack_variable(self.lora_kernel_a)\n+            self.lora_kernel_b = self._untrack_variable(self.lora_kernel_b)\n+            self._tracker.lock()\n+            self.lora_rank = None\n\n@@ -310,6 +310,13 @@ class DenseTest(testing.TestCase):\n         layer.build((None, 8))\n         layer.quantize(\"int8\")\n \n+        # Verify weights dtype\n+        self.assertEqual(backend.standardize_dtype(layer._kernel.dtype), \"int8\")\n+        self.assertEqual(\n+            backend.standardize_dtype(layer.kernel_scale.dtype),\n+            layer.variable_dtype,\n+        )\n+\n         # Try eager call\n         x = np.random.random((2, 8))\n         _ = layer(x)\n@@ -337,6 +344,14 @@ class DenseTest(testing.TestCase):\n         x = np.random.random((2, 8))\n         _ = layer(x)\n \n+        # Try building with quantized dtype policy\n+        layer = layers.Dense(units=16, dtype=\"int8_from_mixed_bfloat16\")\n+        layer.build((None, 8))\n+        self.assertEqual(backend.standardize_dtype(layer._kernel.dtype), \"int8\")\n+        self.assertEqual(\n+            backend.standardize_dtype(layer.kernel_scale.dtype), \"float32\"\n+        )\n+\n     @pytest.mark.requires_trainable_backend\n     def test_quantize_dtype_argument(self):\n         self.run_layer_test(\n\n@@ -153,6 +153,13 @@ class EinsumDense(Layer):\n         )\n         kernel_shape, bias_shape, full_output_shape = shape_data\n         self.full_output_shape = tuple(full_output_shape)\n+        # `quantized_build` needs `self.input_spec`\n+        self.input_spec = InputSpec(ndim=len(input_shape))\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            self.quantized_build(\n+                input_shape, mode=self.dtype_policy.quantization_mode\n+            )\n+        else:\n             self._kernel = self.add_weight(\n                 name=\"kernel\",\n                 shape=tuple(kernel_shape),\n@@ -162,7 +169,6 @@ class EinsumDense(Layer):\n                 dtype=self.dtype,\n                 trainable=True,\n             )\n-\n         if bias_shape is not None:\n             self.bias = self.add_weight(\n                 name=\"bias\",\n@@ -175,12 +181,12 @@ class EinsumDense(Layer):\n             )\n         else:\n             self.bias = None\n-        self.input_spec = InputSpec(ndim=len(input_shape))\n         self.built = True\n         if self.lora_rank:\n             self.enable_lora(self.lora_rank)\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n-            self.quantize(self.dtype_policy.quantization_mode)\n+            if self.bias is not None:\n+                self.bias.trainable = False\n \n     @property\n     def kernel(self):\n@@ -197,31 +203,6 @@ class EinsumDense(Layer):\n     def compute_output_shape(self, _):\n         return self.full_output_shape\n \n-    def get_config(self):\n-        base_config = super().get_config()\n-        config = {\n-            \"output_shape\": self.partial_output_shape,\n-            \"equation\": self.equation,\n-            \"activation\": activations.serialize(self.activation),\n-            \"bias_axes\": self.bias_axes,\n-            \"kernel_initializer\": initializers.serialize(\n-                self.kernel_initializer\n-            ),\n-            \"bias_initializer\": initializers.serialize(self.bias_initializer),\n-            \"kernel_regularizer\": regularizers.serialize(\n-                self.kernel_regularizer\n-            ),\n-            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n-            \"activity_regularizer\": regularizers.serialize(\n-                self.activity_regularizer\n-            ),\n-            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n-            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n-        }\n-        if self.lora_rank:\n-            config[\"lora_rank\"] = self.lora_rank\n-        return {**base_config, **config}\n-\n     def call(self, inputs):\n         x = ops.einsum(self.equation, inputs, self.kernel)\n         if self.bias is not None:\n@@ -230,30 +211,6 @@ class EinsumDense(Layer):\n             x = self.activation(x)\n         return x\n \n-    def quantized_call(self, inputs):\n-        if self.lora_enabled:\n-            raise ValueError(\"`quantized_call` doesn't support lora weights\")\n-        inputs, inputs_scale = self.inputs_quantizer(inputs)\n-        x = ops.einsum(self.equation, inputs, self.kernel)\n-        # Deal with `inputs_scale`\n-        inputs_scale = ops.transpose(inputs_scale, self._input_transpose_axes)\n-        if self._input_expand_axes:\n-            inputs_scale = ops.expand_dims(\n-                inputs_scale, axis=self._input_expand_axes\n-            )\n-        if self._input_squeeze_axes:\n-            inputs_scale = ops.squeeze(\n-                inputs_scale, axis=self._input_squeeze_axes\n-            )\n-        # De-scale outputs\n-        x = ops.cast(x, self.compute_dtype)\n-        x = ops.divide(x, ops.multiply(inputs_scale, self.kernel_scale))\n-        if self.bias is not None:\n-            x += self.bias\n-        if self.activation is not None:\n-            x = self.activation(x)\n-        return x\n-\n     def enable_lora(\n         self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n     ):\n@@ -290,6 +247,117 @@ class EinsumDense(Layer):\n         self.lora_enabled = True\n         self.lora_rank = rank\n \n+    def save_own_variables(self, store):\n+        if not self.lora_enabled:\n+            return super().save_own_variables(store)\n+\n+        kernel_value = self.kernel\n+        store[\"0\"] = kernel_value\n+        if self.bias is not None:\n+            store[\"1\"] = self.bias\n+\n+    def load_own_variables(self, store):\n+        if not self.lora_enabled:\n+            return super().load_own_variables(store)\n+        self._kernel.assign(store[\"0\"])\n+        if self.bias is not None:\n+            self.bias.assign(store[\"1\"])\n+        self.lora_kernel_a.assign(np.zeros(self.lora_kernel_a.shape))\n+        self.lora_kernel_b.assign(np.zeros(self.lora_kernel_b.shape))\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"output_shape\": self.partial_output_shape,\n+            \"equation\": self.equation,\n+            \"activation\": activations.serialize(self.activation),\n+            \"bias_axes\": self.bias_axes,\n+            \"kernel_initializer\": initializers.serialize(\n+                self.kernel_initializer\n+            ),\n+            \"bias_initializer\": initializers.serialize(self.bias_initializer),\n+            \"kernel_regularizer\": regularizers.serialize(\n+                self.kernel_regularizer\n+            ),\n+            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n+            \"activity_regularizer\": regularizers.serialize(\n+                self.activity_regularizer\n+            ),\n+            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n+            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n+        }\n+        if self.lora_rank:\n+            config[\"lora_rank\"] = self.lora_rank\n+        return {**base_config, **config}\n+\n+    \"\"\"Quantization-related methods\"\"\"\n+\n+    def quantized_build(self, input_shape, mode):\n+        shape_data = _analyze_einsum_string(\n+            self.equation,\n+            self.bias_axes,\n+            input_shape,\n+            self.partial_output_shape,\n+        )\n+        kernel_shape, _, _ = shape_data\n+        if mode == \"int8\":\n+            (\n+                self._input_reduced_axes,\n+                self._kernel_reduced_axes,\n+                self._input_transpose_axes,\n+                self._kernel_transpose_axes,\n+                self._input_expand_axes,\n+                self._kernel_expand_axes,\n+                self._input_squeeze_axes,\n+                self._kernel_squeeze_axes,\n+            ) = _analyze_quantization_info(self.equation, self.input_spec.ndim)\n+            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n+            self._kernel = self.add_weight(\n+                name=\"kernel\",\n+                shape=kernel_shape,\n+                initializer=\"zeros\",\n+                dtype=\"int8\",\n+                trainable=False,\n+            )\n+            kernel_scale_shape = np.array(kernel_shape)\n+            kernel_scale_shape[self._kernel_reduced_axes] = 1\n+            kernel_scale_shape = kernel_scale_shape[self._kernel_transpose_axes]\n+            kernel_scale_shape = kernel_scale_shape.tolist()\n+            for a in sorted(self._kernel_expand_axes):\n+                kernel_scale_shape.insert(a, 1)\n+            for a in sorted(self._kernel_squeeze_axes, reverse=True):\n+                kernel_scale_shape.pop(a)\n+            self.kernel_scale = self.add_weight(\n+                name=\"kernel_scale\",\n+                shape=kernel_scale_shape,\n+                initializer=\"ones\",\n+                trainable=False,\n+            )\n+\n+    def quantized_call(self, inputs):\n+        if self.lora_enabled:\n+            raise ValueError(\"`quantized_call` doesn't support lora weights\")\n+        inputs, inputs_scale = self.inputs_quantizer(inputs)\n+        x = ops.einsum(self.equation, inputs, self.kernel)\n+        # Deal with `inputs_scale`\n+        inputs_scale = ops.transpose(inputs_scale, self._input_transpose_axes)\n+        if self._input_expand_axes:\n+            inputs_scale = ops.expand_dims(\n+                inputs_scale, axis=self._input_expand_axes\n+            )\n+        if self._input_squeeze_axes:\n+            inputs_scale = ops.squeeze(\n+                inputs_scale, axis=self._input_squeeze_axes\n+            )\n+        # De-scale outputs\n+        x = ops.cast(x, self.compute_dtype)\n+        x = ops.divide(x, ops.multiply(inputs_scale, self.kernel_scale))\n+        if self.bias is not None:\n+            x += self.bias\n+        if self.activation is not None:\n+            x = self.activation(x)\n+        return x\n+\n     def quantize(self, mode):\n         self._check_quantize_args(mode, self.compute_dtype)\n         if mode == \"int8\":\n@@ -297,11 +365,6 @@ class EinsumDense(Layer):\n                 raise ValueError(\"`quantize` can only be done once per layer.\")\n             # Merge lora-related parameters to make use of fully int8 kernel\n             self._merge_lora_into_kernel()\n-\n-            if self.input_spec is None:\n-                raise ValueError(\n-                    f\"Cannot quantize {self.name} that isn't yet built.\"\n-                )\n             (\n                 self._input_reduced_axes,\n                 self._kernel_reduced_axes,\n@@ -347,7 +410,6 @@ class EinsumDense(Layer):\n                 shape=kernel_scale.shape,\n                 # Prevent adding a large constant to the computation graph\n                 initializer=lambda shape, dtype: kernel_scale,\n-                dtype=self.compute_dtype,\n                 trainable=False,\n             )\n             if self.bias is not None:\n@@ -376,24 +438,6 @@ class EinsumDense(Layer):\n             self._tracker.lock()\n             self.lora_rank = None\n \n-    def save_own_variables(self, store):\n-        if not self.lora_enabled:\n-            return super().save_own_variables(store)\n-\n-        kernel_value = self.kernel\n-        store[\"0\"] = kernel_value\n-        if self.bias is not None:\n-            store[\"1\"] = self.bias\n-\n-    def load_own_variables(self, store):\n-        if not self.lora_enabled:\n-            return super().load_own_variables(store)\n-        self._kernel.assign(store[\"0\"])\n-        if self.bias is not None:\n-            self.bias.assign(store[\"1\"])\n-        self.lora_kernel_a.assign(np.zeros(self.lora_kernel_a.shape))\n-        self.lora_kernel_b.assign(np.zeros(self.lora_kernel_b.shape))\n-\n \n def _analyze_einsum_string(equation, bias_axes, input_shape, output_shape):\n     \"\"\"Analyzes an einsum string to determine the required weight shape.\"\"\"\n\n@@ -4,6 +4,7 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n+from keras import backend\n from keras import constraints\n from keras import layers\n from keras import models\n@@ -382,6 +383,13 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         layer.build((None, 3))\n         layer.quantize(\"int8\")\n \n+        # Verify weights dtype\n+        self.assertEqual(backend.standardize_dtype(layer._kernel.dtype), \"int8\")\n+        self.assertEqual(\n+            backend.standardize_dtype(layer.kernel_scale.dtype),\n+            layer.variable_dtype,\n+        )\n+\n         # Try eager call\n         x = np.random.random((2, 3))\n         _ = layer(x)\n@@ -413,6 +421,39 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         x = np.random.random((2, 3))\n         _ = layer(x)\n \n+        # Try building with quantized dtype policy\n+        layer = layers.EinsumDense(\n+            equation=\"abcde,afce->acdbf\",  # Test reduce and transpose\n+            output_shape=(2, 4, 8, 16),\n+            bias_axes=\"d\",\n+            dtype=\"int8_from_mixed_bfloat16\",\n+        )\n+        layer.build((1, 8, 2, 4, 32))\n+        self.assertEqual(backend.standardize_dtype(layer._kernel.dtype), \"int8\")\n+        self.assertEqual(\n+            backend.standardize_dtype(layer.kernel_scale.dtype), \"float32\"\n+        )\n+        layer = layers.EinsumDense(\n+            equation=\"a,b->ab\",  # Test expand\n+            output_shape=(4,),\n+            dtype=\"int8_from_float32\",\n+        )\n+        layer.build((None,))\n+        self.assertEqual(backend.standardize_dtype(layer._kernel.dtype), \"int8\")\n+        self.assertEqual(\n+            backend.standardize_dtype(layer.kernel_scale.dtype), \"float32\"\n+        )\n+        layer = layers.EinsumDense(\n+            equation=\"ab,ab->a\",  # Test squeeze\n+            output_shape=(2,),\n+            dtype=\"int8_from_float32\",\n+        )\n+        layer.build((2, 4))\n+        self.assertEqual(backend.standardize_dtype(layer._kernel.dtype), \"int8\")\n+        self.assertEqual(\n+            backend.standardize_dtype(layer.kernel_scale.dtype), \"float32\"\n+        )\n+\n     @pytest.mark.requires_trainable_backend\n     def test_quantize_dtype_argument(self):\n         self.run_layer_test(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ddba5d8e96db3bc40aa77fd721fe263f4b654154", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 129 | Lines Deleted: 24 | Files Changed: 3 | Hunks: 14 | Methods Changed: 10 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 153 | Churn Cumulative: 1242 | Contributors (this commit): 11 | Commits (past 90d): 22 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -436,8 +436,64 @@ def unstack(x, num=None, axis=0):\n     return x.unbind(axis)\n \n \n-def custom_gradient(fun):\n-    # TODO: Support this function\n-    raise NotImplementedError(\n-        \"`custom_gradient` is not supported with torch backend\"\n-    )\n+class custom_gradient:\n+    \"\"\"\n+    Decorator for custom gradients.\n+\n+    Args:\n+        forward_fn: Function to compute forward pass.\n+    \"\"\"\n+    def __init__(self, forward_fn):\n+        self.forward_fn = forward_fn\n+\n+    def __call__(self, *args, **kwargs):\n+        return CustomGradientFunction.apply(self.forward_fn, *args, **kwargs)\n+\n+class CustomGradientFunction(torch.autograd.Function):\n+    \"\"\"\n+    CustomGradientFunction is a PyTorch autograd function enabling\n+    custom forward and backward passes for gradient computation.\n+\n+    Forward pass computation via:\n+    `CustomGradientFunction.forward`\n+\n+    Args:\n+        ctx: Context object.\n+        forward_fn: Function to compute forward pass.\n+        *args: Arguments for the forward pass.\n+        **kwargs: Keyword arguments for the forward pass.\n+\n+    Returns:\n+        torch.Tensor: Output of the forward pass.\n+\n+    Backward pass computation via\n+    `CustomGradientFunction.backward`\n+\n+    Args:\n+        ctx: Context object.\n+        grad_output: Gradient with respect to the output.\n+\n+    Returns:\n+        Tuple: Gradients with respect to the input arguments.\n+    \"\"\"\n+    @staticmethod\n+    def forward(ctx, forward_fn, *args, **kwargs):\n+        ctx.forward_fn = forward_fn\n+        ctx.save_for_backward(*args)\n+        try:\n+            output, ctx.grad_fn = forward_fn(*args, **kwargs)\n+        except:\n+            output = forward_fn(*args, **kwargs)\n+            ctx.grad_fn = lambda *args, **kwargs: torch.full((), float('nan'))\n+        return output\n+\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        args = ctx.saved_tensors\n+        grad_fn = ctx.grad_fn\n+        if grad_fn is None:\n+            raise ValueError(\"grad_fn must be provided for custom gradient\")\n+        grads = grad_fn(*args, upstream=grad_output)\n+        if not isinstance(grads, tuple):\n+            grads = (grads,)\n+        return (None,) + grads\n\n@@ -635,36 +635,71 @@ def custom_gradient(f):\n     a more efficient or numerically stable gradient for a sequence of\n     operations.\n \n-    Note that `custom_gradient` only supports TensorFlow and JAX backends.\n-\n     Args:\n-        f: Function `f(*x)` that returns a tuple `(y, grad_fn)` where:\n-            - `x` is a sequence of (nested structures of) tensor inputs to the\n-                function.\n-            - `y` is a (nested structure of) tensor outputs of applying\n-                operations in `f` to `x`.\n-            - `grad_fn` is a function with the signature `g(*grad_ys)` which\n-                returns a list of tensors the same size as (flattened) `x`: the\n-                derivatives of tensors in `y` with respect to the tensors in\n-                `x`. `grad_ys` is a sequence of tensors the same size as\n-                (flattened) `y` holding the initial value gradients for each\n-                tensor in `y`.\n+        f: Function `f(*args)` that returns a tuple\n+            `(output, grad_fn)`, where:\n+            - `args` is a sequence of (nested structures of) tensor inputs to\n+                the function.\n+            - `output` is a (nested structure of) tensor outputs of applying\n+                operations in `forward_fn` to `args`.\n+            - `grad_fn` is a function with the signature `grad_fn(*args,\n+                upstream)` which returns a tuple of tensors the same size as\n+                (flattened) `args`: the derivatives of tensors in `output` with\n+                respect to the tensors in `args`. `upstream` is a tensor or\n+                sequence of tensors holding the initial value gradients for each\n+                tensor in `output`.\n \n     Returns:\n-        A function `h(x)` which returns the same value as `f(x)[0]` and whose\n-        gradient is determined by `f(x)[1]`.\n+        A function `h(*args)` which returns the same value as\n+        `f(*args)[0]` and whose gradient is determined by\n+        `f(*args)[1]`.\n+\n \n     Example:\n \n+    [Backend-invariant]\n     ```python\n     @ops.custom_gradient\n     def log1pexp(x):\n         e = ops.exp(x)\n \n-        def grad(upstream):\n+        def grad(*args, upstream=None):\n+            if upstream == None:\n+                upstream, = args\n+                \n             return ops.multiply(upstream, 1.0 - 1.0 / ops.add(1, e))\n \n         return ops.log(1 + e), grad\n     ```\n+\n+    Note that the grad function that returns gradient computations \n+    requires `args` as well as an `upstream` keyword argument, depending \n+    upon the backend being set. In JAX and TensorFlow backends, \n+    it requires only one argument, whereas it might use `upstream` keyword \n+    arguments in the case of PyTorch backend.\n+\n+    Example: When working with TensorFlow/JAX backend, `grad(upstream)`\n+    is sufficient. In PyTorch `grad` function requires `*args` as well\n+    as `upstream`, `def grad(*args, upstream)`. Follow the previous\n+    example to use `@ops.custom_gradient` in all three backends.\n+\n+    [JAX, TensorFlow-specific backend example]\n+    ```python\n+    @ops.custom_gradient\n+    def log1pexp(x):\n+        e = ops.exp(x)\n+        def grad(upstream=None):\n+            return ops.multiply(upstream, 1.0 - 1.0 / ops.add(1, e))\n+        return ops.log(1 + e), grad\n+    ```\n+    [PyTorch-specific backend example]\n+    ```python\n+    @ops.custom_gradient\n+    def log1pexp(x):\n+        e = ops.exp(x)\n+        def grad(*args, upstream):\n+            return ops.multiply(upstream, 1.0 - 1.0 / ops.add(1, e))\n+        return ops.log(1 + e), grad\n+    ```\n     \"\"\"\n     return backend.core.custom_gradient(f)\n\n@@ -501,15 +501,20 @@ class CoreOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertFalse(ops.is_tensor([1, 2, 3]))\n \n     @pytest.mark.skipif(\n-        backend.backend() not in (\"tensorflow\", \"jax\"),\n+        backend.backend() not in (\"tensorflow\", \"jax\", \"torch\"),\n         reason=f\"{backend.backend()} doesn't support `custom_gradient`.\",\n     )\n+\n     def test_custom_gradient(self):\n+\n+        # function to test custom_gradient on\n         @ops.custom_gradient\n         def log1pexp(x):\n             e = ops.exp(x)\n \n-            def grad(upstream):\n+            def grad(*args, upstream = None):\n+                if upstream == None:\n+                    upstream, = args\n                 return ops.multiply(upstream, 1.0 - 1.0 / ops.add(1, e))\n \n             return ops.log(1 + e), grad\n@@ -529,14 +534,23 @@ class CoreOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 z = log1pexp_nan(x)\n             dy_dx = tape1.gradient(y, x)\n             dz_dx = tape2.gradient(z, x)\n+            self.assertEqual(ops.convert_to_numpy(dy_dx), 1.0)\n         elif backend.backend() == \"jax\":\n             import jax\n \n             dy_dx = jax.grad(log1pexp)(x)\n             dz_dx = jax.grad(log1pexp_nan)(x)\n-\n             self.assertEqual(ops.convert_to_numpy(dy_dx), 1.0)\n             self.assertTrue(ops.isnan(dz_dx))\n+        elif backend.backend() == \"torch\":\n+            import torch\n+\n+            x = torch.tensor(100.0, requires_grad = True) # x = ops.convert_to_tensor(100.0) is NOT supported Yet!\n+            z = log1pexp(x)\n+            z.sum().backward()\n+            self.assertEqual(ops.convert_to_numpy(x.grad), 1.0)\n+\n+\n \n \n class CoreOpsDtypeTest(testing.TestCase, parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba43e3b876d1053cd0388e4fc2c9599eeb44b394", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 47 | Files Changed: 3 | Hunks: 22 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 87 | Churn Cumulative: 1329 | Contributors (this commit): 11 | Commits (past 90d): 25 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -437,58 +437,49 @@ def unstack(x, num=None, axis=0):\n \n \n class custom_gradient:\n-    \"\"\"\n-    Decorator for custom gradients.\n+    \"\"\"Decorator for custom gradients.\n \n     Args:\n-        forward_fn: Function to compute forward pass.\n+        forward_fn: Forward pass function.\n     \"\"\"\n+\n     def __init__(self, forward_fn):\n         self.forward_fn = forward_fn\n \n     def __call__(self, *args, **kwargs):\n         return CustomGradientFunction.apply(self.forward_fn, *args, **kwargs)\n \n-class CustomGradientFunction(torch.autograd.Function):\n-    \"\"\"\n-    CustomGradientFunction is a PyTorch autograd function enabling\n-    custom forward and backward passes for gradient computation.\n \n-    Forward pass computation via:\n-    `CustomGradientFunction.forward`\n+class CustomGradientFunction(torch.autograd.Function):\n+    \"\"\"Enables custom forward & backward passes for gradient computation.\"\"\"\n+\n+    @staticmethod\n+    def forward(ctx, forward_fn, *args, **kwargs):\n+        \"\"\"Forward pass computation specification.\n \n         Args:\n             ctx: Context object.\n             forward_fn: Function to compute forward pass.\n             *args: Arguments for the forward pass.\n             **kwargs: Keyword arguments for the forward pass.\n-\n-    Returns:\n-        torch.Tensor: Output of the forward pass.\n-\n-    Backward pass computation via\n-    `CustomGradientFunction.backward`\n-\n-    Args:\n-        ctx: Context object.\n-        grad_output: Gradient with respect to the output.\n-\n-    Returns:\n-        Tuple: Gradients with respect to the input arguments.\n         \"\"\"\n-    @staticmethod\n-    def forward(ctx, forward_fn, *args, **kwargs):\n         ctx.forward_fn = forward_fn\n         ctx.save_for_backward(*args)\n         try:\n             output, ctx.grad_fn = forward_fn(*args, **kwargs)\n         except:\n             output = forward_fn(*args, **kwargs)\n-            ctx.grad_fn = lambda *args, **kwargs: torch.full((), float('nan'))\n+            ctx.grad_fn = lambda *args, **kwargs: torch.full((), float(\"nan\"))\n         return output\n \n     @staticmethod\n     def backward(ctx, grad_output):\n+        \"\"\"Backward pass computation specification.\n+\n+        Args:\n+            ctx: Context object.\n+            grad_output: Gradient with respect to the output.\n+        \"\"\"\n         args = ctx.saved_tensors\n         grad_fn = ctx.grad_fn\n         if grad_fn is None:\n\n@@ -655,44 +655,49 @@ def custom_gradient(f):\n         `f(*args)[1]`.\n \n \n-    Example:\n+    Examples:\n+\n+    1. Backend-agnostic example.\n \n-    [Backend-invariant]\n     ```python\n     @ops.custom_gradient\n     def log1pexp(x):\n         e = ops.exp(x)\n \n         def grad(*args, upstream=None):\n-            if upstream == None:\n-                upstream, = args\n-                \n+            if upstream is None:\n+                (upstream,) = args\n             return ops.multiply(upstream, 1.0 - 1.0 / ops.add(1, e))\n \n         return ops.log(1 + e), grad\n     ```\n \n-    Note that the grad function that returns gradient computations \n+    Note that the grad function that returns gradient computation\n     requires `args` as well as an `upstream` keyword argument, depending\n-    upon the backend being set. In JAX and TensorFlow backends, \n-    it requires only one argument, whereas it might use `upstream` keyword \n-    arguments in the case of PyTorch backend.\n+    on the backend being set. With the JAX and TensorFlow backends,\n+    it requires only one argument, whereas it might use the `upstream`\n+    argument in the case of the PyTorch backend.\n \n-    Example: When working with TensorFlow/JAX backend, `grad(upstream)`\n-    is sufficient. In PyTorch `grad` function requires `*args` as well\n-    as `upstream`, `def grad(*args, upstream)`. Follow the previous\n-    example to use `@ops.custom_gradient` in all three backends.\n+    When working with TensorFlow/JAX backend, `grad(upstream)`\n+    is sufficient. With PyTorch, the `grad` function requires\n+    `*args` as well as `upstream`, e.g. `def grad(*args, upstream)`.\n+    Follow the previous example to use `@ops.custom_gradient` in\n+    a way that is compatible with all backends.\n+\n+    2. Here's JAX & TensorFlow-specific example:\n \n-    [JAX, TensorFlow-specific backend example]\n     ```python\n     @ops.custom_gradient\n     def log1pexp(x):\n         e = ops.exp(x)\n-        def grad(upstream=None):\n+        def grad(upstream):\n             return ops.multiply(upstream, 1.0 - 1.0 / ops.add(1, e))\n         return ops.log(1 + e), grad\n     ```\n-    [PyTorch-specific backend example]\n+\n+    3. Lastly, here's a PyTorch-specific example,\n+    using `*args` & `upstream`:\n+\n     ```python\n     @ops.custom_gradient\n     def log1pexp(x):\n\n@@ -504,7 +504,6 @@ class CoreOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         backend.backend() not in (\"tensorflow\", \"jax\", \"torch\"),\n         reason=f\"{backend.backend()} doesn't support `custom_gradient`.\",\n     )\n-\n     def test_custom_gradient(self):\n \n         # function to test custom_gradient on\n@@ -513,8 +512,8 @@ class CoreOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             e = ops.exp(x)\n \n             def grad(*args, upstream=None):\n-                if upstream == None:\n-                    upstream, = args\n+                if upstream is None:\n+                    (upstream,) = args\n                 return ops.multiply(upstream, 1.0 - 1.0 / ops.add(1, e))\n \n             return ops.log(1 + e), grad\n@@ -545,14 +544,12 @@ class CoreOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         elif backend.backend() == \"torch\":\n             import torch\n \n-            x = torch.tensor(100.0, requires_grad = True) # x = ops.convert_to_tensor(100.0) is NOT supported Yet!\n+            x = torch.tensor(100.0, requires_grad=True)\n             z = log1pexp(x)\n             z.sum().backward()\n             self.assertEqual(ops.convert_to_numpy(x.grad), 1.0)\n \n \n-\n-\n class CoreOpsDtypeTest(testing.TestCase, parameterized.TestCase):\n     import jax  # enable bfloat16 for numpy\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
