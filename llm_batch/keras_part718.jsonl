{"custom_id": "keras#ecf5d086201cd41d7fee113972f7b74abae8b260", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 248 | Lines Deleted: 325 | Files Changed: 7 | Hunks: 24 | Methods Changed: 45 | Complexity Δ (Sum/Max): -2/13 | Churn Δ: 573 | Churn Cumulative: 4376 | Contributors (this commit): 18 | Commits (past 90d): 62 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,20 +7,25 @@ import tree\n from jax.tree_util import Partial\n \n from keras_core.backend.common import KerasVariable\n+from keras_core.backend.common import global_state\n from keras_core.backend.common import standardize_dtype\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.backend.jax import distribution\n+from keras_core.backend.jax import distribution_lib\n from keras_core.utils.nest import pack_sequence_as\n \n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n         value = jnp.array(value, dtype=self._dtype)\n-        if distribution.get_global_distribution() is not None:\n-            value = distribution.get_global_distribution().distribute_variable(\n-                value\n-            )\n+        # Note that variable.shape is needed by distribution_lib\n+        self._shape = tuple(value.shape)\n+        # We can't import the keras_core/distribution/distribution_lib\n+        # due to circular dependency.\n+        distribution = global_state.get_global_attribute(\"distribution\")\n+        if distribution is not None:\n+            layout = distribution.get_variable_layout(self)\n+            value = distribution_lib.distribute_value(value, layout)\n         self._value = value\n \n     def _direct_assign(self, value):\n\n@@ -1,121 +0,0 @@\n-\"\"\"!!!DO NOT USE!!!\n-\n-Distribution related class for JAX backend.\n-\n-This is just a prototype and we might want to unify it\n-with other backends in the future.\n-\"\"\"\n-\n-import contextlib\n-\n-import jax\n-import numpy as np\n-from absl import logging\n-\n-from keras_core.backend.common import global_state\n-\n-DEFAULT_BATCH_DIM_NAME = \"batch\"\n-GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n-\n-\n-def get_global_distribution():\n-    \"\"\"Retrieve the current distribution from global context.\"\"\"\n-    return global_state.get_global_attribute(GLOBAL_ATTRIBUTE_NAME)\n-\n-\n-def set_global_distribution(distribution):\n-    \"\"\"Set the distribution as the global distribution setting.\"\"\"\n-    # TODO(qlzh727): Do type checking for input once we have a base class.\n-    distribution.as_global_distribution()\n-\n-\n-class DataParallelDistribution:\n-    def __init__(self, mesh=None, devices=None):\n-        \"\"\"Create the data parallel distribution.\n-\n-        You can choose to create this instance by either specifing\n-        the `Mesh` or `devices` parameters (but not both).\n-\n-        The mesh is expected to be a `jax.sharding.Mesh` instance, and is\n-        expected to be 1D only. In case that the mesh has multiple axes, then\n-        the first axis will be treated as the data parallel dimension\n-        (and a warning will be raised).\n-\n-        When a list of `devices` are provided, they will be used to construct a\n-        1D mesh.\n-\n-        When both `mesh` and `devices` are absent, then `jax.devices()`\n-        will be used to detect any available devices\n-        and create a 1D mesh from them.\n-        \"\"\"\n-        super().__init__()\n-        if mesh:\n-            self._initialize_with_mesh(mesh)\n-        elif devices:\n-            self._initialize_mesh_from_devices(devices)\n-        else:\n-            self._initialize_mesh_from_jax_devices()\n-\n-        self._configure_sharding_spec()\n-        self._batch_dim_name = self.mesh.axis_names[0]\n-\n-    @contextlib.contextmanager\n-    def scope(self):\n-        original_scope = global_state.get_global_attribute(\n-            GLOBAL_ATTRIBUTE_NAME\n-        )\n-        global_state.set_global_attribute(GLOBAL_ATTRIBUTE_NAME, self)\n-        try:\n-            yield\n-        finally:\n-            global_state.set_global_attribute(\n-                GLOBAL_ATTRIBUTE_NAME, original_scope\n-            )\n-\n-    def as_global_distribution(self):\n-        global_state.set_global_attribute(GLOBAL_ATTRIBUTE_NAME, self)\n-\n-    def distribute_data(self, data):\n-        return jax.device_put(data, self._data_sharding)\n-\n-    def distribute_variable(self, variable):\n-        return jax.device_put(variable, self._variable_sharding)\n-\n-    def _initialize_with_mesh(self, mesh):\n-        if not isinstance(mesh, jax.sharding.Mesh):\n-            raise ValueError(\n-                \"Expect `mesh` to be an instance of `jax.sharding.Mesh`. \"\n-                f\"Received: mesh={mesh} (of type {type(mesh)})\"\n-            )\n-        self._user_provide_devices = None\n-        self.mesh = mesh\n-        if self.mesh.devices.ndim != 1:\n-            logging.warning(\n-                \"Expect the input mesh to be 1D, but received \"\n-                f\"mesh.devices.ndim={mesh.devices.ndim}. \"\n-                \"The first axis will be used for data-parallel sharding.\"\n-            )\n-\n-    def _initialize_mesh_from_devices(self, devices):\n-        self._user_provide_devices = devices\n-        self.mesh = jax.sharding.Mesh(np.array(devices), DEFAULT_BATCH_DIM_NAME)\n-\n-    def _initialize_mesh_from_jax_devices(self):\n-        self._user_provide_devices = None\n-        self.mesh = jax.sharding.Mesh(\n-            np.array(jax.devices()), DEFAULT_BATCH_DIM_NAME\n-        )\n-\n-    def _configure_sharding_spec(self):\n-        variable_shard_spec = [\n-            None\n-        ] * self.mesh.devices.ndim  # Fully replicated\n-        data_shard_spec = variable_shard_spec.copy()\n-        data_shard_spec[0] = self.mesh.axis_names[0]  # Shard on the first dim\n-\n-        self._data_sharding = jax.sharding.NamedSharding(\n-            self.mesh, jax.sharding.PartitionSpec(*data_shard_spec)\n-        )\n-        self._variable_sharding = jax.sharding.NamedSharding(\n-            self.mesh, jax.sharding.PartitionSpec(*variable_shard_spec)\n-        )\n\n@@ -54,3 +54,16 @@ def to_jax_layout(tensor_layout):\n     partition_spec = jax.sharding.PartitionSpec(*tensor_layout.axes)\n     jax_mesh = to_jax_mesh(tensor_layout.device_mesh)\n     return jax.sharding.NamedSharding(jax_mesh, partition_spec)\n+\n+\n+def distribute_value(value, tensor_layout):\n+    \"\"\"Distribute the value based on the layout.\n+\n+    Args:\n+        value: `jax.Array` that need to be distributed.\n+        tensor_layout: `TensorLayout` for the distribution information.\n+\n+    Returns:\n+        Distributed value.\n+    \"\"\"\n+    return jax.device_put(value, to_jax_layout(tensor_layout))\n\n@@ -1,183 +0,0 @@\n-\"\"\"Tests for JAX based distribution.\"\"\"\n-import os\n-\n-import jax\n-import numpy as np\n-import pytest\n-\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.backend.common import global_state\n-from keras_core.backend.jax import distribution\n-\n-# Due to https://github.com/google/jax/issues/17188, we can't\n-# override the XLA flag after the JAX back init. We have to\n-# run this at top level to let JAX pick the flag value.\n-xla_flags = os.getenv(\"XLA_FLAGS\") or \"\"\n-# Don't override user-specified device count, or other XLA flags.\n-if \"xla_force_host_platform_device_count\" not in xla_flags:\n-    os.environ[\"XLA_FLAGS\"] = (\n-        xla_flags + \" --xla_force_host_platform_device_count=8\"\n-    )\n-\n-\n-@pytest.mark.skipif(\n-    backend.backend() != \"jax\",\n-    reason=\"Only JAX backend support distribution API for now.\",\n-)\n-class DataParallelDistributionTest(testing.TestCase):\n-    def test_create_with_devices(self):\n-        devices = jax.devices()\n-        self.assertEqual(len(devices), 8)\n-        ds = distribution.DataParallelDistribution(devices=devices)\n-\n-        mesh = ds.mesh\n-        self.assertEqual(len(mesh.devices), 8)\n-        self.assertEqual(mesh.axis_names, (\"batch\",))\n-        self.assertEqual(\n-            ds._data_sharding,\n-            jax.sharding.NamedSharding(\n-                mesh, jax.sharding.PartitionSpec(\"batch\")\n-            ),\n-        )\n-        self.assertEqual(\n-            ds._variable_sharding,\n-            jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(None)),\n-        )\n-\n-    def test_create_with_mesh(self):\n-        mesh = jax.sharding.Mesh(jax.devices(), \"data\")\n-        ds = distribution.DataParallelDistribution(mesh=mesh)\n-        self.assertEqual(ds.mesh, mesh)\n-\n-        self.assertEqual(\n-            ds._data_sharding,\n-            jax.sharding.NamedSharding(\n-                mesh, jax.sharding.PartitionSpec(\"data\")\n-            ),\n-        )\n-        self.assertEqual(\n-            ds._variable_sharding,\n-            jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(None)),\n-        )\n-\n-    def test_create_with_available_devices(self):\n-        ds = distribution.DataParallelDistribution()\n-\n-        mesh = ds.mesh\n-        self.assertEqual(len(mesh.devices), 8)\n-        self.assertEqual(mesh.axis_names, (\"batch\",))\n-\n-        self.assertEqual(\n-            ds._data_sharding,\n-            jax.sharding.NamedSharding(\n-                mesh, jax.sharding.PartitionSpec(\"batch\")\n-            ),\n-        )\n-        self.assertEqual(\n-            ds._variable_sharding,\n-            jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(None)),\n-        )\n-\n-    def test_mesh_with_rank_2(self):\n-        mesh = jax.sharding.Mesh(\n-            np.array(jax.devices()).reshape(4, 2), (\"data\", \"model\")\n-        )\n-        ds = distribution.DataParallelDistribution(mesh=mesh)\n-        self.assertEqual(ds.mesh, mesh)\n-\n-        self.assertEqual(\n-            ds._data_sharding,\n-            jax.sharding.NamedSharding(\n-                mesh, jax.sharding.PartitionSpec(\"data\", None)\n-            ),\n-        )\n-        self.assertEqual(\n-            ds._variable_sharding,\n-            jax.sharding.NamedSharding(\n-                mesh, jax.sharding.PartitionSpec(None, None)\n-            ),\n-        )\n-\n-    def test_distribute_data(self):\n-        ds = distribution.DataParallelDistribution()\n-\n-        data = np.arange(16).reshape((8, 2))\n-        distributed_data = ds.distribute_data(data)\n-        self.assertEqual(distributed_data.sharding, ds._data_sharding)\n-\n-    def test_distribute_variable(self):\n-        ds = distribution.DataParallelDistribution()\n-\n-        weights = np.arange(16).reshape((8, 2))\n-        distributed_weights = ds.distribute_variable(weights)\n-        self.assertEqual(distributed_weights.sharding, ds._variable_sharding)\n-\n-    def test_scope(self):\n-        self.assertIsNone(distribution.get_global_distribution())\n-        data_distribution = distribution.DataParallelDistribution()\n-        with data_distribution.scope():\n-            self.assertIs(\n-                distribution.get_global_distribution(), data_distribution\n-            )\n-            data_distribution_2 = distribution.DataParallelDistribution()\n-            with data_distribution_2.scope():\n-                self.assertIs(\n-                    distribution.get_global_distribution(), data_distribution_2\n-                )\n-\n-            self.assertIs(\n-                distribution.get_global_distribution(), data_distribution\n-            )\n-\n-        self.assertIsNone(distribution.get_global_distribution())\n-\n-    def test_as_global_distribution(self):\n-        try:\n-            self.assertIsNone(distribution.get_global_distribution())\n-\n-            data_distribution = distribution.DataParallelDistribution()\n-            data_distribution.as_global_distribution()\n-            self.assertIs(\n-                distribution.get_global_distribution(), data_distribution\n-            )\n-        finally:\n-            # Cleanup the global state\n-            global_state.set_global_attribute(\n-                distribution.GLOBAL_ATTRIBUTE_NAME, None\n-            )\n-\n-    def test_set_global_distribution(self):\n-        try:\n-            self.assertIsNone(distribution.get_global_distribution())\n-\n-            data_distribution = distribution.DataParallelDistribution()\n-            distribution.set_global_distribution(data_distribution)\n-            self.assertIs(\n-                distribution.get_global_distribution(), data_distribution\n-            )\n-        finally:\n-            # Cleanup the global state\n-            global_state.set_global_attribute(\n-                distribution.GLOBAL_ATTRIBUTE_NAME, None\n-            )\n-\n-    def test_e2e_model(self):\n-        data_distribution = distribution.DataParallelDistribution()\n-        with data_distribution.scope():\n-            inputs = layers.Input(shape=[28, 28, 1])\n-            y = layers.Flatten()(inputs)\n-            y = layers.Dense(units=200, use_bias=False, activation=\"relu\")(y)\n-            y = layers.Dropout(0.4)(y)\n-            y = layers.Dense(units=10, activation=\"softmax\")(y)\n-            model = models.Model(inputs=inputs, outputs=y)\n-\n-        # Make sure all the weights are properly sharded.\n-        for weight in model.weights:\n-            self.assertEqual(\n-                weight._value.sharding, data_distribution._variable_sharding\n-            )\n-\n-        # TODO(qlzh727): Need to validate the data sharding\n\n@@ -6,7 +6,8 @@ from keras_core import backend\n from keras_core import callbacks as callbacks_module\n from keras_core import ops\n from keras_core import optimizers as optimizers_module\n-from keras_core.backend.jax import distribution\n+from keras_core.backend import distribution_lib as jax_distribution_lib\n+from keras_core.distribution import distribution_lib\n from keras_core.trainers import trainer as base_trainer\n from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.epoch_iterator import EpochIterator\n@@ -753,7 +754,13 @@ class JAXTrainer(base_trainer.Trainer):\n                 ref_v.assign(v)\n \n     def _distribute_data(self, data):\n-        if distribution.get_global_distribution() is not None:\n-            distribute = distribution.get_global_distribution()\n-            data = distribute.distribute_data(data)\n+        distribution = distribution_lib.distribution()\n+        if distribution is not None:\n+\n+            def distribute_single_value(d):\n+                layout = distribution.get_data_layout(d.shape)\n+                return jax_distribution_lib.distribute_value(d, layout)\n+\n+            return jax.tree_util.tree_map(distribute_single_value, data)\n+        else:\n             return data\n\n@@ -6,6 +6,8 @@ Currently only the JAX backend has been implemented, and the Tensorflow backend\n will be implemented in future (via tf.dtensor API).\n \"\"\"\n \n+import logging\n+\n import contextlib\n \n import numpy as np\n@@ -13,6 +15,7 @@ import numpy as np\n from keras_core.backend import distribution_lib\n from keras_core.backend.common import global_state\n \n+DEFAULT_BATCH_DIM_NAME = \"batch\"\n GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n \n \n@@ -75,7 +78,7 @@ class DeviceMesh:\n                 \"Shape and axis_names should have same size,\"\n                 f\"got shape: {shape} and axis_names: {axis_names}\"\n             )\n-        if not devices:\n+        if devices is None:\n             devices = list_devices()\n         devices = np.array(devices)\n         if np.prod(shape) != np.prod(devices.shape):\n@@ -173,7 +176,7 @@ class Distribution:\n     \"\"\"\n \n     def __init__(self, device_mesh):\n-        pass\n+        self._device_mesh = device_mesh\n \n     def get_data_layout(self, data_shape):\n         \"\"\"Retrieve the `TensorLayout` for the input data.\n@@ -183,28 +186,106 @@ class Distribution:\n \n         Returns:\n             The `TensorLayout` for the data, which can be used by\n-            `backend.distribute_tensor()` to redistribute a input data.\n+            `backend.distribute_value()` to redistribute a input data.\n         \"\"\"\n-        pass\n+        raise NotImplementedError()\n \n-    def get_variable_layout(self, variable_path):\n-        \"\"\"Retrieve the `TensorLayout` for the variable based on the path.\n-\n-        The path of the variable is available by `variable.path`.\n+    def get_variable_layout(self, variable):\n+        \"\"\"Retrieve the `TensorLayout` for the variable.\n \n         Args:\n-            variable_path: string, the path for the variable to be distributed.\n+            variable: A `KerasVariable` to retrieve the `TensorLayout`.\n \n         return:\n             The `TensorLayout` for the variable, which can be used by\n-            `backend.distribute_tensor()` to redistribute a variable.\n+            `backend.distribute_value()` to redistribute a variable.\n         \"\"\"\n-        pass\n+        raise NotImplementedError()\n \n     @contextlib.contextmanager\n     def scope(self):\n         \"\"\"Context manager to make the `Distribution` current.\"\"\"\n-        pass\n+        original_scope = distribution()\n+        set_distribution(self)\n+        try:\n+            yield\n+        finally:\n+            set_distribution(original_scope)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+\n+class DataParallel(Distribution):\n+    def __init__(self, device_mesh=None, devices=None):\n+        \"\"\"Create the data parallel distribution.\n+\n+        You can choose to create this instance by either specifing\n+        the `device_mesh` or `devices` parameters (but not both).\n+\n+        The device_mesh is expected to be a `DeviceMesh` instance, and is\n+        expected to be 1D only. In case that the mesh has multiple axes, then\n+        the first axis will be treated as the data parallel dimension\n+        (and a warning will be raised).\n+\n+        When a list of `devices` are provided, they will be used to construct a\n+        1D mesh.\n+\n+        When both `mesh` and `devices` are absent, then `list_devices()`\n+        will be used to detect any available devices and create a 1D mesh from\n+        them.\n+        \"\"\"\n+        if device_mesh:\n+            self._initialize_with_device_mesh(device_mesh)\n+        elif devices:\n+            self._initialize_mesh_from_devices(devices)\n+        else:\n+            self._initialize_mesh_from_list_devices()\n+\n+        self._batch_dim_name = self.device_mesh.axis_names[0]\n+\n+    def _initialize_with_device_mesh(self, device_mesh):\n+        if not isinstance(device_mesh, DeviceMesh):\n+            raise ValueError(\n+                \"Expect `mesh` to be an instance of `DeviceMesh`. \"\n+                f\"Received: mesh={device_mesh} (of type {type(device_mesh)})\"\n+            )\n+        super().__init__(device_mesh)\n+        if self.device_mesh.devices.ndim != 1:\n+            logging.warning(\n+                \"Expect the input mesh to be 1D, but received \"\n+                \"mesh.devices.ndim=%d. \"\n+                \"The first axis will be used for data-parallel sharding.\",\n+                device_mesh.devices.ndim,\n+            )\n+\n+    def _initialize_mesh_from_devices(self, devices):\n+        devices = np.array(devices)\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(device_mesh)\n+\n+    def _initialize_mesh_from_list_devices(self):\n+        devices = np.array(list_devices())\n+        device_mesh = DeviceMesh(\n+            shape=devices.shape,\n+            axis_names=[DEFAULT_BATCH_DIM_NAME],\n+            devices=devices,\n+        )\n+        super().__init__(device_mesh)\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self._batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n \n \n def distribution():\n\n@@ -1,11 +1,15 @@\n \"\"\"Test for distribution_lib.py.\"\"\"\n \n import os\n+from unittest import mock\n \n import jax\n+import numpy as np\n import pytest\n \n from keras_core import backend\n+from keras_core import layers\n+from keras_core import models\n from keras_core import testing\n from keras_core.backend import distribution_lib as backend_dlib\n from keras_core.distribution import distribution_lib\n@@ -96,6 +100,99 @@ class TensorLayoutTest(testing.TestCase):\n             layout.device_mesh = self.mesh\n \n \n+class DistributionTest(testing.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        devices = [\"CPU:{i}\" for i in range(8)]\n+        shape = (4, 2)\n+        axis_names = [\"batch\", \"model\"]\n+\n+        self.device_mesh = distribution_lib.DeviceMesh(\n+            shape, axis_names, devices\n+        )\n+\n+    def test_init_with_device_mesh(self):\n+        distribution = distribution_lib.Distribution(self.device_mesh)\n+        self.assertIs(distribution.device_mesh, self.device_mesh)\n+\n+    def test_scope(self):\n+        distribution_1 = distribution_lib.Distribution(self.device_mesh)\n+        distribution_2 = distribution_lib.Distribution(self.device_mesh)\n+\n+        self.assertIsNone(distribution_lib.distribution())\n+        with distribution_1.scope():\n+            self.assertIs(distribution_lib.distribution(), distribution_1)\n+            with distribution_2.scope():\n+                self.assertIs(distribution_lib.distribution(), distribution_2)\n+\n+            self.assertIs(distribution_lib.distribution(), distribution_1)\n+\n+        self.assertIsNone(distribution_lib.distribution())\n+\n+\n+class DataParallelDistributionTest(testing.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        self.devices = [\"CPU:{i}\" for i in range(8)]\n+        shape = (8,)\n+        axis_names = [\"data\"]\n+\n+        self.device_mesh = distribution_lib.DeviceMesh(\n+            shape, axis_names, self.devices\n+        )\n+\n+    def test_create_with_device_mesh(self):\n+        distribution = distribution_lib.DataParallel(\n+            device_mesh=self.device_mesh\n+        )\n+\n+        device_mesh = distribution.device_mesh\n+        self.assertEqual(len(device_mesh.devices), 8)\n+        self.assertEqual(device_mesh.axis_names, [\"data\"])\n+        self.assertEqual(distribution._batch_dim_name, \"data\")\n+\n+    def test_create_with_devices(self):\n+        distribution = distribution_lib.DataParallel(devices=self.devices)\n+        device_mesh = distribution.device_mesh\n+        self.assertEqual(len(device_mesh.devices), 8)\n+        self.assertEqual(device_mesh.axis_names, [\"batch\"])\n+        self.assertEqual(distribution._batch_dim_name, \"batch\")\n+\n+    @mock.patch.object(\n+        distribution_lib,\n+        \"list_devices\",\n+        return_value=[\"CPU:{i}\" for i in range(8)],\n+    )\n+    def test_create_with_list_devices(self, mock_list_devices):\n+        distribution = distribution_lib.DataParallel()\n+        mock_list_devices.assert_called_once()\n+\n+        device_mesh = distribution.device_mesh\n+        self.assertEqual(len(device_mesh.devices), 8)\n+        self.assertEqual(device_mesh.axis_names, [\"batch\"])\n+        self.assertEqual(distribution._batch_dim_name, \"batch\")\n+\n+    def test_get_data_layout(self):\n+        distribution = distribution_lib.DataParallel(\n+            device_mesh=self.device_mesh\n+        )\n+\n+        data = np.arange(16).reshape((4, 2, 2))\n+        data_layout = distribution.get_data_layout(data.shape)\n+        self.assertIs(data_layout.device_mesh, self.device_mesh)\n+        self.assertEqual(data_layout.axes, [\"data\", None, None])\n+\n+    def test_get_variable_layout(self):\n+        distribution = distribution_lib.DataParallel(\n+            device_mesh=self.device_mesh\n+        )\n+\n+        variable = backend.Variable(initializer=[1, 2, 3])\n+        variable_layout = distribution.get_variable_layout(variable)\n+        self.assertIs(variable_layout.device_mesh, self.device_mesh)\n+        self.assertEqual(variable_layout.axes, [None])\n+\n+\n @pytest.mark.skipif(\n     backend.backend() != \"jax\",\n     reason=\"Backend specific test\",\n@@ -141,3 +238,27 @@ class JaxDistributionLibTest(testing.TestCase):\n             ValueError, \"Cannot create sharding when device mesh is not set\"\n         ):\n             backend_dlib.to_jax_layout(layout)\n+\n+    def test_e2e_model(self):\n+        distribution = distribution_lib.DataParallel(\n+            devices=backend_dlib.list_devices()\n+        )\n+\n+        with distribution.scope():\n+            inputs = layers.Input(shape=[28, 28, 1])\n+            y = layers.Flatten()(inputs)\n+            y = layers.Dense(units=200, use_bias=False, activation=\"relu\")(y)\n+            y = layers.Dropout(0.4)(y)\n+            y = layers.Dense(units=10, activation=\"softmax\")(y)\n+            model = models.Model(inputs=inputs, outputs=y)\n+\n+        # Make sure all the weights are properly sharded.\n+        for weight in model.weights:\n+            self.assertTrue(weight._value.sharding.is_fully_replicated)\n+\n+        inputs = np.random.normal(size=(32, 28, 28, 1))\n+        labels = np.random.normal(size=(32, 10))\n+\n+        with distribution.scope():\n+            model.compile(loss=\"mse\")\n+            model.fit(inputs, labels)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4da630253d45c8c2f9ea23be6c8f82cf8393c21f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 6893 | Contributors (this commit): 18 | Commits (past 90d): 91 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -772,7 +772,7 @@ def sort(x, axis=-1):\n \n def split(x, indices_or_sections, axis=0):\n     x = convert_to_tensor(x)\n-    if isinstance(indices_or_sections, list):\n+    if isinstance(indices_or_sections, (list, tuple)):\n         idxs = convert_to_tensor(indices_or_sections)\n         start_size = indices_or_sections[0]\n         end_size = x.shape[axis] - indices_or_sections[-1]\n\n@@ -4369,6 +4369,8 @@ def sort(x, axis=-1):\n class Split(Operation):\n     def __init__(self, indices_or_sections, axis=0):\n         super().__init__()\n+        if not isinstance(indices_or_sections, int):\n+            indices_or_sections = tuple(indices_or_sections)\n         self.indices_or_sections = indices_or_sections\n         self.axis = axis\n \n@@ -4399,7 +4401,7 @@ class Split(Operation):\n                 for _ in range(self.indices_or_sections)\n             ]\n \n-        indices_or_sections = [0] + self.indices_or_sections\n+        indices_or_sections = (0, *self.indices_or_sections, x_size_on_axis)\n         output_size = np.diff(indices_or_sections)\n         outputs = []\n         for i in range(len(output_size)):\n\n@@ -1169,7 +1169,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3, 3])\n         self.assertEqual(knp.split(x, 2)[0].shape, (None, 3, 3))\n         self.assertEqual(knp.split(x, 3, axis=1)[0].shape, (None, 1, 3))\n+        self.assertEqual(len(knp.split(x, [1, 3], axis=1)), 3)\n+        self.assertEqual(knp.split(x, [1, 3], axis=1)[0].shape, (None, 1, 3))\n         self.assertEqual(knp.split(x, [1, 3], axis=1)[1].shape, (None, 2, 3))\n+        self.assertEqual(knp.split(x, [1, 3], axis=1)[2].shape, (None, 0, 3))\n \n     def test_sqrt(self):\n         x = KerasTensor([None, 3])\n@@ -1631,7 +1634,10 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(len(knp.split(x, 2)), 2)\n         self.assertEqual(knp.split(x, 2)[0].shape, (1, 3))\n         self.assertEqual(knp.split(x, 3, axis=1)[0].shape, (2, 1))\n+        self.assertEqual(len(knp.split(x, [1, 3], axis=1)), 3)\n+        self.assertEqual(knp.split(x, [1, 3], axis=1)[0].shape, (2, 1))\n         self.assertEqual(knp.split(x, [1, 3], axis=1)[1].shape, (2, 2))\n+        self.assertEqual(knp.split(x, [1, 3], axis=1)[2].shape, (2, 0))\n \n         with self.assertRaises(ValueError):\n             knp.split(x, 2, axis=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a600ddd23b4f6b06905351d5703d2364979ae448", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 15/11 | Churn Δ: 69 | Churn Cumulative: 1735 | Contributors (this commit): 8 | Commits (past 90d): 24 | Contributors (cumulative): 10 | DMM Complexity: 0.5636363636363636\n\nDIFF:\n@@ -31,16 +31,45 @@ class ActivationsTest(testing.TestCase):\n             expected[:, i] = _ref_softmax(x[:, i])\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n-    # TODO: Fails on Tuple Axis\n-    # ops/nn_ops.py:3824: TypeError:\n-    # '<=' not supported between instances of 'int' and 'tuple'\n-    # def test_softmax_3d_axis_tuple(self):\n-    #     x = np.random.random((2, 3, 5))\n-    #     result = activations.softmax([x], axis=(1, 2))[0]\n-    #     expected = np.zeros((2, 3, 5))\n-    #     for i in range(2):\n-    #         expected[i, :, :] = _ref_softmax(x[i, :, :])\n-    #     self.assertAllClose(result, expected, rtol=1e-05)\n+    def test_softmax_3d_axis_tuple(self):\n+        x = np.random.random((2, 3, 5))\n+        result = activations.softmax(x, axis=(1, 2))\n+        expected = np.zeros((2, 3, 5))\n+        for i in range(2):\n+            expected[i, :, :] = _ref_softmax(x[i, :, :])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_softmax_1d(self):\n+        x = np.random.random(5)\n+        result = activations.softmax(x)\n+        expected = _ref_softmax(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_softmax_higher_dim(self):\n+        x = np.random.random((2, 3, 4, 5))\n+        result = activations.softmax(x, axis=(2, 3))\n+        expected = np.zeros((2, 3, 4, 5))\n+        for i in range(2):\n+            for j in range(3):\n+                expected[i, j, :, :] = _ref_softmax(x[i, j, :, :])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_softmax_higher_dim_multiple_axes(self):\n+        x = np.random.random((2, 3, 4, 5, 6))\n+        result = activations.softmax(x, axis=(2, 3, 4))\n+        expected = np.zeros((2, 3, 4, 5, 6))\n+        for i in range(2):\n+            for j in range(3):\n+                expected[i, j, :, :, :] = _ref_softmax(x[i, j, :, :, :])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_softmax_negative_axis(self):\n+        x = np.random.random((2, 5))\n+        result = activations.softmax(x, axis=-1)\n+        expected = np.zeros((2, 5))\n+        for i in range(2):\n+            expected[i, :] = _ref_softmax(x[i, :])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n \n     def test_temporal_softmax(self):\n         x = np.random.random((2, 2, 3)) * 10\n\n@@ -494,6 +494,26 @@ def softmax(x, axis=-1):\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Softmax(axis).symbolic_call(x)\n+    if isinstance(axis, tuple):\n+        original_shape = x.shape\n+        new_shape = []\n+        skip_dims = set(axis)\n+        i = 0\n+        while i < len(original_shape):\n+            if i in skip_dims:\n+                size = 1\n+                while i in skip_dims:\n+                    size *= original_shape[i]\n+                    i += 1\n+                new_shape.append(size)\n+            else:\n+                new_shape.append(original_shape[i])\n+                i += 1\n+        x = x.reshape(new_shape)\n+        x = backend.nn.softmax(x, axis=-1)\n+        x = x.reshape(original_shape)\n+        return x\n+    else:\n         return backend.nn.softmax(x, axis=axis)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#75b83f6554531b06eaa41473e661a3184c2f7599", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 206 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 14 | Complexity Δ (Sum/Max): 23/15 | Churn Δ: 208 | Churn Cumulative: 814 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,9 +6,10 @@ Currently only the JAX backend has been implemented, and the Tensorflow backend\n will be implemented in future (via tf.dtensor API).\n \"\"\"\n \n-import logging\n-\n+import collections\n import contextlib\n+import logging\n+import re\n \n import numpy as np\n \n@@ -288,6 +289,103 @@ class DataParallel(Distribution):\n         return TensorLayout(variable_shard_spec, self.device_mesh)\n \n \n+class LayoutMap(collections.abc.MutableMapping):\n+    \"\"\"A dict-like object that maps string to `TensorLayout` instances.\n+\n+    `LayoutMap` uses a string as key and a `TensorLayout` as value. There is a\n+    behavior difference between a normal Python dict and this class. The string\n+    key will be treated as a regex when retrieving the value. See the docstring\n+    of `get` for more details.\n+\n+    See below for a usage example. You can define the naming schema\n+    of the `TensorLayout`, and then retrieve the corresponding\n+    `TensorLayout` instance.\n+\n+    In the normal case, the key to query is usually the `variable.path`, which\n+    is the idenifier of the variable.\n+\n+    ```python\n+    layout_map = LayoutMap(device_mesh=None)\n+    layout_map['.*dense.*kernel'] = layout_2d\n+    layout_map['.*dense.*bias'] = layout_1d\n+    layout_map['.*conv2d.*kernel'] = layout_4d\n+    layout_map['.*conv2d.*bias'] = layout_1d\n+\n+    layout_1 = layout_map['dense_1.kernel']             # layout_1 == layout_2d\n+    layout_2 = layout_map['dense_1.bias']               # layout_2 == layout_1d\n+    layout_3 = layout_map['dense_2.kernel']             # layout_3 == layout_2d\n+    layout_4 = layout_map['dense_2.bias']               # layout_4 == layout_1d\n+    layout_5 = layout_map['my_model/conv2d_123/kernel'] # layout_5 == layout_4d\n+    layout_6 = layout_map['my_model/conv2d_123/bias']   # layout_6 == layout_1d\n+    layout_7 = layout_map['my_model/conv3d_1/kernel']   # layout_7 == None\n+    layout_8 = layout_map['my_model/conv3d_1/bias']     # layout_8 == None\n+    ```\n+\n+    Args:\n+        device_mesh: An optional `DeviceMesh` that can be used to populate the\n+            `TensorLayout.device_mesh` if the `TensorLayout.device_mesh` is not\n+            set.\n+    \"\"\"\n+\n+    def __init__(self, device_mesh=None):\n+        self._layout_map = collections.OrderedDict()\n+        self._device_mesh = device_mesh\n+\n+    def __getitem__(self, key):\n+        \"\"\"Retrieve the corresponding layout by the string key.\n+\n+        When there isn't an exact match, all the existing keys in the layout map\n+        will be treated as a regex and map against the input key again. The\n+        first match will be returned, based on the key insertion order. Return\n+        None if there isn't any match found.\n+\n+        Args:\n+            key: the string key as the query for the layout.\n+\n+        Returns:\n+            Corresponding layout based on the query.\n+        \"\"\"\n+        if key in self._layout_map:\n+            return self._layout_map[key]\n+\n+        for k in self._layout_map:\n+            if re.match(k, key):\n+                return self._layout_map[k]\n+        return None\n+\n+    def __setitem__(self, key, layout):\n+        if key in self._layout_map:\n+            raise ValueError(\n+                f\"{key} already exist in the LayoutMap with \"\n+                f\"value {self._layout_map[key]}. Please make sure to \"\n+                \"not use duplicated keys.\"\n+            )\n+        if not isinstance(layout, TensorLayout):\n+            raise ValueError(\n+                f\"{layout} should be a TensorLayout type, got {type(layout)}\"\n+            )\n+        self._maybe_populate_device_mesh(layout)\n+        self._layout_map[key] = layout\n+\n+    def __delitem__(self, key):\n+        # let the dict to handle the key missing error\n+        return self._layout_map.pop(key)\n+\n+    def __len__(self):\n+        return len(self._layout_map)\n+\n+    def __iter__(self):\n+        return iter(self._layout_map)\n+\n+    @property\n+    def device_mesh(self):\n+        return self._device_mesh\n+\n+    def _maybe_populate_device_mesh(self, layout):\n+        if layout.device_mesh is None and self.device_mesh is not None:\n+            layout.device_mesh = self.device_mesh\n+\n+\n def distribution():\n     \"\"\"Retrieve the current distribution from global context.\"\"\"\n     return global_state.get_global_attribute(GLOBAL_ATTRIBUTE_NAME)\n\n@@ -193,6 +193,112 @@ class DataParallelDistributionTest(testing.TestCase):\n         self.assertEqual(variable_layout.axes, [None])\n \n \n+class LayoutMapTest(testing.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        self.devices = [\"CPU:{i}\" for i in range(8)]\n+        shape = (4, 2)\n+        axis_names = [\"data\", \"model\"]\n+\n+        self.device_mesh = distribution_lib.DeviceMesh(\n+            shape, axis_names, self.devices\n+        )\n+        self.sharded_2d = distribution_lib.TensorLayout([None, \"model\"])\n+        self.sharded_1d = distribution_lib.TensorLayout([\"model\"])\n+\n+        self.replicated_2d = distribution_lib.TensorLayout([None, None])\n+        self.replicated_1d = distribution_lib.TensorLayout([None])\n+\n+    def test_add(self):\n+        layout_map = distribution_lib.LayoutMap(self.device_mesh)\n+        layout_map[\"dense/kernel\"] = self.sharded_2d\n+        layout_map[\"dense/bias\"] = self.sharded_1d\n+\n+        # Make there are two items in the map, and we access them via the\n+        # underlying container at layout_map._layout_map\n+        self.assertLen(layout_map, 2)\n+\n+        kernel_layout = layout_map[\"dense/kernel\"]\n+        self.assertEqual(kernel_layout.axes, [None, \"model\"])\n+        self.assertIs(kernel_layout.device_mesh, self.device_mesh)\n+\n+        bias_layout = layout_map[\"dense/bias\"]\n+        self.assertEqual(bias_layout.axes, [\"model\"])\n+        self.assertIs(bias_layout.device_mesh, self.device_mesh)\n+\n+        with self.assertRaisesRegex(ValueError, \"dense/kernel already exist\"):\n+            layout_map[\"dense/kernel\"] = self.sharded_2d\n+\n+        with self.assertRaisesRegex(ValueError, \"should be a TensorLayout\"):\n+            layout_map[\"conv.kernel\"] = [1, 2, 3]\n+\n+    def test_get(self):\n+        layout_map = distribution_lib.LayoutMap(self.device_mesh)\n+        layout_map[\"dense/kernel\"] = self.sharded_2d\n+        layout_map[\"dense/bias\"] = self.sharded_1d\n+\n+        layout_map[\"dense.*kernel\"] = self.replicated_2d\n+        layout_map[\"dense.*bias\"] = self.replicated_1d\n+\n+        layout_map[\".*bias\"] = self.sharded_1d\n+\n+        self.assertEqual(layout_map[\"dense/kernel\"], self.sharded_2d)\n+        self.assertEqual(layout_map[\"dense/bias\"], self.sharded_1d)\n+\n+        # Map against the wildcard bias rule for dense, and based on the order\n+        # of insertion, it will not use .*bias.\n+        self.assertEqual(layout_map[\"dense_2/kernel\"], self.replicated_2d)\n+        self.assertEqual(layout_map[\"dense_2/bias\"], self.replicated_1d)\n+\n+        self.assertIsNone(layout_map[\"conv2d/kernel\"])\n+        self.assertEqual(layout_map[\"conv2d/bias\"], self.sharded_1d)\n+\n+    def test_delete(self):\n+        layout_map = distribution_lib.LayoutMap(self.device_mesh)\n+\n+        layout_map[\"dense/kernel\"] = self.sharded_2d\n+        layout_map[\"dense/bias\"] = self.sharded_1d\n+\n+        self.assertEqual(layout_map.pop(\"dense/kernel\"), self.sharded_2d)\n+        # Make sure to match against the exact string, not the regex\n+        with self.assertRaises(KeyError):\n+            layout_map.pop(\".*bias\")\n+\n+        # Make sure del also works\n+        del layout_map[\"dense/bias\"]\n+\n+        self.assertLen(layout_map, 0)\n+\n+    def test_len(self):\n+        layout_map = distribution_lib.LayoutMap(self.device_mesh)\n+        self.assertLen(layout_map, 0)\n+\n+        layout_map[\"dense/kernel\"] = self.sharded_2d\n+        layout_map[\"dense/bias\"] = self.sharded_1d\n+\n+        self.assertLen(layout_map, 2)\n+\n+    def test_iter(self):\n+        layout_map = distribution_lib.LayoutMap(self.device_mesh)\n+\n+        layout_map[\"dense/kernel\"] = self.sharded_2d\n+        layout_map[\"dense/bias\"] = self.sharded_1d\n+\n+        # Make sure the items are ordered based on the insertion order.\n+        self.assertEqual(\n+            list(layout_map.keys()), [\"dense/kernel\", \"dense/bias\"]\n+        )\n+\n+        keys = []\n+        values = []\n+        for k, v in layout_map.items():\n+            keys.append(k)\n+            values.append(v)\n+\n+        self.assertEqual(keys, [\"dense/kernel\", \"dense/bias\"])\n+        self.assertEqual(values, [self.sharded_2d, self.sharded_1d])\n+\n+\n @pytest.mark.skipif(\n     backend.backend() != \"jax\",\n     reason=\"Backend specific test\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#de886b35e0c39444392f5d1ae244c02101845763", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 446 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -8,8 +8,8 @@ will be implemented in future (via tf.dtensor API).\n \n import collections\n import contextlib\n-import logging\n import re\n+import warnings\n \n import numpy as np\n \n@@ -254,7 +254,7 @@ class DataParallel(Distribution):\n             )\n         super().__init__(device_mesh)\n         if self.device_mesh.devices.ndim != 1:\n-            logging.warning(\n+            warnings.warn(\n                 \"Expect the input mesh to be 1D, but received \"\n                 \"mesh.devices.ndim=%d. \"\n                 \"The first axis will be used for data-parallel sharding.\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1eb778896614dbdbed84866b10e7e3bfa874469", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): 19/15 | Churn Δ: 81 | Churn Cumulative: 1816 | Contributors (this commit): 8 | Commits (past 90d): 26 | Contributors (cumulative): 10 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -15,6 +15,13 @@ def _ref_softplus(x):\n     return np.log(np.ones_like(x) + np.exp(x))\n \n \n+def _ref_log_softmax(values):\n+    max_val = np.max(values)  # for numerical stability\n+    stabilized_values = values - max_val\n+    log_sum_exp = np.log(np.sum(np.exp(stabilized_values)))\n+    return stabilized_values - log_sum_exp\n+\n+\n class ActivationsTest(testing.TestCase):\n     def test_softmax(self):\n         x = np.random.random((2, 5))\n@@ -77,6 +84,60 @@ class ActivationsTest(testing.TestCase):\n         expected = _ref_softmax(x[0, 0])\n         self.assertAllClose(result[0, 0], expected, rtol=1e-05)\n \n+    def test_log_softmax_2d_axis_0(self):\n+        x = np.random.random((2, 5))\n+        result = activations.log_softmax(x[np.newaxis, :], axis=1)[0]\n+        expected = np.zeros((2, 5))\n+        for i in range(5):\n+            expected[:, i] = _ref_log_softmax(x[:, i])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_log_softmax_3d_axis_tuple(self):\n+        x = np.random.random((2, 3, 5))\n+        result = activations.log_softmax(x, axis=(1, 2))\n+        expected = np.zeros((2, 3, 5))\n+        for i in range(2):\n+            expected[i, :, :] = _ref_log_softmax(x[i, :, :])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_log_softmax_1d(self):\n+        x = np.random.random(5)\n+        result = activations.log_softmax(x)\n+        expected = _ref_log_softmax(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_log_softmax_higher_dim(self):\n+        x = np.random.random((2, 3, 4, 5))\n+        result = activations.log_softmax(x, axis=(2, 3))\n+        expected = np.zeros((2, 3, 4, 5))\n+        for i in range(2):\n+            for j in range(3):\n+                expected[i, j, :, :] = _ref_log_softmax(x[i, j, :, :])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_log_softmax_higher_dim_multiple_axes(self):\n+        x = np.random.random((2, 3, 4, 5, 6))\n+        result = activations.log_softmax(x, axis=(2, 3, 4))\n+        expected = np.zeros((2, 3, 4, 5, 6))\n+        for i in range(2):\n+            for j in range(3):\n+                expected[i, j, :, :, :] = _ref_log_softmax(x[i, j, :, :, :])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_log_softmax_negative_axis(self):\n+        x = np.random.random((2, 5))\n+        result = activations.log_softmax(x, axis=-1)\n+        expected = np.zeros((2, 5))\n+        for i in range(2):\n+            expected[i, :] = _ref_log_softmax(x[i, :])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_temporal_log_softmax(self):\n+        x = np.random.random((2, 2, 3)) * 10\n+        result = activations.log_softmax(x[np.newaxis, :])[0]\n+        expected = _ref_log_softmax(x[0, 0])\n+        self.assertAllClose(result[0, 0], expected, rtol=1e-05)\n+\n     def test_selu(self):\n         alpha = 1.6732632423543772848170429916717\n         scale = 1.0507009873554804934193349852946\n\n@@ -559,6 +559,26 @@ def log_softmax(x, axis=-1):\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return LogSoftmax(axis).symbolic_call(x)\n+    if isinstance(axis, tuple):\n+        original_shape = x.shape\n+        new_shape = []\n+        skip_dims = set(axis)\n+        i = 0\n+        while i < len(original_shape):\n+            if i in skip_dims:\n+                size = 1\n+                while i in skip_dims:\n+                    size *= original_shape[i]\n+                    i += 1\n+                new_shape.append(size)\n+            else:\n+                new_shape.append(original_shape[i])\n+                i += 1\n+        x = x.reshape(new_shape)\n+        x = backend.nn.log_softmax(x, axis=-1)\n+        x = x.reshape(original_shape)\n+        return x\n+    else:\n         return backend.nn.log_softmax(x, axis=axis)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f5c3e3c831364af7e05a8cb38fa90f2654dbad0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 576 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 9 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 576 | Churn Cumulative: 576 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,576 @@\n+\"\"\"\n+Title: Structured data learning with TabTransformer\n+Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n+Date created: 2022/01/18\n+Last modified: 2022/01/18\n+Description: Using contextual embeddings for structured data classification.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+This example demonstrates how to do structured data classification using\n+[TabTransformer](https://arxiv.org/abs/2012.06678), a deep tabular data modeling\n+architecture for supervised and semi-supervised learning.\n+The TabTransformer is built upon self-attention based Transformers.\n+The Transformer layers transform the embeddings of categorical features\n+into robust contextual embeddings to achieve higher predictive accuracy.\n+\n+\n+\n+## Setup\n+\"\"\"\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import ops\n+\n+import math\n+import numpy as np\n+import pandas as pd\n+from tensorflow import data as tf_data\n+import matplotlib.pyplot as plt\n+from functools import partial\n+\n+\"\"\"\n+## Prepare the data\n+\n+This example uses the\n+[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n+provided by the\n+[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n+The task is binary classification\n+to predict whether a person is likely to be making over USD 50,000 a year.\n+\n+The dataset includes 48,842 instances with 14 input features: 5 numerical features and 9 categorical features.\n+\n+First, let's load the dataset from the UCI Machine Learning Repository into a Pandas\n+DataFrame:\n+\"\"\"\n+\n+CSV_HEADER = [\n+    \"age\",\n+    \"workclass\",\n+    \"fnlwgt\",\n+    \"education\",\n+    \"education_num\",\n+    \"marital_status\",\n+    \"occupation\",\n+    \"relationship\",\n+    \"race\",\n+    \"gender\",\n+    \"capital_gain\",\n+    \"capital_loss\",\n+    \"hours_per_week\",\n+    \"native_country\",\n+    \"income_bracket\",\n+]\n+\n+train_data_url = (\n+    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n+)\n+train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n+\n+test_data_url = (\n+    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n+)\n+test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n+\n+print(f\"Train dataset shape: {train_data.shape}\")\n+print(f\"Test dataset shape: {test_data.shape}\")\n+\n+\"\"\"\n+Remove the first record (because it is not a valid data example) and a trailing 'dot' in the class labels.\n+\"\"\"\n+\n+test_data = test_data[1:]\n+test_data.income_bracket = test_data.income_bracket.apply(\n+    lambda value: value.replace(\".\", \"\")\n+)\n+\n+\"\"\"\n+Now we store the training and test data in separate CSV files.\n+\"\"\"\n+\n+train_data_file = \"train_data.csv\"\n+test_data_file = \"test_data.csv\"\n+\n+train_data.to_csv(train_data_file, index=False, header=False)\n+test_data.to_csv(test_data_file, index=False, header=False)\n+\n+\"\"\"\n+## Define dataset metadata\n+\n+Here, we define the metadata of the dataset that will be useful for reading and parsing\n+the data into input features, and encoding the input features with respect to their types.\n+\"\"\"\n+\n+# A list of the numerical feature names.\n+NUMERIC_FEATURE_NAMES = [\n+    \"age\",\n+    \"education_num\",\n+    \"capital_gain\",\n+    \"capital_loss\",\n+    \"hours_per_week\",\n+]\n+# A dictionary of the categorical features and their vocabulary.\n+CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n+    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n+    \"education\": sorted(list(train_data[\"education\"].unique())),\n+    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n+    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n+    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n+    \"race\": sorted(list(train_data[\"race\"].unique())),\n+    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n+    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n+}\n+# Name of the column to be used as instances weight.\n+WEIGHT_COLUMN_NAME = \"fnlwgt\"\n+# A list of the categorical feature names.\n+CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n+# A list of all the input features.\n+FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n+# A list of column default values for each feature.\n+COLUMN_DEFAULTS = [\n+    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n+    for feature_name in CSV_HEADER\n+]\n+# The name of the target feature.\n+TARGET_FEATURE_NAME = \"income_bracket\"\n+# A list of the labels of the target features.\n+TARGET_LABELS = [\" <=50K\", \" >50K\"]\n+\n+\"\"\"\n+## Configure the hyperparameters\n+\n+The hyperparameters includes model architecture and training configurations.\n+\"\"\"\n+\n+LEARNING_RATE = 0.001\n+WEIGHT_DECAY = 0.0001\n+DROPOUT_RATE = 0.2\n+BATCH_SIZE = 265\n+NUM_EPOCHS = 15\n+\n+NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n+NUM_HEADS = 4  # Number of attention heads.\n+EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n+MLP_HIDDEN_UNITS_FACTORS = [\n+    2,\n+    1,\n+]  # MLP hidden layer units, as factors of the number of inputs.\n+NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n+\n+\"\"\"\n+## Implement data reading pipeline\n+\n+We define an input function that reads and parses the file, then converts features\n+and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n+for training or evaluation.\n+\"\"\"\n+\n+target_label_lookup = layers.StringLookup(\n+    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n+)\n+\n+\n+def prepare_example(features, target):\n+    target_index = target_label_lookup(target)\n+    weights = features.pop(WEIGHT_COLUMN_NAME)\n+    return features, target_index, weights\n+\n+\n+lookup_dict = {}\n+for feature_name in CATEGORICAL_FEATURE_NAMES:\n+    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n+    # Create a lookup to convert a string values to an integer indices.\n+    # Since we are not using a mask token, nor expecting any out of vocabulary\n+    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n+    lookup = layers.StringLookup(\n+        vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n+    )\n+    lookup_dict[feature_name] = lookup\n+\n+\n+def encode_categorical(batch_x, batch_y, weights):\n+    for feature_name in CATEGORICAL_FEATURE_NAMES:\n+        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n+\n+    return batch_x, batch_y, weights\n+\n+\n+def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n+    dataset = (\n+        tf.data.experimental.make_csv_dataset(\n+            csv_file_path,\n+            batch_size=batch_size,\n+            column_names=CSV_HEADER,\n+            column_defaults=COLUMN_DEFAULTS,\n+            label_name=TARGET_FEATURE_NAME,\n+            num_epochs=1,\n+            header=False,\n+            na_value=\"?\",\n+            shuffle=shuffle,\n+        )\n+        .map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n+        .map(encode_categorical)\n+    )\n+    return dataset.cache()\n+\n+\n+\"\"\"\n+## Implement a training and evaluation procedure\n+\"\"\"\n+\n+\n+def run_experiment(\n+    model,\n+    train_data_file,\n+    test_data_file,\n+    num_epochs,\n+    learning_rate,\n+    weight_decay,\n+    batch_size,\n+):\n+    optimizer = keras.optimizers.AdamW(\n+        learning_rate=learning_rate, weight_decay=weight_decay\n+    )\n+\n+    model.compile(\n+        optimizer=optimizer,\n+        loss=keras.losses.BinaryCrossentropy(),\n+        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n+    )\n+\n+    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n+    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n+\n+    print(\"Start training the model...\")\n+    history = model.fit(\n+        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n+    )\n+    print(\"Model training finished\")\n+\n+    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n+\n+    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n+\n+    return history\n+\n+\n+\"\"\"\n+## Create model inputs\n+\n+Now, define the inputs for the models as a dictionary, where the key is the feature name,\n+and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n+and data type.\n+\"\"\"\n+\n+\n+def create_model_inputs():\n+    inputs = {}\n+    for feature_name in FEATURE_NAMES:\n+        if feature_name in NUMERIC_FEATURE_NAMES:\n+            inputs[feature_name] = layers.Input(\n+                name=feature_name, shape=(), dtype=\"float32\"\n+            )\n+        else:\n+            inputs[feature_name] = layers.Input(\n+                name=feature_name, shape=(), dtype=\"float32\"\n+            )\n+    return inputs\n+\n+\n+\"\"\"\n+## Encode features\n+\n+The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n+We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n+regardless their vocabulary sizes. This is required for the Transformer model.\n+\"\"\"\n+\n+\n+def encode_inputs(inputs, embedding_dims):\n+    encoded_categorical_feature_list = []\n+    numerical_feature_list = []\n+\n+    for feature_name in inputs:\n+        if feature_name in CATEGORICAL_FEATURE_NAMES:\n+            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n+            # Create a lookup to convert a string values to an integer indices.\n+            # Since we are not using a mask token, nor expecting any out of vocabulary\n+            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n+\n+            # Convert the string input values into integer indices.\n+\n+            # Create an embedding layer with the specified dimensions.\n+            embedding = layers.Embedding(\n+                input_dim=len(vocabulary), output_dim=embedding_dims\n+            )\n+\n+            # Convert the index values to embedding representations.\n+            encoded_categorical_feature = embedding(inputs[feature_name])\n+            encoded_categorical_feature_list.append(encoded_categorical_feature)\n+\n+        else:\n+            # Use the numerical features as-is.\n+            numerical_feature = ops.expand_dims(inputs[feature_name], -1)\n+            numerical_feature_list.append(numerical_feature)\n+\n+    return encoded_categorical_feature_list, numerical_feature_list\n+\n+\n+\"\"\"\n+## Implement an MLP block\n+\"\"\"\n+\n+\n+def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n+    mlp_layers = []\n+    for units in hidden_units:\n+        mlp_layers.append(normalization_layer()),\n+        mlp_layers.append(layers.Dense(units, activation=activation))\n+        mlp_layers.append(layers.Dropout(dropout_rate))\n+\n+    return keras.Sequential(mlp_layers, name=name)\n+\n+\n+\"\"\"\n+## Experiment 1: a baseline model\n+\n+In the first experiment, we create a simple multi-layer feed-forward network.\n+\"\"\"\n+\n+\n+def create_baseline_model(\n+    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n+):\n+    # Create model inputs.\n+    inputs = create_model_inputs()\n+    # encode features.\n+    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n+        inputs, embedding_dims\n+    )\n+    # Concatenate all features.\n+    features = layers.concatenate(\n+        encoded_categorical_feature_list + numerical_feature_list\n+    )\n+    # Compute Feedforward layer units.\n+    feedforward_units = [features.shape[-1]]\n+\n+    # Create several feedforwad layers with skip connections.\n+    for layer_idx in range(num_mlp_blocks):\n+        features = create_mlp(\n+            hidden_units=feedforward_units,\n+            dropout_rate=dropout_rate,\n+            activation=keras.activations.gelu,\n+            normalization_layer=layers.LayerNormalization,\n+            name=f\"feedforward_{layer_idx}\",\n+        )(features)\n+\n+    # Compute MLP hidden_units.\n+    mlp_hidden_units = [\n+        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n+    ]\n+    # Create final MLP.\n+    features = create_mlp(\n+        hidden_units=mlp_hidden_units,\n+        dropout_rate=dropout_rate,\n+        activation=keras.activations.selu,\n+        normalization_layer=layers.BatchNormalization,\n+        name=\"MLP\",\n+    )(features)\n+\n+    # Add a sigmoid as a binary classifer.\n+    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+baseline_model = create_baseline_model(\n+    embedding_dims=EMBEDDING_DIMS,\n+    num_mlp_blocks=NUM_MLP_BLOCKS,\n+    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n+    dropout_rate=DROPOUT_RATE,\n+)\n+\n+print(\"Total model weights:\", baseline_model.count_params())\n+keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n+\n+\"\"\"\n+Let's train and evaluate the baseline model:\n+\"\"\"\n+\n+history = run_experiment(\n+    model=baseline_model,\n+    train_data_file=train_data_file,\n+    test_data_file=test_data_file,\n+    num_epochs=NUM_EPOCHS,\n+    learning_rate=LEARNING_RATE,\n+    weight_decay=WEIGHT_DECAY,\n+    batch_size=BATCH_SIZE,\n+)\n+\n+\"\"\"\n+The baseline linear model achieves ~81% validation accuracy.\n+\"\"\"\n+\n+\"\"\"\n+## Experiment 2: TabTransformer\n+\n+The TabTransformer architecture works as follows:\n+\n+1. All the categorical features are encoded as embeddings, using the same `embedding_dims`.\n+This means that each value in each categorical feature will have its own embedding vector.\n+2. A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding.\n+3. The embedded categorical features are fed into a stack of Transformer blocks.\n+Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n+3. The outputs of the final Transformer layer, which are the *contextual embeddings* of the categorical features,\n+are concatenated with the input numerical features, and fed into a final MLP block.\n+4. A `softmax` classifer is applied at the end of the model.\n+\n+The [paper](https://arxiv.org/abs/2012.06678) discusses both addition and concatenation of the column embedding in the\n+*Appendix: Experiment and Model Details* section.\n+The architecture of TabTransformer is shown below, as presented in the paper.\n+\n+<img src=\"https://raw.githubusercontent.com/keras-team/keras-io/master/examples/structured_data/img/tabtransformer/tabtransformer.png\" width=\"500\"/>\n+\"\"\"\n+\n+\n+def create_tabtransformer_classifier(\n+    num_transformer_blocks,\n+    num_heads,\n+    embedding_dims,\n+    mlp_hidden_units_factors,\n+    dropout_rate,\n+    use_column_embedding=False,\n+):\n+    # Create model inputs.\n+    inputs = create_model_inputs()\n+    # encode features.\n+    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n+        inputs, embedding_dims\n+    )\n+    # Stack categorical feature embeddings for the Tansformer.\n+    encoded_categorical_features = ops.stack(encoded_categorical_feature_list, axis=1)\n+    # Concatenate numerical features.\n+    numerical_features = layers.concatenate(numerical_feature_list)\n+\n+    # Add column embedding to categorical feature embeddings.\n+    if use_column_embedding:\n+        num_columns = encoded_categorical_features.shape[1]\n+        column_embedding = layers.Embedding(\n+            input_dim=num_columns, output_dim=embedding_dims\n+        )\n+        column_indices = ops.arange(start=0, stop=num_columns, step=1)\n+        encoded_categorical_features = encoded_categorical_features + column_embedding(\n+            column_indices\n+        )\n+\n+    # Create multiple layers of the Transformer block.\n+    for block_idx in range(num_transformer_blocks):\n+        # Create a multi-head attention layer.\n+        attention_output = layers.MultiHeadAttention(\n+            num_heads=num_heads,\n+            key_dim=embedding_dims,\n+            dropout=dropout_rate,\n+            name=f\"multihead_attention_{block_idx}\",\n+        )(encoded_categorical_features, encoded_categorical_features)\n+        # Skip connection 1.\n+        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n+            [attention_output, encoded_categorical_features]\n+        )\n+        # Layer normalization 1.\n+        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n+        # Feedforward.\n+        feedforward_output = create_mlp(\n+            hidden_units=[embedding_dims],\n+            dropout_rate=dropout_rate,\n+            activation=keras.activations.gelu,\n+            normalization_layer=partial(\n+                layers.LayerNormalization, epsilon=1e-6\n+            ),  # using partial to provide keyword arguments before initialization\n+            name=f\"feedforward_{block_idx}\",\n+        )(x)\n+        # Skip connection 2.\n+        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n+        # Layer normalization 2.\n+        encoded_categorical_features = layers.LayerNormalization(\n+            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n+        )(x)\n+\n+    # Flatten the \"contextualized\" embeddings of the categorical features.\n+    categorical_features = layers.Flatten()(encoded_categorical_features)\n+    # Apply layer normalization to the numerical features.\n+    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n+    # Prepare the input for the final MLP block.\n+    features = layers.concatenate([categorical_features, numerical_features])\n+\n+    # Compute MLP hidden_units.\n+    mlp_hidden_units = [\n+        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n+    ]\n+    # Create final MLP.\n+    features = create_mlp(\n+        hidden_units=mlp_hidden_units,\n+        dropout_rate=dropout_rate,\n+        activation=keras.activations.selu,\n+        normalization_layer=layers.BatchNormalization,\n+        name=\"MLP\",\n+    )(features)\n+\n+    # Add a sigmoid as a binary classifer.\n+    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+tabtransformer_model = create_tabtransformer_classifier(\n+    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n+    num_heads=NUM_HEADS,\n+    embedding_dims=EMBEDDING_DIMS,\n+    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n+    dropout_rate=DROPOUT_RATE,\n+)\n+\n+print(\"Total model weights:\", tabtransformer_model.count_params())\n+keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n+\n+\"\"\"\n+Let's train and evaluate the TabTransformer model:\n+\"\"\"\n+\n+history = run_experiment(\n+    model=tabtransformer_model,\n+    train_data_file=train_data_file,\n+    test_data_file=test_data_file,\n+    num_epochs=NUM_EPOCHS,\n+    learning_rate=LEARNING_RATE,\n+    weight_decay=WEIGHT_DECAY,\n+    batch_size=BATCH_SIZE,\n+)\n+\n+\"\"\"\n+The TabTransformer model achieves ~85% validation accuracy.\n+Note that, with the default parameter configurations, both the baseline and the TabTransformer\n+have similar number of trainable weights: 109,629 and 92,151 respectively, and both use the same training hyperparameters.\n+\"\"\"\n+\n+\"\"\"\n+## Conclusion\n+\n+TabTransformer significantly outperforms MLP and recent\n+deep networks for tabular data while matching the performance of tree-based ensemble models.\n+TabTransformer can be learned in end-to-end supervised training using labeled examples.\n+For a scenario where there are a few labeled examples and a large number of unlabeled\n+examples, a pre-training procedure can be employed to train the Transformer layers using unlabeled data.\n+This is followed by fine-tuning of the pre-trained Transformer layers along with\n+the top MLP layer using the labeled data.\n+\n+Example available on HuggingFace.\n+\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/🤗%20Model-TabTransformer-black.svg)](https://huggingface.co/keras-io/tab_transformer) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-TabTransformer-black.svg)](https://huggingface.co/spaces/keras-io/TabTransformer_Classification) |\n+\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
