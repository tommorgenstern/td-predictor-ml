{"custom_id": "scrapy#67adb2a05fbc0d8b0844cbfe615ec6566b08591a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 116 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@ Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n version_info = (0, 11, 0, 'dev')\n-__version__ = \"0.11\"\n+__version__ = \"0.11.0\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a034d078c85e53ff1021ee065d0ab740fb5a5728", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 36 | Churn Cumulative: 36 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,36 @@\n+import sys, os, glob\n+from subprocess import check_call\n+\n+def build(suffix):\n+    for ifn in glob.glob(\"debian/scrapy.*\") + glob.glob(\"debian/scrapyd.*\"):\n+        s = open(ifn).read()\n+        s = s.replace('SUFFIX', suffix)\n+        pre, suf = ifn.split('.', 1)\n+        ofn = \"%s-%s.%s\" % (pre, suffix, suf)\n+        with open(ofn, 'w') as of:\n+            of.write(s)\n+\n+    for ifn in ['debian/control', 'debian/changelog']:\n+        s = open(ifn).read()\n+        s = s.replace('SUFFIX', suffix)\n+        with open(ifn, 'w') as of:\n+            of.write(s)\n+\n+    check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)-r$(hg tip --template \"{rev}\")+$(date +%s) \"Automatic build\"', shell=True)\n+    check_call('debuild -us -uc -b', shell=True)\n+\n+def clean(suffix):\n+    for f in glob.glob(\"debian/scrapy-%s.*\" % suffix) + \\\n+            glob.glob(\"debian/scrapyd-%s.*\" % suffix):\n+        os.remove(f)\n+\n+def main():\n+    cmd = sys.argv[1]\n+    suffix = '%s.%s' % __import__('scrapy').version_info[:2]\n+    if cmd == 'build':\n+        build(suffix)\n+    elif cmd == 'clean':\n+        clean(suffix)\n+\n+if __name__ == '__main__':\n+    main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91a7c25797479bbe182694694cf267d6fe659c27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 27 | Files Changed: 5 | Hunks: 21 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 64 | Churn Cumulative: 931 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -50,6 +50,8 @@ class RedirectMiddleware(object):\n         if ttl and redirects <= self.max_redirect_times:\n             redirected.meta['redirect_times'] = redirects\n             redirected.meta['redirect_ttl'] = ttl - 1\n+            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + \\\n+                [request.url]\n             redirected.dont_filter = request.dont_filter\n             redirected.priority = request.priority + self.priority_adjust\n             log.msg(\"Redirecting (%s) to %s from %s\" % (reason, redirected, request),\n\n@@ -13,23 +13,24 @@ from scrapy.http.common import deprecated_setter\n \n class Response(object_ref):\n \n-    __slots__ = ['_url', 'headers', 'status', '_body', 'request', '_meta', \\\n+    __slots__ = ['_url', 'headers', 'status', '_body', 'request', \\\n         'flags', '__weakref__']\n \n-    def __init__(self, url, status=200, headers=None, body='', meta=None, flags=None):\n+    def __init__(self, url, status=200, headers=None, body='', flags=None, request=None):\n         self.headers = Headers(headers or {})\n         self.status = int(status)\n         self._set_body(body)\n         self._set_url(url)\n-        self.request = None\n+        self.request = request\n         self.flags = [] if flags is None else list(flags)\n-        self._meta = dict(meta) if meta else None\n \n     @property\n     def meta(self):\n-        if self._meta is None:\n-            self._meta = {}\n-        return self._meta\n+        try:\n+            return self.request.meta\n+        except AttributeError:\n+            raise AttributeError(\"Response.meta not available, this response \" \\\n+                \"is not tied to any request\")\n \n     def _get_url(self):\n         return self._url\n@@ -61,7 +62,7 @@ class Response(object_ref):\n     body = property(_get_body, deprecated_setter(_set_body, 'body'))\n \n     def __repr__(self):\n-        attrs = ['url', 'status', 'body', 'headers', 'meta', 'flags']\n+        attrs = ['url', 'status', 'body', 'headers', 'request', 'flags']\n         args = \", \".join([\"%s=%r\" % (a, getattr(self, a)) for a in attrs])\n         return \"%s(%s)\" % (self.__class__.__name__, args)\n \n@@ -76,7 +77,7 @@ class Response(object_ref):\n         \"\"\"Create a new Response with the same attributes except for those\n         given new values.\n         \"\"\"\n-        for x in ['url', 'status', 'headers', 'body', 'meta', 'flags']:\n+        for x in ['url', 'status', 'headers', 'body', 'request', 'flags']:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n\n@@ -26,12 +26,11 @@ class TextResponse(Response):\n \n     __slots__ = ['_encoding', '_cached_benc', '_cached_ubody']\n \n-    def __init__(self, url, status=200, headers=None, body=None, meta=None, \\\n-            flags=None, encoding=None):\n-        self._encoding = encoding\n+    def __init__(self, *args, **kwargs):\n+        self._encoding = kwargs.pop('encoding', None)\n         self._cached_benc = None\n         self._cached_ubody = None\n-        super(TextResponse, self).__init__(url, status, headers, body, meta, flags)\n+        super(TextResponse, self).__init__(*args, **kwargs)\n \n     def _set_url(self, url):\n         if isinstance(url, unicode):\n\n@@ -145,5 +145,17 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert isinstance(req, Request)\n         self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)\n \n+    def test_redirect_urls(self):\n+        req1 = Request('http://scrapytest.org/first')\n+        rsp1 = Response('http://scrapytest.org/first', headers={'Location': '/redirected'}, status=302)\n+        req2 = self.mw.process_response(req1, rsp1, self.spider)\n+        rsp2 = Response('http://scrapytest.org/redirected', headers={'Location': '/redirected2'}, status=302)\n+        req3 = self.mw.process_response(req2, rsp2, self.spider)\n+\n+        self.assertEqual(req2.url, 'http://scrapytest.org/redirected')\n+        self.assertEqual(req2.meta['redirect_urls'], ['http://scrapytest.org/first'])\n+        self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')\n+        self.assertEqual(req3.meta['redirect_urls'], ['http://scrapytest.org/first', 'http://scrapytest.org/redirected'])\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,7 +1,7 @@\n import unittest\n import weakref\n \n-from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers\n+from scrapy.http import Request, Response, TextResponse, HtmlResponse, XmlResponse, Headers\n from scrapy.utils.encoding import resolve_encoding\n \n \n@@ -26,15 +26,11 @@ class BaseResponseTest(unittest.TestCase):\n \n         assert isinstance(r.headers, Headers)\n         self.assertEqual(r.headers, {})\n-        self.assertEqual(r.meta, {})\n \n-        meta = {\"lala\": \"lolo\"}\n         headers = {\"caca\": \"coco\"}\n         body = \"a body\"\n-        r = self.response_class(\"http://www.example.com\", meta=meta, headers=headers, body=body)\n+        r = self.response_class(\"http://www.example.com\", headers=headers, body=body)\n \n-        assert r.meta is not meta\n-        self.assertEqual(r.meta, meta)\n         assert r.headers is not headers\n         self.assertEqual(r.headers[\"caca\"], \"coco\")\n \n@@ -48,17 +44,12 @@ class BaseResponseTest(unittest.TestCase):\n         \"\"\"Test Response copy\"\"\"\n \n         r1 = self.response_class(\"http://www.example.com\", body=\"Some body\")\n-        r1.meta['foo'] = 'bar'\n         r1.flags.append('cached')\n         r2 = r1.copy()\n \n         self.assertEqual(r1.status, r2.status)\n         self.assertEqual(r1.body, r2.body)\n \n-        # make sure meta dict is shallow copied\n-        assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n-        self.assertEqual(r1.meta, r2.meta)\n-\n         # make sure flags list is shallow copied\n         assert r1.flags is not r2.flags, \"flags must be a shallow copy, not identical\"\n         self.assertEqual(r1.flags, r2.flags)\n@@ -67,6 +58,12 @@ class BaseResponseTest(unittest.TestCase):\n         assert r1.headers is not r2.headers, \"headers must be a shallow copy, not identical\"\n         self.assertEqual(r1.headers, r2.headers)\n \n+    def test_copy_meta(self):\n+        req = Request(\"http://www.example.com\")\n+        req.meta['foo'] = 'bar'\n+        r1 = self.response_class(\"http://www.example.com\", body=\"Some body\", request=req)\n+        assert r1.meta is req.meta\n+\n     def test_copy_inherited_classes(self):\n         \"\"\"Test Response children copies preserve their class\"\"\"\n \n@@ -90,10 +87,9 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertEqual((r1.headers, r2.headers), ({}, hdrs))\n \n         # Empty attributes (which may fail if not compared properly)\n-        r3 = self.response_class(\"http://www.example.com\", meta={'a': 1}, flags=['cached'])\n-        r4 = r3.replace(body='', meta={}, flags=[])\n+        r3 = self.response_class(\"http://www.example.com\", flags=['cached'])\n+        r4 = r3.replace(body='', flags=[])\n         self.assertEqual(r4.body, '')\n-        self.assertEqual(r4.meta, {})\n         self.assertEqual(r4.flags, [])\n \n     def test_weakref_slots(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d59ef4823100f51bcf24831b0bc0154fece99528", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 18 | Churn Cumulative: 270 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_unicode\n from scrapy.utils.url import safe_url_string, urljoin_rfc, canonicalize_url, url_is_from_any_domain\n+from scrapy.utils.response import get_base_url\n \n class BaseSgmlLinkExtractor(FixedSGMLParser):\n \n@@ -20,13 +21,14 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n         self.current_link = None\n         self.unique = unique\n \n-    def _extract_links(self, response_text, response_url, response_encoding):\n+    def _extract_links(self, response_text, response_url, response_encoding, base_url=None):\n         \"\"\" Do the real extraction work \"\"\"\n         self.reset()\n         self.feed(response_text)\n         self.close()\n \n         ret = []\n+        if base_url is None:\n             base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n         for link in self.links:\n             link.url = urljoin_rfc(base_url, link.url, response_encoding)\n@@ -100,14 +102,16 @@ class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n             unique=unique, process_value=process_value)\n \n     def extract_links(self, response):\n+        base_url = None\n         if self.restrict_xpaths:\n             hxs = HtmlXPathSelector(response)\n             html = ''.join(''.join(html_fragm for html_fragm in hxs.select(xpath_expr).extract()) \\\n                 for xpath_expr in self.restrict_xpaths)\n+            base_url = get_base_url(response)\n         else:\n             html = response.body\n \n-        links = self._extract_links(html, response.url, response.encoding)\n+        links = self._extract_links(html, response.url, response.encoding, base_url)\n         links = self._process_links(links)\n         return links\n \n\n@@ -221,6 +221,16 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://example.org/other/page.html', text='Link text')])\n \n+    def test_base_url_with_restrict_xpaths(self):\n+        html = \"\"\"<html><head><title>Page title<title><base href=\"http://otherdomain.com/base/\" />\n+        <body><p><a href=\"item/12.html\">Item 12</a></p>\n+        </body></html>\"\"\"\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n+        lx = SgmlLinkExtractor(restrict_xpaths=\"//p\") \n+        self.assertEqual(lx.extract_links(response),\n+                         [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n+\n+\n class HTMLImageLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n         body = get_testdata('link_extractor', 'image_linkextractor.html')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f82ea19de1e61ef71373817f2ca9687b900744d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 136 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -34,7 +34,7 @@ class JsonRpcResource(JsonResource):\n         return self.get_target()\n \n     def render_POST(self, txrequest):\n-        reqstr = txrequest.content.read()\n+        reqstr = txrequest.content.getvalue()\n         target = self.get_target()\n         return jsonrpc_server_call(target, reqstr, self.json_decoder)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d726063d6b241c96444094551583698ae156e0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 163 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 6 | Methods Changed: 21 | Complexity Δ (Sum/Max): 22/8 | Churn Δ: 163 | Churn Cumulative: 660 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,6 +47,8 @@ class Shell(object):\n         elif response:\n             request = response.request\n             self.populate_vars(request.url, response, request, spider)\n+        else:\n+            self.populate_vars()\n         if self.code:\n             print eval(self.code, globals(), self.vars)\n         else:\n\n@@ -0,0 +1,22 @@\n+from twisted.trial import unittest\n+from twisted.internet import defer\n+\n+from scrapy.utils.testsite import SiteTest\n+from scrapy.utils.testproc import ProcessTest\n+\n+\n+class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n+\n+    command = 'fetch'\n+\n+    @defer.inlineCallbacks\n+    def test_output(self):\n+        _, out, _ = yield self.execute([self.url('/text')])\n+        self.assertEqual(out.strip(), 'Works')\n+\n+    @defer.inlineCallbacks\n+    def test_headers(self):\n+        _, out, _ = yield self.execute([self.url('/text'), '--headers'])\n+        headers = eval(out)\n+        assert 'TwistedWeb' in headers['Server'][0]\n+        self.assertEqual(headers['Content-Type'], ['text/plain'])\n\n@@ -0,0 +1,47 @@\n+from twisted.trial import unittest\n+from twisted.internet import defer\n+\n+from scrapy.utils.testsite import SiteTest\n+from scrapy.utils.testproc import ProcessTest\n+\n+\n+class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n+\n+    command = 'shell'\n+\n+    @defer.inlineCallbacks\n+    def test_empty(self):\n+        _, out, _ = yield self.execute(['-c', 'item'])\n+        assert 'Item' in out\n+\n+    @defer.inlineCallbacks\n+    def test_response_body(self):\n+        _, out, _ = yield self.execute([self.url('/text'), '-c', 'response.body'])\n+        assert 'Works' in out\n+\n+    @defer.inlineCallbacks\n+    def test_response_type_text(self):\n+        _, out, _ = yield self.execute([self.url('/text'), '-c', 'type(response)'])\n+        assert 'TextResponse' in out\n+\n+    @defer.inlineCallbacks\n+    def test_response_type_html(self):\n+        _, out, _ = yield self.execute([self.url('/html'), '-c', 'type(response)'])\n+        assert 'HtmlResponse' in out\n+\n+    @defer.inlineCallbacks\n+    def test_response_selector_html(self):\n+        xpath = 'hxs.select(\"//p[@class=\\'one\\']/text()\").extract()[0]'\n+        _, out, _ = yield self.execute([self.url('/html'), '-c', xpath])\n+        self.assertEqual(out.strip(), 'Works')\n+\n+    @defer.inlineCallbacks\n+    def test_response_encoding_gb18030(self):\n+        _, out, _ = yield self.execute([self.url('/enc-gb18030'), '-c', 'response.encoding'])\n+        self.assertEqual(out.strip(), 'gb18030')\n+\n+    @defer.inlineCallbacks\n+    def test_redirect(self):\n+        _, out, _ = yield self.execute([self.url('/redirect'), '-c', 'response.url'])\n+        assert out.strip().endswith('/redirected')\n+\n\n@@ -0,0 +1,15 @@\n+from twisted.trial import unittest\n+from twisted.internet import defer\n+\n+import scrapy\n+from scrapy.utils.testproc import ProcessTest\n+\n+\n+class VersionTest(ProcessTest, unittest.TestCase):\n+\n+    command = 'version'\n+\n+    @defer.inlineCallbacks\n+    def test_output(self):\n+        _, out, _ = yield self.execute([])\n+        self.assertEqual(out.strip(), \"Scrapy %s\" % scrapy.__version__)\n\n@@ -0,0 +1,46 @@\n+import sys\n+import os\n+\n+from twisted.internet import reactor, defer, protocol\n+\n+class ProcessTest(object):\n+\n+    command = None\n+    prefix = [sys.executable, '-m', 'scrapy.cmdline']\n+\n+    def execute(self, args, check_code=True, settings='missing'):\n+        env = os.environ.copy()\n+        env['SCRAPY_SETTINGS_MODULE'] = settings\n+        cmd = self.prefix + [self.command] + list(args)\n+        pp = TestProcessProtocol()\n+        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n+        reactor.spawnProcess(pp, cmd[0], cmd, env=env)\n+        return pp.deferred\n+\n+    def _process_finished(self, pp, cmd, check_code):\n+        if pp.exitcode and check_code:\n+            msg = \"process %s exit with code %d\" % (cmd, pp.exitcode)\n+            msg += \"\\n>>> stdout <<<\\n%s\" % pp.out\n+            msg += \"\\n\"\n+            msg += \"\\n>>> stderr <<<\\n%s\" % pp.err\n+            raise RuntimeError(msg)\n+        return pp.exitcode, pp.out, pp.err\n+\n+\n+class TestProcessProtocol(protocol.ProcessProtocol):\n+\n+    def __init__(self):\n+        self.deferred = defer.Deferred()\n+        self.out = ''\n+        self.err = ''\n+        self.exitcode = None\n+\n+    def outReceived(self, data):\n+        self.out += data\n+\n+    def errReceived(self, data):\n+        self.err += data\n+\n+    def processEnded(self, status):\n+        self.exitcode = status.value.exitCode\n+        self.deferred.callback(self)\n\n@@ -0,0 +1,31 @@\n+import urlparse\n+\n+from twisted.internet import reactor\n+from twisted.web import server, resource, static, util\n+\n+class SiteTest(object):\n+\n+    def setUp(self):\n+        self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n+        self.baseurl = \"http://localhost:%d/\" % self.site.getHost().port\n+\n+    def tearDown(self):\n+        self.site.stopListening()\n+\n+    def url(self, path):\n+        return urlparse.urljoin(self.baseurl, path)\n+\n+def test_site():\n+    r = resource.Resource()\n+    r.putChild(\"text\", static.Data(\"Works\", \"text/plain\"))\n+    r.putChild(\"html\", static.Data(\"<body><p class='one'>Works</p><p class='two'>World</p></body>\", \"text/html\"))\n+    r.putChild(\"enc-gb18030\", static.Data(\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"))\n+    r.putChild(\"redirect\", util.Redirect(\"/redirected\"))\n+    r.putChild(\"redirected\", static.Data(\"Redirected here\", \"text/plain\"))\n+    return server.Site(r)\n+    \n+\n+if __name__ == '__main__':\n+    port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n+    print \"http://localhost:%d/\" % port.getHost().port\n+    reactor.run()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3cda6817551e751e9f1757a83099a02ae1d560b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ class ProcessTest(object):\n \n     command = None\n     prefix = [sys.executable, '-m', 'scrapy.cmdline']\n+    cwd = os.getcwd() # trial chdirs to temp dir\n \n     def execute(self, args, check_code=True, settings='missing'):\n         env = os.environ.copy()\n@@ -14,7 +15,7 @@ class ProcessTest(object):\n         cmd = self.prefix + [self.command] + list(args)\n         pp = TestProcessProtocol()\n         pp.deferred.addBoth(self._process_finished, cmd, check_code)\n-        reactor.spawnProcess(pp, cmd[0], cmd, env=env)\n+        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n         return pp.deferred\n \n     def _process_finished(self, pp, cmd, check_code):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42e8346d06894362413ca9d54c1c8ba8dce71524", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 23 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,7 @@ class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_headers(self):\n         _, out, _ = yield self.execute([self.url('/text'), '--headers'])\n+        out = out.replace('\\r', '') # required on win32\n         headers = eval(out)\n         assert 'TwistedWeb' in headers['Server'][0]\n         self.assertEqual(headers['Content-Type'], ['text/plain'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bbffa594975f896cb88e9dc9830988b0b4c2a71a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 11 | Files Changed: 6 | Hunks: 15 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 61 | Churn Cumulative: 383 | Contributors (this commit): 1 | Commits (past 90d): 18 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,12 +7,16 @@ from twisted.application.service import Service\n from twisted.python import log\n \n from scrapy.utils.py26 import cpu_count\n+from scrapy.utils.python import unicode_to_str\n+from scrapyd.utils import get_crawl_args\n from .interfaces import IPoller, IEggStorage, IEnvironment\n \n class Launcher(Service):\n \n     def __init__(self, config, app):\n-        self.max_proc = config.getint('max_proc', 0) or cpu_count()\n+        self.max_proc = config.getint('max_proc', 0)\n+        if not self.max_proc:\n+            self.max_proc = cpu_count() * config.getint('max_proc_per_cpu')\n         self.egg_runner = config.get('egg_runner', 'scrapyd.eggrunner')\n         self.app = app\n \n@@ -39,12 +43,14 @@ class Launcher(Service):\n         return eggpath\n \n     def _spawn_process(self, message, slot):\n-        project = message['project']\n+        project = unicode_to_str(message['project'])\n+        spider = unicode_to_str(message['spider'])\n         eggpath = self._get_eggpath(project)\n         args = [sys.executable, '-m', self.egg_runner, 'crawl']\n+        args += get_crawl_args(message)\n         e = self.app.getComponent(IEnvironment)\n         env = e.get_environment(message, slot, eggpath)\n-        pp = ScrapyProcessProtocol(eggpath, slot)\n+        pp = ScrapyProcessProtocol(eggpath, slot, project, spider)\n         pp.deferred.addBoth(self._process_finished, eggpath, slot)\n         reactor.spawnProcess(pp, sys.executable, args=args, env=env)\n \n@@ -56,10 +62,12 @@ class Launcher(Service):\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n-    def __init__(self, eggfile, slot):\n+    def __init__(self, eggfile, slot, project, spider):\n         self.eggfile = eggfile\n         self.slot = slot\n         self.pid = None\n+        self.project = project\n+        self.spider = spider\n         self.deferred = defer.Deferred()\n \n     def outReceived(self, data):\n@@ -80,5 +88,6 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.deferred.callback(self)\n \n     def log(self, msg):\n-        msg += \"slot=%r pid=%r egg=%r\" % (self.slot, self.pid, self.eggfile)\n+        msg += \"project=%r spider=%r slot=%r pid=%r egg=%r\" % (self.project, \\\n+            self.spider, self.slot, self.pid, self.eggfile)\n         log.msg(msg, system=\"Launcher\")\n\n@@ -18,7 +18,8 @@ class QueuePoller(object):\n             return\n         for p, q in self.queues.iteritems():\n             if q.count():\n-                return self.dq.put(self._message(p))\n+                msg = q.pop()\n+                return self.dq.put(self._message(msg, p))\n \n     def next(self):\n         return self.dq.get()\n@@ -26,5 +27,8 @@ class QueuePoller(object):\n     def update_projects(self):\n         self.queues = get_spider_queues(self.config)\n \n-    def _message(self, project):\n-        return {'project': str(project)}\n+    def _message(self, queue_msg, project):\n+        d = queue_msg.copy()\n+        d['project'] = project\n+        d['spider'] = d.pop('name')\n+        return d\n\n@@ -37,5 +37,5 @@ class QueuePollerTest(unittest.TestCase):\n         self.poller.poll()\n         self.queues['mybot1'].pop()\n         self.poller.poll()\n-        self.failUnlessEqual(d1.result, {'project': 'mybot1'})\n-        self.failUnlessEqual(d2.result, {'project': 'mybot2'})\n+        self.failUnlessEqual(d1.result, {'project': 'mybot1', 'spider': 'spider1'})\n+        self.failUnlessEqual(d2.result, {'project': 'mybot2', 'spider': 'spider2'})\n\n@@ -0,0 +1,13 @@\n+import unittest\n+\n+from scrapyd.utils import get_crawl_args\n+\n+class UtilsTest(unittest.TestCase):\n+\n+    def test_get_crawl_args(self):\n+        msg = {'project': 'lolo', 'spider': 'lala'}\n+        self.assertEqual(get_crawl_args(msg), ['lala'])\n+        msg = {'project': 'lolo', 'spider': 'lala', 'arg1': 'val1'}\n+        cargs = get_crawl_args(msg)\n+        self.assertEqual(cargs, ['lala', '-a', 'arg1=val1'])\n+        assert all(isinstance(x, str) for x in cargs), cargs\n\n@@ -2,6 +2,7 @@ import os\n from ConfigParser import NoSectionError\n \n from scrapy.spiderqueue import SqliteSpiderQueue\n+from scrapy.utils.python import stringify_dict, unicode_to_str\n \n def get_spider_queues(config):\n     \"\"\"Return a dict of Spider Quees keyed by project name\"\"\"\n@@ -28,3 +29,15 @@ def get_project_list(config):\n     except NoSectionError:\n         pass\n     return projects\n+\n+def get_crawl_args(message):\n+    \"\"\"Return the command-line arguments to use for the scrapy crawl process\n+    that will be started for this message\n+    \"\"\"\n+    msg = message.copy()\n+    args = [unicode_to_str(msg['spider'])]\n+    del msg['project'], msg['spider']\n+    for k, v in stringify_dict(msg).items():\n+        args += ['-a']\n+        args += ['%s=%s' % (k, v)]\n+    return args\n\n@@ -46,7 +46,7 @@ class AddVersion(WsResource):\n         eggstorage = self.root.app.getComponent(IEggStorage)\n         eggstorage.put(eggf, project, version)\n         self.root.update_projects()\n-        return {\"status\": \"ok\", \"spiders\": spiders}\n+        return {\"status\": \"ok\", \"spiders\": len(spiders)}\n \n class ListProjects(WsResource):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#46e5d694e6396dcf10aac0fce144dfbf6f9a4d70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 120 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,7 +46,8 @@ class AddVersion(WsResource):\n         eggstorage = self.root.app.getComponent(IEggStorage)\n         eggstorage.put(eggf, project, version)\n         self.root.update_projects()\n-        return {\"status\": \"ok\", \"spiders\": len(spiders)}\n+        return {\"status\": \"ok\", \"project\": project, \"version\": version, \\\n+            \"spiders\": len(spiders)}\n \n class ListProjects(WsResource):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df54ed004193a9e85607ff48bb198644f9b4e3df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 144 | Lines Deleted: 54 | Files Changed: 9 | Hunks: 35 | Methods Changed: 28 | Complexity Δ (Sum/Max): 12/11 | Churn Δ: 198 | Churn Cumulative: 663 | Contributors (this commit): 1 | Commits (past 90d): 31 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ from .eggstorage import FilesystemEggStorage\n from .scheduler import SpiderScheduler\n from .poller import QueuePoller\n from .environ import Environment\n-from .webservice import Root\n+from .website import Root\n from .config import Config\n \n def application():\n\n@@ -11,6 +11,7 @@ class Environment(object):\n     def __init__(self, config, initenv=os.environ):\n         self.dbs_dir = config.get('dbs_dir', 'dbs')\n         self.logs_dir = config.get('logs_dir', 'logs')\n+        self.logs_to_keep = config.getint('logs_to_keep', 5)\n         if config.cp.has_section('settings'):\n             self.settings = dict(config.cp.items('settings'))\n         else:\n@@ -27,7 +28,15 @@ class Environment(object):\n             env['SCRAPY_SETTINGS_MODULE'] = self.settings[project]\n         dbpath = os.path.join(self.dbs_dir, '%s.db' % project)\n         env['SCRAPY_SQLITE_DB'] = dbpath\n-        logpath = os.path.join(self.logs_dir, 'slot%s.log' % slot)\n-        env['SCRAPY_LOG_FILE'] = logpath\n+        env['SCRAPY_LOG_FILE'] = self._get_log_file(message)\n         return env\n \n+    def _get_log_file(self, message):\n+        logsdir = os.path.join(self.logs_dir, message['project'], \\\n+            message['spider'])\n+        if not os.path.exists(logsdir):\n+            os.makedirs(logsdir)\n+        to_delete = sorted(os.listdir(logsdir), reverse=True)[:-self.logs_to_keep]\n+        for x in to_delete:\n+            os.remove(os.path.join(logsdir, x))\n+        return os.path.join(logsdir, \"%s.log\" % message['_id'])\n\n@@ -65,7 +65,9 @@ class IEnvironment(Interface):\n     def get_environment(message, slot, eggpath):\n         \"\"\"Return the environment variables to use for running the process.\n \n-        `message` is the message received from the IPoller.next()\n+        `message` is the message received from the IPoller.next() augmented to\n+           contain (at least) the following keys: project, spider, _id (where\n+           _id is a unique identifier for this run)\n         `slot` is the Launcher slot where the process will be running.\n         `eggpath` is the path to an eggfile that contains the project code. The\n            `eggpath` may be `None` if no egg was found for the project, in\n\n@@ -1,22 +1,26 @@\n import sys, os\n from shutil import copyfileobj\n from tempfile import mkstemp\n+from datetime import datetime\n \n from twisted.internet import reactor, defer, protocol, error\n from twisted.application.service import Service\n from twisted.python import log\n \n from scrapy.utils.py26 import cpu_count\n-from scrapy.utils.python import unicode_to_str\n+from scrapy.utils.python import stringify_dict\n from scrapyd.utils import get_crawl_args\n from .interfaces import IPoller, IEggStorage, IEnvironment\n \n class Launcher(Service):\n \n+    name = 'launcher'\n+\n     def __init__(self, config, app):\n+        self.processes = {}\n         self.max_proc = config.getint('max_proc', 0)\n         if not self.max_proc:\n-            self.max_proc = cpu_count() * config.getint('max_proc_per_cpu')\n+            self.max_proc = cpu_count() * config.getint('max_proc_per_cpu', 4)\n         self.egg_runner = config.get('egg_runner', 'scrapyd.eggrunner')\n         self.app = app\n \n@@ -43,31 +47,35 @@ class Launcher(Service):\n         return eggpath\n \n     def _spawn_process(self, message, slot):\n-        project = unicode_to_str(message['project'])\n-        spider = unicode_to_str(message['spider'])\n+        msg = stringify_dict(message, keys_only=False)\n+        project = msg['project']\n         eggpath = self._get_eggpath(project)\n         args = [sys.executable, '-m', self.egg_runner, 'crawl']\n-        args += get_crawl_args(message)\n+        args += get_crawl_args(msg)\n         e = self.app.getComponent(IEnvironment)\n-        env = e.get_environment(message, slot, eggpath)\n-        pp = ScrapyProcessProtocol(eggpath, slot, project, spider)\n+        env = e.get_environment(msg, slot, eggpath)\n+        pp = ScrapyProcessProtocol(eggpath, slot, project, msg['spider'], msg['_id'])\n         pp.deferred.addBoth(self._process_finished, eggpath, slot)\n         reactor.spawnProcess(pp, sys.executable, args=args, env=env)\n+        self.processes[slot] = pp\n \n     def _process_finished(self, _, eggpath, slot):\n         if eggpath:\n             os.remove(eggpath)\n+        self.processes.pop(slot)\n         self._wait_for_project(slot)\n \n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n-    def __init__(self, eggfile, slot, project, spider):\n+    def __init__(self, eggfile, slot, project, spider, job):\n         self.eggfile = eggfile\n         self.slot = slot\n         self.pid = None\n         self.project = project\n         self.spider = spider\n+        self.job = job\n+        self.start_time = datetime.now()\n         self.deferred = defer.Deferred()\n \n     def outReceived(self, data):\n@@ -88,6 +96,6 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.deferred.callback(self)\n \n     def log(self, msg):\n-        msg += \"project=%r spider=%r slot=%r pid=%r egg=%r\" % (self.project, \\\n-            self.spider, self.slot, self.pid, self.eggfile)\n+        msg += \"project=%r spider=%r job=%r pid=%r egg=%r\" % (self.project, \\\n+            self.spider, self.job, self.pid, self.eggfile)\n         log.msg(msg, system=\"Launcher\")\n\n@@ -22,21 +22,21 @@ class EggStorageTest(unittest.TestCase):\n         verifyObject(IEnvironment, self.environ)\n \n     def test_get_environment_with_eggfile(self):\n-        msg = {'project': 'mybot'}\n+        msg = {'project': 'mybot', 'spider': 'myspider', '_id': 'ID'}\n         slot = 3\n         env = self.environ.get_environment(msg, slot, '/path/to/file.egg')\n         self.assertEqual(env['SCRAPY_PROJECT'], 'mybot')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n-        self.assert_(env['SCRAPY_LOG_FILE'].endswith('slot3.log'))\n+        self.assert_(env['SCRAPY_LOG_FILE'].endswith('/mybot/myspider/ID.log'))\n         self.assert_(env['SCRAPY_EGGFILE'].endswith('/path/to/file.egg'))\n         self.failIf('SCRAPY_SETTINGS_MODULE' in env)\n \n     def test_get_environment_without_eggfile(self):\n-        msg = {'project': 'newbot'}\n+        msg = {'project': 'newbot', 'spider': 'myspider', '_id': 'ID'}\n         slot = 3\n         env = self.environ.get_environment(msg, slot, None)\n         self.assertEqual(env['SCRAPY_PROJECT'], 'newbot')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('newbot.db'))\n-        self.assert_(env['SCRAPY_LOG_FILE'].endswith('slot3.log'))\n+        self.assert_(env['SCRAPY_LOG_FILE'].endswith('/newbot/myspider/ID.log'))\n         self.assertEqual(env['SCRAPY_SETTINGS_MODULE'], 'newbot.settings')\n         self.failIf('SCRAPY_EGGFILE' in env)\n\n@@ -7,7 +7,7 @@ class UtilsTest(unittest.TestCase):\n     def test_get_crawl_args(self):\n         msg = {'project': 'lolo', 'spider': 'lala'}\n         self.assertEqual(get_crawl_args(msg), ['lala'])\n-        msg = {'project': 'lolo', 'spider': 'lala', 'arg1': 'val1'}\n+        msg = {'project': 'lolo', 'spider': 'lala', 'arg1': u'val1'}\n         cargs = get_crawl_args(msg)\n         self.assertEqual(cargs, ['lala', '-a', 'arg1=val1'])\n         assert all(isinstance(x, str) for x in cargs), cargs\n\n@@ -37,7 +37,7 @@ def get_crawl_args(message):\n     msg = message.copy()\n     args = [unicode_to_str(msg['spider'])]\n     del msg['project'], msg['spider']\n-    for k, v in stringify_dict(msg).items():\n+    for k, v in stringify_dict(msg, keys_only=False).items():\n         args += ['-a']\n         args += ['%s=%s' % (k, v)]\n     return args\n\n@@ -1,11 +1,9 @@\n import cgi\n import traceback\n+import uuid\n from cStringIO import StringIO\n \n-from twisted.web.resource import Resource\n-\n from scrapy.utils.txweb import JsonResource\n-from .interfaces import IPoller, IEggStorage, ISpiderScheduler\n from .eggutils import get_spider_list_from_eggfile\n \n class WsResource(JsonResource):\n@@ -29,9 +27,11 @@ class Schedule(WsResource):\n         args = dict((k, v[0]) for k, v in txrequest.args.items())\n         project = args.pop('project')\n         spider = args.pop('spider')\n-        sched = self.root.app.getComponent(ISpiderScheduler)\n-        sched.schedule(project, spider, **args)\n-        return {\"status\": \"ok\"}\n+        jobid = uuid.uuid1().hex\n+        args['_id'] = jobid\n+        self.root.scheduler.schedule(project, spider, **args)\n+        jobids = {spider: jobid}\n+        return {\"status\": \"ok\", \"jobs\": jobids}\n \n class AddVersion(WsResource):\n \n@@ -43,8 +43,7 @@ class AddVersion(WsResource):\n         version = d['version'][0]\n         eggf = StringIO(d['egg'][0])\n         spiders = get_spider_list_from_eggfile(eggf, project)\n-        eggstorage = self.root.app.getComponent(IEggStorage)\n-        eggstorage.put(eggf, project, version)\n+        self.root.eggstorage.put(eggf, project, version)\n         self.root.update_projects()\n         return {\"status\": \"ok\", \"project\": project, \"version\": version, \\\n             \"spiders\": len(spiders)}\n@@ -52,23 +51,21 @@ class AddVersion(WsResource):\n class ListProjects(WsResource):\n \n     def render_GET(self, txrequest):\n-        projects = self.root.app.getComponent(ISpiderScheduler).list_projects()\n+        projects = self.root.scheduler.list_projects()\n         return {\"status\": \"ok\", \"projects\": projects}\n \n class ListVersions(WsResource):\n \n     def render_GET(self, txrequest):\n         project = txrequest.args['project'][0]\n-        eggstorage = self.root.app.getComponent(IEggStorage)\n-        versions = eggstorage.list(project)\n+        versions = self.root.eggstorage.list(project)\n         return {\"status\": \"ok\", \"versions\": versions}\n \n class ListSpiders(WsResource):\n \n     def render_GET(self, txrequest):\n         project = txrequest.args['project'][0]\n-        eggstorage = self.root.app.getComponent(IEggStorage)\n-        _, eggf = eggstorage.get(project)\n+        _, eggf = self.root.eggstorage.get(project)\n         spiders = get_spider_list_from_eggfile(eggf, project, \\\n             eggrunner=self.root.egg_runner)\n         return {\"status\": \"ok\", \"spiders\": spiders}\n@@ -81,8 +78,7 @@ class DeleteProject(WsResource):\n         return {\"status\": \"ok\"}\n \n     def _delete_version(self, project, version=None):\n-        eggstorage = self.root.app.getComponent(IEggStorage)\n-        eggstorage.delete(project, version)\n+        self.root.eggstorage.delete(project, version)\n         self.root.update_projects()\n \n class DeleteVersion(DeleteProject):\n@@ -93,22 +89,3 @@ class DeleteVersion(DeleteProject):\n         self._delete_version(project, version)\n         return {\"status\": \"ok\"}\n \n-class Root(Resource):\n-\n-    def __init__(self, config, app):\n-        Resource.__init__(self)\n-        self.debug = config.getboolean('debug', False)\n-        self.eggrunner = config.get('egg_runner')\n-        self.app = app\n-        self.putChild('schedule.json', Schedule(self))\n-        self.putChild('addversion.json', AddVersion(self))\n-        self.putChild('listprojects.json', ListProjects(self))\n-        self.putChild('listversions.json', ListVersions(self))\n-        self.putChild('listspiders.json', ListSpiders(self))\n-        self.putChild('delproject.json', DeleteProject(self))\n-        self.putChild('delversion.json', DeleteVersion(self))\n-        self.update_projects()\n-\n-    def update_projects(self):\n-        self.app.getComponent(IPoller).update_projects()\n-        self.app.getComponent(ISpiderScheduler).update_projects()\n\n@@ -0,0 +1,94 @@\n+from datetime import datetime\n+\n+from twisted.web import resource, static\n+from twisted.application.service import IServiceCollection\n+from .interfaces import IPoller, IEggStorage, ISpiderScheduler\n+\n+from . import webservice\n+\n+class Root(resource.Resource):\n+\n+    def __init__(self, config, app):\n+        resource.Resource.__init__(self)\n+        self.debug = config.getboolean('debug', False)\n+        self.eggrunner = config.get('egg_runner')\n+        logsdir = config.get('logs_dir')\n+        self.app = app\n+        self.putChild('', Home())\n+        self.putChild('schedule.json', webservice.Schedule(self))\n+        self.putChild('addversion.json', webservice.AddVersion(self))\n+        self.putChild('listprojects.json', webservice.ListProjects(self))\n+        self.putChild('listversions.json', webservice.ListVersions(self))\n+        self.putChild('listspiders.json', webservice.ListSpiders(self))\n+        self.putChild('delproject.json', webservice.DeleteProject(self))\n+        self.putChild('delversion.json', webservice.DeleteVersion(self))\n+        self.putChild('logs', static.File(logsdir, 'text/plain'))\n+        self.putChild('procmon', ProcessMonitor(self))\n+        self.update_projects()\n+\n+    def update_projects(self):\n+        self.poller.update_projects()\n+        self.scheduler.update_projects()\n+\n+    @property\n+    def launcher(self):\n+        app = IServiceCollection(self.app, self.app)\n+        return app.getServiceNamed('launcher')\n+\n+    @property\n+    def scheduler(self):\n+        return self.app.getComponent(ISpiderScheduler)\n+\n+    @property\n+    def eggstorage(self):\n+        return self.app.getComponent(IEggStorage)\n+\n+    @property\n+    def poller(self):\n+        return self.app.getComponent(IPoller)\n+\n+\n+class Home(resource.Resource):\n+\n+    def render_GET(self, txrequest):\n+        return \"\"\"\n+<html>\n+<head><title>Scrapyd</title></head>\n+<body>\n+<h1>Scrapyd</h1>\n+<ul>\n+<li><a href=\"procmon\">Process monitor</a></li>\n+<li><a href=\"logs/\">Logs</li>\n+<li><a href=\"http://doc.scrapy.org/topics/scrapyd.html\">Documentation</a></li>\n+</ul>\n+</body>\n+</html>\n+\"\"\"\n+\n+\n+class ProcessMonitor(resource.Resource):\n+\n+    def __init__(self, root):\n+        resource.Resource.__init__(self)\n+        self.root = root\n+\n+    def render(self, txrequest):\n+        s = \"<html><head><title>Scrapyd</title></title>\"\n+        s += \"<body>\"\n+        s += \"<h1>Process monitor</h1>\"\n+        s += \"<p><a href='..'>Go back</a></p>\"\n+        s += \"<table border='1'>\"\n+        s += \"<tr>\"\n+        s += \"<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th>\"\n+        s += \"</tr>\"\n+        for p in self.root.launcher.processes.values():\n+            s += \"<tr>\"\n+            for a in ['project', 'spider', 'job', 'pid']:\n+                s += \"<td>%s</td>\" % getattr(p, a)\n+            s += \"<td>%s</td>\" % (datetime.now() - p.start_time)\n+            s += \"</tr>\"\n+        s += \"</table>\"\n+        s += \"</body>\"\n+        s += \"</html>\"\n+        return s\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c4f562ec47d0e3fa588f38140791addd6a88d9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 27 | Files Changed: 10 | Hunks: 20 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 59 | Churn Cumulative: 764 | Contributors (this commit): 1 | Commits (past 90d): 44 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,7 +19,7 @@ class Environment(object):\n         self.initenv = initenv\n \n     def get_environment(self, message, slot, eggpath):\n-        project = message['project']\n+        project = message['_project']\n         env = self.initenv.copy()\n         env['SCRAPY_PROJECT'] = project\n         if eggpath:\n@@ -32,11 +32,11 @@ class Environment(object):\n         return env\n \n     def _get_log_file(self, message):\n-        logsdir = os.path.join(self.logs_dir, message['project'], \\\n-            message['spider'])\n+        logsdir = os.path.join(self.logs_dir, message['_project'], \\\n+            message['_spider'])\n         if not os.path.exists(logsdir):\n             os.makedirs(logsdir)\n         to_delete = sorted(os.listdir(logsdir), reverse=True)[:-self.logs_to_keep]\n         for x in to_delete:\n             os.remove(os.path.join(logsdir, x))\n-        return os.path.join(logsdir, \"%s.log\" % message['_id'])\n+        return os.path.join(logsdir, \"%s.log\" % message['_job'])\n\n@@ -35,9 +35,11 @@ class IPoller(Interface):\n         project that needs to run, or already fired if there was a project\n         waiting to run already.\n \n-        The message is a dict containing (at least) the name of the project to\n-        be run in the 'project' key. This message will be passed later to\n-        IEnvironment.get_environment().\n+        The message is a dict containing (at least):\n+        * the name of the project to be run in the '_project' key\n+        * the name of the spider to be run in the '_spider' key\n+        * a unique identifier for this run in the `_job` key\n+        This message will be passed later to IEnvironment.get_environment().\n         \"\"\"\n \n     def update_projects():\n@@ -65,9 +67,7 @@ class IEnvironment(Interface):\n     def get_environment(message, slot, eggpath):\n         \"\"\"Return the environment variables to use for running the process.\n \n-        `message` is the message received from the IPoller.next() augmented to\n-           contain (at least) the following keys: project, spider, _id (where\n-           _id is a unique identifier for this run)\n+        `message` is the message received from the IPoller.next() method\n         `slot` is the Launcher slot where the process will be running.\n         `eggpath` is the path to an eggfile that contains the project code. The\n            `eggpath` may be `None` if no egg was found for the project, in\n\n@@ -48,13 +48,15 @@ class Launcher(Service):\n \n     def _spawn_process(self, message, slot):\n         msg = stringify_dict(message, keys_only=False)\n-        project = msg['project']\n+        project = msg['_project']\n         eggpath = self._get_eggpath(project)\n         args = [sys.executable, '-m', self.egg_runner, 'crawl']\n         args += get_crawl_args(msg)\n         e = self.app.getComponent(IEnvironment)\n         env = e.get_environment(msg, slot, eggpath)\n-        pp = ScrapyProcessProtocol(eggpath, slot, project, msg['spider'], msg['_id'])\n+        env = stringify_dict(env, keys_only=False)\n+        pp = ScrapyProcessProtocol(eggpath, slot, project, msg['_spider'], \\\n+            msg['_job'], env)\n         pp.deferred.addBoth(self._process_finished, eggpath, slot)\n         reactor.spawnProcess(pp, sys.executable, args=args, env=env)\n         self.processes[slot] = pp\n@@ -68,7 +70,7 @@ class Launcher(Service):\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n-    def __init__(self, eggfile, slot, project, spider, job):\n+    def __init__(self, eggfile, slot, project, spider, job, env):\n         self.eggfile = eggfile\n         self.slot = slot\n         self.pid = None\n@@ -76,6 +78,8 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.spider = spider\n         self.job = job\n         self.start_time = datetime.now()\n+        self.env = env\n+        self.logfile = env['SCRAPY_LOG_FILE']\n         self.deferred = defer.Deferred()\n \n     def outReceived(self, data):\n\n@@ -29,6 +29,6 @@ class QueuePoller(object):\n \n     def _message(self, queue_msg, project):\n         d = queue_msg.copy()\n-        d['project'] = project\n-        d['spider'] = d.pop('name')\n+        d['_project'] = project\n+        d['_spider'] = d.pop('name')\n         return d\n\n@@ -22,7 +22,7 @@ class EggStorageTest(unittest.TestCase):\n         verifyObject(IEnvironment, self.environ)\n \n     def test_get_environment_with_eggfile(self):\n-        msg = {'project': 'mybot', 'spider': 'myspider', '_id': 'ID'}\n+        msg = {'_project': 'mybot', '_spider': 'myspider', '_job': 'ID'}\n         slot = 3\n         env = self.environ.get_environment(msg, slot, '/path/to/file.egg')\n         self.assertEqual(env['SCRAPY_PROJECT'], 'mybot')\n@@ -32,7 +32,7 @@ class EggStorageTest(unittest.TestCase):\n         self.failIf('SCRAPY_SETTINGS_MODULE' in env)\n \n     def test_get_environment_without_eggfile(self):\n-        msg = {'project': 'newbot', 'spider': 'myspider', '_id': 'ID'}\n+        msg = {'_project': 'newbot', '_spider': 'myspider', '_job': 'ID'}\n         slot = 3\n         env = self.environ.get_environment(msg, slot, None)\n         self.assertEqual(env['SCRAPY_PROJECT'], 'newbot')\n\n@@ -37,5 +37,5 @@ class QueuePollerTest(unittest.TestCase):\n         self.poller.poll()\n         self.queues['mybot1'].pop()\n         self.poller.poll()\n-        self.failUnlessEqual(d1.result, {'project': 'mybot1', 'spider': 'spider1'})\n-        self.failUnlessEqual(d2.result, {'project': 'mybot2', 'spider': 'spider2'})\n+        self.failUnlessEqual(d1.result, {'_project': 'mybot1', '_spider': 'spider1'})\n+        self.failUnlessEqual(d2.result, {'_project': 'mybot2', '_spider': 'spider2'})\n\n@@ -5,9 +5,9 @@ from scrapyd.utils import get_crawl_args\n class UtilsTest(unittest.TestCase):\n \n     def test_get_crawl_args(self):\n-        msg = {'project': 'lolo', 'spider': 'lala'}\n+        msg = {'_project': 'lolo', '_spider': 'lala'}\n         self.assertEqual(get_crawl_args(msg), ['lala'])\n-        msg = {'project': 'lolo', 'spider': 'lala', 'arg1': u'val1'}\n+        msg = {'_project': 'lolo', '_spider': 'lala', 'arg1': u'val1'}\n         cargs = get_crawl_args(msg)\n         self.assertEqual(cargs, ['lala', '-a', 'arg1=val1'])\n         assert all(isinstance(x, str) for x in cargs), cargs\n\n@@ -35,8 +35,8 @@ def get_crawl_args(message):\n     that will be started for this message\n     \"\"\"\n     msg = message.copy()\n-    args = [unicode_to_str(msg['spider'])]\n-    del msg['project'], msg['spider']\n+    args = [unicode_to_str(msg['_spider'])]\n+    del msg['_project'], msg['_spider']\n     for k, v in stringify_dict(msg, keys_only=False).items():\n         args += ['-a']\n         args += ['%s=%s' % (k, v)]\n\n@@ -28,7 +28,7 @@ class Schedule(WsResource):\n         project = args.pop('project')\n         spider = args.pop('spider')\n         jobid = uuid.uuid1().hex\n-        args['_id'] = jobid\n+        args['_job'] = jobid\n         self.root.scheduler.schedule(project, spider, **args)\n         jobids = {spider: jobid}\n         return {\"status\": \"ok\", \"jobs\": jobids}\n\n@@ -57,8 +57,8 @@ class Home(resource.Resource):\n <body>\n <h1>Scrapyd</h1>\n <ul>\n-<li><a href=\"procmon\">Process monitor</a></li>\n-<li><a href=\"logs/\">Logs</li>\n+<li><a href=\"/procmon\">Process monitor</a></li>\n+<li><a href=\"/logs/\">Logs</li>\n <li><a href=\"http://doc.scrapy.org/topics/scrapyd.html\">Documentation</a></li>\n </ul>\n </body>\n@@ -79,13 +79,14 @@ class ProcessMonitor(resource.Resource):\n         s += \"<p><a href='..'>Go back</a></p>\"\n         s += \"<table border='1'>\"\n         s += \"<tr>\"\n-        s += \"<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th>\"\n+        s += \"<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th><th>Log</th>\"\n         s += \"</tr>\"\n         for p in self.root.launcher.processes.values():\n             s += \"<tr>\"\n             for a in ['project', 'spider', 'job', 'pid']:\n                 s += \"<td>%s</td>\" % getattr(p, a)\n             s += \"<td>%s</td>\" % (datetime.now() - p.start_time)\n+            s += \"<td><a href='/logs/%s/%s/%s.log'>Log</a></td>\" % (p.project, p.spider, p.job)\n             s += \"</tr>\"\n         s += \"</table>\"\n         s += \"</body>\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
