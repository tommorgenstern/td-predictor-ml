{"custom_id": "keras#5f176b1f4b8833eb5c5cd2bde60d5100a6bdde49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 47/26 | Churn Δ: 28 | Churn Cumulative: 2295 | Contributors (this commit): 29 | Commits (past 90d): 11 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -58,17 +58,17 @@ def load_data(\n           ranked by how often they occur (in the training set) and only\n           the `num_words` most frequent words are kept. Any less frequent word\n           will appear as `oov_char` value in the sequence data. If None,\n-          all words are kept. Defaults to None, so all words are kept.\n+          all words are kept. Defaults to `None`.\n       skip_top: skip the top N most frequently occurring words\n           (which may not be informative). These words will appear as\n-          `oov_char` value in the dataset. Defaults to 0, so no words are\n-          skipped.\n+          `oov_char` value in the dataset. When 0, no words are\n+          skipped. Defaults to `0`.\n       maxlen: int or None. Maximum sequence length.\n-          Any longer sequence will be truncated. Defaults to None, which\n-          means no truncation.\n+          Any longer sequence will be truncated. None, means no truncation.\n+          Defaults to `None`.\n       seed: int. Seed for reproducible data shuffling.\n       start_char: int. The start of a sequence will be marked with this\n-          character. Defaults to 1 because 0 is usually the padding character.\n+          character. 0 is usually the padding character. Defaults to `1`.\n       oov_char: int. The out-of-vocabulary character.\n           Words that were cut out because of the `num_words` or\n           `skip_top` limits will be replaced with this character.\n\n@@ -65,20 +65,20 @@ def load_data(\n           ranked by how often they occur (in the training set) and only\n           the `num_words` most frequent words are kept. Any less frequent word\n           will appear as `oov_char` value in the sequence data. If None,\n-          all words are kept. Defaults to None, so all words are kept.\n+          all words are kept. Defaults to `None`.\n       skip_top: skip the top N most frequently occurring words\n           (which may not be informative). These words will appear as\n-          `oov_char` value in the dataset. Defaults to 0, so no words are\n-          skipped.\n+          `oov_char` value in the dataset. 0 means no words are\n+          skipped. Defaults to 0\n       maxlen: int or None. Maximum sequence length.\n-          Any longer sequence will be truncated. Defaults to None, which\n-          means no truncation.\n+          Any longer sequence will be truncated. None means no truncation.\n+          Defaults to `None`.\n       test_split: Float between 0 and 1. Fraction of the dataset to be used\n-        as test data. Defaults to 0.2, meaning 20% of the dataset is used as\n-        test data.\n+        as test data. 0.2 means that 20% of the dataset is used as\n+        test data. Defaults to 0.2\n       seed: int. Seed for reproducible data shuffling.\n       start_char: int. The start of a sequence will be marked with this\n-          character. Defaults to 1 because 0 is usually the padding character.\n+          character. 0 is usually the padding character. Defaults to `1`.\n       oov_char: int. The out-of-vocabulary character.\n           Words that were cut out because of the `num_words` or\n           `skip_top` limits will be replaced with this character.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ad7371a082a3df70c3b1e1e999cbb8d749d2417", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 42 | Files Changed: 9 | Hunks: 32 | Methods Changed: 18 | Complexity Δ (Sum/Max): 2615/504 | Churn Δ: 87 | Churn Cumulative: 72213 | Contributors (this commit): 174 | Commits (past 90d): 57 | Contributors (cumulative): 298 | DMM Complexity: None\n\nDIFF:\n@@ -458,7 +458,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         # Whether the layer will track any layers that is set as attribute on\n         # itself as sub-layers, the weights from the sub-layers will be included\n-        # in the parent layer's variables() as well.  Default to True, which\n+        # in the parent layer's variables() as well.  Defaults to `True`, which\n         # means auto tracking is turned on. Certain subclass might want to turn\n         # it off, like Sequential model.\n         self._auto_track_sub_layers = True\n@@ -3830,9 +3830,9 @@ class BaseRandomLayer(Layer):\n           force_generator: boolean, default to False, whether to force the\n             RandomGenerator to use the code branch of tf.random.Generator.\n           rng_type: string, the rng type that will be passed to backend\n-            RandomGenerator. Default to `None`, which will allow RandomGenerator\n-            to choose types by itself. Valid values are \"stateful\", \"stateless\",\n-            \"legacy_stateful\".\n+            RandomGenerator. `None` will allow RandomGenerator to choose\n+            types by itself. Valid values are \"stateful\", \"stateless\",\n+            \"legacy_stateful\". Defaults to `None`.\n           **kwargs: other keyword arguments that will be passed to the parent\n             *class\n         \"\"\"\n\n@@ -98,8 +98,8 @@ def make_variable(\n         or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev).\n         Note, if the current variable scope is marked as non-trainable\n         then this parameter is ignored and any added variables are also\n-        marked as non-trainable. `trainable` defaults to `True` unless\n-        `synchronization` is set to `ON_READ`.\n+        marked as non-trainable. `trainable` becomes `True` unless\n+        `synchronization` is set to `ON_READ`. Defaults to `None`.\n       caching_device: Passed to `tf.Variable`.\n       validate_shape: Passed to `tf.Variable`.\n       constraint: Constraint instance (callable).\n\n@@ -237,7 +237,7 @@ class Layer(base_layer.Layer):\n \n         # Whether the layer will track any layers that are set as attribute on\n         # itself as sub-layers, the weights from the sub-layers will be included\n-        # in the parent layer's variables() as well.  Default to True, which\n+        # in the parent layer's variables() as well.  Defaults to `True`, which\n         # means auto tracking is turned on. Certain subclass might want to turn\n         # it off, like the Sequential model.\n         self._auto_track_sub_layers = True\n\n@@ -140,14 +140,14 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n         \"\"\"Configures the layer for `adapt`.\n \n         Arguments:\n-          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n+          run_eagerly: Bool. If `True`, this `Model`'s\n             logic will not be wrapped in a `tf.function`. Recommended to leave\n             this as `None` unless your `Model` cannot be run inside a\n-            `tf.function`.\n-          steps_per_execution: Int. Defaults to 1. The number of batches to run\n+            `tf.function`. Defaults to `False`.\n+          steps_per_execution: Int. The number of batches to run\n             during each `tf.function` call. Running multiple batches inside a\n             single `tf.function` call can greatly improve performance on TPUs or\n-            small models with a large Python overhead.\n+            small models with a large Python overhead. Defaults to `1`.\n         \"\"\"\n         if steps_per_execution is None:\n             steps_per_execution = 1\n\n@@ -268,7 +268,7 @@ class TensorLikeDataAdapter(DataAdapter):\n         _check_data_cardinality(inputs)\n \n         # If batch_size is not passed but steps is, calculate from the input\n-        # data.  Default to 32 for backwards compat.\n+        # data.  Defaults to `32` for backwards compatibility.\n         if not batch_size:\n             batch_size = int(math.ceil(num_samples / steps)) if steps else 32\n \n@@ -645,7 +645,7 @@ class CompositeTensorDataAdapter(DataAdapter):\n             dataset = dataset.shuffle(num_samples)\n \n         # If batch_size is not passed but steps is, calculate from the input\n-        # data.  Default to 32 for backwards compatibility.\n+        # data.  Defaults to `32` for backwards compatibility.\n         if not batch_size:\n             batch_size = int(math.ceil(num_samples / steps)) if steps else 32\n \n\n@@ -1647,8 +1647,8 @@ class ModuleWrapper(base_layer.Layer):\n         Args:\n           module: The `tf.Module` instance to be wrapped.\n           method_name: (Optional) str. The name of the method to use as the\n-            forward pass of the module. If not set, defaults to '__call__' if\n-            defined, or 'call'.\n+            forward pass of the module. If not set, becomes '__call__' if\n+            defined, or 'call'. Defaults to `None`.\n           **kwargs: Additional keywrod arguments. See `tf.keras.layers.Layer`.\n \n         Raises:\n\n@@ -88,12 +88,12 @@ class InputLayer(base_layer.Layer):\n             will use the `tf.TypeSpec` of this tensor rather\n             than creating a new placeholder tensor.\n         sparse: Boolean, whether the placeholder created is meant to be sparse.\n-            Default to `False`.\n+            Defaults to `False`.\n         ragged: Boolean, whether the placeholder created is meant to be ragged.\n             In this case, values of `None` in the `shape` argument represent\n             ragged dimensions. For more information about `tf.RaggedTensor`, see\n             [this guide](https://www.tensorflow.org/guide/ragged_tensor).\n-            Default to `False`.\n+            Defaults to `False`.\n         type_spec: A `tf.TypeSpec` object to create Input from. This\n             `tf.TypeSpec` represents the entire batch. When provided, all other\n             args except name must be `None`.\n\n@@ -673,12 +673,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               coefficients.\n             weighted_metrics: List of metrics to be evaluated and weighted by\n               `sample_weight` or `class_weight` during training and testing.\n-            run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n-              logic will not be wrapped in a `tf.function`. Recommended to leave\n-              this as `None` unless your `Model` cannot be run inside a\n-              `tf.function`. `run_eagerly=True` is not supported when using\n-              `tf.distribute.experimental.ParameterServerStrategy`.\n-            steps_per_execution: Int. Defaults to 1. The number of batches to\n+            run_eagerly: Bool. If `True`, this `Model`'s logic will not be\n+              wrapped in a `tf.function`. Recommended to leave this as `None`\n+              unless your `Model` cannot be run inside a `tf.function`.\n+              `run_eagerly=True` is not supported when using\n+              `tf.distribute.experimental.ParameterServerStrategy`. Defaults to\n+               `False`.\n+            steps_per_execution: Int. The number of batches to\n               run during each `tf.function` call. Running multiple batches\n               inside a single `tf.function` call can greatly improve performance\n               on TPUs or small models with a large Python overhead. At most, one\n@@ -687,7 +688,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               the size of the epoch. Note that if `steps_per_execution` is set\n               to `N`, `Callback.on_batch_begin` and `Callback.on_batch_end`\n               methods will only be called every `N` batches (i.e. before/after\n-              each `tf.function` execution).\n+              each `tf.function` execution). Defaults to `1`.\n             jit_compile: If `True`, compile the model training step with XLA.\n               [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n               for machine learning.\n@@ -708,9 +709,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               not process the same data. The number of shards should be at least\n               the number of workers for good performance. A value of 'auto'\n               turns on exact evaluation and uses a heuristic for the number of\n-              shards based on the number of workers. Defaults to 0, meaning no\n+              shards based on the number of workers. 0, meaning no\n               visitation guarantee is provided. NOTE: Custom implementations of\n               `Model.test_step` will be ignored when doing exact evaluation.\n+              Defaults to `0`.\n             **kwargs: Arguments supported for backwards compatibility only.\n         \"\"\"\n         if jit_compile and not tf_utils.can_jit_compile(warn=True):\n@@ -1457,11 +1459,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 of index `epochs` is reached.\n             verbose: 'auto', 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = progress bar, 2 = one line per epoch.\n-                'auto' defaults to 1 for most cases, but 2 when used with\n+                'auto' becomes 1 for most cases, but 2 when used with\n                 `ParameterServerStrategy`. Note that the progress bar is not\n                 particularly useful when logged to a file, so verbose=2 is\n                 recommended when not running interactively (eg, in a production\n-                environment).\n+                environment). Defaults to 'auto'.\n             callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during training.\n                 See `tf.keras.callbacks`. Note\n@@ -2059,11 +2061,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               they generate batches).\n             verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = progress bar, 2 = single line.\n-                `\"auto\"` defaults to 1 for most cases, and to 2 when used with\n+                `\"auto\"` becomes 1 for most cases, and to 2 when used with\n                 `ParameterServerStrategy`. Note that the progress bar is not\n                 particularly useful when logged to a file, so `verbose=2` is\n                 recommended when not running interactively (e.g. in a production\n-                environment).\n+                environment). Defaults to 'auto'.\n             sample_weight: Optional Numpy array of weights for the test samples,\n               used for weighting the loss function. You can either pass a flat\n               (1D) Numpy array with the same length as the input samples\n@@ -2419,11 +2421,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 (since they generate batches).\n             verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = progress bar, 2 = single line.\n-                `\"auto\"` defaults to 1 for most cases, and to 2 when used with\n+                `\"auto\"` becomes 1 for most cases, and to 2 when used with\n                 `ParameterServerStrategy`. Note that the progress bar is not\n                 particularly useful when logged to a file, so `verbose=2` is\n                 recommended when not running interactively (e.g. in a production\n-                environment).\n+                environment). Defaults to 'auto'.\n             steps: Total number of steps (batches of samples)\n                 before declaring the prediction round finished.\n                 Ignored with the default value of `None`. If x is a `tf.data`\n@@ -2958,7 +2960,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         SavedModel format arguments:\n             include_optimizer: Only applied to SavedModel and legacy HDF5\n                 formats. If False, do not save the optimizer state.\n-                Defaults to True.\n+                Defaults to `True`.\n             signatures: Only applies to SavedModel format. Signatures to save\n                 with the SavedModel. See the `signatures` argument in\n                 `tf.saved_model.save` for details.\n@@ -3051,7 +3053,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 target location, or provide the user with a manual prompt.\n             save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n                 '.keras' will default to HDF5 if `save_format` is `None`.\n-                Otherwise `None` defaults to 'tf'.\n+                Otherwise, `None` becomes 'tf'. Defaults to `None`.\n             options: Optional `tf.train.CheckpointOptions` object that specifies\n                 options for saving weights.\n \n@@ -3366,17 +3368,17 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 (e.g. set this to adapt the display to different\n                 terminal window sizes).\n             positions: Relative or absolute positions of log elements\n-                in each line. If not provided,\n-                defaults to `[0.3, 0.6, 0.70, 1.]`\n+                in each line. If not provided, becomes\n+                `[0.3, 0.6, 0.70, 1.]`. Defaults to `None`.\n             print_fn: Print function to use. By default, prints to `stdout`.\n                 If `stdout` doesn't work in your environment, change to `print`.\n                 It will be called on each line of the summary.\n                 You can set it to a custom function\n                 in order to capture the string summary.\n             expand_nested: Whether to expand the nested models.\n-                If not provided, defaults to `False`.\n+                Defaults to `False`.\n             show_trainable: Whether to show if a layer is trainable.\n-                If not provided, defaults to `False`.\n+                Defaults to `False`.\n             layer_range: a list or tuple of 2 strings,\n                 which is the starting layer name and ending layer name\n                 (both inclusive) indicating the range of layers to be printed\n@@ -3942,7 +3944,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n         Args:\n           user_metrics: Whether to return user-supplied metrics or `Metric`\n-            objects. Defaults to returning the user-supplied metrics.\n+            objects. If True, returns the user-supplied metrics.\n+            Defaults to `True`.\n \n         Returns:\n           Dictionary of arguments that were used when compiling the model.\n@@ -4186,11 +4189,11 @@ def _get_verbosity(verbose, distribute_strategy):\n             distribute_strategy._should_use_with_coordinator\n             or not io_utils.is_interactive_logging_enabled()\n         ):\n-            # Default to epoch-level logging for PSStrategy or using absl\n+            # Defaults to epoch-level logging for PSStrategy or using absl\n             # logging.\n             return 2\n         else:\n-            return 1  # Default to batch-level logging otherwise.\n+            return 1  # Defaults to batch-level logging otherwise.\n     return verbose\n \n \n\n@@ -269,10 +269,10 @@ class Model(training_lib.Model):\n                 output names (strings) to scalar coefficients.\n             sample_weight_mode: If you need to do timestep-wise\n                 sample weighting (2D weights), set this to `\"temporal\"`.\n-                `None` defaults to sample-wise weights (1D).\n+                `None` becomes sample-wise weights (1D).\n                 If the model has multiple outputs, you can use a different\n                 `sample_weight_mode` on each output by passing a\n-                dictionary or a list of modes.\n+                dictionary or a list of modes. Defaults to `None`.\n             weighted_metrics: List of metrics to be evaluated and weighted\n                 by sample_weight or class_weight during training and testing.\n             target_tensors: By default, Keras will create placeholders for the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#037fc4afe78ce8a0702a531eac3cd7cd6c2a11b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 19 | Churn Cumulative: 987 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -114,18 +114,17 @@ def model_to_estimator(\n         `tempfile.mkdtemp`\n       config: `RunConfig` to config `Estimator`. Allows setting up things in\n         `model_fn` based on configuration such as `num_ps_replicas`, or\n-        `model_dir`. Defaults to `None`. If both `config.model_dir` and the\n+        `model_dir`. If both `config.model_dir` and the\n         `model_dir` argument (above) are specified the `model_dir` **argument**\n-        takes precedence.\n+        takes precedence. Defaults to `None`.\n       checkpoint_format: Sets the format of the checkpoint saved by the\n         estimator when training. May be `saver` or `checkpoint`, depending on\n         whether to save checkpoints from `tf.train.Saver` or\n-        `tf.train.Checkpoint`. This argument currently defaults to `saver`. When\n-        2.0 is released, the default will be `checkpoint`. Estimators use\n-        name-based `tf.train.Saver` checkpoints, while Keras models use\n-        object-based checkpoints from `tf.train.Checkpoint`. Currently, saving\n-        object-based checkpoints from `model_to_estimator` is only supported by\n-        Functional and Sequential models. Defaults to 'saver'.\n+        `tf.train.Checkpoint`. Estimators use name-based `tf.train.Saver`\n+        checkpoints, while Keras models use object-based checkpoints from\n+        `tf.train.Checkpoint`. Currently, saving object-based checkpoints\n+        from `model_to_estimator` is only supported by Functional and\n+        Sequential models. Defaults to 'saver'.\n       metric_names_map: Optional dictionary mapping Keras model output metric\n         names to custom names. This can be used to override the default Keras\n         model output metrics names in a multi IO model use case and provide\n@@ -312,9 +311,9 @@ def model_to_estimator_v2(\n         `tempfile.mkdtemp`\n       config: `RunConfig` to config `Estimator`. Allows setting up things in\n         `model_fn` based on configuration such as `num_ps_replicas`, or\n-        `model_dir`. Defaults to `None`. If both `config.model_dir` and the\n+        `model_dir`. If both `config.model_dir` and the\n         `model_dir` argument (above) are specified the `model_dir` **argument**\n-        takes precedence.\n+        takes precedence. Defaults to `None`.\n       checkpoint_format: Sets the format of the checkpoint saved by the\n         estimator when training. May be `saver` or `checkpoint`, depending on\n         whether to save checkpoints from `tf.compat.v1.train.Saver` or\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7157c028d258751a195e6f7ffc6e0360509b4de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 22/12 | Churn Δ: 10 | Churn Cumulative: 996 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -90,7 +90,7 @@ class DenseFeatures(kfc._BaseFeaturesLayer):\n           trainable:  Boolean, whether the layer's variables will be updated via\n             gradient descent during training.\n           name: Name to give to the DenseFeatures.\n-          partitioner: Partitioner for input layer. Defaults to None.\n+          partitioner: Partitioner for input layer. Defaults to `None`.\n           **kwargs: Keyword arguments to construct a layer.\n \n         Raises:\n@@ -150,8 +150,8 @@ class DenseFeatures(kfc._BaseFeaturesLayer):\n             method of any `FeatureColumn` that takes a `training` argument. For\n             example, if a `FeatureColumn` performed dropout, the column could\n             expose a `training` argument to control whether the dropout should\n-            be applied. If `None`, defaults to\n-            `tf.keras.backend.learning_phase()`.\n+            be applied. If `None`, becomes `tf.keras.backend.learning_phase()`.\n+            Defaults to `None`.\n \n \n         Returns:\n\n@@ -122,8 +122,8 @@ class SequenceFeatures(kfc._BaseFeaturesLayer):\n             method of any `FeatureColumn` that takes a `training` argument. For\n             example, if a `FeatureColumn` performed dropout, the column could\n             expose a `training` argument to control whether the dropout should\n-            be applied. If `None`, defaults to\n-            `tf.keras.backend.learning_phase()`.\n+            be applied. If `None`, becomes `tf.keras.backend.learning_phase()`.\n+            Defaults to `None`.\n \n \n         Returns:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d1b6b77d5e8c6f925b407f3585ddfb17cb7702e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 17 | Churn Cumulative: 790 | Contributors (this commit): 9 | Commits (past 90d): 14 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,7 @@ class LeakyReLU(Layer):\n       Same shape as the input.\n \n     Args:\n-      alpha: Float >= 0. Negative slope coefficient. Default to 0.3.\n+      alpha: Float >= 0. Negative slope coefficient. Defaults to `0.3`.\n \n     \"\"\"\n \n\n@@ -65,11 +65,11 @@ class ReLU(Layer):\n       Same shape as the input.\n \n     Args:\n-      max_value: Float >= 0. Maximum activation value. Default to None, which\n-        means unlimited.\n-      negative_slope: Float >= 0. Negative slope coefficient. Default to 0.\n-      threshold: Float >= 0. Threshold value for thresholded activation. Default\n-        to 0.\n+      max_value: Float >= 0. Maximum activation value. None means unlimited.\n+        Defaults to `None`.\n+      negative_slope: Float >= 0. Negative slope coefficient. Defaults to `0.`.\n+      threshold: Float >= 0. Threshold value for thresholded activation.\n+        Defaults to `0.`.\n     \"\"\"\n \n     def __init__(\n\n@@ -72,8 +72,9 @@ class Softmax(Layer):\n         normalization is applied.\n     Call arguments:\n       inputs: The inputs, or logits to the softmax layer.\n-      mask: A boolean mask of the same shape as `inputs`. Defaults to `None`.\n-        The mask specifies 1 to keep and 0 to mask.\n+      mask: A boolean mask of the same shape as `inputs`. The mask\n+        specifies 1 to keep and 0 to mask. Defaults to `None`.\n+\n \n     Returns:\n       softmaxed output with the same shape as `inputs`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7bcf63e3dab8fb257eaff216bc2817ed1aa461f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 30 | Files Changed: 9 | Hunks: 11 | Methods Changed: 0 | Complexity Δ (Sum/Max): 96/23 | Churn Δ: 60 | Churn Cumulative: 5676 | Contributors (this commit): 11 | Commits (past 90d): 27 | Contributors (cumulative): 64 | DMM Complexity: None\n\nDIFF:\n@@ -65,10 +65,10 @@ class DepthwiseConv(Conv):\n         `channels_first`.  The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape `(batch_size, height,\n         width, channels)` while `channels_first` corresponds to inputs with\n-        shape `(batch_size, channels, height, width)`. It defaults to the\n-        `image_data_format` value found in your Keras config file at\n-        `~/.keras/keras.json`. If you never set it, then it will be\n-        'channels_last'.\n+        shape `(batch_size, channels, height, width)`. If left unspecified,\n+        uses `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: An integer or tuple/list of 2 integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n\n@@ -54,7 +54,7 @@ class Conv1DTranspose(Conv1D):\n       kernel_size: An integer length of the 1D convolution window.\n       strides: An integer specifying the stride of the convolution along the\n         time dimension. Specifying a stride value != 1 is incompatible with\n-        specifying a `dilation_rate` value != 1. Defaults to 1.\n+        specifying a `dilation_rate` value != 1. Defaults to `1`.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n         `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n         evenly to the left/right or up/down of the input such that output has\n\n@@ -101,11 +101,11 @@ class Conv2D(Conv):\n         `channels_first`.  The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape `(batch_size, height,\n         width, channels)` while `channels_first` corresponds to inputs with\n-        shape `(batch_size, channels, height, width)`. It defaults to the\n-        `image_data_format` value found in your Keras config file at\n-        `~/.keras/keras.json`. If you never set it, then it will be\n-        `channels_last`. Note that the `channels_first` format is currently not\n-        supported by TensorFlow on CPU.\n+        shape `(batch_size, channels, height, width)`. If left unspecified, it\n+        uses the `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Note that the `channels_first` format is currently not\n+        supported by TensorFlow on CPU. Defaults to 'channels_last'.\n       dilation_rate: an integer or tuple/list of 2 integers, specifying the\n         dilation rate to use for dilated convolution. Can be a single integer to\n         specify the same value for all spatial dimensions. Currently, specifying\n\n@@ -82,9 +82,9 @@ class Conv2DTranspose(Conv2D):\n         `(batch_size, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n         `(batch_size, channels, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses `image_data_format` value found in your Keras\n+        config file at `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to \"channels_last\".\n       dilation_rate: an integer, specifying the dilation rate for all spatial\n         dimensions for dilated convolution. Specifying different dilation rates\n         for different dimensions is not supported.\n\n@@ -83,11 +83,11 @@ class Conv3D(Conv):\n         `channels_last` corresponds to inputs with shape `batch_shape +\n         (spatial_dim1, spatial_dim2, spatial_dim3, channels)` while\n         `channels_first` corresponds to inputs with shape `batch_shape +\n-        (channels, spatial_dim1, spatial_dim2, spatial_dim3)`. It defaults to\n-        the `image_data_format` value found in your Keras config file at\n-        `~/.keras/keras.json`. If you never set it, then it will be\n-        \"channels_last\". Note that the `channels_first` format is currently not\n-        supported by TensorFlow on CPU.\n+        (channels, spatial_dim1, spatial_dim2, spatial_dim3)`. When unspecified,\n+        uses `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json` (if exists) else 'channels_last'. Note that the\n+        `channels_first` format is currently not supported by TensorFlow on CPU.\n+        Defaults to 'channels_last'.\n       dilation_rate: an integer or tuple/list of 3 integers, specifying the\n         dilation rate to use for dilated convolution. Can be a single integer to\n         specify the same value for all spatial dimensions. Currently, specifying\n\n@@ -82,9 +82,9 @@ class Conv3DTranspose(Conv3D):\n         `(batch_size, depth, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n         `(batch_size, channels, depth, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses `image_data_format` value found in your Keras\n+        config file at `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: an integer or tuple/list of 3 integers, specifying\n         the dilation rate to use for dilated convolution.\n         Can be a single integer to specify the same value for\n\n@@ -67,10 +67,10 @@ class DepthwiseConv1D(DepthwiseConv):\n         `channels_first`.  The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape `(batch_size, height,\n         width, channels)` while `channels_first` corresponds to inputs with\n-        shape `(batch_size, channels, height, width)`. It defaults to the\n+        shape `(batch_size, channels, height, width)`. When unspecified, uses\n         `image_data_format` value found in your Keras config file at\n-        `~/.keras/keras.json`. If you never set it, then it will be\n-        'channels_last'.\n+        `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: A single integer, specifying the dilation rate to use for\n         dilated convolution. Currently, specifying any `dilation_rate`\n         value != 1 is incompatible with specifying any stride value != 1.\n\n@@ -68,10 +68,10 @@ class DepthwiseConv2D(DepthwiseConv):\n         `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape `(batch_size, height,\n         width, channels)` while `channels_first` corresponds to inputs with\n-        shape `(batch_size, channels, height, width)`. It defaults to the\n+        shape `(batch_size, channels, height, width)`. When unspecified, uses\n         `image_data_format` value found in your Keras config file at\n-        `~/.keras/keras.json`. If you never set it, then it will be\n-        'channels_last'.\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: An integer or tuple/list of 2 integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n\n@@ -70,9 +70,9 @@ class SeparableConv2D(SeparableConv):\n         `(batch_size, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n         `(batch_size, channels, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses `image_data_format` value found in your Keras\n+        config file at `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: An integer or tuple/list of 2 integers, specifying\n         the dilation rate to use for dilated convolution.\n       depth_multiplier: The number of depthwise convolution output channels\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83dbe711f6e18d0fe2e770264ecd845a2bf58522", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 55/29 | Churn Δ: 34 | Churn Cumulative: 1628 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -50,18 +50,19 @@ class GroupNormalization(Layer):\n     Args:\n       groups: Integer, the number of groups for Group Normalization. Can be in\n         the range [1, N] where N is the input dimension. The input dimension\n-        must be divisible by the number of groups. Defaults to 32.\n+        must be divisible by the number of groups. Defaults to `32`.\n       axis: Integer or List/Tuple. The axis or axes to normalize across.\n-        Typically this is the features axis/axes. The left-out axes are\n-        typically the batch axis/axes. This argument defaults to `-1`, the last\n-        dimension in the input.\n+        Typically, this is the features axis/axes. The left-out axes are\n+        typically the batch axis/axes. `-1` is the last dimension in the\n+        input. Defaults to `-1`.\n       epsilon: Small float added to variance to avoid dividing by zero. Defaults\n         to 1e-3\n       center: If True, add offset of `beta` to normalized tensor. If False,\n-        `beta` is ignored. Defaults to True.\n+        `beta` is ignored. Defaults to `True`.\n       scale: If True, multiply by `gamma`. If False, `gamma` is not used.\n-        Defaults to True. When the next layer is linear (also e.g. `nn.relu`),\n-        this can be disabled since the scaling will be done by the next layer.\n+        When the next layer is linear (also e.g. `nn.relu`), this can be\n+        disabled since the scaling will be done by the next layer.\n+        Defaults to `True`.\n       beta_initializer: Initializer for the beta weight. Defaults to zeros.\n       gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n       beta_regularizer: Optional regularizer for the beta weight. None by\n\n@@ -120,16 +120,17 @@ class LayerNormalization(Layer):\n \n     Args:\n       axis: Integer or List/Tuple. The axis or axes to normalize across.\n-        Typically this is the features axis/axes. The left-out axes are\n-        typically the batch axis/axes. This argument defaults to `-1`, the last\n-        dimension in the input.\n+        Typically, this is the features axis/axes. The left-out axes are\n+        typically the batch axis/axes. `-1` is the last dimension in the\n+        input. Defaults to `-1`.\n       epsilon: Small float added to variance to avoid dividing by zero. Defaults\n         to 1e-3\n       center: If True, add offset of `beta` to normalized tensor. If False,\n-        `beta` is ignored. Defaults to True.\n+        `beta` is ignored. Defaults to `True`.\n       scale: If True, multiply by `gamma`. If False, `gamma` is not used.\n-        Defaults to True. When the next layer is linear (also e.g. `nn.relu`),\n-        this can be disabled since the scaling will be done by the next layer.\n+        When the next layer is linear (also e.g. `nn.relu`), this can be\n+        disabled since the scaling will be done by the next layer.\n+        Defaults to `True`.\n       beta_initializer: Initializer for the beta weight. Defaults to zeros.\n       gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n       beta_regularizer: Optional regularizer for the beta weight. None by\n\n@@ -40,9 +40,9 @@ class UnitNormalization(base_layer.Layer):\n \n     Args:\n       axis: Integer or list/tuple. The axis or axes to normalize across.\n-        Typically this is the features axis or axes. The left-out axes are\n-        typically the batch axis or axes. Defaults to `-1`, the last dimension\n-        in the input.\n+        Typically, this is the features axis or axes. The left-out axes are\n+        typically the batch axis or axes. `-1` is the last dimension\n+        in the input. Defaults to `-1`.\n     \"\"\"\n \n     def __init__(self, axis=-1, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#60043ea048cc7f19702ba0a9800fe8e6fa68a05c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 24 | Files Changed: 8 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 12/2 | Churn Δ: 55 | Churn Cumulative: 1939 | Contributors (this commit): 8 | Commits (past 90d): 20 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -108,9 +108,10 @@ class AveragePooling2D(Pooling2D):\n         `(batch, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n         `(batch, channels, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Input shape:\n       - If `data_format='channels_last'`:\n\n@@ -48,9 +48,10 @@ class AveragePooling3D(Pooling3D):\n         `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n         while `channels_first` corresponds to inputs with shape\n         `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Input shape:\n       - If `data_format='channels_last'`:\n\n@@ -44,9 +44,9 @@ class GlobalAveragePooling2D(GlobalPooling2D):\n           `(batch, height, width, channels)` while `channels_first`\n           corresponds to inputs with shape\n           `(batch, channels, height, width)`.\n-          It defaults to the `image_data_format` value found in your\n-          Keras config file at `~/.keras/keras.json`.\n-          If you never set it, then it will be \"channels_last\".\n+          When unspecified, uses `image_data_format` value found\n+          in your Keras config file at `~/.keras/keras.json`\n+          (if exists) else 'channels_last'. Defaults to 'channels_last'.\n         keepdims: A boolean, whether to keep the spatial dimensions or not.\n           If `keepdims` is `False` (default), the rank of the tensor is reduced\n           for spatial dimensions.\n\n@@ -36,9 +36,10 @@ class GlobalAveragePooling3D(GlobalPooling3D):\n         `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n         while `channels_first` corresponds to inputs with shape\n         `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       keepdims: A boolean, whether to keep the spatial dimensions or not.\n         If `keepdims` is `False` (default), the rank of the tensor is reduced\n         for spatial dimensions.\n\n@@ -42,9 +42,10 @@ class GlobalMaxPooling2D(GlobalPooling2D):\n         `(batch, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n         `(batch, channels, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       keepdims: A boolean, whether to keep the spatial dimensions or not.\n         If `keepdims` is `False` (default), the rank of the tensor is reduced\n         for spatial dimensions.\n\n@@ -34,9 +34,10 @@ class GlobalMaxPooling3D(GlobalPooling3D):\n         `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n         while `channels_first` corresponds to inputs with shape\n         `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       keepdims: A boolean, whether to keep the spatial dimensions or not.\n         If `keepdims` is `False` (default), the rank of the tensor is reduced\n         for spatial dimensions.\n\n@@ -127,9 +127,10 @@ class MaxPooling2D(Pooling2D):\n         `(batch, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n         `(batch, channels, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Input shape:\n       - If `data_format='channels_last'`:\n\n@@ -48,9 +48,10 @@ class MaxPooling3D(Pooling3D):\n         `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n         while `channels_first` corresponds to inputs with shape\n         `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Input shape:\n       - If `data_format='channels_last'`:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2aec8c152bf8097a60442e7601b3cb748aca15bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 56 | Files Changed: 11 | Hunks: 45 | Methods Changed: 2 | Complexity Δ (Sum/Max): 731/166 | Churn Δ: 118 | Churn Cumulative: 34938 | Contributors (this commit): 35 | Commits (past 90d): 34 | Contributors (cumulative): 146 | DMM Complexity: None\n\nDIFF:\n@@ -90,7 +90,7 @@ class CategoryEncoding(base_layer.Layer):\n         inputs to the layer must integers in the range `0 <= value <\n         num_tokens`, or an error will be thrown.\n       output_mode: Specification for the output of the layer.\n-        Defaults to `\"multi_hot\"`. Values can be `\"one_hot\"`, `\"multi_hot\"` or\n+        Values can be `\"one_hot\"`, `\"multi_hot\"` or\n         `\"count\"`, configuring the layer as follows:\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array of `num_tokens` size, containing a 1 at the element index. If\n@@ -105,6 +105,7 @@ class CategoryEncoding(base_layer.Layer):\n           - `\"count\"`: Like `\"multi_hot\"`, but the int array contains a count of\n             the number of times the token at that index appeared in the sample.\n         For all output modes, currently only output up to rank 2 is supported.\n+        Defaults to `\"multi_hot\"`.\n       sparse: Boolean. If true, returns a `SparseTensor` instead of a dense\n         `Tensor`. Defaults to `False`.\n \n\n@@ -164,8 +164,8 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n         0.01). Higher values of epsilon increase the quantile approximation, and\n         hence result in more unequal buckets, but could improve performance\n         and resource consumption.\n-      output_mode: Specification for the output of the layer. Defaults to\n-        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or\n+      output_mode: Specification for the output of the layer. Values can be\n+       `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or\n         `\"count\"` configuring the layer as follows:\n           - `\"int\"`: Return the discretized bin indices directly.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n@@ -180,9 +180,10 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n             will be `(..., num_tokens)`.\n           - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of\n             the number of times the bin index appeared in the sample.\n+        Defaults to `\"int\"`.\n       sparse: Boolean. Only applicable to `\"one_hot\"`, `\"multi_hot\"`,\n         and `\"count\"` output modes. If True, returns a `SparseTensor` instead of\n-        a dense `Tensor`. Defaults to False.\n+        a dense `Tensor`. Defaults to `False`.\n \n     Examples:\n \n\n@@ -51,15 +51,15 @@ class HashedCrossing(base_layer.Layer):\n \n     Args:\n       num_bins: Number of hash bins.\n-      output_mode: Specification for the output of the layer. Defaults to\n-        `\"int\"`.  Values can be `\"int\"`, or `\"one_hot\"` configuring the layer as\n-        follows:\n+      output_mode: Specification for the output of the layer. Values can be\n+        `\"int\"`, or `\"one_hot\"` configuring the layer as follows:\n           - `\"int\"`: Return the integer bin indices directly.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as `num_bins`, containing a 1 at the input's bin\n             index.\n+        Defaults to `\"int\"`.\n       sparse: Boolean. Only applicable to `\"one_hot\"` mode. If True, returns a\n-        `SparseTensor` instead of a dense `Tensor`. Defaults to False.\n+        `SparseTensor` instead of a dense `Tensor`. Defaults to `False`.\n       **kwargs: Keyword arguments to construct a layer.\n \n     Examples:\n\n@@ -109,17 +109,16 @@ class Hashing(base_layer.Layer):\n         bin, so the effective number of bins is `(num_bins - 1)` if `mask_value`\n         is set.\n       mask_value: A value that represents masked inputs, which are mapped to\n-        index 0. Defaults to None, meaning no mask term will be added and the\n-        hashing will start at index 0.\n+        index 0. None means no mask term will be added and the\n+        hashing will start at index 0. Defaults to `None`.\n       salt: A single unsigned integer or None.\n         If passed, the hash function used will be SipHash64, with these values\n         used as an additional input (known as a \"salt\" in cryptography).\n-        These should be non-zero. Defaults to `None` (in that\n-        case, the FarmHash64 hash function is used). It also supports\n-        tuple/list of 2 unsigned integer numbers, see reference paper for\n-        details.\n-      output_mode: Specification for the output of the layer. Defaults to\n-        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or\n+        These should be non-zero. If None, uses the FarmHash64 hash function.\n+        It also supports tuple/list of 2 unsigned integer numbers, see\n+        reference paper for details. Defaults to `None`.\n+      output_mode: Specification for the output of the layer. Values can bes\n+        `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or\n         `\"count\"` configuring the layer as follows:\n           - `\"int\"`: Return the integer bin indices directly.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n@@ -134,9 +133,10 @@ class Hashing(base_layer.Layer):\n             will be `(..., num_tokens)`.\n           - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of\n             the number of times the bin index appeared in the sample.\n+        Defaults to `\"int\"`.\n       sparse: Boolean. Only applicable to `\"one_hot\"`, `\"multi_hot\"`,\n         and `\"count\"` output modes. If True, returns a `SparseTensor` instead of\n-        a dense `Tensor`. Defaults to False.\n+        a dense `Tensor`. Defaults to `False`.\n       **kwargs: Keyword arguments to construct a layer.\n \n     Input shape:\n\n@@ -65,9 +65,9 @@ class Resizing(base_layer.Layer):\n         height: Integer, the height of the output shape.\n         width: Integer, the width of the output shape.\n         interpolation: String, the interpolation method.\n-            Defaults to `\"bilinear\"`.\n             Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n             `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n+            Defaults to `\"bilinear\"`.\n         crop_to_aspect_ratio: If True, resize the images without aspect\n             ratio distortion. When the original aspect ratio differs\n             from the target aspect ratio, the output image will be\n@@ -420,9 +420,9 @@ class RandomFlip(base_layer.BaseRandomLayer):\n \n     Args:\n         mode: String indicating which flip mode to use. Can be `\"horizontal\"`,\n-            `\"vertical\"`, or `\"horizontal_and_vertical\"`. Defaults to\n-            `\"horizontal_and_vertical\"`. `\"horizontal\"` is a left-right flip and\n-            `\"vertical\"` is a top-bottom flip.\n+            `\"vertical\"`, or `\"horizontal_and_vertical\"`. `\"horizontal\"` is a\n+            left-right flip and `\"vertical\"` is a top-bottom flip. Defaults to\n+            `\"horizontal_and_vertical\"`\n         seed: Integer. Used to create a random seed.\n     \"\"\"\n \n@@ -1055,9 +1055,9 @@ class RandomZoom(base_layer.BaseRandomLayer):\n             result in an output\n             zooming out between 20% to 30%.\n             `width_factor=(-0.3, -0.2)` result in an\n-            output zooming in between 20% to 30%. Defaults to `None`,\n+            output zooming in between 20% to 30%. `None` means\n             i.e., zooming vertical and horizontal directions\n-            by preserving the aspect ratio.\n+            by preserving the aspect ratio. Defaults to `None`.\n         fill_mode: Points outside the boundaries of the input are\n             filled according to the given mode\n             (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n@@ -1377,9 +1377,9 @@ class RandomBrightness(base_layer.BaseRandomLayer):\n             will be used for upper bound.\n         value_range: Optional list/tuple of 2 floats\n             for the lower and upper limit\n-            of the values of the input data. Defaults to [0.0, 255.0].\n-            Can be changed to e.g. [0.0, 1.0] if the image input\n-            has been scaled before this layer.\n+            of the values of the input data.\n+            To make no change, use [0.0, 1.0], e.g., if the image input\n+            has been scaled before this layer. Defaults to [0.0, 255.0].\n             The brightness adjustment will be scaled to this range, and the\n             output values will be clipped to this range.\n         seed: optional integer, for fixed RNG behavior.\n@@ -1539,9 +1539,9 @@ class RandomHeight(base_layer.BaseRandomLayer):\n             `factor=0.2` results in an output with\n             height changed by a random amount in the range `[-20%, +20%]`.\n         interpolation: String, the interpolation method.\n-            Defaults to `\"bilinear\"`.\n             Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n             `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n+            Defaults to `\"bilinear\"`.\n         seed: Integer. Used to create a random seed.\n \n     Input shape:\n@@ -1661,9 +1661,9 @@ class RandomWidth(base_layer.BaseRandomLayer):\n             `factor=0.2` results in an output with width changed\n             by a random amount in the range `[-20%, +20%]`.\n         interpolation: String, the interpolation method.\n-            Defaults to `bilinear`.\n             Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n             `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n+            Defaults to `bilinear`.\n         seed: Integer. Used to create a random seed.\n \n     Input shape:\n\n@@ -2233,7 +2233,7 @@ class LearningPhaseTest(test_combinations.TestCase):\n         layer = image_preprocessing.RandomWidth(0.5, seed=123)\n         shape = (12, 12, 3)\n         img = np.random.random((12,) + shape)\n-        out = layer(img)  # Default to training=True\n+        out = layer(img)  # Defaults to training=True\n         self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)\n \n         out = layer(img, training=True)\n@@ -2249,7 +2249,7 @@ class LearningPhaseTest(test_combinations.TestCase):\n \n         shape = (12, 12, 3)\n         img = np.random.random((12,) + shape)\n-        out = seq(img)  # Default to training=True\n+        out = seq(img)  # Defaults to training=True\n         self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)\n \n         out = seq(img, training=True)\n\n@@ -134,10 +134,10 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         `\"tf_idf\"`, this argument must be supplied.\n       invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n         map indices to vocabulary items instead of mapping vocabulary items to\n-        indices. Default to False.\n-      output_mode: Specification for the output of the layer. Defaults to\n-        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`,\n-        or `\"tf_idf\"` configuring the layer as follows:\n+        indices. Defaults to `False`.\n+      output_mode: Specification for the output of the layer. Values can be\n+        `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"`\n+        configuring the layer as follows:\n           - `\"int\"`: Return the raw integer indices of the input tokens.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as the vocabulary, containing a 1 at the element\n@@ -153,6 +153,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n             the number of times the token at that index appeared in the sample.\n           - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n             find the value in each token slot.\n+        Defaults to `\"int\"`.\n       pad_to_max_tokens: Only valid when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n         padded to `max_tokens` even if the number of unique tokens in the\n@@ -161,7 +162,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         False.\n       sparse: Boolean. Only applicable to `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`\n         and `\"tf-idf\"` output modes. If True, returns a `SparseTensor` instead\n-        of a dense `Tensor`. Defaults to False.\n+        of a dense `Tensor`. Defaults to `False`.\n     \"\"\"\n \n     def __init__(\n\n@@ -71,18 +71,18 @@ class IntegerLookup(index_lookup.IndexLookup):\n         only be specified when adapting the vocabulary or when setting\n         `pad_to_max_tokens=True`. If None, there is no cap on the size of the\n         vocabulary. Note that this size includes the OOV and mask tokens.\n-        Defaults to None.\n+        Defaults to `None`.\n       num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n         value is more than 1, OOV inputs are modulated to determine their OOV\n         value. If this value is 0, OOV inputs will cause an error when calling\n-        the layer. Defaults to 1.\n+        the layer. Defaults to `1`.\n       mask_token: An integer token that represents masked inputs. When\n         `output_mode` is `\"int\"`, the token is included in vocabulary and mapped\n         to index 0. In other output modes, the token will not appear in the\n         vocabulary and instances of the mask token in the input will be dropped.\n-        If set to None, no mask term will be added. Defaults to None.\n+        If set to None, no mask term will be added. Defaults to `None`.\n       oov_token: Only used when `invert` is True. The token to return for OOV\n-        indices. Defaults to -1.\n+        indices. Defaults to `-1`.\n       vocabulary: Optional. Either an array of integers or a string path to a\n         text file. If passing an array, can pass a tuple, list, 1D numpy array,\n         or 1D tensor containing the integer vocbulary terms. If passing a file\n@@ -98,10 +98,10 @@ class IntegerLookup(index_lookup.IndexLookup):\n         `\"tf_idf\"`, this argument must be supplied.\n       invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n         map indices to vocabulary items instead of mapping vocabulary items to\n-        indices. Default to False.\n-      output_mode: Specification for the output of the layer. Defaults to\n-        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`,\n-        or `\"tf_idf\"` configuring the layer as follows:\n+        indices. Defaults to `False`.\n+      output_mode: Specification for the output of the layer. Values can be\n+        `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"`\n+        configuring the layer as follows:\n           - `\"int\"`: Return the vocabulary indices of the input tokens.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as the vocabulary, containing a 1 at the element\n@@ -119,6 +119,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n             find the value in each token slot.\n         For `\"int\"` output, any shape of input and output is supported. For all\n         other output modes, currently only output up to rank 2 is supported.\n+        Defaults to `\"int\"`.\n       pad_to_max_tokens: Only applicable when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n         padded to `max_tokens` even if the number of unique tokens in the\n@@ -127,7 +128,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n         False.\n       sparse: Boolean. Only applicable when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, returns a `SparseTensor` instead of a\n-        dense `Tensor`. Defaults to False.\n+        dense `Tensor`. Defaults to `False`.\n \n     Examples:\n \n\n@@ -52,11 +52,12 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n           example, if shape is `(None, 5)` and `axis=1`, the layer will track 5\n           separate mean and variance values for the last axis. If `axis` is set\n           to `None`, the layer will normalize all elements in the input by a\n-          scalar mean and variance. Defaults to -1, where the last axis of the\n+          scalar mean and variance. When `-1` the last axis of the\n           input is assumed to be a feature dimension and is normalized per\n           index. Note that in the specific case of batched scalar inputs where\n           the only axis is the batch axis, the default will normalize each index\n           in the batch separately. In this case, consider passing `axis=None`.\n+          Defaults to `-1`.\n         mean: The mean value(s) to use during normalization. The passed value(s)\n           will be broadcast to the shape of the kept axes above; if the value(s)\n           cannot be broadcast, an error will be raised when this layer's\n\n@@ -68,11 +68,11 @@ class StringLookup(index_lookup.IndexLookup):\n         only be specified when adapting the vocabulary or when setting\n         `pad_to_max_tokens=True`. If None, there is no cap on the size of the\n         vocabulary. Note that this size includes the OOV and mask tokens.\n-        Defaults to None.\n+        Defaults to `None`.\n       num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n         value is more than 1, OOV inputs are hashed to determine their OOV\n         value. If this value is 0, OOV inputs will cause an error when calling\n-        the layer.  Defaults to 1.\n+        the layer.  Defaults to `1`.\n       mask_token: A token that represents masked inputs. When `output_mode` is\n         `\"int\"`, the token is included in vocabulary and mapped to index 0. In\n         other output modes, the token will not appear in the vocabulary and\n@@ -93,10 +93,10 @@ class StringLookup(index_lookup.IndexLookup):\n         `\"tf_idf\"`, this argument must be supplied.\n       invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n         map indices to vocabulary items instead of mapping vocabulary items to\n-        indices. Default to False.\n-      output_mode: Specification for the output of the layer. Defaults to\n-        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`,\n-        or `\"tf_idf\"` configuring the layer as follows:\n+        indices. Defaults to `False`.\n+      output_mode: Specification for the output of the layer. Values can be\n+        `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"`\n+        configuring the layer as follows:\n           - `\"int\"`: Return the raw integer indices of the input tokens.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as the vocabulary, containing a 1 at the element\n@@ -114,6 +114,7 @@ class StringLookup(index_lookup.IndexLookup):\n             find the value in each token slot.\n         For `\"int\"` output, any shape of input and output is supported. For all\n         other output modes, currently only output up to rank 2 is supported.\n+        Defaults to `\"int\"`\n       pad_to_max_tokens: Only applicable when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n         padded to `max_tokens` even if the number of unique tokens in the\n@@ -122,7 +123,7 @@ class StringLookup(index_lookup.IndexLookup):\n         False.\n       sparse: Boolean. Only applicable when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, returns a `SparseTensor` instead of a\n-        dense `Tensor`. Defaults to False.\n+        dense `Tensor`. Defaults to `False`.\n       encoding: Optional. The text encoding to use to interpret the input\n         strings. Defaults to `\"utf-8\"`.\n \n\n@@ -152,12 +152,12 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         have its time dimension padded or truncated to exactly\n         `output_sequence_length` values, resulting in a tensor of shape\n         `(batch_size, output_sequence_length)` regardless of how many tokens\n-        resulted from the splitting step. Defaults to None.\n+        resulted from the splitting step. Defaults to `None`.\n       pad_to_max_tokens: Only valid in  `\"multi_hot\"`, `\"count\"`, and `\"tf_idf\"`\n         modes. If True, the output will have its feature axis padded to\n         `max_tokens` even if the number of unique tokens in the vocabulary is\n         less than max_tokens, resulting in a tensor of shape `(batch_size,\n-        max_tokens)` regardless of vocabulary size. Defaults to False.\n+        max_tokens)` regardless of vocabulary size. Defaults to `False`.\n       vocabulary: Optional. Either an array of strings or a string path to a\n         text file. If passing an array, can pass a tuple, list, 1D numpy array,\n         or 1D tensor containing the string vocabulary terms. If passing a file\n@@ -171,10 +171,10 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         `\"tf_idf\"`, this argument must be supplied.\n       ragged: Boolean. Only applicable to `\"int\"` output mode. If True, returns\n         a `RaggedTensor` instead of a dense `Tensor`, where each sequence may\n-        have a different length after string splitting. Defaults to False.\n+        have a different length after string splitting. Defaults to `False`.\n       sparse: Boolean. Only applicable to `\"multi_hot\"`, `\"count\"`, and\n         `\"tf_idf\"` output modes. If True, returns a `SparseTensor` instead of a\n-        dense `Tensor`. Defaults to False.\n+        dense `Tensor`. Defaults to `False`.\n       encoding: Optional. The text encoding to use to interpret the input\n         strings. Defaults to `\"utf-8\"`.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
