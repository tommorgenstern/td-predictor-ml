{"custom_id": "scrapy#b76335034054d77e4b55350d761fea4946dea3f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 444 | Files Changed: 6 | Hunks: 6 | Methods Changed: 37 | Complexity Δ (Sum/Max): -32/0 | Churn Δ: 444 | Churn Cumulative: 456 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 10 | DMM Complexity: 0.08444444444444445\n\nDIFF:\n@@ -1,3 +0,0 @@\n-from scrapy.contrib.history.history import ItemHistory\n-from scrapy.contrib.history.scheduler import RulesScheduler\n-from scrapy.contrib.history.store import SQLHistoryStore\n\n@@ -1,162 +0,0 @@\n-\"\"\"\n-History management\n-\"\"\"\n-import re\n-from datetime import datetime\n-\n-import MySQLdb\n-\n-class History(object):\n-    \"\"\"Base class for tracking different kinds of histories\"\"\"\n-    \n-    def __init__(self, db_uri):\n-        self.db_uri = db_uri\n-        self._mysql_conn = None\n-\n-    def connect(self):\n-        \"\"\"\n-        Connect to PDB and open mysql connect to PRODUCT_DB\n-        \"\"\"\n-        m = re.search(r\"mysql:\\/\\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/]+)/(?P<db>.*)$\", self.db_uri)\n-        if m:\n-            d = m.groupdict()\n-            if d['passwd'] is None:\n-                del(d['passwd'])\n-\n-            d['charset'] = \"utf8\"\n-            self._mysql_conn = MySQLdb.connect(**d)\n-\n-    def get_mysql_conn(self):\n-        if self._mysql_conn is None:\n-            self.connect()\n-        return self._mysql_conn\n-    mysql_conn = property(get_mysql_conn)\n-\n-\n-class ItemHistory(History):\n-    \"\"\"\n-    Class instance registers item guids, and versions with check_item method.\n-    It also gives access to stored ItemTicket and ItemVersion objects.\n-    \"\"\"\n-    NEW = 0\n-    UPDATE = 1\n-    DUPLICATE = 2\n-    \n-    def check_item(self, domain, item):\n-        \"\"\"\n-        Store item's guid and version to the database along\n-        with date and time of last occurence.\n-        Return:\n-         * ItemHistory.NEW       - if guid hasn't been met\n-         * ItemHistory.UPDATE    - if history already contains guid, but versions doesn't match\n-         * ItemHistory.DUPLICATE - both guid and version are not new\n-        \"\"\"\n-        version = item.version\n-\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-\n-        def add_version(version):\n-            insert = \"INSERT INTO version (guid, version, seen) VALUES (%s,%s,%s)\"\n-            c.execute(insert, (item.guid, version, datetime.now()))\n-\n-        select = \"SELECT * FROM ticket WHERE guid=%s\"\n-        c.execute(select, item.guid)\n-        r = c.fetchone()\n-        if r:\n-            select = \"SELECT * FROM version WHERE version=%s\"\n-            if c.execute(select, version):\n-                update = \"UPDATE version SET seen=%s WHERE version=%s\"\n-                c.execute(update, (datetime.now(), version))\n-                self.mysql_conn.commit()\n-                return ItemHistory.DUPLICATE\n-            else:\n-                add_version(version)\n-                self.mysql_conn.commit()\n-                return ItemHistory.UPDATE\n-        else:\n-            insert = \"INSERT INTO ticket (guid, domain, url, url_hash) VALUES (%s,%s,%s,%s)\"\n-            c.execute(insert, (item.guid, domain, item.url, hash(item.url)))\n-            add_version(version)\n-            self.mysql_conn.commit()\n-            return ItemHistory.NEW\n-        \n-    def get_ticket(self, guid):\n-        \"\"\"\n-        Return ItemTicket object for guid. \n-        ItemVersion objects can be accessed via 'versions' list.\n-        \"\"\"\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-        select = \"SELECT * FROM ticket WHERE guid=%s\"\n-        c.execute(select, guid)\n-        ticket = c.fetchone()\n-        if not ticket:\n-            raise Exception(\"Item ticket with guid = '%s' not found\" % guid)\n-        ticket['versions'] = []\n-        select = \"SELECT * FROM version WHERE guid=%s\"\n-        c.execute(select, guid)\n-        for version in c.fetchall():\n-            ticket['versions'].append(version)\n-        return ticket\n-\n-    def delete_ticket(self, guid):\n-        \"\"\"Delete item ticket and associated versions from DB\"\"\"\n-        c = self.mysql_conn.cursor()\n-        delete = \"DELETE FROM ticket WHERE guid=%s\"\n-        c.execute(delete, guid)\n-        self.mysql_conn.commit()\n-\n-\n-class URLHistory(History):\n-    \"\"\"\n-    Access URL status and history for the scraping engine\n-    \n-    This is degsigned to have an instance per domain where typically\n-    a call will be made to get_url_status, followed by either\n-    update_checked or record_version.\n-    \"\"\"\n-    \n-    def get_url_status(self, urlkey):\n-        \"\"\"\n-        Get the url status (url, last_version, last_checked), \n-        or None if the url data has not been seen before\n-        \"\"\"\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-        select = \"SELECT * FROM url_status WHERE url_hash=%s\"\n-        c.execute(select, urlkey)\n-        r = c.fetchone()\n-        return (r['url'], r['last_version'], r['last_checked']) if r else None\n-\n-    def record_version(self, urlkey, url, parent_key, version, postdata_hash=None):\n-        \"\"\"\n-        Record a version of a page and update the last checked time. \n-\n-        If the same version (or None) is passed, the last checked time is still updated.\n-        \"\"\"\n-        now = datetime.now()\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-        select = \"SELECT * FROM url_status WHERE url_hash=%s\"\n-        c.execute(select, urlkey)\n-        r = c.fetchone()\n-\n-        if not r:\n-            insert = \"INSERT INTO url_status (url_hash, url, parent_hash, last_version, last_checked) VALUES (%s,%s,%s,%s,%s)\"\n-            c.execute(insert, (urlkey, url, parent_key, version, now))\n-        else:\n-            update = \"UPDATE url_status SET last_version=%s, last_checked=%s WHERE url_hash=%s\"\n-            c.execute(update, (version, now, urlkey))\n-        self.mysql_conn.commit()\n-\n-        last_version = r['last_version'] if r else None\n-        if version and version != last_version:\n-            if not c.execute(\"SELECT url_hash FROM url_history WHERE version=%s\", version):\n-                insert = \"INSERT INTO url_history (url_hash, version, postdata_hash, created) VALUES (%s,%s,%s,%s)\"\n-                c.execute(insert, (urlkey, version, postdata_hash, now))\n-                self.mysql_conn.commit()\n-\n-    def get_version_info(self, version):\n-        \"\"\"Simple accessor method\"\"\"\n-        c = self.mysql_conn.cursor(MySQLdb.cursors.DictCursor)\n-        select = \"SELECT * FROM url_history WHERE version=%s\"\n-        c.execute(select, version)\n-        r = c.fetchone()\n-        return (r['url_hash'], r['created']) if r else None\n\n@@ -1,35 +0,0 @@\n-\"\"\"\n-The datastore module contains implementations of data storage engines for the\n-crawling process. These are used to track metrics on each site and on each\n-page visited.\n-\"\"\"\n-from datetime import datetime\n-\n-class MemoryStore(object) :\n-    \"\"\"Simple implementation of a data store. This is useful if no persistent \n-    history is required (e.g. unit testing or development) and provides a \n-    simpler reference implementation.\n-    \"\"\"\n-    def __init__(self) :\n-        \"\"\"The store will just be a dict with an entry for each site, that \n-        entry will contain dicts and lists of data.\n-        \"\"\"\n-        self._store = {}\n-\n-    def open(self, site):\n-        self._store[site] = {}\n-\n-    def close_site(self, site):\n-        del self._store[site]\n-\n-    def store(self, site, key, url, parent=None, version=None, post_version=None):\n-        checked = datetime.now()\n-        self._store[key] = (version, checked)\n-\n-    def status(self, site, key):\n-        \"\"\"Get the version and last checked time for a key.\n-\n-        this will be changed later to support checking (lazily) the last modified \n-        time and perhaps other statistics needed by the scheduling algorithms\n-        \"\"\"\n-        return self._store.get(key)\n\n@@ -1,92 +0,0 @@\n-import hashlib\n-from datetime import datetime\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy.utils.misc import load_object\n-from scrapy.core import signals\n-from scrapy import log\n-from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n-from scrapy.conf import settings\n-\n-class HistoryMiddleware(object):\n-    # How often we should re-check links we know about\n-    MIN_CHECK_DAYS = 4\n-    # How often we should process pages that have not changed (need to include depth)\n-    MIN_PROCESS_UNCHANGED_DAYS = 12\n-\n-    MEMORYSTORE = 'scrapy.contrib_exp.history.memorystore.MemoryStore'\n-\n-    def __init__(self):\n-        historycls = load_object(self.MEMORYSTORE)\n-        if not historycls:\n-            raise NotConfigured\n-        self.historydata = historycls()\n-        dispatcher.connect(self.open_domain, signal=signals.domain_opened)\n-        dispatcher.connect(self.close_domain, signal=signals.domain_closed)\n-\n-    def process_request(self, request, spider):\n-        key = urlkey(request.url)\n-        status = self.historydata.status(domain, key)\n-        if status:\n-            _url, version, last_checked = status\n-            d = datetime.now() - last_checked\n-            if d.days < self.MIN_CHECK_DAYS:\n-                raise IgnoreRequest(\"Not scraping %s (scraped %s ago)\" % (request.url, d))\n-            request.meta['history_response_version'] = version\n-\n-    def process_response(self, request, response, spider):\n-        version = request.meta.get('history_response_version')\n-        if version == self.get_version(response):\n-            del request.content['history_response_version']\n-            hist = self.historydata.version_info(domain, version)\n-            if hist:\n-                versionkey, created = hist\n-                # if versionkey != urlkey(url) this means\n-                # the same content is available on a different url\n-                delta = datetime.now() - created\n-                if delta.days < self.MIN_PROCESS_UNCHANGED_DAYS:\n-                    message = \"skipping %s: unchanged for %s\" % (response.url, delta)\n-                    raise IgnoreRequest(message)\n-        self.record_visit(domain, request, response)\n-        return response\n-\n-    def process_exception(self, request, exception, spider):\n-        self.record_visit(spider.domain_name, request, None)\n-\n-    def open_domain(self, domain):\n-        self.historydata.open(domain)\n-\n-    def close_domain(self, domain):\n-        self.historydata.close_site(domain)\n-\n-    def record_visit(self, domain, request, response):\n-        \"\"\"record the fact that the url has been visited\"\"\"\n-        url = request.url\n-        post_version = hash(request.body)\n-        key = urlkey(url)\n-        if response:\n-            redirect_url = response.url\n-            parentkey = urlkey(response.request.headers.get('referer')) if response.request else None\n-            version = self.get_version(response)\n-        else:\n-            redirect_url, parentkey, version = url, None, None\n-        self.historydata.store(domain, key, url, parentkey, version, post_version)\n-\n-    def get_version(self, response):\n-        key = hashlib.sha1(response.body).hexdigest()\n-\n-def urlkey(url):\n-    \"\"\"Generate a 'key' for a given url\n-\n-    >>> urlkey(\"http://www.example.com/\")\n-    '89e6a0649e06d83370cdf2cbfb05f363934a8d0c'\n-    >>> urlkey(\"http://www.example.com/\") == urlkey(\"http://www.example.com/?\")\n-    True\n-    \"\"\"\n-    from scrapy.utils.c14n import canonicalize\n-    return hash(canonicalize(url))\n-\n-\n-def hash(value):\n-    return hashlib.sha1(value).hexdigest() if value else None\n\n@@ -1,105 +0,0 @@\n-\"\"\"\n-WARNING: This Scheduler code is obsolete and needs to be rewritten \n-\"\"\"\n-\n-import hashlib\n-from datetime import datetime\n-\n-from twisted.internet import defer\n-\n-from scrapy import log\n-from scrapy.core.scheduler import Scheduler\n-from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.utils.request import request_fingerprint\n-\n-class RulesScheduler(Scheduler):\n-    \"\"\"Scheduler that uses rules to determine if we should follow links\n-\n-     TODO:\n-     * take into account where in chain of links we are (less depth should\n-       be crawled more often)\n-     * Be more strict about scraping product pages that rarely lead to new\n-       versions of products. The same applies to pages with links. Particularly\n-       useful for filtering out when there are many urls for the same product.\n-       (but be careful to also filter out pages that almost always lead to new\n-       output).\n-    \"\"\"\n-\n-    # if these parameters change, then update bin/unavailable.py\n-\n-    # How often we should re-check links we know about\n-    MIN_CHECK_DAYS = 4\n-\n-    # How often we should process pages that have not changed (need to include depth)\n-    MIN_PROCESS_UNCHANGED_DAYS = 12\n-\n-    def enqueue_request(self, domain, request):\n-        \"\"\"Add a page to be scraped for a domain that is currently being scraped.\n-\n-        The url will only be added if we have not checked it already within\n-        a specified time period.\n-        \"\"\"\n-        requestid = request_fingerprint(request)\n-        added = self.groupfilter.add(domain, requestid)\n-\n-        if request.dont_filter or added:\n-            key = urlkey(request.url) # we can not use fingerprint unless lost crawled history\n-            status = self.historydata.status(domain, key)\n-            now = datetime.now()\n-            version = None\n-            if status:\n-                _url, version, last_checked = status\n-                d = now - last_checked\n-                if d.days < self.MIN_CHECK_DAYS:\n-                    log.msg(\"Not scraping %s (scraped %s ago)\" % (request.url, d), level=log.DEBUG)\n-                    return\n-            # put the version in the pending pages to avoid querying DB again\n-            record = (request, version, now)\n-            self.pending_requests[domain].push(record, request.priority)\n-\n-    def next_request(self, domain):\n-        \"\"\"Get the next page from the superclass. This will add a callback\n-        to prevent processing the page unless its content has been\n-        changed.\n-\n-        In the event that it a page is not processed, the record_visit method\n-        is called to update the last_checked time.\n-        \"\"\"\n-        pending_list = self.pending_requests.get(domain)\n-        if not pending_list :\n-            return None\n-        request, version, timestamp = pending_list.get_nowait()[1]\n-        post_version = hash(request.body)\n-\n-        def callback(pagedata):\n-            \"\"\"process other callback if we pass the checks\"\"\"\n-            \n-            if version == self.get_version(pagedata):\n-                hist = self.historydata.version_info(domain, version)\n-                if hist:\n-                    versionkey, created = hist\n-                    # if versionkey != urlkey(url) this means\n-                    # the same content is available on a different url\n-                    delta = timestamp - created\n-                    if delta.days < self.MIN_PROCESS_UNCHANGED_DAYS:\n-                        message = \"skipping %s: unchanged for %s\" % (pagedata.url, delta)\n-                        raise IgnoreRequest(message)\n-            self.record_visit(domain, request.url, pagedata.url,\n-                              pagedata.parent,  self.get(pagedata),\n-                              post_version)\n-            return pagedata\n-\n-        def errback(error) :\n-            self.record_visit(domain, request.url, request.url, None, None,\n-                              post_version)\n-            return error\n-\n-        d = defer.Deferred()\n-        d.addCallbacks(callback, errback)\n-        # prepend_callback Request method was removed (it never worked properly anyways) \n-        #request.prepend_callback(d)\n-\n-        return request\n-\n-    def get_version(self, response):\n-        key = hashlib.sha1(response.body).hexdigest()\n\n@@ -1,47 +0,0 @@\n-\"\"\"\n-SQLHistoryStore\n-\n-Persistent history storage using relational database\n-\"\"\"\n-from scrapy.contrib.history.history import URLHistory\n-from scrapy import log\n-from scrapy.conf import settings\n-\n-class SQLHistoryStore(object) :\n-    \"\"\"Implementation of a data store that stores information in a relation \n-    database\n-\n-    This maintains a URLHistory object per site. That means each domain\n-    has it's own session and is isolated from the others.\n-    \"\"\"\n-    \n-    def __init__(self):\n-        self._store = {}\n-        self._dbinfo = settings['SCRAPING_DB']\n-        self._debug = settings['DEBUG_SQL_HISTORY_STORE']\n-   \n-    def open(self, site):\n-        self._store[site] = URLHistory(self._dbinfo)\n-\n-    def close_site(self, site):\n-        self._store[site].close()\n-        del self._store[site]\n-    \n-    def store(self, site, key, url, parent=None, version=None, post_version=None):\n-        history = self._store[site]\n-        if self._debug:\n-            log.msg(\"record_version(key=%s, url=%s, parent=%s, version=%s, post_version=%s)\" % \n-                    (key, url, parent, version, post_version), domain=site, level=log.DEBUG)\n-        history.record_version(key, url, parent, version, post_version)\n-\n-    def has_site(self, site):\n-        return site in self._store\n-\n-    def status(self, site, key):\n-        if site in self._store:\n-            history = self._store[site]\n-            return history.get_url_status(key)\n-\n-    def version_info(self, site, version):\n-        history = self._store[site]\n-        return history.get_version_info(version)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#119bb11090d245488c99f44928074b8065aba525", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 18 | Churn Cumulative: 94 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,14 +5,18 @@ Helper functions for dealing with Twisted deferreds\n from twisted.internet import defer, reactor, task\n from twisted.python import failure\n \n-def defer_fail(_failure):\n-    \"\"\"same as twsited.internet.defer.fail, but delay calling errback \"\"\"\n+def defer_failed(_failure):\n+    \"\"\"Same as twisted.internet.defer.fail, but delay calling errback until\n+    next reactor loop\n+    \"\"\"\n     d = defer.Deferred()\n     reactor.callLater(0, d.errback, _failure)\n     return d\n \n def defer_succeed(result):\n-    \"\"\"same as twsited.internet.defer.succed, but delay calling callback\"\"\"\n+    \"\"\"Same as twsited.internet.defer.succed, but delay calling callback until\n+    next reactor loop\n+    \"\"\"\n     d = defer.Deferred()\n     reactor.callLater(0, d.callback, result)\n     return d\n@@ -26,7 +30,9 @@ def defer_result(result):\n         return defer_succeed(result)\n \n def mustbe_deferred(f, *args, **kw):\n-    \"\"\"same as twisted.internet.defer.maybeDeferred, but delay calling callback/errback\"\"\"\n+    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n+    callback/errback to next reactor loop\n+    \"\"\"\n     try:\n         result = f(*args, **kw)\n     except:\n@@ -34,9 +40,6 @@ def mustbe_deferred(f, *args, **kw):\n     else:\n         return defer_result(result)\n \n-def chain_deferred(d1, d2):\n-    return d1.chainDeferred(d2).addBoth(lambda _:d2)\n-\n def parallel(iterable, count, callable, *args, **named):\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n@@ -46,4 +49,3 @@ def parallel(iterable, count, callable, *args, **named):\n     coop = task.Cooperator()\n     work = (callable(elem, *args, **named) for elem in iterable)\n     return defer.DeferredList([coop.coiterate(work) for i in xrange(count)])\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eeac040fceb04cbde94ea9ca1585153da5be98b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 3 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1 +1,2 @@\n-\"\"\"This directory contains third party modules used by Scrapy\"\"\"\n+\"\"\"This package contains some third party modules that are distributed along\n+with Scrapy\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8fad0a57441a24fc3b672f909bd99570e8a91a7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 98 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,7 @@ def defer_result(result):\n     if isinstance(result, defer.Deferred):\n         return result\n     elif isinstance(result, failure.Failure):\n-        return defer_fail(result)\n+        return defer_failed(result)\n     else:\n         return defer_succeed(result)\n \n@@ -36,7 +36,7 @@ def mustbe_deferred(f, *args, **kw):\n     try:\n         result = f(*args, **kw)\n     except:\n-        return defer_fail(failure.Failure())\n+        return defer_failed(failure.Failure())\n     else:\n         return defer_result(result)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cbed0f1ea6bae2447081b900087d398b9783cc96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 38 | Files Changed: 7 | Hunks: 28 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 88 | Churn Cumulative: 1456 | Contributors (this commit): 5 | Commits (past 90d): 82 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,7 @@ from scrapy.http import Headers\n from scrapy.core.exceptions import NotSupported\n from scrapy.utils.defer import defer_succeed\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.signal import send_catch_log\n from scrapy.core.downloader.dnscache import DNSCache\n from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n@@ -55,8 +56,10 @@ def create_factory(request, spider):\n         headers = Headers(factory.response_headers)\n         respcls = responsetypes.from_args(headers=headers, url=url)\n         r = respcls(url=request.url, status=status, headers=headers, body=body)\n-        signals.send_catch_log(signal=signals.request_uploaded, sender='download_http', request=request, spider=spider)\n-        signals.send_catch_log(signal=signals.response_downloaded, sender='download_http', response=r, spider=spider)\n+        send_catch_log(signal=signals.request_uploaded, sender='download_http', \\\n+            request=request, spider=spider)\n+        send_catch_log(signal=signals.response_downloaded, sender='download_http', \\\n+            response=r, spider=spider)\n         return r\n \n     factory.deferred.addCallbacks(_create_response)\n\n@@ -7,6 +7,7 @@ docs/topics/downloader-middleware.rst\n \"\"\"\n \n from scrapy.core import signals\n+from scrapy.utils.signal import send_catch_log\n from scrapy import log\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import NotConfigured\n@@ -76,9 +77,11 @@ class DownloaderMiddlewareManager(object):\n                     'Middleware %s.process_response must return Response or Request, got %s' % \\\n                     (method.im_self.__class__.__name__, type(response))\n                 if isinstance(response, Request):\n-                    signals.send_catch_log(signal=signals.response_received, sender=self.__class__, response=response, spider=spider)\n+                    send_catch_log(signal=signals.response_received, \\\n+                        sender=self.__class__, response=response, spider=spider)\n                     return response\n-            signals.send_catch_log(signal=signals.response_received, sender=self.__class__, response=response, spider=spider)\n+            send_catch_log(signal=signals.response_received, sender=self.__class__, \\\n+                response=response, spider=spider)\n             return response\n \n         def process_exception(_failure):\n\n@@ -15,13 +15,13 @@ from scrapy import log\n from scrapy.stats import stats\n from scrapy.conf import settings\n from scrapy.core import signals\n-from scrapy.core.scheduler import Scheduler\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n from scrapy.core.exceptions import IgnoreRequest, DontCloseDomain\n from scrapy.http import Response, Request\n from scrapy.spider import spiders\n from scrapy.utils.misc import load_object\n+from scrapy.utils.signal import send_catch_log\n from scrapy.utils.defer import mustbe_deferred\n \n class ExecutionEngine(object):\n@@ -95,7 +95,7 @@ class ExecutionEngine(object):\n             self.control_reactor = control_reactor\n             reactor.callLater(0, self._mainloop)\n             self.start_time = datetime.utcnow()\n-            signals.send_catch_log(signal=signals.engine_started, sender=self.__class__)\n+            send_catch_log(signal=signals.engine_started, sender=self.__class__)\n             self.addtask(self._mainloop, 5.0)\n             for tsk, interval, now in self.tasks:\n                 tsk.start(interval, now)\n@@ -114,7 +114,7 @@ class ExecutionEngine(object):\n             self.running = False\n             for domain in self.open_domains:\n                 spider = spiders.fromdomain(domain)\n-                signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, \\\n+                send_catch_log(signal=signals.domain_closed, sender=self.__class__, \\\n                     domain=domain, spider=spider, reason='shutdown')\n                 stats.close_domain(domain, reason='shutdown')\n             for tsk, _, _ in self.tasks: # stop looping calls\n@@ -125,7 +125,7 @@ class ExecutionEngine(object):\n                 p.stopListening()\n             if self.control_reactor and reactor.running:\n                 reactor.stop()\n-            signals.send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n+            send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n \n     def pause(self):\n         \"\"\"Pause the execution engine\"\"\"\n@@ -280,26 +280,29 @@ class ExecutionEngine(object):\n         stats.open_domain(domain)\n \n         # XXX: sent for backwards compatibility (will be removed in Scrapy 0.8)\n-        signals.send_catch_log(signals.domain_open, sender=self.__class__, domain=domain, spider=spider)\n+        send_catch_log(signals.domain_open, sender=self.__class__, \\\n+            domain=domain, spider=spider)\n \n-        signals.send_catch_log(signals.domain_opened, sender=self.__class__, domain=domain, spider=spider)\n+        send_catch_log(signals.domain_opened, sender=self.__class__, \\\n+            domain=domain, spider=spider)\n \n     def _domain_idle(self, domain):\n-        \"\"\"Called when a domain gets idle. This function is called when there are no\n-        remaining pages to download or schedule. It can be called multiple\n-        times. If some extension raises a DontCloseDomain exception (in the\n-        domain_idle signal handler) the domain is not closed until the next\n-        loop and this function is guaranteed to be called (at least) once again\n-        for this domain.\n+        \"\"\"Called when a domain gets idle. This function is called when there\n+        are no remaining pages to download or schedule. It can be called\n+        multiple times. If some extension raises a DontCloseDomain exception\n+        (in the domain_idle signal handler) the domain is not closed until the\n+        next loop and this function is guaranteed to be called (at least) once\n+        again for this domain.\n         \"\"\"\n         spider = spiders.fromdomain(domain)\n         try:\n-            dispatcher.send(signal=signals.domain_idle, sender=self.__class__, domain=domain, spider=spider)\n+            dispatcher.send(signal=signals.domain_idle, sender=self.__class__, \\\n+                domain=domain, spider=spider)\n         except DontCloseDomain:\n             self.next_request(domain)\n             return\n         except:\n-            log.exc(\"Exception catched on domain_idle signal dispatch\")\n+            log.err(_why=\"Exception catched on domain_idle signal dispatch\")\n         if self.domain_is_idle(domain):\n             self.close_domain(domain, reason='finished')\n \n@@ -330,7 +333,7 @@ class ExecutionEngine(object):\n         self.scheduler.close_domain(domain)\n         self.scraper.close_domain(domain)\n         reason = self.closing.pop(domain, 'finished')\n-        signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, \\\n+        send_catch_log(signal=signals.domain_closed, sender=self.__class__, \\\n             domain=domain, spider=spider, reason=reason)\n         stats.close_domain(domain, reason=reason)\n         log.msg(\"Domain closed (%s)\" % reason, domain=domain) \n\n@@ -6,6 +6,7 @@ from twisted.internet import defer\n \n from scrapy.utils.defer import defer_result, defer_succeed, parallel\n from scrapy.utils.misc import arg_to_iter, load_object\n+from scrapy.utils.signal import send_catch_log\n from scrapy.core.exceptions import IgnoreRequest, DropItem\n from scrapy.core import signals\n from scrapy.http import Request, Response\n@@ -153,13 +154,13 @@ class Scraper(object):\n         if domain in self.engine.closing:\n             return\n         elif isinstance(output, Request):\n-            signals.send_catch_log(signal=signals.request_received, request=output, \\\n+            send_catch_log(signal=signals.request_received, request=output, \\\n                 spider=spider)\n             self.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n             log.msg(\"Scraped %s in <%s>\" % (output, request.url), level=log.DEBUG, \\\n                 domain=domain)\n-            signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, \\\n+            send_catch_log(signal=signals.item_scraped, sender=self.__class__, \\\n                 item=output, spider=spider, response=response)\n             self.sites[domain].itemproc_size += 1\n             stats.max_value('scraper/max_itemproc_size', \\\n@@ -196,13 +197,13 @@ class Scraper(object):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 log.msg(\"Dropped %s - %s\" % (item, str(ex)), level=log.WARNING, domain=domain)\n-                signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, \\\n+                send_catch_log(signal=signals.item_dropped, sender=self.__class__, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.msg('Error processing %s - %s' % (item, output), \\\n                     log.ERROR, domain=domain)\n         else:\n             log.msg(\"Passed %s\" % item, log.INFO, domain=domain)\n-            signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, \\\n+            send_catch_log(signal=signals.item_passed, sender=self.__class__, \\\n                 item=item, spider=spider, output=output)\n \n\n@@ -5,10 +5,6 @@ These signals are documented in docs/topics/signals.rst. Please don't add new\n signals here without documenting them there.\n \"\"\"\n \n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy import log\n-\n engine_started = object()\n engine_stopped = object()\n domain_opened = object()\n@@ -25,11 +21,3 @@ item_dropped = object()\n # XXX: deprecated signals (will be removed in Scrapy 0.8)\n domain_open = object()\n \n-def send_catch_log(signal, sender=None, **kwargs):\n-    \"\"\"\n-    Send a signal and log any exceptions raised by its listeners\n-    \"\"\"\n-    try:\n-        dispatcher.send(signal=signal, sender=sender, **kwargs)\n-    except:\n-        log.exc(\"Exception catched on signal dispatch\")\n\n@@ -7,6 +7,7 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.stats.signals import stats_domain_opened, stats_domain_closing, \\\n     stats_domain_closed\n+from scrapy.utils.signal import send_catch_log\n from scrapy.core import signals\n from scrapy import log\n from scrapy.conf import settings\n@@ -49,14 +50,14 @@ class StatsCollector(object):\n \n     def open_domain(self, domain):\n         self._stats[domain] = {}\n-        signals.send_catch_log(stats_domain_opened, domain=domain)\n+        send_catch_log(stats_domain_opened, domain=domain)\n \n     def close_domain(self, domain, reason):\n         if self._dump:\n             log.msg(\"Dumping stats:\\n\" + pprint.pformat(self.get_stats(domain)), \\\n                 domain=domain)\n         stats = self._stats.pop(domain)\n-        signals.send_catch_log(stats_domain_closed, domain=domain, reason=reason, \\\n+        send_catch_log(stats_domain_closed, domain=domain, reason=reason, \\\n             domain_stats=stats)\n \n     def engine_stopped(self):\n@@ -64,7 +65,7 @@ class StatsCollector(object):\n             log.msg(\"Dumping global stats:\\n\" + pprint.pformat(self.get_stats()))\n \n     def _start_closing_domain(self, domain, reason):\n-        signals.send_catch_log(stats_domain_closing, domain=domain, reason=reason)\n+        send_catch_log(stats_domain_closing, domain=domain, reason=reason)\n         self.close_domain(domain, reason)\n \n class MemoryStatsCollector(StatsCollector):\n\n@@ -0,0 +1,13 @@\n+\"\"\"Helper functinos for working with signals\"\"\"\n+\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy import log\n+\n+def send_catch_log(*args, **kwargs):\n+    \"\"\"Same as dispatcher.send but logs any exceptions raised by the signal\n+    handlers\n+    \"\"\"\n+    try:\n+        dispatcher.send(*args, **kwargs)\n+    except:\n+        log.exc(\"Exception catched on signal dispatch\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a767b7044e8a2b1311ec624098be764fd586b627", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 62 | Files Changed: 4 | Hunks: 11 | Methods Changed: 10 | Complexity Δ (Sum/Max): -1/6 | Churn Δ: 98 | Churn Cumulative: 432 | Contributors (this commit): 3 | Commits (past 90d): 24 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,52 +0,0 @@\n-\"\"\"\n-Dns cache module. \n-This module implements a dns cache to improve the performance of the \n-crawler, reducing the dns lookups.\n-\"\"\"\n-import socket\n-from scrapy.core import signals\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-class DNSCache(object):\n-    \"\"\"\n-    DNSCache.\n-\n-    Impelements a DNS Cache to improve the performance of the \n-    DNS lookup that the crawler request when it's running.\n-    Use:\n-     Create a single instance:\n-       >>> import dnscache\n-       >>> cachedns = dnscache.DNSCache()\n-     \n-     To get (and set a new host if not exists) we only call \n-       >> ip = cachedns.get('python.org')\n-       >>> print ip\n-       '82.94.164.162'\n-       >>> \n-    \"\"\"\n-    def __init__(self):\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n-        self._cache = {}\n-    \n-    def get(self, host):\n-        \"\"\"\n-        Returns the ip associated with the host and save it in\n-        the cache.\n-        \"\"\"\n-        try:\n-            ips = self._cache[host]\n-        except KeyError:\n-            # The socket.gethostbyname_ex throw an\n-            # exception when it can't get the host \n-            # ip. So we save the hostname anyway.\n-            # we use socket.getaddrinfo that support IP4/IP6\n-            try:\n-                ips = list(set([x[4][0] for x in socket.getaddrinfo(host,None)])) \n-            except socket.gaierror:\n-                ips = [host]\n-            self._cache[host] = ips\n-        return ips[0]\n-\n-    def domain_closed(self, domain, spider, reason):\n-        if domain in self._cache:\n-            del self._cache[domain]\n\n@@ -18,7 +18,6 @@ from scrapy.core.exceptions import NotSupported\n from scrapy.utils.defer import defer_succeed\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.signal import send_catch_log\n-from scrapy.core.downloader.dnscache import DNSCache\n from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n from scrapy.conf import settings\n@@ -27,9 +26,6 @@ from scrapy.conf import settings\n default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n ssl_supported = 'ssl' in optional_features\n \n-# Cache for dns lookups.\n-dnscache = DNSCache()\n-\n def download_any(request, spider):\n     scheme = urlparse_cached(request).scheme\n     if scheme == 'http':\n@@ -69,19 +65,17 @@ def download_http(request, spider):\n     \"\"\"Return a deferred for the HTTP download\"\"\"\n     factory = create_factory(request, spider)\n     url = urlparse_cached(request)\n-    ip = dnscache.get(url.hostname)\n     port = url.port\n-    reactor.connectTCP(ip, port or 80, factory)\n+    reactor.connectTCP(url.hostname, port or 80, factory)\n     return factory.deferred\n \n def download_https(request, spider):\n     \"\"\"Return a deferred for the HTTPS download\"\"\"\n     factory = create_factory(request, spider)\n     url = urlparse_cached(request)\n-    ip = dnscache.get(url.hostname)\n     port = url.port\n     contextFactory = ssl.ClientContextFactory()\n-    reactor.connectSSL(ip, port or 443, factory, contextFactory)\n+    reactor.connectSSL(url.hostname, port or 443, factory, contextFactory)\n     return factory.deferred\n \n def download_file(request, spider) :\n\n@@ -8,11 +8,12 @@ from twisted.internet import reactor, defer\n \n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.spider import spiders\n-from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n-from scrapy.core.downloader.handlers import download_any\n from scrapy.conf import settings\n from scrapy.utils.defer import mustbe_deferred\n from scrapy import log\n+from .middleware import DownloaderMiddlewareManager\n+from .handlers import download_any\n+from .resolver import CachingThreadedResolver\n \n \n class SiteInfo(object):\n@@ -54,6 +55,8 @@ class Downloader(object):\n         self.sites = {}\n         self.middleware = DownloaderMiddlewareManager()\n         self.concurrent_domains = settings.getint('CONCURRENT_DOMAINS')\n+        cached_resolver = CachingThreadedResolver(reactor)\n+        reactor.installResolver(cached_resolver)\n \n     def fetch(self, request, spider):\n         \"\"\" Main method to use to request a download\n\n@@ -0,0 +1,29 @@\n+\"\"\"DNS resolver with cache to use with Twisted reactors\"\"\"\n+\n+from twisted.internet import defer\n+from twisted.internet.base import ThreadedResolver\n+\n+from scrapy.core import signals\n+from scrapy.xlib.pydispatch import dispatcher\n+\n+class CachingThreadedResolver(ThreadedResolver):\n+\n+    def __init__(self, reactor):\n+        ThreadedResolver.__init__(self, reactor)\n+        self._cache = {}\n+        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+\n+    def getHostByName(self, name, timeout = (1, 3, 11, 45)):\n+        if name in self._cache:\n+            return defer.succeed(self._cache[name])\n+        dfd = ThreadedResolver.getHostByName(self, name, timeout)\n+        dfd.addCallback(self._cache_result, name)\n+        return dfd\n+\n+    def _cache_result(self, result, name):\n+        self._cache[name] = result\n+        return result\n+\n+    def domain_closed(self, spider):\n+        for domain in [spider.domain_name] + spider.extra_domain_names:\n+            self._cache.pop(domain, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#297a8ac76b94d73431ef4069fb86e87306fb2230", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 4 | Methods Changed: 11 | Complexity Δ (Sum/Max): 2/8 | Churn Δ: 77 | Churn Cumulative: 394 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,45 @@\n+from collections import defaultdict\n+\n+from twisted.internet import reactor, defer\n+from twisted.internet.base import ThreadedResolver\n+\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.core import signals\n+\n+\n+class CachingResolver(object):\n+    \"\"\"Scrapy extension to use a caching resolver, instead of default one\"\"\"\n+\n+    def __init__(self):\n+        self.spider_hostnames = defaultdict(set)\n+        self.resolver = _CachingThreadedResolver(reactor)\n+        reactor.installResolver(self.resolver)\n+        dispatcher.connect(self.request_received, signals.request_received)\n+        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+\n+    def request_received(self, request, spider):\n+        url_hostname = urlparse_cached(request).hostname\n+        self.spider_hostnames[spider.domain_name].add(url_hostname)\n+\n+    def domain_closed(self, spider):\n+        for hostname in self.spider_hostnames:\n+            self.resolver._cache.pop(hostname, None)\n+\n+\n+class _CachingThreadedResolver(ThreadedResolver):\n+\n+    def __init__(self, *args, **kwargs):\n+        ThreadedResolver.__init__(self, *args, **kwargs)\n+        self._cache = {}\n+\n+    def getHostByName(self, name, timeout = (1, 3, 11, 45)):\n+        if name in self._cache:\n+            return defer.succeed(self._cache[name])\n+        dfd = ThreadedResolver.getHostByName(self, name, timeout)\n+        dfd.addCallback(self._cache_result, name)\n+        return dfd\n+\n+    def _cache_result(self, result, name):\n+        self._cache[name] = result\n+        return result\n\n@@ -13,7 +13,6 @@ from scrapy.utils.defer import mustbe_deferred\n from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n from .handlers import download_any\n-from .resolver import CachingThreadedResolver\n \n \n class SiteInfo(object):\n@@ -55,8 +54,6 @@ class Downloader(object):\n         self.sites = {}\n         self.middleware = DownloaderMiddlewareManager()\n         self.concurrent_domains = settings.getint('CONCURRENT_DOMAINS')\n-        cached_resolver = CachingThreadedResolver(reactor)\n-        reactor.installResolver(cached_resolver)\n \n     def fetch(self, request, spider):\n         \"\"\" Main method to use to request a download\n\n@@ -1,29 +0,0 @@\n-\"\"\"DNS resolver with cache to use with Twisted reactors\"\"\"\n-\n-from twisted.internet import defer\n-from twisted.internet.base import ThreadedResolver\n-\n-from scrapy.core import signals\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-class CachingThreadedResolver(ThreadedResolver):\n-\n-    def __init__(self, reactor):\n-        ThreadedResolver.__init__(self, reactor)\n-        self._cache = {}\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n-\n-    def getHostByName(self, name, timeout = (1, 3, 11, 45)):\n-        if name in self._cache:\n-            return defer.succeed(self._cache[name])\n-        dfd = ThreadedResolver.getHostByName(self, name, timeout)\n-        dfd.addCallback(self._cache_result, name)\n-        return dfd\n-\n-    def _cache_result(self, result, name):\n-        self._cache[name] = result\n-        return result\n-\n-    def domain_closed(self, spider):\n-        for domain in [spider.domain_name] + spider.extra_domain_names:\n-            self._cache.pop(domain, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#41f1972e4754dfbcd1cad9762fbbff021c6c0382", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 5 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n  \n # Logging levels\n-SILENT, CRITICAL, ERROR, WARNING, INFO, DEBUG, TRACE = range(7)\n+SILENT, CRITICAL, ERROR, WARNING, INFO, DEBUG = range(6)\n level_names = {\n     0: \"SILENT\",\n     1: \"CRITICAL\",\n@@ -21,7 +21,6 @@ level_names = {\n     3: \"WARNING\",\n     4: \"INFO\",\n     5: \"DEBUG\",\n-    6: \"TRACE\",\n }\n \n BOT_NAME = settings['BOT_NAME']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7e1cba4a833d2d2592081c932ed279a28027bc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 207 | Contributors (this commit): 3 | Commits (past 90d): 17 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -136,6 +136,7 @@ class BaseImagesPipeline(MediaPipeline):\n             self.store_image(key, image, buf, info)\n             if first_buf is None:\n                 first_buf = buf\n+        first_buf.seek(0)\n         return md5sum(first_buf)\n \n     def get_images(self, response, request, info):\n\n@@ -3,7 +3,7 @@ import rfc822\n from scrapy.http import Request\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.contrib.pipeline.images import BaseImagesPipeline, md5sum\n+from scrapy.contrib.pipeline.images import BaseImagesPipeline\n from scrapy.conf import settings\n \n \n\n@@ -64,18 +64,17 @@ def extract_regex(regex, text, encoding):\n     else:\n         return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n \n-def md5sum(buffer):\n-    \"\"\"Calculate the md5 checksum of a file\n+def md5sum(file):\n+    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n+    whole content in memory.\n \n     >>> from StringIO import StringIO\n     >>> md5sum(StringIO('file content to hash'))\n     '784406af91dd5a54fbb9c84c2236595a'\n-\n     \"\"\"\n     m = hashlib.md5()\n-    buffer.seek(0)\n     while 1:\n-        d = buffer.read(8096)\n+        d = file.read(8096)\n         if not d:\n             break\n         m.update(d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bdc0ec68632565373bc8b16bf83a9ec1a4f8507a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 39 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -66,8 +66,8 @@ class BaseImagesPipeline(MediaPipeline):\n         referer = request.headers.get('Referer')\n \n         if response.status != 200:\n-            msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' \\\n-                    % (mtype, request, referer, errmsg)\n+            msg = 'Image (http-error): Error downloading %s from %s referred in <%s>' \\\n+                    % (mtype, request, referer)\n             log.msg(msg, level=log.WARNING, domain=info.domain)\n             raise ImageException(msg)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
