{"custom_id": "scrapy#683ef2a8d9666bbf20462a35d01d517ad1eca73e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 513 | Contributors (this commit): 15 | Commits (past 90d): 8 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -7,9 +7,9 @@ See documentation in topics/media-pipeline.rst\n import hashlib\n import os\n import os.path\n-import rfc822\n import time\n import logging\n+from email.utils import parsedate_tz, mktime_tz\n from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n import six\n@@ -91,8 +91,8 @@ class S3FilesStore(object):\n         def _onsuccess(boto_key):\n             checksum = boto_key.etag.strip('\"')\n             last_modified = boto_key.last_modified\n-            modified_tuple = rfc822.parsedate_tz(last_modified)\n-            modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n+            modified_tuple = parsedate_tz(last_modified)\n+            modified_stamp = int(mktime_tz(modified_tuple))\n             return {'checksum': checksum, 'last_modified': modified_stamp}\n \n         return self._get_boto_key(path).addCallback(_onsuccess)\n\n@@ -97,7 +97,7 @@ def iter_errback(iterable, errback, *a, **kw):\n     iterating it.\n     \"\"\"\n     it = iter(iterable)\n-    while 1:\n+    while True:\n         try:\n             yield next(it)\n         except StopIteration:\n\n@@ -106,7 +106,7 @@ def md5sum(file):\n     '784406af91dd5a54fbb9c84c2236595a'\n     \"\"\"\n     m = hashlib.md5()\n-    while 1:\n+    while True:\n         d = file.read(8096)\n         if not d:\n             break\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ab1648a363090d8fc22547cb3b763d79ed30c7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 9 | Churn Cumulative: 15 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from twisted.trial import unittest\n from scrapy.settings import Settings\n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n+import six\n \n class M1(object):\n \n@@ -65,12 +66,20 @@ class MiddlewareManagerTest(unittest.TestCase):\n \n     def test_methods(self):\n         mwman = TestMiddlewareManager(M1(), M2(), M3())\n+        if six.PY2:\n             self.assertEqual([x.im_class for x in mwman.methods['open_spider']],\n                 [M1, M2])\n             self.assertEqual([x.im_class for x in mwman.methods['close_spider']],\n                 [M2, M1])\n             self.assertEqual([x.im_class for x in mwman.methods['process']],\n                 [M1, M3])\n+        else:\n+            self.assertEqual([x.__self__.__class__ for x in mwman.methods['open_spider']],\n+                [M1, M2])\n+            self.assertEqual([x.__self__.__class__ for x in mwman.methods['close_spider']],\n+                [M2, M1])\n+            self.assertEqual([x.__self__.__class__ for x in mwman.methods['process']],\n+                [M1, M3])\n \n     def test_enabled(self):\n         m1, m2, m3 = M1(), M2(), M3()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec8afbc060fdd4b5159470c3964f4482ccf4dbe3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 55 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -192,7 +192,7 @@ def _create_item_with_files(*files):\n def _prepare_request_object(item_url):\n     return Request(\n         item_url,\n-        meta={'response': Response(item_url, status=200, body='data')})\n+        meta={'response': Response(item_url, status=200, body=b'data')})\n \n \n if __name__ == \"__main__\":\n\n@@ -44,7 +44,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n     def test_default_media_downloaded(self):\n         request = Request('http://url')\n-        response = Response('http://url', body='')\n+        response = Response('http://url', body=b'')\n         assert self.pipe.media_downloaded(response, request, self.info) is response\n \n     def test_default_media_failed(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e044bfa60f072aca25e5561136dc0b9e89b2a1ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 58 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -92,9 +92,9 @@ class ResponseTypes(object):\n         chunk = body[:5000]\n         if isbinarytext(chunk):\n             return self.from_mimetype('application/octet-stream')\n-        elif \"<html>\" in chunk.lower():\n+        elif b\"<html>\" in chunk.lower():\n             return self.from_mimetype('text/html')\n-        elif \"<?xml\" in chunk.lower():\n+        elif b\"<?xml\" in chunk.lower():\n             return self.from_mimetype('text/xml')\n         else:\n             return self.from_mimetype('text')\n\n@@ -39,7 +39,7 @@ class DecompressionMiddlewareTest(TestCase):\n         assert_samelines(self, new.body, rsp.body)\n \n     def test_empty_response(self):\n-        rsp = Response(url='http://test.com', body='')\n+        rsp = Response(url='http://test.com', body=b'')\n         new = self.mw.process_response(None, rsp, self.spider)\n         assert new is rsp\n         assert not rsp.body\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d17d594712fdfd0da9e240c79901f7cb8d1441f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 52 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ import six\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n-from scrapy.utils.python import isbinarytext\n+from scrapy.utils.python import isbinarytext, to_bytes, to_native_str\n \n class ResponseTypes(object):\n \n@@ -59,7 +59,7 @@ class ResponseTypes(object):\n \n     def from_content_disposition(self, content_disposition):\n         try:\n-            filename = content_disposition.split(';')[1].split('=')[1]\n+            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36ae635fbe47d68025b475762cece2a6c9e8257f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 54 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,7 @@ class ResponseTypes(object):\n         header \"\"\"\n         if content_encoding:\n             return Response\n-        mimetype = content_type.split(';')[0].strip().lower()\n+        mimetype = to_native_str(content_type).split(';')[0].strip().lower()\n         return self.from_mimetype(mimetype)\n \n     def from_content_disposition(self, content_disposition):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42b8988eb2ca7bc31c0a3ab2346b584bed162e18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 55 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -90,6 +90,7 @@ class ResponseTypes(object):\n         it's not meant to be used except for special cases where response types\n         cannot be guess using more straightforward methods.\"\"\"\n         chunk = body[:5000]\n+        chunk = to_bytes(chunk)\n         if isbinarytext(chunk):\n             return self.from_mimetype('application/octet-stream')\n         elif b\"<html>\" in chunk.lower():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c12a3f23adeea263da870010d6e5ec207e98c8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 90 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,6 @@ from os import path\n # is relative to the documentation root, use os.path.abspath to make it\n # absolute, like shown here.\n sys.path.append(path.join(path.dirname(__file__), \"_ext\"))\n-sys.path.append(path.join(path.dirname(path.dirname(__file__)), \"scrapy\"))\n sys.path.insert(0, path.dirname(path.dirname(__file__)))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#06b91da943c1355ddc8a1018b67ebd5f35bc833e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 9 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -43,10 +43,10 @@ class ResponseTypesTest(unittest.TestCase):\n \n     def test_from_body(self):\n         mappings = [\n-            ('\\x03\\x02\\xdf\\xdd\\x23', Response),\n-            ('Some plain text\\ndata with tabs\\t and null bytes\\0', TextResponse),\n-            ('<html><head><title>Hello</title></head>', HtmlResponse),\n-            ('<?xml version=\"1.0\" encoding=\"utf-8\"', XmlResponse),\n+            (b'\\x03\\x02\\xdf\\xdd\\x23', Response),\n+            (b'Some plain text\\ndata with tabs\\t and null bytes\\0', TextResponse),\n+            (b'<html><head><title>Hello</title></head>', HtmlResponse),\n+            (b'<?xml version=\"1.0\" encoding=\"utf-8\"', XmlResponse),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_body(source)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de6e013b9a8080cf759096e793272f6814e3617d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 6 | Churn Cumulative: 169 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -551,7 +551,7 @@ class FormRequestTest(RequestTest):\n         r1 = self.request_class.from_response(response, formid=\"form2\")\n         self.assertEqual(r1.method, 'POST')\n         fs = _qs(r1)\n-        self.assertEqual(fs, {'four': ['4'], 'three': ['3']})\n+        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})\n \n     def test_from_response_formname_notexists_fallback_formid(self):\n         response = _buildresponse(\n@@ -566,7 +566,7 @@ class FormRequestTest(RequestTest):\n         r1 = self.request_class.from_response(response, formname=\"form3\", formid=\"form2\")\n         self.assertEqual(r1.method, 'POST')\n         fs = _qs(r1)\n-        self.assertEqual(fs, {'four': ['4'], 'three': ['3']})\n+        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})\n \n     def test_from_response_formid_notexist(self):\n         response = _buildresponse(\n@@ -579,7 +579,7 @@ class FormRequestTest(RequestTest):\n         r1 = self.request_class.from_response(response, formid=\"form3\")\n         self.assertEqual(r1.method, 'POST')\n         fs = _qs(r1)\n-        self.assertEqual(fs, {'one': ['1']})\n+        self.assertEqual(fs, {b'one': [b'1']})\n \n     def test_from_response_formid_errors_formnumber(self):\n         response = _buildresponse(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02b5182608bb09dbf14486c939d70b641ad50660", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 74 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,8 @@\n \"\"\"\n This module implements a class which returns the appropriate Response class\n based on different criteria.\n-\n \"\"\"\n-\n+from __future__ import absolute_import\n from mimetypes import MimeTypes\n from pkgutil import get_data\n from io import StringIO\n@@ -13,6 +12,7 @@ from scrapy.http import Response\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import isbinarytext, to_bytes, to_native_str\n \n+\n class ResponseTypes(object):\n \n     CLASSES = {\n@@ -69,11 +69,13 @@ class ResponseTypes(object):\n         \"\"\"Return the most appropriate Response class by looking at the HTTP\n         headers\"\"\"\n         cls = Response\n-        if 'Content-Type' in headers:\n-            cls = self.from_content_type(headers['Content-type'], \\\n-                headers.get('Content-Encoding'))\n-        if cls is Response and 'Content-Disposition' in headers:\n-            cls = self.from_content_disposition(headers['Content-Disposition'])\n+        if b'Content-Type' in headers:\n+            cls = self.from_content_type(\n+                content_type=headers[b'Content-type'],\n+                content_encoding=headers.get(b'Content-Encoding')\n+            )\n+        if cls is Response and b'Content-Disposition' in headers:\n+            cls = self.from_content_disposition(headers[b'Content-Disposition'])\n         return cls\n \n     def from_filename(self, filename):\n@@ -101,7 +103,8 @@ class ResponseTypes(object):\n             return self.from_mimetype('text')\n \n     def from_args(self, headers=None, url=None, filename=None, body=None):\n-        \"\"\"Guess the most appropriate Response class based on the given arguments\"\"\"\n+        \"\"\"Guess the most appropriate Response class based on\n+        the given arguments.\"\"\"\n         cls = Response\n         if headers is not None:\n             cls = self.from_headers(headers)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f927f68e1d3eae71550a48c5484099ca7dd9ce1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 28 | Files Changed: 2 | Hunks: 26 | Methods Changed: 9 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 76 | Churn Cumulative: 328 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,6 @@\n This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n-\n import os\n import re\n import weakref\n@@ -11,6 +10,7 @@ import tempfile\n \n from twisted.web import http\n from twisted.web.http import RESPONSES\n+from scrapy.utils.python import to_bytes\n from w3lib import html\n \n from scrapy.utils.decorators import deprecated\n@@ -27,10 +27,11 @@ def get_base_url(response):\n     \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n     if response not in _baseurl_cache:\n         text = response.body_as_unicode()[0:4096]\n-        _baseurl_cache[response] = html.get_base_url(text, response.url, \\\n+        _baseurl_cache[response] = html.get_base_url(text, response.url,\n             response.encoding)\n     return _baseurl_cache[response]\n \n+\n _noscript_re = re.compile(u'<noscript>.*?</noscript>', re.IGNORECASE | re.DOTALL)\n _script_re = re.compile(u'<script.*?>.*?</script>', re.IGNORECASE | re.DOTALL)\n _metaref_cache = weakref.WeakKeyDictionary()\n@@ -40,10 +41,11 @@ def get_meta_refresh(response):\n         text = response.body_as_unicode()[0:4096]\n         text = _noscript_re.sub(u'', text)\n         text = _script_re.sub(u'', text)\n-        _metaref_cache[response] = html.get_meta_refresh(text, response.url, \\\n+        _metaref_cache[response] = html.get_meta_refresh(text, response.url,\n             response.encoding)\n     return _metaref_cache[response]\n \n+\n def response_status_message(status):\n     \"\"\"Return status code plus status text descriptive message\n \n@@ -55,19 +57,21 @@ def response_status_message(status):\n     \"\"\"\n     return '%s %s' % (status, http.responses.get(int(status)))\n \n+\n def response_httprepr(response):\n-    \"\"\"Return raw HTTP representation (as string) of the given response. This\n+    \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n     is provided only for reference, since it's not the exact stream of bytes\n     that was received (that's not exposed by Twisted).\n     \"\"\"\n-\n-    s = \"HTTP/1.1 %d %s\\r\\n\" % (response.status, RESPONSES.get(response.status, ''))\n+    s = b\"HTTP/1.1 \" + to_bytes(str(response.status)) + b\" \" + \\\n+        to_bytes(RESPONSES.get(response.status, b'')) + b\"\\r\\n\"\n     if response.headers:\n-        s += response.headers.to_string() + \"\\r\\n\"\n-    s += \"\\r\\n\"\n+        s += response.headers.to_string() + b\"\\r\\n\"\n+    s += b\"\\r\\n\"\n     s += response.body\n     return s\n \n+\n def open_in_browser(response, _openfunc=webbrowser.open):\n     \"\"\"Open the given response in a local web browser, populating the <base>\n     tag for external links to work\n@@ -76,13 +80,14 @@ def open_in_browser(response, _openfunc=webbrowser.open):\n     # XXX: this implementation is a bit dirty and could be improved\n     body = response.body\n     if isinstance(response, HtmlResponse):\n-        if '<base' not in body:\n-            body = body.replace('<head>', '<head><base href=\"%s\">' % response.url)\n+        if b'<base' not in body:\n+            repl = '<head><base href=\"%s\">' % response.url\n+            body = body.replace(b'<head>', to_bytes(repl))\n         ext = '.html'\n     elif isinstance(response, TextResponse):\n         ext = '.txt'\n     else:\n-        raise TypeError(\"Unsupported response type: %s\" % \\\n+        raise TypeError(\"Unsupported response type: %s\" %\n                         response.__class__.__name__)\n     fd, fname = tempfile.mkstemp(ext)\n     os.write(fd, body)\n\n@@ -3,53 +3,59 @@ import unittest\n from six.moves.urllib.parse import urlparse\n \n from scrapy.http import Response, TextResponse, HtmlResponse\n-from scrapy.utils.response import response_httprepr, open_in_browser, get_meta_refresh\n+from scrapy.utils.python import to_bytes\n+from scrapy.utils.response import (response_httprepr, open_in_browser,\n+                                   get_meta_refresh, get_base_url)\n \n __doctests__ = ['scrapy.utils.response']\n \n+\n class ResponseUtilsTest(unittest.TestCase):\n-    dummy_response = TextResponse(url='http://example.org/', body='dummy_response')\n+    dummy_response = TextResponse(url='http://example.org/', body=b'dummy_response')\n \n     def test_response_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n-        self.assertEqual(response_httprepr(r1), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n+        self.assertEqual(response_httprepr(r1), b'HTTP/1.1 200 OK\\r\\n\\r\\n')\n \n-        r1 = Response(\"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n-        self.assertEqual(response_httprepr(r1), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+        r1 = Response(\"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=b\"Some body\")\n+        self.assertEqual(response_httprepr(r1), b'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n-        r1 = Response(\"http://www.example.com\", status=6666, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n-        self.assertEqual(response_httprepr(r1), 'HTTP/1.1 6666 \\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+        r1 = Response(\"http://www.example.com\", status=6666, headers={\"Content-type\": \"text/html\"}, body=b\"Some body\")\n+        self.assertEqual(response_httprepr(r1), b'HTTP/1.1 6666 \\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n     def test_open_in_browser(self):\n         url = \"http:///www.example.com/some/page.html\"\n-        body = \"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n+        body = b\"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n+\n         def browser_open(burl):\n             path = urlparse(burl).path\n             if not os.path.exists(path):\n                 path = burl.replace('file://', '')\n-            bbody = open(path).read()\n-            assert '<base href=\"%s\">' % url in bbody, \"<base> tag not added\"\n+            with open(path, \"rb\") as f:\n+                bbody = f.read()\n+            self.assertIn(b'<base href=\"' + to_bytes(url) + b'\">', bbody)\n             return True\n         response = HtmlResponse(url, body=body)\n         assert open_in_browser(response, _openfunc=browser_open), \\\n             \"Browser not called\"\n-        self.assertRaises(TypeError, open_in_browser, Response(url, body=body), \\\n-            debug=True)\n+\n+        resp = Response(url, body=body)\n+        self.assertRaises(TypeError, open_in_browser, resp, debug=True)\n \n     def test_get_meta_refresh(self):\n-        r1 = HtmlResponse(\"http://www.example.com\", body=\"\"\"\n+        r1 = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n         <html>\n         <head><title>Dummy</title><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n         <body>blahablsdfsal&amp;</body>\n         </html>\"\"\")\n-        r2 = HtmlResponse(\"http://www.example.com\", body=\"\"\"\n+        r2 = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n         <html>\n         <head><title>Dummy</title><noScript>\n         <meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n         </noSCRIPT>\n         <body>blahablsdfsal&amp;</body>\n         </html>\"\"\")\n-        r3 = HtmlResponse(\"http://www.example.com\", body=\"\"\"\n+        r3 = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n     <noscript><meta http-equiv=\"REFRESH\" content=\"0;url=http://www.example.com/newpage</noscript>\n     <script type=\"text/javascript\">\n     if(!checkCookies()){\n@@ -61,5 +67,14 @@ class ResponseUtilsTest(unittest.TestCase):\n         self.assertEqual(get_meta_refresh(r2), (None, None))\n         self.assertEqual(get_meta_refresh(r3), (None, None))\n \n-if __name__ == \"__main__\":\n-    unittest.main()\n+    def test_get_base_url(self):\n+        resp = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n+        <html>\n+        <head><base href=\"http://www.example.com/img/\" target=\"_blank\"></head>\n+        <body>blahablsdfsal&amp;</body>\n+        </html>\"\"\")\n+        self.assertEqual(get_base_url(resp), \"http://www.example.com/img/\")\n+\n+        resp2 = HtmlResponse(\"http://www.example.com\", body=b\"\"\"\n+        <html><body>blahablsdfsal&amp;</body></html>\"\"\")\n+        self.assertEqual(get_base_url(resp2), \"http://www.example.com\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b01292df02a379a5d587ddfd4149914f83a20204", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 116 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -71,7 +71,7 @@ def request_authenticate(request, username, password):\n \n \n def request_httprepr(request):\n-    \"\"\"Return the raw HTTP representation (as string) of the given request.\n+    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n     This is provided only for reference since it's not the actual stream of\n     bytes that will be send when performing the request (that's controlled\n     by Twisted).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9316b2317bdf9b66ef90abccfcee16169bc537f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 125 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,7 @@ def _py_files(folder):\n \n \n collect_ignore = [\n+    # deprecated or moved modules\n     \"scrapy/conf.py\",\n     \"scrapy/stats.py\",\n     \"scrapy/project.py\",\n@@ -19,6 +20,10 @@ collect_ignore = [\n     \"scrapy/command.py\",\n     \"scrapy/linkextractor.py\",\n     \"scrapy/spider.py\",\n+\n+    # not a test, but looks like a test\n+    \"scrapy/utils/testsite.py\",\n+\n ] + _py_files(\"scrapy/contrib\") + _py_files(\"scrapy/contrib_exp\")\n \n \n\n@@ -4,6 +4,7 @@ from six.moves.urllib.parse import urljoin\n from twisted.internet import reactor\n from twisted.web import server, resource, static, util\n \n+\n class SiteTest(object):\n \n     def setUp(self):\n@@ -18,6 +19,7 @@ class SiteTest(object):\n     def url(self, path):\n         return urljoin(self.baseurl, path)\n \n+\n def test_site():\n     r = resource.Resource()\n     r.putChild(\"text\", static.Data(\"Works\", \"text/plain\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b2fd9bdb63c45024872813587f61007c5ae06396", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 38 | Churn Cumulative: 583 | Contributors (this commit): 9 | Commits (past 90d): 13 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,13 +31,6 @@ del _monkeypatches\n optional_features = set()\n # TODO: backwards compatibility, remove for Scrapy 0.20\n optional_features.add('ssl')\n-try:\n-    import boto\n-    del boto\n-except ImportError:\n-    pass\n-else:\n-    optional_features.add('boto')\n \n from twisted import version as _txv\n twisted_version = (_txv.major, _txv.minor, _txv.micro)\n\n@@ -1,22 +1,23 @@\n from urlparse import unquote\n \n-from scrapy import optional_features\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n from .http import HTTPDownloadHandler\n \n+\n+def get_s3_connection():\n     try:\n         from boto.s3.connection import S3Connection\n     except ImportError:\n-    S3Connection = object\n+        return None\n \n     class _v19_S3Connection(S3Connection):\n-    \"\"\"A dummy S3Connection wrapper that doesn't do any syncronous download\"\"\"\n+        \"\"\"A dummy S3Connection wrapper that doesn't do any synchronous download\"\"\"\n         def _mexe(self, method, bucket, key, headers, *args, **kwargs):\n             return headers\n \n     class _v20_S3Connection(S3Connection):\n-    \"\"\"A dummy S3Connection wrapper that doesn't do any syncronous download\"\"\"\n+        \"\"\"A dummy S3Connection wrapper that doesn't do any synchronous download\"\"\"\n         def _mexe(self, http_request, *args, **kwargs):\n             http_request.authorize(connection=self)\n             return http_request.headers\n@@ -28,12 +29,16 @@ except ImportError:\n     else:\n         _S3Connection = _v20_S3Connection\n \n+    return _S3Connection\n+\n \n class S3DownloadHandler(object):\n \n     def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n             httpdownloadhandler=HTTPDownloadHandler):\n-        if 'boto' not in optional_features:\n+\n+        _S3Connection = get_s3_connection()\n+        if _S3Connection is None:\n             raise NotConfigured(\"missing boto library\")\n \n         if not aws_access_key_id:\n\n@@ -84,6 +84,11 @@ class S3FilesStore(object):\n     }\n \n     def __init__(self, uri):\n+        try:\n+            from boto.s3.connection import S3Connection\n+            self.S3Connection = S3Connection\n+        except ImportError:\n+            raise NotConfigured(\"missing boto library\")\n         assert uri.startswith('s3://')\n         self.bucket, self.prefix = uri[5:].split('/', 1)\n \n@@ -98,10 +103,9 @@ class S3FilesStore(object):\n         return self._get_boto_key(path).addCallback(_onsuccess)\n \n     def _get_boto_bucket(self):\n-        from boto.s3.connection import S3Connection\n         # disable ssl (is_secure=False) because of this python bug:\n         # http://bugs.python.org/issue5103\n-        c = S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)\n+        c = self.S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)\n         return c.get_bucket(self.bucket, validate=False)\n \n     def _get_boto_key(self, path):\n\n@@ -395,7 +395,13 @@ class HttpDownloadHandlerMock(object):\n         return request\n \n class S3TestCase(unittest.TestCase):\n-    skip = 'boto' not in optional_features and 'missing boto library'\n+    download_handler_cls = S3DownloadHandler\n+    try:\n+        # can't instance without settings, but ignore that\n+        download_handler_cls({})\n+    except NotConfigured:\n+        skip = 'missing boto library'\n+    except KeyError: pass\n \n     # test use same example keys than amazon developer guide\n     # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f02ef82e8560242eb34b336f385addfdef3211d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 25 | Files Changed: 3 | Hunks: 29 | Methods Changed: 18 | Complexity Δ (Sum/Max): 19/15 | Churn Δ: 71 | Churn Cumulative: 205 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,9 @@\n import time\n-from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE\n+from six.moves.http_cookiejar import (\n+    CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE\n+)\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.python import to_native_str\n \n \n class CookieJar(object):\n@@ -97,6 +100,7 @@ def potential_domain_matches(domain):\n         pass\n     return matches + ['.' + d for d in matches]\n \n+\n class _DummyLock(object):\n     def acquire(self):\n         pass\n@@ -133,6 +137,11 @@ class WrappedRequest(object):\n         \"\"\"\n         return self.request.meta.get('is_unverifiable', False)\n \n+    # python3 uses request.unverifiable\n+    @property\n+    def unverifiable(self):\n+        return self.is_unverifiable()\n+\n     def get_origin_req_host(self):\n         return urlparse_cached(self.request).hostname\n \n@@ -140,14 +149,16 @@ class WrappedRequest(object):\n         return name in self.request.headers\n \n     def get_header(self, name, default=None):\n-        return self.request.headers.get(name, default)\n+        return to_native_str(self.request.headers.get(name, default))\n \n     def header_items(self):\n-        return self.request.headers.items()\n+        return [\n+            (to_native_str(k), [to_native_str(x) for x in v])\n+            for k, v in self.request.headers.items()\n+        ]\n \n     def add_unredirected_header(self, name, value):\n         self.request.headers.appendlist(name, value)\n-        #print 'add_unredirected_header', self.request.headers\n \n \n class WrappedResponse(object):\n@@ -158,5 +169,8 @@ class WrappedResponse(object):\n     def info(self):\n         return self\n \n-    def getheaders(self, name):\n-        return self.response.headers.getlist(name)\n+    # python3 cookiejars calls get_all\n+    def get_all(self, name, default=None):\n+        return [to_native_str(v) for v in self.response.headers.getlist(name)]\n+    # python2 cookiejars calls getheaders\n+    getheaders = get_all\n\n@@ -9,7 +9,7 @@ from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n class CookiesMiddlewareTest(TestCase):\n \n     def assertCookieValEqual(self, first, second, msg=None):\n-        cookievaleq = lambda cv: re.split(';\\s*', cv)\n+        cookievaleq = lambda cv: re.split(';\\s*', cv.decode('latin1'))\n         return self.assertEqual(\n             sorted(cookievaleq(first)),\n             sorted(cookievaleq(second)), msg)\n@@ -34,7 +34,7 @@ class CookiesMiddlewareTest(TestCase):\n \n         req2 = Request('http://scrapytest.org/sub1/')\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertEquals(req2.headers.get('Cookie'), \"C1=value1\")\n+        self.assertEquals(req2.headers.get('Cookie'), b\"C1=value1\")\n \n     def test_dont_merge_cookies(self):\n         # merge some cookies into jar\n@@ -55,12 +55,12 @@ class CookiesMiddlewareTest(TestCase):\n         # check that cookies are merged back\n         req = Request('http://scrapytest.org/mergeme')\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers.get('Cookie'), 'C1=value1')\n+        self.assertEquals(req.headers.get('Cookie'), b'C1=value1')\n \n         # check that cookies are merged when dont_merge_cookies is passed as 0\n         req = Request('http://scrapytest.org/mergeme', meta={'dont_merge_cookies': 0})\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers.get('Cookie'), 'C1=value1')\n+        self.assertEquals(req.headers.get('Cookie'), b'C1=value1')\n \n     def test_complex_cookies(self):\n         # merge some cookies into jar\n@@ -76,12 +76,12 @@ class CookiesMiddlewareTest(TestCase):\n         # embed C1 and C3 for scrapytest.org/foo\n         req = Request('http://scrapytest.org/foo')\n         self.mw.process_request(req, self.spider)\n-        assert req.headers.get('Cookie') in ('C1=value1; C3=value3', 'C3=value3; C1=value1')\n+        assert req.headers.get('Cookie') in (b'C1=value1; C3=value3', b'C3=value3; C1=value1')\n \n         # embed C2 for scrapytest.org/bar\n         req = Request('http://scrapytest.org/bar')\n         self.mw.process_request(req, self.spider)\n-        self.assertEquals(req.headers.get('Cookie'), 'C2=value2')\n+        self.assertEquals(req.headers.get('Cookie'), b'C2=value2')\n \n         # embed nothing for scrapytest.org/baz\n         req = Request('http://scrapytest.org/baz')\n@@ -91,7 +91,7 @@ class CookiesMiddlewareTest(TestCase):\n     def test_merge_request_cookies(self):\n         req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'})\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers.get('Cookie'), 'galleta=salada')\n+        self.assertEquals(req.headers.get('Cookie'), b'galleta=salada')\n \n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n         res = Response('http://scrapytest.org/', headers=headers)\n@@ -100,12 +100,12 @@ class CookiesMiddlewareTest(TestCase):\n         req2 = Request('http://scrapytest.org/sub1/')\n         assert self.mw.process_request(req2, self.spider) is None\n \n-        self.assertCookieValEqual(req2.headers.get('Cookie'), \"C1=value1; galleta=salada\")\n+        self.assertCookieValEqual(req2.headers.get('Cookie'), b\"C1=value1; galleta=salada\")\n \n     def test_cookiejar_key(self):\n         req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'}, meta={'cookiejar': \"store1\"})\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers.get('Cookie'), 'galleta=salada')\n+        self.assertEquals(req.headers.get('Cookie'), b'galleta=salada')\n \n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n         res = Response('http://scrapytest.org/', headers=headers, request=req)\n@@ -113,11 +113,11 @@ class CookiesMiddlewareTest(TestCase):\n \n         req2 = Request('http://scrapytest.org/', meta=res.meta)\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertCookieValEqual(req2.headers.get('Cookie'),'C1=value1; galleta=salada')\n+        self.assertCookieValEqual(req2.headers.get('Cookie'), b'C1=value1; galleta=salada')\n \n         req3 = Request('http://scrapytest.org/', cookies={'galleta': 'dulce'}, meta={'cookiejar': \"store2\"})\n         assert self.mw.process_request(req3, self.spider) is None\n-        self.assertEquals(req3.headers.get('Cookie'), 'galleta=dulce')\n+        self.assertEquals(req3.headers.get('Cookie'), b'galleta=dulce')\n \n         headers = {'Set-Cookie': 'C2=value2; path=/'}\n         res2 = Response('http://scrapytest.org/', headers=headers, request=req3)\n@@ -125,7 +125,7 @@ class CookiesMiddlewareTest(TestCase):\n \n         req4 = Request('http://scrapytest.org/', meta=res2.meta)\n         assert self.mw.process_request(req4, self.spider) is None\n-        self.assertCookieValEqual(req4.headers.get('Cookie'), 'C2=value2; galleta=dulce')\n+        self.assertCookieValEqual(req4.headers.get('Cookie'), b'C2=value2; galleta=dulce')\n \n         #cookies from hosts with port\n         req5_1 = Request('http://scrapytest.org:1104/')\n@@ -137,11 +137,11 @@ class CookiesMiddlewareTest(TestCase):\n \n         req5_2 = Request('http://scrapytest.org:1104/some-redirected-path')\n         assert self.mw.process_request(req5_2, self.spider) is None\n-        self.assertEquals(req5_2.headers.get('Cookie'), 'C1=value1')\n+        self.assertEquals(req5_2.headers.get('Cookie'), b'C1=value1')\n \n         req5_3 = Request('http://scrapytest.org/some-redirected-path')\n         assert self.mw.process_request(req5_3, self.spider) is None\n-        self.assertEquals(req5_3.headers.get('Cookie'), 'C1=value1')\n+        self.assertEquals(req5_3.headers.get('Cookie'), b'C1=value1')\n \n         #skip cookie retrieval for not http request\n         req6 = Request('file:///scrapy/sometempfile')\n@@ -152,5 +152,4 @@ class CookiesMiddlewareTest(TestCase):\n         request = Request(\"http://example-host/\", cookies={'currencyCookie': 'USD'})\n         assert self.mw.process_request(request, self.spider) is None\n         self.assertIn('Cookie', request.headers)\n-        self.assertIn('currencyCookie', request.headers['Cookie'])\n-\n+        self.assertEqual(b'currencyCookie=USD', request.headers['Cookie'])\n\n@@ -8,7 +8,7 @@ from scrapy.http.cookies import WrappedRequest, WrappedResponse\n class WrappedRequestTest(TestCase):\n \n     def setUp(self):\n-        self.request = Request(\"http://www.example.com/page.html\", \\\n+        self.request = Request(\"http://www.example.com/page.html\",\n                                headers={\"Content-Type\": \"text/html\"})\n         self.wrapped = WrappedRequest(self.request)\n \n@@ -23,10 +23,12 @@ class WrappedRequestTest(TestCase):\n \n     def test_is_unverifiable(self):\n         self.assertFalse(self.wrapped.is_unverifiable())\n+        self.assertFalse(self.wrapped.unverifiable)\n \n     def test_is_unverifiable2(self):\n         self.request.meta['is_unverifiable'] = True\n         self.assertTrue(self.wrapped.is_unverifiable())\n+        self.assertTrue(self.wrapped.unverifiable)\n \n     def test_get_origin_req_host(self):\n         self.assertEqual(self.wrapped.get_origin_req_host(), 'www.example.com')\n@@ -40,11 +42,13 @@ class WrappedRequestTest(TestCase):\n         self.assertEqual(self.wrapped.get_header('xxxxx', 'def'), 'def')\n \n     def test_header_items(self):\n-        self.assertEqual(self.wrapped.header_items(), [('Content-Type', ['text/html'])])\n+        self.assertEqual(self.wrapped.header_items(),\n+                         [('Content-Type', ['text/html'])])\n \n     def test_add_unredirected_header(self):\n         self.wrapped.add_unredirected_header('hello', 'world')\n-        self.assertEqual(self.request.headers['hello'], 'world')\n+        self.assertEqual(self.request.headers['hello'], b'world')\n+\n \n class WrappedResponseTest(TestCase):\n \n@@ -58,3 +62,7 @@ class WrappedResponseTest(TestCase):\n \n     def test_getheaders(self):\n         self.assertEqual(self.wrapped.getheaders('content-type'), ['text/html'])\n+\n+    def test_get_all(self):\n+        # get_all result must be native string\n+        self.assertEqual(self.wrapped.get_all('content-type'), ['text/html'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dba7e39f61cbe2c22d3c9064f32f6e36d74f14b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 218 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -149,11 +149,13 @@ class WrappedRequest(object):\n         return name in self.request.headers\n \n     def get_header(self, name, default=None):\n-        return to_native_str(self.request.headers.get(name, default))\n+        return to_native_str(self.request.headers.get(name, default),\n+                             errors='replace')\n \n     def header_items(self):\n         return [\n-            (to_native_str(k), [to_native_str(x) for x in v])\n+            (to_native_str(k, errors='replace'),\n+             [to_native_str(x, errors='replace') for x in v])\n             for k, v in self.request.headers.items()\n         ]\n \n@@ -171,6 +173,7 @@ class WrappedResponse(object):\n \n     # python3 cookiejars calls get_all\n     def get_all(self, name, default=None):\n-        return [to_native_str(v) for v in self.response.headers.getlist(name)]\n+        return [to_native_str(v, errors='replace')\n+                for v in self.response.headers.getlist(name)]\n     # python2 cookiejars calls getheaders\n     getheaders = get_all\n\n@@ -22,20 +22,32 @@ class CookiesMiddlewareTest(TestCase):\n         del self.mw\n \n     def test_basic(self):\n-        headers = {'Set-Cookie': 'C1=value1; path=/'}\n         req = Request('http://scrapytest.org/')\n         assert self.mw.process_request(req, self.spider) is None\n         assert 'Cookie' not in req.headers\n \n+        headers = {'Set-Cookie': 'C1=value1; path=/'}\n         res = Response('http://scrapytest.org/', headers=headers)\n         assert self.mw.process_response(req, res, self.spider) is res\n \n-        #assert res.cookies\n-\n         req2 = Request('http://scrapytest.org/sub1/')\n         assert self.mw.process_request(req2, self.spider) is None\n         self.assertEquals(req2.headers.get('Cookie'), b\"C1=value1\")\n \n+    def test_do_not_break_on_non_utf8_header(self):\n+        req = Request('http://scrapytest.org/')\n+        assert self.mw.process_request(req, self.spider) is None\n+        assert 'Cookie' not in req.headers\n+\n+        headers = {'Set-Cookie': b'C1=in\\xa3valid; path=/',\n+                   'Other': b'ignore\\xa3me'}\n+        res = Response('http://scrapytest.org/', headers=headers)\n+        assert self.mw.process_response(req, res, self.spider) is res\n+\n+        req2 = Request('http://scrapytest.org/sub1/')\n+        assert self.mw.process_request(req2, self.spider) is None\n+        self.assertIn('Cookie', req2.headers)\n+\n     def test_dont_merge_cookies(self):\n         # merge some cookies into jar\n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6adf648dcfe59b52c0e4663e701a232e78d7bf2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 30/25 | Churn Δ: 82 | Churn Cumulative: 365 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from collections import defaultdict\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n+from scrapy.utils.python import to_native_str\n \n logger = logging.getLogger(__name__)\n \n@@ -52,18 +53,20 @@ class CookiesMiddleware(object):\n \n     def _debug_cookie(self, request, spider):\n         if self.debug:\n-            cl = request.headers.getlist('Cookie')\n+            cl = [to_native_str(c, errors='replace')\n+                  for c in request.headers.getlist('Cookie')]\n             if cl:\n-                msg = \"Sending cookies to: %s\" % request + os.linesep\n-                msg += os.linesep.join(\"Cookie: %s\" % c for c in cl)\n+                cookies = \"\\n\".join(\"Cookie: {}\\n\".format(c) for c in cl)\n+                msg = \"Sending cookies to: {}\\n{}\".format(request, cookies)\n                 logger.debug(msg, extra={'spider': spider})\n \n     def _debug_set_cookie(self, response, spider):\n         if self.debug:\n-            cl = response.headers.getlist('Set-Cookie')\n+            cl = [to_native_str(c, errors='replace')\n+                  for c in response.headers.getlist('Set-Cookie')]\n             if cl:\n-                msg = \"Received cookies from: %s\" % response + os.linesep\n-                msg += os.linesep.join(\"Set-Cookie: %s\" % c for c in cl)\n+                cookies = \"\\n\".join(\"Set-Cookie: {}\\n\".format(c) for c in cl)\n+                msg = \"Received cookies from: {}\\n{}\".format(response, cookies)\n                 logger.debug(msg, extra={'spider': spider})\n \n     def _format_cookie(self, cookie):\n\n@@ -20,7 +20,6 @@ else:\n     from email import encoders as Encoders\n \n from twisted.internet import defer, reactor, ssl\n-from twisted.mail.smtp import ESMTPSenderFactory\n \n logger = logging.getLogger(__name__)\n \n@@ -102,6 +101,8 @@ class MailSender(object):\n                       'mailattachs': nattachs, 'mailerr': errstr})\n \n     def _sendmail(self, to_addrs, msg):\n+        # Import twisted.mail here because it is not available in python3\n+        from twisted.mail.smtp import ESMTPSenderFactory\n         msg = StringIO(msg)\n         d = defer.Deferred()\n         factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, \\\n\n@@ -1,8 +1,12 @@\n-from unittest import TestCase\n import re\n+import logging\n+from unittest import TestCase\n+from testfixtures import LogCapture\n \n from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n+from scrapy.utils.test import get_crawler\n+from scrapy.exceptions import NotConfigured\n from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n \n \n@@ -34,6 +38,64 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(req2, self.spider) is None\n         self.assertEquals(req2.headers.get('Cookie'), b\"C1=value1\")\n \n+    def test_setting_false_cookies_enabled(self):\n+        self.assertRaises(\n+            NotConfigured,\n+            CookiesMiddleware.from_crawler,\n+            get_crawler(settings_dict={'COOKIES_ENABLED': False})\n+        )\n+\n+    def test_setting_default_cookies_enabled(self):\n+        self.assertIsInstance(\n+            CookiesMiddleware.from_crawler(get_crawler()),\n+            CookiesMiddleware\n+        )\n+\n+    def test_setting_true_cookies_enabled(self):\n+        self.assertIsInstance(\n+            CookiesMiddleware.from_crawler(\n+                get_crawler(settings_dict={'COOKIES_ENABLED': True})\n+            ),\n+            CookiesMiddleware\n+        )\n+\n+    def test_setting_enabled_cookies_debug(self):\n+        crawler = get_crawler(settings_dict={'COOKIES_DEBUG': True})\n+        mw = CookiesMiddleware.from_crawler(crawler)\n+        with LogCapture('scrapy.downloadermiddlewares.cookies',\n+                        level=logging.DEBUG) as l:\n+            req = Request('http://scrapytest.org/')\n+            res = Response('http://scrapytest.org/',\n+                           headers={'Set-Cookie': 'C1=value1; path=/'})\n+            mw.process_response(req, res, crawler.spider)\n+            req2 = Request('http://scrapytest.org/sub1/')\n+            mw.process_request(req2, crawler.spider)\n+\n+            l.check(\n+                ('scrapy.downloadermiddlewares.cookies',\n+                 'DEBUG',\n+                 'Received cookies from: <200 http://scrapytest.org/>\\n'\n+                 'Set-Cookie: C1=value1; path=/\\n'),\n+                ('scrapy.downloadermiddlewares.cookies',\n+                 'DEBUG',\n+                 'Sending cookies to: <GET http://scrapytest.org/sub1/>\\n'\n+                 'Cookie: C1=value1\\n'),\n+            )\n+\n+    def test_setting_disabled_cookies_debug(self):\n+        crawler = get_crawler(settings_dict={'COOKIES_DEBUG': False})\n+        mw = CookiesMiddleware.from_crawler(crawler)\n+        with LogCapture('scrapy.downloadermiddlewares.cookies',\n+                        level=logging.DEBUG) as l:\n+            req = Request('http://scrapytest.org/')\n+            res = Response('http://scrapytest.org/',\n+                           headers={'Set-Cookie': 'C1=value1; path=/'})\n+            mw.process_response(req, res, crawler.spider)\n+            req2 = Request('http://scrapytest.org/sub1/')\n+            mw.process_request(req2, crawler.spider)\n+\n+            l.check()\n+\n     def test_do_not_break_on_non_utf8_header(self):\n         req = Request('http://scrapytest.org/')\n         assert self.mw.process_request(req, self.spider) is None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
