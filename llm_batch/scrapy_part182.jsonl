{"custom_id": "scrapy#6f50cf55a4ded1de1238a71b4dac7ebd85f07a75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 16 | Churn Cumulative: 77 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,16 +13,16 @@ def start_python_console(namespace=None, noipython=False, banner=''):\n                 raise ImportError()\n \n             try:\n-                try:\n-                    from IPython.terminal import embed\n+                from IPython.terminal.embed import InteractiveShellEmbed\n+                from IPython.terminal.ipapp import load_default_config\n             except ImportError:\n-                    from IPython.frontend.terminal import embed\n-                sh = embed.InteractiveShellEmbed(banner1=banner)\n-            except ImportError:\n-                from IPython.Shell import IPShellEmbed\n-                sh = IPShellEmbed(banner=banner)\n+                from IPython.frontend.terminal.embed import InteractiveShellEmbed\n+                from IPython.frontend.terminal.ipapp import load_default_config\n \n-            sh(global_ns={}, local_ns=namespace)\n+            config = load_default_config()\n+            shell = InteractiveShellEmbed(\n+                banner1=banner, user_ns=namespace, config=config)\n+            shell()\n         except ImportError:\n             import code\n             try: # readline module is only available on unix systems\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8360380db00b8b55ffe242bd4965f467530c113d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 114 | Files Changed: 1 | Hunks: 1 | Methods Changed: 13 | Complexity Δ (Sum/Max): -24/0 | Churn Δ: 114 | Churn Cumulative: 175 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,114 +0,0 @@\n-#!/usr/bin/env python\n-\"\"\"\n-Example script to control a Scrapy server using its JSON-RPC web service.\n-\n-It only provides a reduced functionality as its main purpose is to illustrate\n-how to write a web service client. Feel free to improve or write you own.\n-\n-Also, keep in mind that the JSON-RPC API is not stable. The recommended way for\n-controlling a Scrapy server is through the execution queue (see the \"queue\"\n-command).\n-\n-\"\"\"\n-\n-from __future__ import print_function\n-import sys, optparse, urllib, json\n-from six.moves.urllib.parse import urljoin\n-\n-from scrapy.utils.jsonrpc import jsonrpc_client_call, JsonRpcError\n-\n-def get_commands():\n-    return {\n-        'help': cmd_help,\n-        'stop': cmd_stop,\n-        'list-available': cmd_list_available,\n-        'list-running': cmd_list_running,\n-        'list-resources': cmd_list_resources,\n-        'get-global-stats': cmd_get_global_stats,\n-        'get-spider-stats': cmd_get_spider_stats,\n-    }\n-\n-def cmd_help(args, opts):\n-    \"\"\"help - list available commands\"\"\"\n-    print(\"Available commands:\")\n-    for _, func in sorted(get_commands().items()):\n-        print(\"  \", func.__doc__)\n-\n-def cmd_stop(args, opts):\n-    \"\"\"stop <spider> - stop a running spider\"\"\"\n-    jsonrpc_call(opts, 'crawler/engine', 'close_spider', args[0])\n-\n-def cmd_list_running(args, opts):\n-    \"\"\"list-running - list running spiders\"\"\"\n-    for x in json_get(opts, 'crawler/engine/open_spiders'):\n-        print(x)\n-\n-def cmd_list_available(args, opts):\n-    \"\"\"list-available - list name of available spiders\"\"\"\n-    for x in jsonrpc_call(opts, 'crawler/spiders', 'list'):\n-        print(x)\n-\n-def cmd_list_resources(args, opts):\n-    \"\"\"list-resources - list available web service resources\"\"\"\n-    for x in json_get(opts, '')['resources']:\n-        print(x)\n-\n-def cmd_get_spider_stats(args, opts):\n-    \"\"\"get-spider-stats <spider> - get stats of a running spider\"\"\"\n-    stats = jsonrpc_call(opts, 'stats', 'get_stats', args[0])\n-    for name, value in stats.items():\n-        print(\"%-40s %s\" % (name, value))\n-\n-def cmd_get_global_stats(args, opts):\n-    \"\"\"get-global-stats - get global stats\"\"\"\n-    stats = jsonrpc_call(opts, 'stats', 'get_stats')\n-    for name, value in stats.items():\n-        print(\"%-40s %s\" % (name, value))\n-\n-def get_wsurl(opts, path):\n-    return urljoin(\"http://%s:%s/\"% (opts.host, opts.port), path)\n-\n-def jsonrpc_call(opts, path, method, *args, **kwargs):\n-    url = get_wsurl(opts, path)\n-    return jsonrpc_client_call(url, method, *args, **kwargs)\n-\n-def json_get(opts, path):\n-    url = get_wsurl(opts, path)\n-    return json.loads(urllib.urlopen(url).read())\n-\n-def parse_opts():\n-    usage = \"%prog [options] <command> [arg] ...\"\n-    description = \"Scrapy web service control script. Use '%prog help' \" \\\n-        \"to see the list of available commands.\"\n-    op = optparse.OptionParser(usage=usage, description=description)\n-    op.add_option(\"-H\", dest=\"host\", default=\"localhost\", \\\n-        help=\"Scrapy host to connect to\")\n-    op.add_option(\"-P\", dest=\"port\", type=\"int\", default=6080, \\\n-        help=\"Scrapy port to connect to\")\n-    opts, args = op.parse_args()\n-    if not args:\n-        op.print_help()\n-        sys.exit(2)\n-    cmdname, cmdargs, opts = args[0], args[1:], opts\n-    commands = get_commands()\n-    if cmdname not in commands:\n-        sys.stderr.write(\"Unknown command: %s\\n\\n\" % cmdname)\n-        cmd_help(None, None)\n-        sys.exit(1)\n-    return commands[cmdname], cmdargs, opts\n-\n-def main():\n-    cmd, args, opts = parse_opts()\n-    try:\n-        cmd(args, opts)\n-    except IndexError:\n-        print(cmd.__doc__)\n-    except JsonRpcError as e:\n-        print(str(e))\n-        if e.data:\n-            print(\"Server Traceback below:\")\n-            print(e.data)\n-\n-\n-if __name__ == '__main__':\n-    main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2a540206a74af8d38a01aaa5a37adc1008cad6ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 28 | Churn Cumulative: 202 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,19 +20,27 @@ def xmliter(obj, nodename):\n     - a unicode string\n     - a string encoded as utf-8\n     \"\"\"\n-    HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename, re.S)\n+    DOCUMENT_HEADER_RE = re.compile(r'<\\?xml[^>]+>\\s*', re.S)\n     HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename, re.S)\n+    END_TAG_RE = re.compile(r'<\\s*/([^\\s>]+)\\s*>', re.S)\n+    NAMESPACE_RE = re.compile(r'((xmlns[:A-Za-z]*)=[^>\\s]+)', re.S)\n     text = _body_or_str(obj)\n \n-    header_start = re.search(HEADER_START_RE, text)\n-    header_start = header_start.group(1).strip() if header_start else ''\n-    header_end = re_rsearch(HEADER_END_RE, text)\n-    header_end = text[header_end[1]:].strip() if header_end else ''\n+    document_header = re.search(DOCUMENT_HEADER_RE, text)\n+    document_header = document_header.group().strip() if document_header else ''\n+    header_end_idx = re_rsearch(HEADER_END_RE, text)\n+    header_end = text[header_end_idx[1]:].strip() if header_end_idx else ''\n+    namespaces = {}\n+    if header_end:\n+        for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n+            tag = re.search(r'<\\s*%s.*?xmlns[:=][^>]*>' % tagname, text[:header_end_idx[1]], re.S)\n+            if tag:\n+                namespaces.update(reversed(x) for x in re.findall(NAMESPACE_RE, tag.group()))\n \n     r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n     for match in r.finditer(text):\n-        nodetext = header_start + match.group() + header_end\n-        yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]\n+        nodetext = document_header + match.group().replace(nodename, '%s %s' % (nodename, ' '.join(namespaces.values())), 1) + header_end\n+        yield Selector(text=nodetext, type='xml')\n \n \n def csviter(obj, delimiter=None, headers=None, encoding=None):\n\n@@ -61,7 +61,6 @@ class XmliterTestCase(unittest.TestCase):\n         \"\"\"\n         response = XmlResponse(url='http://mydummycompany.com', body=body)\n         my_iter = self.xmliter(response, 'item')\n-\n         node = next(my_iter)\n         node.register_namespace('g', 'http://base.google.com/ns/1.0')\n         self.assertEqual(node.xpath('title/text()').extract(), ['Item 1'])\n@@ -74,6 +73,11 @@ class XmliterTestCase(unittest.TestCase):\n         self.assertEqual(node.xpath('id/text()').extract(), [])\n         self.assertEqual(node.xpath('price/text()').extract(), [])\n \n+        my_iter = self.xmliter(response, 'g:image_link')\n+        node = next(my_iter)\n+        node.register_namespace('g', 'http://base.google.com/ns/1.0')\n+        self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])\n+\n     def test_xmliter_exception(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e4689556f01fd74502584c85cba7c72c2b57640d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 410 | Contributors (this commit): 12 | Commits (past 90d): 7 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -67,6 +67,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n         SGMLParser.reset(self)\n         self.links = []\n         self.base_url = None\n+        self.current_link = None\n \n     def unknown_starttag(self, tag, attrs):\n         if tag == 'base':\n\n@@ -284,6 +284,17 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://example.org/foo', text=u'>\\u4eac<\\u4e1c',\n                                fragment='', nofollow=False)])\n \n+    def test_area_tag_with_unicode_present(self):\n+        body = \"\"\"<html><body>\\xbe\\xa9<map><area href=\"http://example.org/foo\" /></map></body></html>\"\"\"\n+        response = HtmlResponse(\"http://example.org\", body=body, encoding='utf-8')\n+        lx = self.extractor_cls()\n+        lx.extract_links(response)\n+        lx.extract_links(response)\n+        lx.extract_links(response)\n+        self.assertEqual(lx.extract_links(response),\n+                         [Link(url='http://example.org/foo', text=u'',\n+                               fragment='', nofollow=False)])\n+\n     def test_encoded_url(self):\n         body = \"\"\"<html><body><div><a href=\"?page=2\">BinB</a></body></html>\"\"\"\n         response = HtmlResponse(\"http://known.fm/AC%2FDC/\", body=body, encoding='utf8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d92914d2978c0fc3acc27c832184eb98b5f6c3cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 74 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n references to live object instances.\n \n If you want live objects for a particular class to be tracked, you only have to\n-subclass form object_ref (instead of object).\n+subclass from object_ref (instead of object).\n \n About performance: This library has a minimal performance impact when enabled,\n and no performance penalty at all when disabled (as object_ref becomes just an\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4932ec43a76a209c6e5d32440e3bc5b41cfbfe0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 1486 | Contributors (this commit): 8 | Commits (past 90d): 24 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -96,7 +96,12 @@ class CrawlerRunner(object):\n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, six.string_types):\n             spidercls = self.spiders.load(spidercls)\n-        crawler = Crawler(spidercls, self.settings.frozencopy())\n+\n+        crawler_settings = self.settings.copy()\n+        spidercls.update_settings(crawler_settings)\n+        crawler_settings.freeze()\n+\n+        crawler = Crawler(spidercls, crawler_settings)\n         return crawler\n \n     def stop(self):\n\n@@ -15,6 +15,7 @@ SETTINGS_PRIORITIES = {\n     'default': 0,\n     'command': 10,\n     'project': 20,\n+    'spider': 30,\n     'cmdline': 40,\n }\n \n\n@@ -20,6 +20,7 @@ class Spider(object_ref):\n     \"\"\"\n \n     name = None\n+    custom_settings = {}\n \n     def __init__(self, name=None, **kwargs):\n         if name is not None:\n@@ -66,6 +67,10 @@ class Spider(object_ref):\n     def parse(self, response):\n         raise NotImplementedError\n \n+    @classmethod\n+    def update_settings(cls, settings):\n+        settings.setdict(cls.custom_settings, priority='spider')\n+\n     @classmethod\n     def handles_request(cls, request):\n         return url_is_from_spider(request.url, cls)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ef3972cfbe5badaff52b75883c1cb889dcd501c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 91 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ except ImportError:\n \n from scrapy import signals\n from scrapy.spider import Spider, BaseSpider\n+from scrapy.settings import Settings\n from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse\n from scrapy.contrib.spiders.init import InitSpider\n from scrapy.contrib.spiders import CrawlSpider, Rule, XMLFeedSpider, \\\n@@ -92,6 +93,16 @@ class SpiderTest(unittest.TestCase):\n                                        spider=spider, reason=None)\n         self.assertTrue(spider.closed_called)\n \n+    def test_update_settings(self):\n+        spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n+        project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n+        self.spider_class.custom_settings = spider_settings\n+        settings = Settings(project_settings, priority='project')\n+\n+        self.spider_class.update_settings(settings)\n+        self.assertEqual(settings.get('TEST1'), 'spider')\n+        self.assertEqual(settings.get('TEST2'), 'spider')\n+        self.assertEqual(settings.get('TEST3'), 'project')\n \n class InitSpiderTest(SpiderTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77bd26a66de7dd80152c89d0c09abf3e7b830612", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 122 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ class Spider(object_ref):\n     \"\"\"\n \n     name = None\n-    custom_settings = {}\n+    custom_settings = None\n \n     def __init__(self, name=None, **kwargs):\n         if name is not None:\n@@ -69,7 +69,7 @@ class Spider(object_ref):\n \n     @classmethod\n     def update_settings(cls, settings):\n-        settings.setdict(cls.custom_settings, priority='spider')\n+        settings.setdict(cls.custom_settings or {}, priority='spider')\n \n     @classmethod\n     def handles_request(cls, request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2592b39fde98ac8ba46c165deb4a1245954f3a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 27 | Churn Cumulative: 51 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,9 @@\n import warnings\n import unittest\n \n-from scrapy.crawler import Crawler\n+from twisted.internet import defer\n+\n+from scrapy.crawler import Crawler, CrawlerRunner\n from scrapy.settings import Settings\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n@@ -22,3 +24,26 @@ class CrawlerTestCase(unittest.TestCase):\n \n             self.crawler.spiders\n             self.assertEqual(len(w), 1, \"Warn deprecated access only once\")\n+\n+\n+class CrawlerRunnerTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.crawler_runner = CrawlerRunner(Settings())\n+\n+    @defer.inlineCallbacks\n+    def test_populate_spidercls_settings(self):\n+        spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n+        project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n+\n+        class CustomSettingsSpider(DefaultSpider):\n+            custom_settings = spider_settings\n+\n+        self.crawler_runner.settings.setdict(project_settings,\n+                                             priority='project')\n+\n+        yield self.crawler_runner.crawl(CustomSettingsSpider)\n+        crawler = self.crawler_runner.crawlers.pop()\n+        self.assertEqual(crawler.settings.get('TEST1'), 'spider')\n+        self.assertEqual(crawler.settings.get('TEST2'), 'spider')\n+        self.assertEqual(crawler.settings.get('TEST3'), 'project')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1dff1fbf75d8277c52cfdd003966fd7e48b972f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 300 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,7 @@ def load_object(path):\n     return obj\n \n \n-def walk_modules(path, load=False):\n+def walk_modules(path):\n     \"\"\"Loads a module and all its submodules from a the given module path and\n     returns them. If *any* module throws an exception while importing, that\n     exception is thrown back.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dfca7b3c8001d8489f8bc7c609b6da5fd072b87a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 103 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ class CookieJar(object):\n         if not IPV4_RE.search(req_host):\n             hosts = potential_domain_matches(req_host)\n             if req_host.find(\".\") == -1:\n-                hosts += req_host + \".local\"\n+                hosts += [req_host + \".local\"]\n         else:\n             hosts = [req_host]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d513b5a542f5581a60a10200e07b58ec294caea5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1027 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -115,6 +115,8 @@ class CrawlerProcess(CrawlerRunner):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n         self.stopping = False\n+        self.log_observer = log.start_from_settings(self.settings)\n+        log.scrapy_info(settings)\n \n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)\n@@ -132,8 +134,6 @@ class CrawlerProcess(CrawlerRunner):\n         reactor.callFromThread(self._stop_reactor)\n \n     def start(self, stop_after_crawl=True, start_reactor=True):\n-        self.log_observer = log.start_from_settings(self.settings)\n-        log.scrapy_info(self.settings)\n         if start_reactor:\n             self._start_reactor(stop_after_crawl)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#51532af69a551292663fa87279eea0317b4ea3a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 9 | Churn Cumulative: 1166 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -58,14 +58,13 @@ class Command(ScrapyCommand):\n         crawler.engine = crawler._create_engine()\n         crawler.engine.start()\n \n-        self.crawler_process.start(start_reactor=False)\n         self._start_crawler_thread()\n \n         shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n         shell.start(url=url)\n \n     def _start_crawler_thread(self):\n-        t = Thread(target=self.crawler_process._start_reactor,\n+        t = Thread(target=self.crawler_process.start,\n                    kwargs={'stop_after_crawl': False})\n         t.daemon = True\n         t.start()\n\n@@ -133,11 +133,7 @@ class CrawlerProcess(CrawlerRunner):\n         self._stop_logging()\n         reactor.callFromThread(self._stop_reactor)\n \n-    def start(self, stop_after_crawl=True, start_reactor=True):\n-        if start_reactor:\n-            self._start_reactor(stop_after_crawl)\n-\n-    def _start_reactor(self, stop_after_crawl=True):\n+    def start(self, stop_after_crawl=True):\n         if stop_after_crawl:\n             d = defer.DeferredList(self.crawl_deferreds)\n             if d.called:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ddf0811a886bf6a604ae5cf946a7180632b51c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 47 | Churn Cumulative: 1213 | Contributors (this commit): 7 | Commits (past 90d): 15 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,7 +53,8 @@ class Command(ScrapyCommand):\n \n         # The crawler is created this way since the Shell manually handles the\n         # crawling engine, so the set up in the crawl method won't work\n-        crawler = self.crawler_process._create_logged_crawler(spidercls)\n+        crawler = self.crawler_process._create_crawler(spidercls)\n+        self.crawler_process._setup_crawler_logging(crawler)\n         # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n         crawler.engine.start()\n\n@@ -76,22 +76,21 @@ class CrawlerRunner(object):\n         smcls = load_object(settings['SPIDER_MANAGER_CLASS'])\n         self.spiders = smcls.from_settings(settings.frozencopy())\n         self.crawlers = set()\n-        self.crawl_deferreds = set()\n+        self._active = set()\n \n     def crawl(self, spidercls, *args, **kwargs):\n-        crawler = self._create_logged_crawler(spidercls)\n-        self.crawlers.add(crawler)\n-\n-        d = crawler.crawl(*args, **kwargs)\n-        self.crawl_deferreds.add(d)\n-        return d\n-\n-    def _create_logged_crawler(self, spidercls):\n         crawler = self._create_crawler(spidercls)\n-        log_observer = log.start_from_crawler(crawler)\n-        if log_observer:\n-            crawler.signals.connect(log_observer.stop, signals.engine_stopped)\n-        return crawler\n+        self._setup_crawler_logging(crawler)\n+        self.crawlers.add(crawler)\n+        d = crawler.crawl(*args, **kwargs)\n+        self._active.add(d)\n+\n+        def _done(result):\n+            self.crawlers.discard(crawler)\n+            self._active.discard(d)\n+            return result\n+\n+        return d.addBoth(_done)\n \n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, six.string_types):\n@@ -100,13 +99,22 @@ class CrawlerRunner(object):\n         crawler_settings = self.settings.copy()\n         spidercls.update_settings(crawler_settings)\n         crawler_settings.freeze()\n+        return Crawler(spidercls, crawler_settings)\n \n-        crawler = Crawler(spidercls, crawler_settings)\n-        return crawler\n+    def _setup_crawler_logging(self, crawler):\n+        log_observer = log.start_from_crawler(crawler)\n+        if log_observer:\n+            crawler.signals.connect(log_observer.stop, signals.engine_stopped)\n \n     def stop(self):\n         return defer.DeferredList(c.stop() for c in self.crawlers)\n \n+    @defer.inlineCallbacks\n+    def join(self):\n+        \"\"\"Wait for all managed crawlers to complete\"\"\"\n+        while self._active:\n+            yield defer.DeferredList(self._active)\n+\n \n class CrawlerProcess(CrawlerRunner):\n     \"\"\"A class to run multiple scrapy crawlers in a process simultaneously\"\"\"\n@@ -135,13 +143,15 @@ class CrawlerProcess(CrawlerRunner):\n \n     def start(self, stop_after_crawl=True):\n         if stop_after_crawl:\n-            d = defer.DeferredList(self.crawl_deferreds)\n-            if d.called:\n+            d = self.join()\n             # Don't start the reactor if the deferreds are already fired\n+            if d.called:\n                 return\n             d.addBoth(lambda _: self._stop_reactor())\n+\n         if self.settings.getbool('DNSCACHE_ENABLED'):\n             reactor.installResolver(CachingThreadedResolver(reactor))\n+\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99971dc8a883fd94909b7e1087ea3b2b450bd319", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 56 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,8 +42,9 @@ class CrawlerRunnerTest(unittest.TestCase):\n         self.crawler_runner.settings.setdict(project_settings,\n                                              priority='project')\n \n-        yield self.crawler_runner.crawl(CustomSettingsSpider)\n-        crawler = self.crawler_runner.crawlers.pop()\n+        d = self.crawler_runner.crawl(CustomSettingsSpider)\n+        crawler = list(self.crawler_runner.crawlers)[0]\n+        yield d\n         self.assertEqual(crawler.settings.get('TEST1'), 'spider')\n         self.assertEqual(crawler.settings.get('TEST2'), 'spider')\n         self.assertEqual(crawler.settings.get('TEST3'), 'project')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce180227fa8a056a4128fd1e381c9ae5b83b1be7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1079 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -107,7 +107,7 @@ class CrawlerRunner(object):\n             crawler.signals.connect(log_observer.stop, signals.engine_stopped)\n \n     def stop(self):\n-        return defer.DeferredList(c.stop() for c in self.crawlers)\n+        return defer.DeferredList([c.stop() for c in list(self.crawlers)])\n \n     @defer.inlineCallbacks\n     def join(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec93c0fdccb09c91f1eb560a538e00565cc0f2ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 59 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,9 @@ class CrawlerRunnerTest(unittest.TestCase):\n     def setUp(self):\n         self.crawler_runner = CrawlerRunner(Settings())\n \n+    def tearDown(self):\n+        return self.crawler_runner.stop()\n+\n     @defer.inlineCallbacks\n     def test_populate_spidercls_settings(self):\n         spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a823207f18a339ffbe66bee6d8a6ef87a447f1a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 1080 | Contributors (this commit): 6 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -156,6 +156,7 @@ class CrawlerProcess(CrawlerRunner):\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n     def _stop_logging(self):\n+        if self.log_observer:\n             self.log_observer.stop()\n \n     def _stop_reactor(self, _=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5bcabfe9c93b649c35fc4b9d1b0441db146bd406", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 182 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,7 +15,7 @@ from scrapy.utils.spider import iter_spider_classes\n class SpiderManager(object):\n \n     def __init__(self, settings):\n-        self.spider_modules = settings['SPIDER_MODULES']\n+        self.spider_modules = settings.getlist('SPIDER_MODULES')\n         self._spiders = {}\n         for name in self.spider_modules:\n             for module in walk_modules(name):\n\n@@ -63,6 +63,13 @@ class SpiderManagerTest(unittest.TestCase):\n         self.spiderman = SpiderManager.from_settings(settings)\n         assert len(self.spiderman._spiders) == 1\n \n+    def test_load_spider_module(self):\n+        prefix = 'tests.test_spidermanager.test_spiders.'\n+        module = ','.join(prefix + s for s in ('spider1', 'spider2'))\n+        settings = Settings({'SPIDER_MODULES': module})\n+        self.spiderman = SpiderManager.from_settings(settings)\n+        assert len(self.spiderman._spiders) == 2\n+\n     def test_load_base_spider(self):\n         module = 'tests.test_spidermanager.test_spiders.spider0'\n         settings = Settings({'SPIDER_MODULES': [module]})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#47b6dff9f1f47fe97de94e2419d8cff38c7bacb6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 43 | Churn Cumulative: 335 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -97,11 +97,12 @@ class CSVFeedSpider(Spider):\n     It receives a CSV file in a response; iterates through each of its rows,\n     and calls parse_row with a dict containing each field's data.\n \n-    You can set some options regarding the CSV file, such as the delimiter\n+    You can set some options regarding the CSV file, such as the delimiter, quotechar\n     and the file's headers.\n     \"\"\"\n \n     delimiter = None # When this is None, python's csv module's default delimiter is used\n+    quotechar = None # When this is None, python's csv module's default quotechar is used\n     headers = None\n \n     def process_results(self, response, results):\n@@ -123,7 +124,7 @@ class CSVFeedSpider(Spider):\n         process_results methods for pre and post-processing purposes.\n         \"\"\"\n \n-        for row in csviter(response, self.delimiter, self.headers):\n+        for row in csviter(response, self.delimiter, self.headers, self.quotechar):\n             ret = self.parse_row(response, row)\n             if isinstance(ret, (BaseItem, Request)):\n                 ret = [ret]\n\n@@ -35,7 +35,7 @@ def xmliter(obj, nodename):\n         yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]\n \n \n-def csviter(obj, delimiter=None, headers=None, encoding=None):\n+def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     \"\"\" Returns an iterator of dictionaries from the given csv object\n \n     obj can be:\n@@ -43,20 +43,24 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n     - a unicode string\n     - a string encoded as utf-8\n \n-    delimiter is the character used to separate field on the given obj.\n+    delimiter is the character used to separate fields on the given obj.\n \n     headers is an iterable that when provided offers the keys\n     for the returned dictionaries, if not the first row is used.\n+    \n+    quotechar is the character used to enclosure fields on the given obj.\n     \"\"\"\n+\n     encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n     def _getrow(csv_r):\n         return [str_to_unicode(field, encoding) for field in next(csv_r)]\n \n     lines = BytesIO(_body_or_str(obj, unicode=False))\n-    if delimiter:\n-        csv_r = csv.reader(lines, delimiter=delimiter)\n-    else:\n-        csv_r = csv.reader(lines)\n+\n+    kwargs = {}\n+    if delimiter: kwargs[\"delimiter\"] = delimiter\n+    if quotechar: kwargs[\"quotechar\"] = quotechar\n+    csv_r = csv.reader(lines, **kwargs)\n \n     if not headers:\n         headers = _getrow(csv_r)\n\n@@ -159,6 +159,28 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n+    def test_csviter_quotechar(self):\n+        body1 = get_testdata('feeds', 'feed-sample6.csv')\n+        body2 = get_testdata('feeds', 'feed-sample6.csv').replace(\",\", '|')\n+        \n+        response1 = TextResponse(url=\"http://example.com/\", body=body1)\n+        csv1 = csviter(response1, quotechar=\"'\")\n+\n+        self.assertEqual([row for row in csv1],\n+                         [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n+                          {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},\n+                          {u'id': u'4', u'name': u'empty',   u'value': u''}])\n+\n+        response2 = TextResponse(url=\"http://example.com/\", body=body2)\n+        csv2 = csviter(response2, delimiter=\"|\", quotechar=\"'\")\n+\n+        self.assertEqual([row for row in csv2],\n+                         [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n+                          {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},\n+                          {u'id': u'4', u'name': u'empty',   u'value': u''}])\n+\n     def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):\n         body = get_testdata('feeds', 'feed-sample3.csv').replace(',', '\\t')\n         response = Response(url=\"http://example.com/\", body=body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22da1783bdc7eac04345261b28a942419391eaab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 41 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -181,6 +181,17 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n+    def test_csviter_wrong_quotechar(self):\n+        body = get_testdata('feeds', 'feed-sample6.csv')\n+        response = TextResponse(url=\"http://example.com/\", body=body)\n+        csv = csviter(response)\n+\n+        self.assertEqual([row for row in csv],\n+                         [{u\"'id'\": u\"1\",   u\"'name'\": u\"'alpha'\",   u\"'value'\": u\"'foobar'\"},\n+                          {u\"'id'\": u\"2\",   u\"'name'\": u\"'unicode'\", u\"'value'\": u\"'\\xfan\\xedc\\xf3d\\xe9\\u203d'\"},\n+                          {u\"'id'\": u\"'3'\", u\"'name'\": u\"'multi'\",   u\"'value'\": u\"'foo\"},\n+                          {u\"'id'\": u\"4\",   u\"'name'\": u\"'empty'\",   u\"'value'\": u\"\"}])\n+\n     def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):\n         body = get_testdata('feeds', 'feed-sample3.csv').replace(',', '\\t')\n         response = Response(url=\"http://example.com/\", body=body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e583c030db440c3ac0e877804f7195aa53aa460f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 12 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,12 @@ from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware\n \n class CookiesMiddlewareTest(TestCase):\n \n+    def test_local_domain(self):\n+        request = Request(\"http://example-host/\", cookies={'currencyCookie': 'USD'})\n+        assert self.mw.process_request(request, self.spider) is None\n+        assert 'Cookie' in request.headers\n+        assert 'currencyCookie' in request.headers['Cookie']\n+\n     def assertCookieValEqual(self, first, second, msg=None):\n         cookievaleq = lambda cv: re.split(';\\s*', cv)\n         return self.assertEqual(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49645d4bf950cbd18f4a2c3839b93b3bf979e2f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 25 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -8,12 +8,6 @@ from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware\n \n class CookiesMiddlewareTest(TestCase):\n \n-    def test_local_domain(self):\n-        request = Request(\"http://example-host/\", cookies={'currencyCookie': 'USD'})\n-        assert self.mw.process_request(request, self.spider) is None\n-        assert 'Cookie' in request.headers\n-        assert 'currencyCookie' in request.headers['Cookie']\n-\n     def assertCookieValEqual(self, first, second, msg=None):\n         cookievaleq = lambda cv: re.split(';\\s*', cv)\n         return self.assertEqual(\n@@ -153,3 +147,10 @@ class CookiesMiddlewareTest(TestCase):\n         req6 = Request('file:///scrapy/sometempfile')\n         assert self.mw.process_request(req6, self.spider) is None\n         self.assertEquals(req6.headers.get('Cookie'), None)\n+\n+    def test_local_domain(self):\n+        request = Request(\"http://example-host/\", cookies={'currencyCookie': 'USD'})\n+        assert self.mw.process_request(request, self.spider) is None\n+        self.assertIn('Cookie', request.headers)\n+        self.assertIn('currencyCookie', request.headers['Cookie'])\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7be3479c204c2b110bc79cd8bdc9c7bdd1519163", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 105 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class CookieJar(object):\n \n         if not IPV4_RE.search(req_host):\n             hosts = potential_domain_matches(req_host)\n-            if req_host.find(\".\") == -1:\n+            if '.' not in req_host:\n                 hosts += [req_host + \".local\"]\n         else:\n             hosts = [req_host]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d11c8595e67680baad002bedf08dfd91f347cfb2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 8 | Churn Cumulative: 725 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -49,14 +49,6 @@ def _get_concurrency_delay(concurrency, spider, settings):\n     if hasattr(spider, 'download_delay'):\n         delay = spider.download_delay\n \n-    # TODO: remove for Scrapy 0.15\n-    c = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n-    if c:\n-        warnings.warn(\"CONCURRENT_REQUESTS_PER_SPIDER setting is deprecated, \"\n-                      \"use CONCURRENT_REQUESTS_PER_DOMAIN instead\", ScrapyDeprecationWarning)\n-        concurrency = c\n-    # ----------------------------\n-\n     if hasattr(spider, 'max_concurrent_requests'):\n         concurrency = spider.max_concurrent_requests\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe6f3efe95eb9f0a33154d239f9b1a6b1c02028d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 161 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,7 +22,6 @@ class RobotsTxtMiddleware(object):\n         self.crawler = crawler\n         self._useragent = crawler.settings.get('USER_AGENT')\n         self._parsers = {}\n-        self._spider_netlocs = set()\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -45,7 +44,6 @@ class RobotsTxtMiddleware(object):\n             robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)\n             dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n-            self._spider_netlocs.add(netloc)\n         return self._parsers[netloc]\n \n     def _parse_robots(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36eec8f4136a314d20c04cd8318a32e75c31a649", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 6 | Methods Changed: 12 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 71 | Churn Cumulative: 232 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,9 +28,10 @@ class RobotsTxtMiddleware(object):\n         return cls(crawler)\n \n     def process_request(self, request, spider):\n-        useragent = self._useragent\n+        if request.meta.get('dont_obey_robotstxt'):\n+            return\n         rp = self.robot_parser(request, spider)\n-        if rp and not rp.can_fetch(useragent, request.url):\n+        if rp and not rp.can_fetch(self._useragent, request.url):\n             log.msg(format=\"Forbidden by robots.txt: %(request)s\",\n                     level=log.DEBUG, request=request)\n             raise IgnoreRequest\n@@ -41,7 +42,11 @@ class RobotsTxtMiddleware(object):\n         if netloc not in self._parsers:\n             self._parsers[netloc] = None\n             robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n-            robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)\n+            robotsreq = Request(\n+                robotsurl,\n+                priority=self.DOWNLOAD_PRIORITY,\n+                meta={'dont_obey_robotstxt': True}\n+            )\n             dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n         return self._parsers[netloc]\n\n@@ -1,3 +1,4 @@\n+from __future__ import absolute_import\n import re\n import mock\n from twisted.internet import reactor\n@@ -11,7 +12,44 @@ from scrapy.settings import Settings\n \n class RobotsTxtMiddlewareTest(unittest.TestCase):\n \n-    def test(self):\n+    def test_robotstxt(self):\n+        middleware = self._get_middleware()\n+        # There is a bit of neglect in robotstxt.py: robots.txt is fetched asynchronously,\n+        # and it is actually fetched only *after* first process_request completes.\n+        # So, first process_request will always succeed.\n+        # We defer test() because otherwise robots.txt download mock will be called after assertRaises failure.\n+        self.assertNotIgnored(Request('http://site.local'), middleware)\n+        def test(r):\n+            self.assertNotIgnored(Request('http://site.local/allowed'), middleware)\n+            self.assertIgnored(Request('http://site.local/admin/main'), middleware)\n+            self.assertIgnored(Request('http://site.local/static/'), middleware)\n+        deferred = Deferred()\n+        deferred.addCallback(test)\n+        reactor.callFromThread(deferred.callback, None)\n+        return deferred\n+\n+    def test_robotstxt_meta(self):\n+        meta = {'dont_obey_robotstxt': True}\n+        middleware = self._get_middleware()\n+        self.assertNotIgnored(Request('http://site.local', meta=meta), middleware)\n+        def test(r):\n+            self.assertNotIgnored(Request('http://site.local/allowed', meta=meta), middleware)\n+            self.assertNotIgnored(Request('http://site.local/admin/main', meta=meta), middleware)\n+            self.assertNotIgnored(Request('http://site.local/static/', meta=meta), middleware)\n+        deferred = Deferred()\n+        deferred.addCallback(test)\n+        reactor.callFromThread(deferred.callback, None)\n+        return deferred\n+\n+    def assertNotIgnored(self, request, middleware):\n+        spider = None  # not actually used\n+        self.assertIsNone(middleware.process_request(request, spider))\n+\n+    def assertIgnored(self, request, middleware):\n+        spider = None  # not actually used\n+        self.assertRaises(IgnoreRequest, middleware.process_request, request, spider)\n+\n+    def _get_crawler(self):\n         crawler = mock.MagicMock()\n         crawler.settings = Settings()\n         crawler.settings.set('USER_AGENT', 'CustomAgent')\n@@ -29,18 +67,8 @@ class RobotsTxtMiddlewareTest(unittest.TestCase):\n             reactor.callFromThread(deferred.callback, response)\n             return deferred\n         crawler.engine.download.side_effect = return_response\n-        middleware = RobotsTxtMiddleware(crawler)\n-        spider = None  # not actually used\n-        # There is a bit of neglect in robotstxt.py: robots.txt is fetched asynchronously,\n-        # and it is actually fetched only *after* first process_request completes.\n-        # So, first process_request will always succeed.\n-        # We defer test() because otherwise robots.txt download mock will be called after assertRaises failure.\n-        self.assertIsNone(middleware.process_request(Request('http://site.local'), spider))  # not affected by robots.txt\n-        def test(r):\n-            self.assertIsNone(middleware.process_request(Request('http://site.local/allowed'), spider))\n-            self.assertRaises(IgnoreRequest, middleware.process_request, Request('http://site.local/admin/main'), spider)\n-            self.assertRaises(IgnoreRequest, middleware.process_request, Request('http://site.local/static/'), spider)\n-        deferred = Deferred()\n-        deferred.addCallback(test)\n-        reactor.callFromThread(deferred.callback, None)\n-        return deferred\n+        return crawler\n+\n+    def _get_middleware(self):\n+        crawler = self._get_crawler()\n+        return RobotsTxtMiddleware(crawler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
