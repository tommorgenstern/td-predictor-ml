{"custom_id": "keras#3379de9c6a5cdf99f15f576178fd74102549ebba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 968 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n import numpy as np\n+import pytest\n \n from keras import layers\n from keras import models\n@@ -95,6 +96,7 @@ class IoUTest(testing.TestCase):\n         expected_result = (1 / (1 + 1 - 1)) / 1\n         self.assertAllClose(result, expected_result, atol=1e-3)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_compilation(self):\n         m_obj = metrics.MeanIoU(num_classes=2)\n         model = models.Sequential(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6618f7a0e14ef37fc9741bc8c3db19619a4d231", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2745 | Contributors (this commit): 18 | Commits (past 90d): 7 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n-import multiprocessing\n import os\n import random\n import time\n import warnings\n+from multiprocessing.pool import ThreadPool\n \n import numpy as np\n \n@@ -574,7 +574,7 @@ def index_directory(\n \n     # Build an index of the files\n     # in the different class subfolders.\n-    pool = multiprocessing.pool.ThreadPool()\n+    pool = ThreadPool()\n     results = []\n     filenames = []\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06d31d9e06d74dcf932d830bbdf1e4bf1447bf87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 6 | Files Changed: 5 | Hunks: 19 | Methods Changed: 14 | Complexity Δ (Sum/Max): 15/7 | Churn Δ: 70 | Churn Cumulative: 540 | Contributors (this commit): 7 | Commits (past 90d): 27 | Contributors (cumulative): 19 | DMM Complexity: 0.6808510638297872\n\nDIFF:\n@@ -579,6 +579,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self._record_training_state_sharding_spec()\n \n         self.make_test_function()\n+        self.stop_evaluating = False\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n@@ -616,6 +617,8 @@ class JAXTrainer(base_trainer.Trainer):\n                 \"metrics_variables\": metrics_variables,\n             }\n             callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+            if self.stop_evaluating:\n+                break\n \n         # Reattach state back to model.\n         self.jax_state_sync()\n@@ -667,6 +670,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self._record_training_state_sharding_spec()\n \n         self.make_predict_function()\n+        self.stop_predicting = False\n         callbacks.on_predict_begin()\n \n         def append_to_outputs(batch_outputs, outputs):\n@@ -696,6 +700,8 @@ class JAXTrainer(base_trainer.Trainer):\n             batch_outputs, state = self.predict_function(state, x)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n+            if self.stop_predicting:\n+                break\n         callbacks.on_predict_end()\n         return tree.map_structure_up_to(batch_outputs, np.concatenate, outputs)\n \n\n@@ -195,6 +195,7 @@ class NumpyTrainer(base_trainer.Trainer):\n             return outputs\n \n         self.make_predict_function()\n+        self.stop_predicting = False\n         callbacks.on_predict_begin()\n         outputs = None\n         for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n@@ -202,6 +203,8 @@ class NumpyTrainer(base_trainer.Trainer):\n             batch_outputs = self.predict_function(data)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n+            if self.stop_predicting:\n+                break\n         callbacks.on_predict_end()\n         return tree.map_structure_up_to(batch_outputs, np.concatenate, outputs)\n \n@@ -257,6 +260,7 @@ class NumpyTrainer(base_trainer.Trainer):\n             )\n \n         self.make_test_function()\n+        self.stop_evaluating = False\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n@@ -264,6 +268,8 @@ class NumpyTrainer(base_trainer.Trainer):\n             callbacks.on_test_batch_begin(step)\n             logs = self.test_function(data)\n             callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+            if self.stop_evaluating:\n+                break\n         logs = self.get_metrics_result()\n         callbacks.on_test_end(logs)\n \n\n@@ -423,6 +423,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             )\n \n         self.make_test_function()\n+        self.stop_evaluating = False\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n@@ -431,6 +432,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 callbacks.on_test_batch_begin(step)\n                 logs = self.test_function(iterator)\n                 callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+                if self.stop_evaluating:\n+                    break\n         logs = self.get_metrics_result()\n         callbacks.on_test_end(logs)\n \n@@ -498,6 +501,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             return data\n \n         self.make_predict_function()\n+        self.stop_predicting = False\n         callbacks.on_predict_begin()\n         outputs = None\n         with epoch_iterator.catch_stop_iteration():\n@@ -507,6 +511,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 batch_outputs = self.predict_function(data)\n                 outputs = append_to_outputs(batch_outputs, outputs)\n                 callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n+                if self.stop_predicting:\n+                    break\n         callbacks.on_predict_end()\n         outputs = tree.map_structure_up_to(\n             batch_outputs, potentially_ragged_concat, outputs\n\n@@ -426,6 +426,7 @@ class TorchTrainer(base_trainer.Trainer):\n         self.eval()\n \n         self.make_test_function()\n+        self.stop_evaluating = False\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n@@ -433,6 +434,8 @@ class TorchTrainer(base_trainer.Trainer):\n             callbacks.on_test_batch_begin(step)\n             logs = self.test_function(data)\n             callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n+            if self.stop_evaluating:\n+                break\n         logs = self.get_metrics_result()\n         callbacks.on_test_end(logs)\n \n@@ -484,6 +487,7 @@ class TorchTrainer(base_trainer.Trainer):\n         self.eval()\n \n         self.make_predict_function()\n+        self.stop_predicting = False\n         callbacks.on_predict_begin()\n         outputs = None\n         for step, data in epoch_iterator.enumerate_epoch():\n@@ -491,6 +495,8 @@ class TorchTrainer(base_trainer.Trainer):\n             batch_outputs = self.predict_function(data)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n+            if self.stop_predicting:\n+                break\n         callbacks.on_predict_end()\n         outputs = tree.map_structure(backend.convert_to_numpy, outputs)\n         return tree.map_structure_up_to(batch_outputs, np.concatenate, outputs)\n\n@@ -1010,9 +1010,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_recompile(self):\n-        inputs = layers.Input((2,))\n-        outputs = layers.Dense(3)(inputs)\n-        model = keras.Model(inputs, outputs)\n+        model = ExampleModel(units=3)\n         model.compile(\n             optimizer=\"sgd\", loss=\"mse\", metrics=[\"mean_squared_error\"]\n         )\n@@ -1072,9 +1070,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         # Test that you can pass an infinite generator to `validation_data`\n         # arg of fit() as well as a `validation_steps` argument and that\n         # validation only runs for the correct number of steps.\n-        inputs = layers.Input((2,))\n-        outputs = layers.Dense(3)(inputs)\n-        model = keras.Model(inputs, outputs)\n+        model = ExampleModel(units=3)\n         model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"mse\"])\n \n         class Recorder(keras.callbacks.Callback):\n@@ -1105,3 +1101,41 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(recorder.train_counter, 3)\n         self.assertEqual(recorder.val_counter, 4)\n+\n+    @parameterized.named_parameters(\n+        [\n+            (\"fit\", \"fit\", \"training\", \"train\"),\n+            (\"evaluate\", \"evaluate\", \"evaluating\", \"test\"),\n+            (\"predict\", \"predict\", \"predicting\", \"predict\"),\n+        ]\n+    )\n+    @pytest.mark.requires_trainable_backend\n+    def test_stop_loop(self, method, method_gerund, on_end_name):\n+        model = ExampleModel(units=3)\n+        model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"mse\"])\n+\n+        class Stopper(keras.callbacks.Callback):\n+            def __init__(self, stop_count):\n+                self.stop_count = stop_count\n+                self.counter = 0\n+                setattr(self, f\"on_{on_end_name}_batch_end\", self.batch_end)\n+\n+            def batch_end(self, *args, **kwargs):\n+                self.counter += 1\n+                if self.counter == self.stop_count:\n+                    setattr(self.model, f\"stop_{method_gerund}\", True)\n+\n+        def infinite_gen():\n+            while True:\n+                x = np.ones((2, 2))\n+                y = np.ones((2, 3))\n+                yield (x,) if method == \"predict\" else (x, y)\n+\n+        stop_count = 5\n+        stopper = Stopper(stop_count)\n+\n+        getattr(model, method)(\n+            infinite_gen(),\n+            callbacks=[stopper],\n+        )\n+        self.assertEqual(stopper.counter, stop_count)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ebe1d052e27166ebcd4fe8ffaa6fb7637c97d8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 4 | Files Changed: 8 | Hunks: 13 | Methods Changed: 13 | Complexity Δ (Sum/Max): 16/8 | Churn Δ: 129 | Churn Cumulative: 2370 | Contributors (this commit): 22 | Commits (past 90d): 30 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from keras.backend.jax.core import compute_output_spec\n from keras.backend.jax.core import cond\n from keras.backend.jax.core import convert_to_numpy\n from keras.backend.jax.core import convert_to_tensor\n+from keras.backend.jax.core import device\n from keras.backend.jax.core import is_tensor\n from keras.backend.jax.core import scatter\n from keras.backend.jax.core import shape\n\n@@ -314,3 +314,18 @@ def unstack(x, num=None, axis=0):\n         jax.lax.index_in_dim(x, i, axis, keepdims=False)\n         for i in range(x.shape[axis])\n     ]\n+\n+\n+def device(device):\n+    if isinstance(device, str):\n+        # We support string value like \"cpu:0\", \"gpu:1\", etc.\n+        jax_device = distribution_lib._to_jax_device(device)\n+    elif not isinstance(device, jax.Device):\n+        raise ValueError(\n+            \"Invalid value for argument `device`. \"\n+            \"Expected a string like 'gpu:0' or a `jax.Device` instance. \"\n+            f\"Received: device={device}\"\n+        )\n+    else:\n+        jax_device = device\n+    return jax.default_device(jax_device)\n\n@@ -13,6 +13,7 @@ from keras.backend.tensorflow.core import compute_output_spec\n from keras.backend.tensorflow.core import cond\n from keras.backend.tensorflow.core import convert_to_numpy\n from keras.backend.tensorflow.core import convert_to_tensor\n+from keras.backend.tensorflow.core import device\n from keras.backend.tensorflow.core import is_tensor\n from keras.backend.tensorflow.core import name_scope\n from keras.backend.tensorflow.core import scatter\n\n@@ -283,3 +283,7 @@ class name_scope(base_name_scope):\n         super().__exit__(*args, **kwargs)\n         if self._pop_on_exit:\n             self._tf_name_scope.__exit__(*args, **kwargs)\n+\n+\n+def device(device):\n+    return tf.device(device)\n\n@@ -0,0 +1,78 @@\n+import pytest\n+\n+from keras import backend\n+from keras import testing\n+\n+\n+class DeviceTest(testing.TestCase):\n+    @pytest.mark.skipif(backend.backend() != \"tensorflow\", reason=\"tf only\")\n+    def test_tf_device_scope(self):\n+        import tensorflow as tf\n+\n+        if not tf.config.list_physical_devices(\"GPU\"):\n+            self.skipTest(\"Need at least one GPU for testing\")\n+\n+        with backend.device(\"cpu:0\"):\n+            t = backend.numpy.ones((2, 1))\n+            self.assertIn(\"CPU:0\", t.device)\n+        # When leaving the scope, the device should be back with gpu:0\n+        t = backend.numpy.ones((2, 1))\n+        self.assertIn(\"GPU:0\", t.device)\n+\n+        # Also verify the explicit gpu device\n+        with backend.device(\"gpu:0\"):\n+            t = backend.numpy.ones((2, 1))\n+            self.assertIn(\"GPU:0\", t.device)\n+\n+    @pytest.mark.skipif(backend.backend() != \"jax\", reason=\"jax only\")\n+    def test_jax_device_scope(self):\n+        import jax\n+        from jax.lib import xla_bridge\n+\n+        platform = xla_bridge.get_backend().platform\n+\n+        if platform != \"gpu\":\n+            self.skipTest(\"Need at least one GPU for testing\")\n+\n+        with backend.device(\"cpu:0\"):\n+            t = backend.numpy.ones((2, 1))\n+            self.assertEqual(t.device(), jax.devices(\"cpu\")[0])\n+\n+        # When leaving the scope, the device should be back with gpu:0\n+        t = backend.numpy.ones((2, 1))\n+        self.assertEqual(t.device(), jax.devices(\"gpu\")[0])\n+\n+        # Also verify the explicit gpu device\n+        with backend.device(\"gpu:0\"):\n+            t = backend.numpy.ones((2, 1))\n+            self.assertEqual(t.device(), jax.devices(\"gpu\")[0])\n+\n+    @pytest.mark.skipif(backend.backend() != \"jax\", reason=\"jax only\")\n+    def test_invalid_jax_device(self):\n+        with self.assertRaisesRegex(ValueError, \"Received: device=123\"):\n+            backend.device(123).__enter__()\n+\n+    @pytest.mark.skipif(backend.backend() != \"torch\", reason=\"torch only\")\n+    def test_torch_device_scope(self):\n+        import torch\n+\n+        if not torch.cuda.device_count():\n+            self.skipTest(\"Need at least one GPU for testing\")\n+\n+        with backend.device(\"cpu:0\"):\n+            t = backend.numpy.ones((2, 1))\n+            self.assertEqual(t.device, torch.device(\"cpu\"))\n+\n+        # When leaving the scope, the device should be back with gpu:0\n+        t = backend.numpy.ones((2, 1))\n+        self.assertEqual(t.device, torch.device(\"cuda\", 0))\n+\n+        # Also verify the explicit gpu -> cuda conversion\n+        with backend.device(\"gpu:0\"):\n+            t = backend.numpy.ones((2, 1))\n+            self.assertEqual(t.device, torch.device(\"cuda\", 0))\n+\n+    @pytest.mark.skipif(backend.backend() != \"torch\", reason=\"torch only\")\n+    def test_invalid_torch_device(self):\n+        with self.assertRaisesRegex(ValueError, \"Received: device=123\"):\n+            backend.device(123).__enter__()\n\n@@ -27,6 +27,7 @@ from keras.backend.torch.core import compute_output_spec\n from keras.backend.torch.core import cond\n from keras.backend.torch.core import convert_to_numpy\n from keras.backend.torch.core import convert_to_tensor\n+from keras.backend.torch.core import device\n from keras.backend.torch.core import is_tensor\n from keras.backend.torch.core import scatter\n from keras.backend.torch.core import shape\n\n@@ -46,10 +46,12 @@ TORCH_DTYPES = {\n \n \n @contextlib.contextmanager\n-def device_scope(device):\n+def device(device):\n     previous_device = global_state.get_global_attribute(\"torch_device\", None)\n+    device = _parse_device_input(device)\n     global_state.set_global_attribute(\"torch_device\", device)\n     try:\n+        with device:\n             yield\n     finally:\n         global_state.set_global_attribute(\"torch_device\", previous_device)\n@@ -62,6 +64,25 @@ def get_device():\n     return device\n \n \n+def _parse_device_input(device):\n+    if isinstance(device, str):\n+        # We support string value like \"cpu:0\", \"gpu:1\", and need to convert\n+        # \"gpu\" to \"cuda\"\n+        if \"gpu\" in device:\n+            device = device.replace(\"gpu\", \"cuda\")\n+        torch_device = torch.device(device)\n+    elif not isinstance(device, torch.device):\n+        raise ValueError(\n+            \"Invalid value for argument `device`. \"\n+            \"Expected a string like 'gpu:0' or a `torch.device` instance. \"\n+            f\"Received: device={device}\"\n+        )\n+    else:\n+        torch_device = device\n+    # The jax.Device instance can be used as context.\n+    return torch_device\n+\n+\n def to_torch_dtype(dtype):\n     standardized_dtype = TORCH_DTYPES.get(standardize_dtype(dtype), None)\n     if standardized_dtype is None:\n@@ -258,14 +279,14 @@ def compute_output_spec(fn, *args, **kwargs):\n             # First try instantiating all tensors on the `\"meta\"` device,\n             # which  should give a \"zero flop\" way to trace shape, but does\n             # not have universal support with torch operations.\n-            with device_scope(\"meta\"):\n+            with device(\"meta\"):\n                 meta_args, meta_kwargs = tree.map_structure(\n                     lambda x: convert_keras_tensor_to_torch(x, fill_value),\n                     (args, kwargs),\n                 )\n                 return fn(*meta_args, **meta_kwargs)\n         except:\n-            with device_scope(DEFAULT_DEVICE):\n+            with device(DEFAULT_DEVICE):\n                 # If the `\"meta\"` device placement fails, fall back to tracing\n                 # eagerly with tensors on the default device. This will be\n                 # more robust, but more expensive.\n\n@@ -105,5 +105,5 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         layer = layers.Normalization()\n         data = np.random.random((32, 4))\n         layer.adapt(data)\n-        with core.device_scope(\"meta\"):\n+        with core.device(\"meta\"):\n             layer(data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c0f944d33fef68c9ffcdc31bdc62863346322e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 43 | Files Changed: 10 | Hunks: 34 | Methods Changed: 20 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 104 | Churn Cumulative: 2490 | Contributors (this commit): 24 | Commits (past 90d): 44 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,3 +27,8 @@ class Variable(BackendVariable):\n @keras_export(\"keras.name_scope\")\n class name_scope(backend_name_scope):\n     pass\n+\n+\n+@keras_export(\"keras.device\")\n+def device(device_name):\n+    return backend.device_scope(device_name)\n\n@@ -12,7 +12,7 @@ from keras.backend.jax.core import compute_output_spec\n from keras.backend.jax.core import cond\n from keras.backend.jax.core import convert_to_numpy\n from keras.backend.jax.core import convert_to_tensor\n-from keras.backend.jax.core import device\n+from keras.backend.jax.core import device_scope\n from keras.backend.jax.core import is_tensor\n from keras.backend.jax.core import scatter\n from keras.backend.jax.core import shape\n\n@@ -316,16 +316,16 @@ def unstack(x, num=None, axis=0):\n     ]\n \n \n-def device(device):\n-    if isinstance(device, str):\n+def device_scope(device_name):\n+    if isinstance(device_name, str):\n         # We support string value like \"cpu:0\", \"gpu:1\", etc.\n-        jax_device = distribution_lib._to_jax_device(device)\n-    elif not isinstance(device, jax.Device):\n+        jax_device = distribution_lib._to_jax_device(device_name)\n+    elif not isinstance(device_name, jax.Device):\n         raise ValueError(\n-            \"Invalid value for argument `device`. \"\n+            \"Invalid value for argument `device_name`. \"\n             \"Expected a string like 'gpu:0' or a `jax.Device` instance. \"\n-            f\"Received: device={device}\"\n+            f\"Received: device_name='{device_name}'\"\n         )\n     else:\n-        jax_device = device\n+        jax_device = device_name\n     return jax.default_device(jax_device)\n\n@@ -13,7 +13,7 @@ from keras.backend.tensorflow.core import compute_output_spec\n from keras.backend.tensorflow.core import cond\n from keras.backend.tensorflow.core import convert_to_numpy\n from keras.backend.tensorflow.core import convert_to_tensor\n-from keras.backend.tensorflow.core import device\n+from keras.backend.tensorflow.core import device_scope\n from keras.backend.tensorflow.core import is_tensor\n from keras.backend.tensorflow.core import name_scope\n from keras.backend.tensorflow.core import scatter\n\n@@ -285,5 +285,5 @@ class name_scope(base_name_scope):\n             self._tf_name_scope.__exit__(*args, **kwargs)\n \n \n-def device(device):\n-    return tf.device(device)\n+def device_scope(device_name):\n+    return tf.device(device_name)\n\n@@ -12,15 +12,19 @@ class DeviceTest(testing.TestCase):\n         if not tf.config.list_physical_devices(\"GPU\"):\n             self.skipTest(\"Need at least one GPU for testing\")\n \n-        with backend.device(\"cpu:0\"):\n+        with backend.device_scope(\"cpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertIn(\"CPU:0\", t.device)\n+        with backend.device_scope(\"CPU:0\"):\n+            t = backend.numpy.ones((2, 1))\n+            self.assertIn(\"CPU:0\", t.device)\n+\n         # When leaving the scope, the device should be back with gpu:0\n         t = backend.numpy.ones((2, 1))\n         self.assertIn(\"GPU:0\", t.device)\n \n         # Also verify the explicit gpu device\n-        with backend.device(\"gpu:0\"):\n+        with backend.device_scope(\"gpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertIn(\"GPU:0\", t.device)\n \n@@ -34,7 +38,10 @@ class DeviceTest(testing.TestCase):\n         if platform != \"gpu\":\n             self.skipTest(\"Need at least one GPU for testing\")\n \n-        with backend.device(\"cpu:0\"):\n+        with backend.device_scope(\"cpu:0\"):\n+            t = backend.numpy.ones((2, 1))\n+            self.assertEqual(t.device(), jax.devices(\"cpu\")[0])\n+        with backend.device_scope(\"CPU:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(t.device(), jax.devices(\"cpu\")[0])\n \n@@ -43,14 +50,14 @@ class DeviceTest(testing.TestCase):\n         self.assertEqual(t.device(), jax.devices(\"gpu\")[0])\n \n         # Also verify the explicit gpu device\n-        with backend.device(\"gpu:0\"):\n+        with backend.device_scope(\"gpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(t.device(), jax.devices(\"gpu\")[0])\n \n     @pytest.mark.skipif(backend.backend() != \"jax\", reason=\"jax only\")\n     def test_invalid_jax_device(self):\n-        with self.assertRaisesRegex(ValueError, \"Received: device=123\"):\n-            backend.device(123).__enter__()\n+        with self.assertRaisesRegex(ValueError, \"Received: device='123'\"):\n+            backend.device_scope(123).__enter__()\n \n     @pytest.mark.skipif(backend.backend() != \"torch\", reason=\"torch only\")\n     def test_torch_device_scope(self):\n@@ -59,7 +66,10 @@ class DeviceTest(testing.TestCase):\n         if not torch.cuda.device_count():\n             self.skipTest(\"Need at least one GPU for testing\")\n \n-        with backend.device(\"cpu:0\"):\n+        with backend.device_scope(\"cpu:0\"):\n+            t = backend.numpy.ones((2, 1))\n+            self.assertEqual(t.device, torch.device(\"cpu\"))\n+        with backend.device_scope(\"CPU:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(t.device, torch.device(\"cpu\"))\n \n@@ -68,11 +78,11 @@ class DeviceTest(testing.TestCase):\n         self.assertEqual(t.device, torch.device(\"cuda\", 0))\n \n         # Also verify the explicit gpu -> cuda conversion\n-        with backend.device(\"gpu:0\"):\n+        with backend.device_scope(\"gpu:0\"):\n             t = backend.numpy.ones((2, 1))\n             self.assertEqual(t.device, torch.device(\"cuda\", 0))\n \n     @pytest.mark.skipif(backend.backend() != \"torch\", reason=\"torch only\")\n     def test_invalid_torch_device(self):\n-        with self.assertRaisesRegex(ValueError, \"Received: device=123\"):\n-            backend.device(123).__enter__()\n+        with self.assertRaisesRegex(ValueError, \"Received: device_name='123'\"):\n+            backend.device_scope(123).__enter__()\n\n@@ -27,7 +27,7 @@ from keras.backend.torch.core import compute_output_spec\n from keras.backend.torch.core import cond\n from keras.backend.torch.core import convert_to_numpy\n from keras.backend.torch.core import convert_to_tensor\n-from keras.backend.torch.core import device\n+from keras.backend.torch.core import device_scope\n from keras.backend.torch.core import is_tensor\n from keras.backend.torch.core import scatter\n from keras.backend.torch.core import shape\n\n@@ -46,12 +46,11 @@ TORCH_DTYPES = {\n \n \n @contextlib.contextmanager\n-def device(device):\n+def device_scope(device_name):\n     previous_device = global_state.get_global_attribute(\"torch_device\", None)\n-    device = _parse_device_input(device)\n-    global_state.set_global_attribute(\"torch_device\", device)\n+    current_device = _parse_device_input(device_name)\n+    global_state.set_global_attribute(\"torch_device\", current_device)\n     try:\n-        with device:\n         yield\n     finally:\n         global_state.set_global_attribute(\"torch_device\", previous_device)\n@@ -64,23 +63,22 @@ def get_device():\n     return device\n \n \n-def _parse_device_input(device):\n-    if isinstance(device, str):\n+def _parse_device_input(device_name):\n+    if isinstance(device_name, str):\n         # We support string value like \"cpu:0\", \"gpu:1\", and need to convert\n         # \"gpu\" to \"cuda\"\n-        if \"gpu\" in device:\n-            device = device.replace(\"gpu\", \"cuda\")\n-        torch_device = torch.device(device)\n-    elif not isinstance(device, torch.device):\n+        device_name = device_name.lower()\n+        if \"gpu\" in device_name:\n+            device_name = device_name.replace(\"gpu\", \"cuda\")\n+        torch_device = torch.device(device_name)\n+    elif not isinstance(device_name, torch.device):\n         raise ValueError(\n-            \"Invalid value for argument `device`. \"\n+            \"Invalid value for argument `device_name`. \"\n             \"Expected a string like 'gpu:0' or a `torch.device` instance. \"\n-            f\"Received: device={device}\"\n+            f\"Received: device_name='{device_name}'\"\n         )\n-    else:\n-        torch_device = device\n-    # The jax.Device instance can be used as context.\n-    return torch_device\n+    # The torch.Device instance can be used directly.\n+    return device_name\n \n \n def to_torch_dtype(dtype):\n@@ -92,6 +90,10 @@ def to_torch_dtype(dtype):\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n+        if isinstance(value, torch.nn.Parameter):\n+            # Reuse same parameter\n+            self._value = value\n+        else:\n             self._value = torch.nn.Parameter(\n                 convert_to_tensor(value, dtype=self._dtype),\n                 requires_grad=self.trainable,\n@@ -128,7 +130,7 @@ class Variable(KerasVariable):\n     def value(self):\n         value = super().value\n         # Create and use a symbolic tensor stub in symbolic calls.\n-        if get_device() == \"meta\" and str(value.device) != \"meta\":\n+        if str(get_device()) == \"meta\" and str(value.device) != \"meta\":\n             return torch.empty(\n                 size=value.shape,\n                 dtype=value.dtype,\n@@ -279,14 +281,14 @@ def compute_output_spec(fn, *args, **kwargs):\n             # First try instantiating all tensors on the `\"meta\"` device,\n             # which  should give a \"zero flop\" way to trace shape, but does\n             # not have universal support with torch operations.\n-            with device(\"meta\"):\n+            with device_scope(\"meta\"):\n                 meta_args, meta_kwargs = tree.map_structure(\n                     lambda x: convert_keras_tensor_to_torch(x, fill_value),\n                     (args, kwargs),\n                 )\n                 return fn(*meta_args, **meta_kwargs)\n         except:\n-            with device(DEFAULT_DEVICE):\n+            with device_scope(DEFAULT_DEVICE):\n                 # If the `\"meta\"` device placement fails, fall back to tracing\n                 # eagerly with tensors on the default device. This will be\n                 # more robust, but more expensive.\n\n@@ -105,5 +105,5 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         layer = layers.Normalization()\n         data = np.random.random((32, 4))\n         layer.adapt(data)\n-        with core.device(\"meta\"):\n+        with core.device_scope(\"meta\"):\n             layer(data)\n\n@@ -101,10 +101,11 @@ class TorchModuleWrapper(Layer):\n         from keras.backend.torch import Variable\n \n         for param in self.module.parameters():\n+            # The Variable will reuse the raw `param`\n+            # and simply wrap it.\n             variable = Variable(\n                 initializer=param, trainable=param.requires_grad\n             )\n-            variable._value = param\n             self._track_variable(variable)\n         self.built = True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#09b9511e734d3cfd6b382ff43f58a6f8e020966b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 32 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -56,7 +56,7 @@ class DeviceTest(testing.TestCase):\n \n     @pytest.mark.skipif(backend.backend() != \"jax\", reason=\"jax only\")\n     def test_invalid_jax_device(self):\n-        with self.assertRaisesRegex(ValueError, \"Received: device='123'\"):\n+        with self.assertRaisesRegex(ValueError, \"Received: device_name='123'\"):\n             backend.device_scope(123).__enter__()\n \n     @pytest.mark.skipif(backend.backend() != \"torch\", reason=\"torch only\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9fff7f90d726190fba8b912b46c5dbba78ee023", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 128 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -70,11 +70,10 @@ def _parse_device_input(device_name):\n         device_name = device_name.lower()\n         if \"gpu\" in device_name:\n             device_name = device_name.replace(\"gpu\", \"cuda\")\n-        torch_device = torch.device(device_name)\n-    elif not isinstance(device_name, torch.device):\n+    else:\n         raise ValueError(\n             \"Invalid value for argument `device_name`. \"\n-            \"Expected a string like 'gpu:0' or a `torch.device` instance. \"\n+            \"Expected a string like 'gpu:0' or 'cpu'. \"\n             f\"Received: device_name='{device_name}'\"\n         )\n     # The torch.Device instance can be used directly.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47727b569f980983bb7a660d49017303e9f626f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 58 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -319,6 +319,7 @@ def unstack(x, num=None, axis=0):\n def device_scope(device_name):\n     if isinstance(device_name, str):\n         # We support string value like \"cpu:0\", \"gpu:1\", etc.\n+        device_name = device_name.lower()\n         jax_device = distribution_lib._to_jax_device(device_name)\n     elif not isinstance(device_name, jax.Device):\n         raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#494a99a51723614d012186a3e227bca9074a469f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 57 | Churn Cumulative: 133 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 0.9333333333333333\n\nDIFF:\n@@ -209,13 +209,16 @@ class PyDatasetAdapter(DataAdapter):\n         self._output_signature = tree.map_structure(get_tensor_spec, batch)\n \n     def _standardize_batch(self, batch):\n+        if isinstance(batch, dict):\n+            return batch\n         if isinstance(batch, np.ndarray):\n             batch = (batch,)\n         if isinstance(batch, list):\n             batch = tuple(batch)\n         if not isinstance(batch, tuple) or len(batch) not in {1, 2, 3}:\n             raise ValueError(\n-                \"PyDataset.__getitem__() must return a tuple, either \"\n+                \"PyDataset.__getitem__() must return a tuple or a dict. \"\n+                \"If a tuple, it must be ordered either \"\n                 \"(input,) or (inputs, targets) or \"\n                 \"(inputs, targets, sample_weights). \"\n                 f\"Received: {str(batch)[:100]}... of type {type(batch)}\"\n\n@@ -39,6 +39,29 @@ class ExamplePyDataset(py_dataset_adapter.PyDataset):\n         return batch_x, batch_y\n \n \n+class DictPyDataset(py_dataset_adapter.PyDataset):\n+    def __init__(\n+        self, inputs, batch_size=32, **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+        self.inputs = inputs\n+        self.batch_size = batch_size\n+\n+    def __len__(self):\n+        return math.ceil(len(self.inputs[\"x\"]) / self.batch_size)\n+\n+    def __getitem__(self, idx):\n+        # Return x, y for batch idx.\n+        low = idx * self.batch_size\n+        # Cap upper bound at array length; the last batch may be smaller\n+        # if the total number of items is not a multiple of batch size.\n+        high = min(low + self.batch_size, len(self.inputs[\"x\"]))\n+        batch_x = self.inputs[\"x\"][low:high]\n+        batch_y = self.inputs[\"y\"][low:high]\n+        batch = {\"x\": batch_x, \"y\": batch_y}\n+        return batch\n+\n+\n class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters(\n         [\n@@ -146,3 +169,32 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n         speedup_time = time.time() - t0\n \n         self.assertLess(speedup_time, no_speedup_time)\n+\n+    def test_dict_inputs(self):\n+        inputs = {\n+            \"x\": np.random.random((40, 4)),\n+            \"y\": np.random.random((40, 2)),\n+        }\n+        py_dataset = DictPyDataset(inputs, batch_size=4)\n+        adapter = py_dataset_adapter.PyDatasetAdapter(\n+            py_dataset, shuffle=False\n+        )\n+        gen = adapter.get_numpy_iterator()\n+        for batch in gen:\n+            self.assertEqual(len(batch), 2)\n+            bx, by = batch[\"x\"], batch[\"y\"]\n+            self.assertIsInstance(bx, np.ndarray)\n+            self.assertIsInstance(by, np.ndarray)\n+            self.assertEqual(bx.dtype, by.dtype)\n+            self.assertEqual(bx.shape, (4, 4))\n+            self.assertEqual(by.shape, (4, 2))\n+        \n+        ds = adapter.get_tf_dataset()\n+        for batch in ds:\n+            self.assertEqual(len(batch), 2)\n+            bx, by = batch[\"x\"], batch[\"y\"]\n+            self.assertIsInstance(bx, tf.Tensor)\n+            self.assertIsInstance(by, tf.Tensor)\n+            self.assertEqual(bx.dtype, by.dtype)\n+            self.assertEqual(tuple(bx.shape), (4, 4))\n+            self.assertEqual(tuple(by.shape), (4, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4223aeadb3d058e15c25f0e5482d2ee3caa4b23a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 15 | Churn Cumulative: 155 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: 0.75\n\nDIFF:\n@@ -389,6 +389,8 @@ def standardize_dtype(dtype):\n         \"torch\" in str(dtype) or \"jax.numpy\" in str(dtype)\n     ):\n         dtype = str(dtype).split(\".\")[-1]\n+    elif hasattr(dtype, \"__name__\"):\n+        dtype = dtype.__name__\n \n     if dtype not in ALLOWED_DTYPES:\n         raise ValueError(f\"Invalid dtype: {dtype}\")\n@@ -414,10 +416,10 @@ def standardize_shape(shape):\n         if config.backend() == \"jax\" and str(e) == \"b\":\n             # JAX2TF tracing represents `None` dimensions as `b`\n             continue\n-        if not isinstance(e, int):\n+        if not is_int_dtype(type(e)):\n             raise ValueError(\n                 f\"Cannot convert '{shape}' to a shape. \"\n-                f\"Found invalid entry '{e}'. \"\n+                f\"Found invalid entry '{e}' of type '{type(e)}'. \"\n             )\n         if e < 0:\n             raise ValueError(\n\n@@ -131,3 +131,12 @@ class InputLayerTest(testing.TestCase, parameterized.TestCase):\n         layer = InputLayer(shape=(32,))\n         output = layer.call()\n         self.assertIsNone(output)\n+\n+    def test_numpy_shape(self):\n+        # non-python int type shapes should be ok\n+        InputLayer(shape=(np.int64(32),))\n+        # float should still raise\n+        with self.assertRaisesRegex(\n+            ValueError, \"Cannot convert\"\n+        ):\n+            InputLayer(shape=(np.float64(32),))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1af1acf8f3a3c870bef9e45212ba075a19d0576a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 162 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -136,7 +136,5 @@ class InputLayerTest(testing.TestCase, parameterized.TestCase):\n         # non-python int type shapes should be ok\n         InputLayer(shape=(np.int64(32),))\n         # float should still raise\n-        with self.assertRaisesRegex(\n-            ValueError, \"Cannot convert\"\n-        ):\n+        with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n             InputLayer(shape=(np.float64(32),))\n\n@@ -40,9 +40,7 @@ class ExamplePyDataset(py_dataset_adapter.PyDataset):\n \n \n class DictPyDataset(py_dataset_adapter.PyDataset):\n-    def __init__(\n-        self, inputs, batch_size=32, **kwargs\n-    ):\n+    def __init__(self, inputs, batch_size=32, **kwargs):\n         super().__init__(**kwargs)\n         self.inputs = inputs\n         self.batch_size = batch_size\n@@ -176,9 +174,7 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n             \"y\": np.random.random((40, 2)),\n         }\n         py_dataset = DictPyDataset(inputs, batch_size=4)\n-        adapter = py_dataset_adapter.PyDatasetAdapter(\n-            py_dataset, shuffle=False\n-        )\n+        adapter = py_dataset_adapter.PyDatasetAdapter(py_dataset, shuffle=False)\n         gen = adapter.get_numpy_iterator()\n         for batch in gen:\n             self.assertEqual(len(batch), 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c62839cbb0e54b7bac09ce20471a0dfaa65ff55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 47 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -135,6 +135,3 @@ class InputLayerTest(testing.TestCase, parameterized.TestCase):\n     def test_numpy_shape(self):\n         # non-python int type shapes should be ok\n         InputLayer(shape=(np.int64(32),))\n-        # float should still raise\n-        with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            InputLayer(shape=(np.float64(32),))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#724321c7b39a90f6125b79931284aa9932c673a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 4 | Files Changed: 6 | Hunks: 18 | Methods Changed: 7 | Complexity Δ (Sum/Max): 10/4 | Churn Δ: 93 | Churn Cumulative: 3956 | Contributors (this commit): 23 | Commits (past 90d): 17 | Contributors (cumulative): 44 | DMM Complexity: 0.3877551020408163\n\nDIFF:\n@@ -4,6 +4,7 @@ from keras.activations.activations import elu\n from keras.activations.activations import exponential\n from keras.activations.activations import gelu\n from keras.activations.activations import hard_sigmoid\n+from keras.activations.activations import hard_swish\n from keras.activations.activations import leaky_relu\n from keras.activations.activations import linear\n from keras.activations.activations import log_softmax\n@@ -36,6 +37,7 @@ ALL_OBJECTS = {\n     sigmoid,\n     exponential,\n     hard_sigmoid,\n+    hard_swish,\n     linear,\n     mish,\n     log_softmax,\n\n@@ -374,6 +374,29 @@ def hard_sigmoid(x):\n     return ops.hard_sigmoid(x)\n \n \n+@keras_export(\"keras.activations.hard_swish\")\n+def hard_swish(x):\n+    \"\"\"Hard swish activation function.\n+\n+    The hard swish activation is defined as:\n+\n+    - `0` if `if x < -3`\n+    - `x` if `x > 3`\n+    - `x * (x + 3) / 6` if `-3 <= x <= 3`\n+\n+    It's a faster, piecewise linear approximation of the swish activation.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Reference:\n+\n+    - [A Howard, 2019](https://arxiv.org/abs/1905.02244)\n+    \"\"\"\n+    x = backend.convert_to_tensor(x)\n+    return x * ops.relu6(x + 3.0) * (1.0 / 6.0)\n+\n+\n @keras_export(\"keras.activations.linear\")\n def linear(x):\n     \"\"\"Linear activation function (pass-through).\n\n@@ -40,6 +40,10 @@ def _ref_hard_sigmoid(x):\n     return z\n \n \n+def _ref_hard_swish(x):\n+    return x * np.minimum(np.maximum(0.0, x + 3.0), 6.0) * (1.0 / 6.0)\n+\n+\n def _ref_sigmoid(x):\n     if x >= 0:\n         return 1 / (1 + np.exp(-x))\n@@ -333,6 +337,39 @@ class ActivationsTest(testing.TestCase):\n             result_positive_above_1, expected_positive_above_1, rtol=1e-05\n         )\n \n+    def test_hard_swish(self):\n+        # Basic test for random values between -3 and 3\n+        x = np.random.uniform(-3, 3, (2, 5)).astype(\"float32\")\n+        result = activations.hard_swish(x[np.newaxis, :])[0]\n+        expected = np.vectorize(_ref_hard_swish)(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5).astype(\"float32\")\n+        result_1d = activations.hard_swish(x_1d)\n+        expected_1d = np.vectorize(_ref_hard_swish)(x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3)).astype(\"float32\")\n+        result_3d = activations.hard_swish(x_3d)\n+        expected_3d = np.vectorize(_ref_hard_swish)(x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test with strictly positive values much larger than 3\n+        x_positive_above_3 = np.random.uniform(5, 10, (2, 5)).astype(\"float32\")\n+        result_positive_above_3 = activations.hard_swish(x_positive_above_3)\n+        expected_positive_above_3 = x_positive_above_3\n+        self.assertAllClose(\n+            result_positive_above_3, expected_positive_above_3, rtol=1e-05\n+        )\n+\n+        # Test with strictly negative values much smaller than -3\n+        x_negatives = np.random.uniform(-10, -5, (2, 5)).astype(\"float32\")\n+        result = activations.hard_swish(x_negatives)\n+        expected_zeros = np.zeros_like(x_negatives)\n+        self.assertAllClose(result, expected_zeros, rtol=1e-05)\n+\n     def test_relu_negative_slope(self):\n         # Define the input tensor\n         x = np.array([-10, -5, 0.0, 5, 10])\n\n@@ -179,10 +179,21 @@ class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.named_parameters(test_parameters)\n     @pytest.mark.skipif(PIL is None, reason=\"Requires PIL.\")\n     def test_application_base(self, app, _, app_module, image_data_format):\n+        import tensorflow as tf\n+\n         if app == nasnet.NASNetMobile and backend.backend() == \"torch\":\n             self.skipTest(\n                 \"NASNetMobile pretrained incorrect with torch backend.\"\n             )\n+        if (\n+            image_data_format == \"channels_first\"\n+            and len(tf.config.list_physical_devices(\"GPU\")) == 0\n+            and backend.backend() == \"tensorflow\"\n+        ):\n+            self.skipTest(\n+                \"Conv2D doesn't support channels_first using CPU with \"\n+                \"tensorflow backend\"\n+            )\n         self.skip_if_invalid_image_data_format_for_model(app, image_data_format)\n         backend.set_image_data_format(image_data_format)\n \n\n@@ -3,6 +3,7 @@ import pytest\n from absl.testing import parameterized\n \n import keras\n+from keras import backend\n from keras import testing\n from keras.applications import imagenet_utils as utils\n from keras.mixed_precision import set_dtype_policy\n@@ -53,8 +54,8 @@ class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n         for mode in [\"torch\", \"tf\"]:\n             x = np.random.uniform(0, 255, (2, 10, 10, 3))\n             xint = x.astype(\"int\")\n-            x2 = utils.preprocess_input(x, mode=mode)\n-            xint2 = utils.preprocess_input(xint)\n+            x2 = utils.preprocess_input(x, \"channels_last\", mode=mode)\n+            xint2 = utils.preprocess_input(xint, \"channels_last\")\n             self.assertAllClose(x, x2)\n             self.assertNotEqual(xint.astype(\"float\").max(), xint2.max())\n \n@@ -64,7 +65,7 @@ class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n         x2 = utils.preprocess_input(\n             x, data_format=\"channels_last\", mode=\"caffe\"\n         )\n-        xint2 = utils.preprocess_input(xint)\n+        xint2 = utils.preprocess_input(xint, data_format=\"channels_last\")\n         self.assertAllClose(x, x2[..., ::-1])\n         self.assertNotEqual(xint.astype(\"float\").max(), xint2.max())\n \n@@ -77,8 +78,12 @@ class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n     )\n     @pytest.mark.requires_trainable_backend\n     def test_preprocess_input_symbolic(self, mode):\n+        backend_data_format = backend.image_data_format()\n         # Test image batch\n+        if backend_data_format == \"channels_last\":\n             x = np.random.uniform(0, 255, (2, 10, 10, 3))\n+        elif backend_data_format == \"channels_first\":\n+            x = np.random.uniform(0, 255, (2, 3, 10, 10))\n         inputs = keras.layers.Input(shape=x.shape[1:])\n         outputs = keras.layers.Lambda(\n             lambda x: utils.preprocess_input(x, mode=mode),\n@@ -87,6 +92,8 @@ class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n         model = keras.Model(inputs, outputs)\n         self.assertEqual(model.predict(x).shape, x.shape)\n \n+        x = np.random.uniform(0, 255, (2, 10, 10, 3))\n+        inputs = keras.layers.Input(shape=x.shape[1:])\n         outputs1 = keras.layers.Lambda(\n             lambda x: utils.preprocess_input(x, \"channels_last\", mode=mode),\n             output_shape=x.shape[1:],\n@@ -104,7 +111,10 @@ class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(out1, out2.transpose(0, 2, 3, 1))\n \n         # Test single image\n+        if backend_data_format == \"channels_last\":\n             x = np.random.uniform(0, 255, (10, 10, 3))\n+        elif backend_data_format == \"channels_first\":\n+            x = np.random.uniform(0, 255, (3, 10, 10))\n         inputs = keras.layers.Input(shape=x.shape)\n         outputs = keras.layers.Lambda(\n             lambda x: utils.preprocess_input(x, mode=mode), output_shape=x.shape\n@@ -112,6 +122,8 @@ class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n         model = keras.Model(inputs, outputs)\n         self.assertEqual(model.predict(x[np.newaxis])[0].shape, x.shape)\n \n+        x = np.random.uniform(0, 255, (10, 10, 3))\n+        inputs = keras.layers.Input(shape=x.shape)\n         outputs1 = keras.layers.Lambda(\n             lambda x: utils.preprocess_input(x, \"channels_last\", mode=mode),\n             output_shape=x.shape,\n\n@@ -540,7 +540,7 @@ def hard_sigmoid(x):\n \n \n def hard_swish(x):\n-    return layers.Multiply()([x, hard_sigmoid(x)])\n+    return layers.Activation(\"hard_swish\")(x)\n \n \n # This function is taken from the original tf repo.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
