{"custom_id": "keras#655b5b9f9fbace30cf71c54106c1fc6d49b169c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 64 | Churn Cumulative: 5696 | Contributors (this commit): 69 | Commits (past 90d): 22 | Contributors (cumulative): 69 | DMM Complexity: None\n\nDIFF:\n@@ -1477,7 +1477,7 @@ class Model(Container):\n                 framework-native tensors (e.g. TensorFlow data tensors).\n             batch_size: Integer or `None`.\n                 Number of samples per gradient update.\n-                If unspecified, it will default to 32.\n+                If unspecified, `batch_size` will default to 32.\n             epochs: Integer. Number of epochs to train the model.\n                 An epoch is an iteration over the entire `x` and `y`\n                 data provided.\n@@ -1486,7 +1486,7 @@ class Model(Container):\n                 The model is not trained for a number of iterations\n                 given by `epochs`, but merely until the epoch\n                 of index `epochs` is reached.\n-            verbose: 0, 1, or 2. Verbosity mode.\n+            verbose: Integer. 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = progress bar, 2 = one line per epoch.\n             callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during training.\n@@ -1503,7 +1503,7 @@ class Model(Container):\n                 `(x_val, y_val, val_sample_weights)` on which to evaluate\n                 the loss and any model metrics at the end of each epoch.\n                 The model will not be trained on this data.\n-                This will override `validation_split`.\n+                `validation_data` will override `validation_split`.\n             shuffle: Boolean (whether to shuffle the training data\n                 before each epoch) or str (for 'batch').\n                 'batch' is a special option for dealing with the\n@@ -1526,9 +1526,11 @@ class Model(Container):\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n                 `sample_weight_mode=\"temporal\"` in `compile()`.\n-            initial_epoch: Epoch at which to start training\n+            initial_epoch: Integer.\n+                Epoch at which to start training\n                 (useful for resuming a previous training run).\n-            steps_per_epoch: Total number of steps (batches of samples)\n+            steps_per_epoch: Integer or `None`.\n+                Total number of steps (batches of samples)\n                 before declaring one epoch finished and starting the\n                 next epoch. When training with input tensors such as\n                 TensorFlow data tensors, the default `None` is equal to\n@@ -1657,25 +1659,41 @@ class Model(Container):\n         Computation is done in batches.\n \n         # Arguments\n-            x: Numpy array of test data,\n-                or list of Numpy arrays if the model has multiple inputs.\n-                If all inputs in the model are named,\n-                you can also pass a dictionary\n-                mapping input names to Numpy arrays.\n-                `x` can be `None` (default) if feeding from framework-native tensors.\n-            y: Numpy array of target data,\n-                or list of Numpy arrays if the model has multiple outputs.\n-                If all outputs in the model are named,\n-                you can also pass a dictionary\n-                mapping output names to Numpy arrays.\n-                `y` can be `None` (default) if feeding from framework-native tensors.\n-            batch_size: Integer. If unspecified, it will default to 32.\n-            verbose: Verbosity mode, 0 or 1.\n-            sample_weight: Array of weights to weight the contribution\n-                of different samples to the loss and metrics.\n-            steps: Total number of steps (batches of samples)\n+            x: Numpy array of test data (if the model has a single input),\n+                or list of Numpy arrays (if the model has multiple inputs).\n+                If input layers in the model are named, you can also pass a\n+                dictionary mapping input names to Numpy arrays.\n+                `x` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n+            y: Numpy array of target (label) data\n+                (if the model has a single output),\n+                or list of Numpy arrays (if the model has multiple outputs).\n+                If output layers in the model are named, you can also pass a\n+                dictionary mapping output names to Numpy arrays.\n+                `y` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n+            batch_size: Integer or `None`.\n+                Number of samples per evaluation step.\n+                If unspecified, `batch_size` will default to 32.\n+            verbose: 0 or 1. Verbosity mode.\n+                0 = silent, 1 = progress bar.\n+            sample_weight: Optional Numpy array of weights for\n+                the test samples, used for weighting the loss function.\n+                You can either pass a flat (1D)\n+                Numpy array with the same length as the input samples\n+                (1:1 mapping between weights and samples),\n+                or in the case of temporal data,\n+                you can pass a 2D array with shape\n+                `(samples, sequence_length)`,\n+                to apply a different weight to every timestep of every sample.\n+                In this case you should make sure to specify\n+                `sample_weight_mode=\"temporal\"` in `compile()`.\n+            steps: Integer or `None`.\n+                Total number of steps (batches of samples)\n                 before declaring the evaluation round finished.\n-                Ignored with the default value of `None`.\n+                The default `None` is equal to the number of unique samples in\n+                your dataset divided by the batch size.\n+\n \n         # Returns\n             Scalar test loss (if the model has a single output and no metrics)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6c951442602034439ae3903069fc44c274b7570", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5698 | Contributors (this commit): 70 | Commits (past 90d): 23 | Contributors (cumulative): 70 | DMM Complexity: None\n\nDIFF:\n@@ -103,7 +103,7 @@ def _standardize_input_data(data, names, shapes=None,\n         arrays = data\n     elif data.__class__.__name__ == 'DataFrame':\n         # test if data is a DataFrame, without pandas installed\n-        data = data.values\n+        arrays = data.values\n     else:\n         if not hasattr(data, 'shape'):\n             raise TypeError('Error when checking model ' +\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#018e55be7c77f168abaea898233ccec035bb39d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 399 | Contributors (this commit): 15 | Commits (past 90d): 13 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -23,4 +23,4 @@ from .layers import Input\n from .models import Model\r\n from .models import Sequential\r\n \r\n-__version__ = '2.1.0'\r\n+__version__ = '2.1.1'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='2.1.0',\n+      version='2.1.1',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/2.1.0',\n+      download_url='https://github.com/fchollet/keras/tarball/2.1.1',\n       license='MIT',\n       install_requires=['numpy>=1.9.1',\n                         'scipy>=0.14',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c41cefc883f1a56994a0a0eba2648dc643be9ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 8340 | Contributors (this commit): 108 | Commits (past 90d): 38 | Contributors (cumulative): 114 | DMM Complexity: None\n\nDIFF:\n@@ -380,7 +380,7 @@ def _make_batches(size, batch_size):\n     # Returns\n         A list of tuples of array indices.\n     \"\"\"\n-    num_batches = int(np.ceil(size / float(batch_size)))\n+    num_batches = (size + batch_size - 1) // batch_size  # round up\n     return [(i * batch_size, min(size, (i + 1) * batch_size))\n             for i in range(num_batches)]\n \n\n@@ -759,7 +759,7 @@ class Iterator(Sequence):\n         return self._get_batches_of_transformed_samples(index_array)\n \n     def __len__(self):\n-        return int(np.ceil(self.n / float(self.batch_size)))\n+        return (self.n + self.batch_size - 1) // self.batch_size  # round up\n \n     def on_epoch_end(self):\n         self._set_index_array()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#370065795e2c8a1c5cc9f14a0e8fdaa7a27d46d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 121 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -49,7 +49,7 @@ def cnn_layers(inputs):\n     x = layers.Flatten()(x)\n     x = layers.Dense(512, activation='relu')(x)\n     x = layers.Dropout(0.5)(x)\n-    predictions = layers.Dense(classes,\n+    predictions = layers.Dense(num_classes,\n                                activation='softmax',\n                                name='x_train_out')(x)\n     return predictions\n@@ -59,12 +59,12 @@ batch_size = 128\n buffer_size = 10000\n steps_per_epoch = np.ceil(60000 / 128).astype('int')  # = 469\n epochs = 5\n-classes = 10\n+num_classes = 10\n \n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n x_train = x_train.astype(np.float32) / 255\n x_train = np.expand_dims(x_train, -1)\n-y_train = tf.one_hot(y_train, classes)\n+y_train = tf.one_hot(y_train, num_classes)\n \n # Create the dataset and its associated one-shot iterator.\n dataset = Dataset.from_tensor_slices((x_train, y_train))\n@@ -109,5 +109,5 @@ test_model.compile(optimizer='rmsprop',\n                    metrics=['accuracy'])\n test_model.summary()\n \n-loss, acc = test_model.evaluate(x_test, y_test, classes)\n+loss, acc = test_model.evaluate(x_test, y_test, num_classes)\n print('\\nTest accuracy: {0}'.format(acc))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#94a0f34f10f2af83258ce4e1d80cf01297f0558f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 99 | Churn Cumulative: 7060 | Contributors (this commit): 73 | Commits (past 90d): 37 | Contributors (cumulative): 87 | DMM Complexity: 1.0\n\nDIFF:\n@@ -101,10 +101,10 @@ def _standardize_input_data(data, names, shapes=None,\n                         'The list you passed was: ' +\n                         str(data)[:200])\n         arrays = data\n-    elif data.__class__.__name__ == 'DataFrame':\n-        # test if data is a DataFrame, without pandas installed\n-        arrays = data.values\n     else:\n+        if data.__class__.__name__ == 'DataFrame':\n+            # test if data is a DataFrame, without pandas installed\n+            data = data.values\n         if not hasattr(data, 'shape'):\n             raise TypeError('Error when checking model ' +\n                             exception_prefix +\n\n@@ -108,13 +108,9 @@ def test_model_methods():\n \n     input_a_np = np.random.random((10, 3))\n     input_b_np = np.random.random((10, 3))\n-    input_a_df = pd.DataFrame(input_a_np)\n-    input_b_df = pd.DataFrame(input_b_np)\n \n     output_a_np = np.random.random((10, 4))\n     output_b_np = np.random.random((10, 3))\n-    output_a_df = pd.DataFrame(output_a_np)\n-    output_b_df = pd.DataFrame(output_b_np)\n \n     # training/testing doesn't work before compiling.\n     with pytest.raises(RuntimeError):\n@@ -130,8 +126,6 @@ def test_model_methods():\n                                [output_a_np, output_b_np])\n     out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                {'dense_1': output_a_np, 'dropout': output_b_np})\n-    out = model.train_on_batch([input_a_df, input_b_df],\n-                               [output_a_df, output_b_df])\n \n     # test fit\n     out = model.fit([input_a_np, input_b_np],\n@@ -141,8 +135,6 @@ def test_model_methods():\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                     {'dense_1': output_a_np, 'dropout': output_b_np},\n                     epochs=1, batch_size=4)\n-    out = model.fit([input_a_df, input_b_df],\n-                    [output_a_df, output_b_df], epochs=1, batch_size=4)\n \n     # test validation_split\n     out = model.fit([input_a_np, input_b_np],\n@@ -175,13 +167,10 @@ def test_model_methods():\n                               [output_a_np, output_b_np])\n     out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                               {'dense_1': output_a_np, 'dropout': output_b_np})\n-    out = model.test_on_batch([input_a_df, input_b_df],\n-                              [output_a_df, output_b_df])\n \n     # predict_on_batch\n     out = model.predict_on_batch([input_a_np, input_b_np])\n     out = model.predict_on_batch({'input_a': input_a_np, 'input_b': input_b_np})\n-    out = model.predict_on_batch([input_a_df, input_b_df])\n \n     # predict, evaluate\n     input_a_np = np.random.random((10, 3))\n@@ -191,9 +180,7 @@ def test_model_methods():\n     output_b_np = np.random.random((10, 3))\n \n     out = model.evaluate([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4)\n-    out = model.evaluate([input_a_df, input_b_df], [output_a_df, output_b_df], batch_size=4)\n     out = model.predict([input_a_np, input_b_np], batch_size=4)\n-    out = model.predict([input_a_df, input_b_df], batch_size=4)\n \n     # with sample_weight\n     input_a_np = np.random.random((10, 3))\n@@ -979,5 +966,85 @@ def test_trainable_weights_count_consistency():\n     assert len(w) == 0, \"Warning raised even when .compile() is called after modifying .trainable\"\n \n \n+@keras_test\n+def test_pandas_dataframe():\n+    input_a = Input(shape=(3,), name='input_a')\n+    input_b = Input(shape=(3,), name='input_b')\n+\n+    x = Dense(4, name='dense_1')(input_a)\n+    y = Dense(3, name='desne_2')(input_b)\n+\n+    model_1 = Model(inputs=input_a, outputs=x)\n+    model_2 = Model(inputs=[input_a, input_b], outputs=[x, y])\n+\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+\n+    model_1.compile(optimizer=optimizer, loss=loss)\n+    model_2.compile(optimizer=optimizer, loss=loss)\n+\n+    input_a_df = pd.DataFrame(np.random.random((10, 3)))\n+    input_b_df = pd.DataFrame(np.random.random((10, 3)))\n+\n+    output_a_df = pd.DataFrame(np.random.random((10, 4)))\n+    output_b_df = pd.DataFrame(np.random.random((10, 3)))\n+\n+    model_1.fit(input_a_df,\n+                output_a_df)\n+    model_2.fit([input_a_df, input_b_df],\n+                [output_a_df, output_b_df])\n+    model_1.fit([input_a_df],\n+                [output_a_df])\n+    model_1.fit({'input_a': input_a_df},\n+                output_a_df)\n+    model_2.fit({'input_a': input_a_df, 'input_b': input_b_df},\n+                [output_a_df, output_b_df])\n+\n+    model_1.predict(input_a_df)\n+    model_2.predict([input_a_df, input_b_df])\n+    model_1.predict([input_a_df])\n+    model_1.predict({'input_a': input_a_df})\n+    model_2.predict({'input_a': input_a_df, 'input_b': input_b_df})\n+\n+    model_1.predict_on_batch(input_a_df)\n+    model_2.predict_on_batch([input_a_df, input_b_df])\n+    model_1.predict_on_batch([input_a_df])\n+    model_1.predict_on_batch({'input_a': input_a_df})\n+    model_2.predict_on_batch({'input_a': input_a_df, 'input_b': input_b_df})\n+\n+    model_1.evaluate(input_a_df,\n+                     output_a_df)\n+    model_2.evaluate([input_a_df, input_b_df],\n+                     [output_a_df, output_b_df])\n+    model_1.evaluate([input_a_df],\n+                     [output_a_df])\n+    model_1.evaluate({'input_a': input_a_df},\n+                     output_a_df)\n+    model_2.evaluate({'input_a': input_a_df, 'input_b': input_b_df},\n+                     [output_a_df, output_b_df])\n+\n+    model_1.train_on_batch(input_a_df,\n+                           output_a_df)\n+    model_2.train_on_batch([input_a_df, input_b_df],\n+                           [output_a_df, output_b_df])\n+    model_1.train_on_batch([input_a_df],\n+                           [output_a_df])\n+    model_1.train_on_batch({'input_a': input_a_df},\n+                           output_a_df)\n+    model_2.train_on_batch({'input_a': input_a_df, 'input_b': input_b_df},\n+                           [output_a_df, output_b_df])\n+\n+    model_1.test_on_batch(input_a_df,\n+                          output_a_df)\n+    model_2.test_on_batch([input_a_df, input_b_df],\n+                          [output_a_df, output_b_df])\n+    model_1.test_on_batch([input_a_df],\n+                          [output_a_df])\n+    model_1.test_on_batch({'input_a': input_a_df},\n+                          output_a_df)\n+    model_2.test_on_batch({'input_a': input_a_df, 'input_b': input_b_df},\n+                          [output_a_df, output_b_df])\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a9685e74a0de27edbd8083963290b5c27d5ba591", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 1156 | Contributors (this commit): 12 | Commits (past 90d): 13 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -369,7 +369,7 @@ class Sequence(object):\n # Global variables to be shared across processes\n _SHARED_SEQUENCES = {}\n # We use a Value to provide unique id to different processes.\n-_SEQUENCE_COUNTER = multiprocessing.Value('i', 0)\n+_SEQUENCE_COUNTER = None\n \n \n def get_index(uid, i):\n@@ -465,9 +465,12 @@ class OrderedEnqueuer(SequenceEnqueuer):\n     def __init__(self, sequence,\n                  use_multiprocessing=False,\n                  shuffle=False):\n-        global _SEQUENCE_COUNTER\n         self.sequence = sequence\n \n+        global _SEQUENCE_COUNTER\n+        if _SEQUENCE_COUNTER is None:\n+            _SEQUENCE_COUNTER = multiprocessing.Value('i', 0)\n+\n         # Doing Multiprocessing.Value += x is not process-safe.\n         with _SEQUENCE_COUNTER.get_lock():\n             self.uid = _SEQUENCE_COUNTER.value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ae233648588141f33225fa8e6cb59df44d494db5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 5733 | Contributors (this commit): 70 | Commits (past 90d): 26 | Contributors (cumulative): 70 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1898,7 +1898,8 @@ class Model(Container):\n         return outputs\n \n     @interfaces.legacy_generator_methods_support\n-    def fit_generator(self, generator,\n+    def fit_generator(self,\n+                      generator,\n                       steps_per_epoch,\n                       epochs=1,\n                       verbose=1,\n@@ -1922,13 +1923,17 @@ class Model(Container):\n         using `use_multiprocessing=True`.\n \n         # Arguments\n-            generator: A generator or an instance of Sequence (keras.utils.Sequence)\n+            generator: A generator or an instance of `Sequence` (`keras.utils.Sequence`)\n                     object in order to avoid duplicate data\n                     when using multiprocessing.\n                 The output of the generator must be either\n-                - a tuple (inputs, targets)\n-                - a tuple (inputs, targets, sample_weights).\n-                All arrays should contain the same number of samples.\n+                - a tuple `(inputs, targets)`\n+                - a tuple `(inputs, targets, sample_weights)`.\n+                This tuple (a single output of the generator) makes a single batch.\n+                Therefore, all arrays in this tuple must have the same length (equal\n+                to the size of this batch). Different batches may have different sizes.\n+                For example, the last batch of the epoch is commonly smaller than the\n+                others, if the size of the dataset is not divisible by the batch size.\n                 The generator is expected to loop over its data\n                 indefinitely. An epoch finishes when `steps_per_epoch`\n                 batches have been seen by the model.\n@@ -1949,10 +1954,13 @@ class Model(Container):\n                 to yield from `generator` before stopping.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n-            max_queue_size: Maximum size for the generator queue\n-            workers: Maximum number of processes to spin up\n-                when using process based threading\n-            use_multiprocessing: If True, use process based threading.\n+            max_queue_size: Integer. Maximum size for the generator queue.\n+                If unspecified, `max_queue_size` will default to 10.\n+            workers: Integer. Maximum number of processes to spin up\n+                when using process based threading.\n+                If unspecified, `workers` will default to 1.\n+            use_multiprocessing: Boolean. If True, use process based threading.\n+                If unspecified, `workers` will default to False.\n                 Note that because\n                 this implementation relies on multiprocessing,\n                 you should not pass\n@@ -2087,6 +2095,7 @@ class Model(Container):\n                                          'a tuple `(x, y, sample_weight)` '\n                                          'or `(x, y)`. Found: ' +\n                                          str(generator_output))\n+\n                     if len(generator_output) == 2:\n                         x, y = generator_output\n                         sample_weight = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f3c49a1fdedcf8371eb3ca64cc4fb6e91cf844e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 557 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -306,9 +306,22 @@ if __name__ == '__main__':\n         generated_images = generator.predict(\n             [noise, sampled_labels], verbose=0)\n \n+        # prepare real images sorted by class label\n+        real_labels = y_train[(epoch - 1) * num_rows * num_classes:\n+                              epoch * num_rows * num_classes]\n+        indices = np.argsort(real_labels, axis=0)\n+        real_images = x_train[(epoch - 1) * num_rows * num_classes:\n+                              epoch * num_rows * num_classes][indices]\n+\n+        # display generated images, white separator, real images\n+        img = np.concatenate(\n+            (generated_images,\n+             np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0),\n+             real_images))\n+\n         # arrange them into a grid\n         img = (np.concatenate([r.reshape(-1, 28)\n-                               for r in np.split(generated_images, num_classes)\n+                               for r in np.split(img, 2 * num_classes + 1)\n                                ], axis=-1) * 127.5 + 127.5).astype(np.uint8)\n \n         Image.fromarray(img).save(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2979250119547754c60cf5c2df1c60708a1bd2df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 15559 | Contributors (this commit): 129 | Commits (past 90d): 40 | Contributors (cumulative): 149 | DMM Complexity: None\n\nDIFF:\n@@ -1534,7 +1534,7 @@ class Model(Container):\n                 before declaring one epoch finished and starting the\n                 next epoch. When training with input tensors such as\n                 TensorFlow data tensors, the default `None` is equal to\n-                the number of unique samples in your dataset divided by\n+                the number of samples in your dataset divided by\n                 the batch size, or 1 if that cannot be determined.\n             validation_steps: Only relevant if `steps_per_epoch`\n                 is specified. Total number of steps (batches of samples)\n@@ -1691,7 +1691,7 @@ class Model(Container):\n             steps: Integer or `None`.\n                 Total number of steps (batches of samples)\n                 before declaring the evaluation round finished.\n-                The default `None` is equal to the number of unique samples in\n+                The default `None` is equal to the number of samples in\n                 your dataset divided by the batch size.\n \n \n@@ -1940,7 +1940,7 @@ class Model(Container):\n             steps_per_epoch: Total number of steps (batches of samples)\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n-                be equal to the number of unique samples of your dataset\n+                be equal to the number of samples of your dataset\n                 divided by the batch size. Not used if using `Sequence`.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n\n@@ -917,7 +917,7 @@ class Sequential(Model):\n                 before declaring one epoch finished and starting the\n                 next epoch. When training with input tensors such as\n                 TensorFlow data tensors, the default `None` is equal to\n-                the number of unique samples in your dataset divided by\n+                the number of samples in your dataset divided by\n                 the batch size, or 1 if that cannot be determined.\n             validation_steps: Only relevant if `steps_per_epoch`\n                 is specified. Total number of steps (batches of samples)\n@@ -1146,7 +1146,7 @@ class Sequential(Model):\n             steps_per_epoch: Total number of steps (batches of samples)\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n-                be equal to the number of unique samples of your dataset\n+                be equal to the number of samples of your dataset\n                 divided by the batch size.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n@@ -1163,7 +1163,7 @@ class Sequential(Model):\n                 is a generator.\n                 Number of steps to yield from validation generator\n                 at the end of every epoch. It should typically\n-                be equal to the number of unique samples of your\n+                be equal to the number of samples of your\n                 validation dataset divided by the batch size.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67a432c273cbd65866b1d2cb1e2c62714b633b6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 44 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,8 @@ def test_to_categorical():\n     expected_shapes = [(3, num_classes),\n                        (4, 3, num_classes),\n                        (5, 4, 3, num_classes),\n-                       (3, num_classes)]\n+                       (3, num_classes),\n+                       (3, 2, num_classes)]\n     labels = [np.random.randint(0, num_classes, shape) for shape in shapes]\n     one_hots = [to_categorical(label, num_classes) for label in labels]\n     for label, one_hot, expected_shape in zip(labels,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f3edf96078d78450b985bdf3bfffe7e0c627169", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 25 | Files Changed: 3 | Hunks: 26 | Methods Changed: 15 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 110 | Churn Cumulative: 17023 | Contributors (this commit): 131 | Commits (past 90d): 55 | Contributors (cumulative): 169 | DMM Complexity: 0.5238095238095238\n\nDIFF:\n@@ -1900,7 +1900,7 @@ class Model(Container):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self,\n                       generator,\n-                      steps_per_epoch,\n+                      steps_per_epoch=None,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,\n@@ -1941,7 +1941,9 @@ class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n-                divided by the batch size. Not used if using `Sequence`.\n+                divided by the batch size.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training.\n@@ -1952,6 +1954,8 @@ class Model(Container):\n             validation_steps: Only relevant if `validation_data`\n                 is a generator. Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(validation_data)` as a number of steps.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Integer. Maximum size for the generator queue.\n@@ -2005,15 +2009,33 @@ class Model(Container):\n         if do_validation:\n             self._make_test_function()\n \n+        is_sequence = isinstance(generator, Sequence)\n+        if not is_sequence and use_multiprocessing and workers > 1:\n+            warnings.warn(\n+                UserWarning('Using a generator with `use_multiprocessing=True`'\n+                            ' and multiple workers may duplicate your data.'\n+                            ' Please consider using the`keras.utils.Sequence'\n+                            ' class.'))\n+        if steps_per_epoch is None:\n+            if is_sequence:\n+                steps_per_epoch = len(generator)\n+            else:\n+                raise ValueError('`steps_per_epoch=None` is only valid for a'\n+                                 ' generator based on the `keras.utils.Sequence`'\n+                                 ' class. Please specify `steps_per_epoch` or use'\n+                                 ' the `keras.utils.Sequence` class.')\n+\n         # python 2 has 'next', 3 has '__next__'\n         # avoid any explicit version checks\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__') or\n                    isinstance(validation_data, Sequence))\n-        if val_gen and not validation_steps:\n-            raise ValueError('When using a generator for validation data, '\n-                             'you must specify a value for '\n-                             '`validation_steps`.')\n+        if (val_gen and not isinstance(validation_data, Sequence) and\n+                not validation_steps):\n+            raise ValueError('`validation_steps=None` is only valid for a'\n+                             ' generator based on the `keras.utils.Sequence`'\n+                             ' class. Please specify `validation_steps` or use'\n+                             ' the `keras.utils.Sequence` class.')\n \n         # Prepare display labels.\n         out_labels = self._get_deduped_metrics_names()\n@@ -2059,15 +2081,6 @@ class Model(Container):\n                 val_data += [0.]\n             for cbk in callbacks:\n                 cbk.validation_data = val_data\n-        is_sequence = isinstance(generator, Sequence)\n-        if not is_sequence and use_multiprocessing and workers > 1:\n-            warnings.warn(\n-                UserWarning('Using a generator with `use_multiprocessing=True`'\n-                            ' and multiple workers may duplicate your data.'\n-                            ' Please consider using the`keras.utils.Sequence'\n-                            ' class.'))\n-        if is_sequence:\n-            steps_per_epoch = len(generator)\n         enqueuer = None\n \n         try:\n@@ -2173,7 +2186,7 @@ class Model(Container):\n         return self.history\n \n     @interfaces.legacy_generator_methods_support\n-    def evaluate_generator(self, generator, steps,\n+    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):\n@@ -2190,7 +2203,8 @@ class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n-                Not used if using Sequence.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading\n@@ -2225,8 +2239,14 @@ class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n+        if steps is None:\n             if is_sequence:\n                 steps = len(generator)\n+            else:\n+                raise ValueError('`steps=None` is only valid for a generator'\n+                                 ' based on the `keras.utils.Sequence` class.'\n+                                 ' Please specify `steps` or use the'\n+                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n \n         try:\n@@ -2288,7 +2308,7 @@ class Model(Container):\n             return averages\n \n     @interfaces.legacy_generator_methods_support\n-    def predict_generator(self, generator, steps,\n+    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,\n@@ -2305,7 +2325,8 @@ class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n-                Not used if using Sequence.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(generator)` as a number of steps.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading\n@@ -2337,8 +2358,14 @@ class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n+        if steps is None:\n             if is_sequence:\n                 steps = len(generator)\n+            else:\n+                raise ValueError('`steps=None` is only valid for a generator'\n+                                 ' based on the `keras.utils.Sequence` class.'\n+                                 ' Please specify `steps` or use the'\n+                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n \n         try:\n\n@@ -1116,7 +1116,7 @@ class Sequential(Model):\n \n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self, generator,\n-                      steps_per_epoch,\n+                      steps_per_epoch=None,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,\n@@ -1148,6 +1148,8 @@ class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is\n@@ -1165,6 +1167,8 @@ class Sequential(Model):\n                 at the end of every epoch. It should typically\n                 be equal to the number of samples of your\n                 validation dataset divided by the batch size.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(validation_data)` as a number of steps.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Maximum size for the generator queue\n@@ -1223,7 +1227,7 @@ class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n \n     @interfaces.legacy_generator_methods_support\n-    def evaluate_generator(self, generator, steps,\n+    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):\n         \"\"\"Evaluates the model on a data generator.\n@@ -1236,6 +1240,8 @@ class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.\n@@ -1263,7 +1269,7 @@ class Sequential(Model):\n                                              use_multiprocessing=use_multiprocessing)\n \n     @interfaces.legacy_generator_methods_support\n-    def predict_generator(self, generator, steps,\n+    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10, workers=1,\n                           use_multiprocessing=False, verbose=0):\n         \"\"\"Generates predictions for the input samples from a data generator.\n@@ -1275,6 +1281,8 @@ class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n+                Optional for `Sequence`: if unspecified, will use\n+                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.\n\n@@ -233,12 +233,17 @@ def test_model_methods():\n \n     # test starting from non-zero initial epoch\n     trained_epochs = []\n+    trained_batches = []\n \n     # define tracer callback\n     def on_epoch_begin(epoch, logs):\n         trained_epochs.append(epoch)\n \n-    tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin)\n+    def on_batch_begin(batch, logs):\n+        trained_batches.append(batch)\n+\n+    tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin,\n+                                on_batch_begin=on_batch_begin)\n \n     out = model.fit([input_a_np, input_b_np],\n                     [output_a_np, output_b_np], epochs=5, batch_size=4,\n@@ -380,10 +385,30 @@ def test_model_methods():\n     model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                   sample_weight_mode=None)\n     trained_epochs = []\n-    out = model.fit_generator(generator=RandomSequence(3), steps_per_epoch=12, epochs=5,\n+    trained_batches = []\n+    out = model.fit_generator(generator=RandomSequence(3), steps_per_epoch=3, epochs=5,\n                               initial_epoch=0, validation_data=RandomSequence(4),\n-                              validation_steps=12, callbacks=[tracker_cb])\n+                              validation_steps=3, callbacks=[tracker_cb])\n     assert trained_epochs == [0, 1, 2, 3, 4]\n+    assert trained_batches == list(range(3)) * 5\n+\n+    # steps_per_epoch will be equal to len of sequence if it's unspecified\n+    trained_epochs = []\n+    trained_batches = []\n+    out = model.fit_generator(generator=RandomSequence(3), epochs=5,\n+                              initial_epoch=0, validation_data=RandomSequence(4),\n+                              callbacks=[tracker_cb])\n+    assert trained_epochs == [0, 1, 2, 3, 4]\n+    assert trained_batches == list(range(12)) * 5\n+\n+    # fit_generator will throw an exception if steps is unspecified for regular generator\n+    with pytest.raises(ValueError):\n+        def gen_data():\n+            while True:\n+                yield (np.asarray([]), np.asarray([]))\n+        out = model.fit_generator(generator=gen_data(), epochs=5,\n+                                  initial_epoch=0, validation_data=gen_data(),\n+                                  callbacks=[tracker_cb])\n \n \n @pytest.mark.skipif(sys.version_info < (3,), reason='Cannot catch warnings in python 2')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86d245cf08499ad824b0f8ba19cf5d5240ae70de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 13 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 50 | Churn Cumulative: 446 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 0.6296296296296297\n\nDIFF:\n@@ -38,8 +38,8 @@ def multi_gpu_model(model, gpus):\n         model: A Keras model instance. To avoid OOM errors,\n             this model could have been built on CPU, for instance\n             (see usage example below).\n-        gpus: Integer >= 2, number of on GPUs on which to create\n-            model replicas.\n+        gpus: Integer >= 2 or list of integers, number of GPUs or\n+            list of GPU IDs on which to create model replicas.\n \n     # Returns\n         A Keras `Model` instance which can be used just like the initial\n@@ -95,14 +95,24 @@ def multi_gpu_model(model, gpus):\n     if K.backend() != 'tensorflow':\n         raise ValueError('`multi_gpu_model` is only available '\n                          'with the TensorFlow backend.')\n+    if isinstance(gpus, (list, tuple)):\n+        if len(gpus) <= 1:\n+            raise ValueError('For multi-gpu usage to be effective, '\n+                             'call `multi_gpu_model` with `len(gpus) >= 2`. '\n+                             'Received: `gpus=%s`' % gpus)\n+        num_gpus = len(gpus)\n+        target_gpu_ids = gpus\n+    else:\n         if gpus <= 1:\n             raise ValueError('For multi-gpu usage to be effective, '\n                              'call `multi_gpu_model` with `gpus >= 2`. '\n                              'Received: `gpus=%d`' % gpus)\n+        num_gpus = gpus\n+        target_gpu_ids = range(num_gpus)\n \n     import tensorflow as tf\n \n-    target_devices = ['/cpu:0'] + ['/gpu:%d' % i for i in range(gpus)]\n+    target_devices = ['/cpu:0'] + ['/gpu:%d' % i for i in target_gpu_ids]\n     available_devices = _get_available_devices()\n     available_devices = [_normalize_device_name(name) for name in available_devices]\n     for device in target_devices:\n@@ -120,7 +130,7 @@ def multi_gpu_model(model, gpus):\n         batch_size = shape[:1]\n         input_shape = shape[1:]\n         step = batch_size // parts\n-        if i == gpus - 1:\n+        if i == num_gpus - 1:\n             size = batch_size - step * i\n         else:\n             size = step\n@@ -135,9 +145,9 @@ def multi_gpu_model(model, gpus):\n \n     # Place a copy of the model on each GPU,\n     # each getting a slice of the inputs.\n-    for i in range(gpus):\n-        with tf.device('/gpu:%d' % i):\n-            with tf.name_scope('replica_%d' % i):\n+    for i, gpu_id in enumerate(target_gpu_ids):\n+        with tf.device('/gpu:%d' % gpu_id):\n+            with tf.name_scope('replica_%d' % gpu_id):\n                 inputs = []\n                 # Retrieve a slice of the input.\n                 for x in model.inputs:\n@@ -145,7 +155,7 @@ def multi_gpu_model(model, gpus):\n                     slice_i = Lambda(get_slice,\n                                      output_shape=input_shape,\n                                      arguments={'i': i,\n-                                                'parts': gpus})(x)\n+                                                'parts': num_gpus})(x)\n                     inputs.append(slice_i)\n \n                 # Apply model on slice\n\n@@ -20,6 +20,7 @@ def multi_gpu_test_simple_model():\n     output_dim = 1\n     hidden_dim = 10\n     gpus = 8\n+    target_gpu_id = [0, 2, 4]\n     epochs = 2\n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(hidden_dim,\n@@ -28,8 +29,12 @@ def multi_gpu_test_simple_model():\n \n     x = np.random.random((num_samples, input_dim))\n     y = np.random.random((num_samples, output_dim))\n-    parallel_model = multi_gpu_model(model, gpus=gpus)\n \n+    parallel_model = multi_gpu_model(model, gpus=gpus)\n+    parallel_model.compile(loss='mse', optimizer='rmsprop')\n+    parallel_model.fit(x, y, epochs=epochs)\n+\n+    parallel_model = multi_gpu_model(model, gpus=target_gpu_id)\n     parallel_model.compile(loss='mse', optimizer='rmsprop')\n     parallel_model.fit(x, y, epochs=epochs)\n \n@@ -43,6 +48,7 @@ def multi_gpu_test_multi_io_model():\n     output_dim_b = 2\n     hidden_dim = 10\n     gpus = 8\n+    target_gpu_id = [0, 2, 4]\n     epochs = 2\n \n     input_a = keras.Input((input_dim_a,))\n@@ -63,6 +69,10 @@ def multi_gpu_test_multi_io_model():\n     parallel_model.compile(loss='mse', optimizer='rmsprop')\n     parallel_model.fit([a_x, b_x], [a_y, b_y], epochs=epochs)\n \n+    parallel_model = multi_gpu_model(model, gpus=target_gpu_id)\n+    parallel_model.compile(loss='mse', optimizer='rmsprop')\n+    parallel_model.fit([a_x, b_x], [a_y, b_y], epochs=epochs)\n+\n \n def multi_gpu_test_invalid_devices():\n     input_shape = (1000, 10)\n@@ -79,6 +89,18 @@ def multi_gpu_test_invalid_devices():\n         parallel_model = multi_gpu_model(model, gpus=10)\n         parallel_model.fit(x, y, epochs=2)\n \n+    with pytest.raises(ValueError):\n+        parallel_model = multi_gpu_model(model, gpus=[0, 2, 4, 6, 8])\n+        parallel_model.fit(x, y, epochs=2)\n+\n+    with pytest.raises(ValueError):\n+        parallel_model = multi_gpu_model(model, gpus=1)\n+        parallel_model.fit(x, y, epochs=2)\n+\n+    with pytest.raises(ValueError):\n+        parallel_model = multi_gpu_model(model, gpus=[0])\n+        parallel_model.fit(x, y, epochs=2)\n+\n \n def multi_gpu_application_np_array_benchmark():\n     print('####### Xception benchmark - np i/o')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
