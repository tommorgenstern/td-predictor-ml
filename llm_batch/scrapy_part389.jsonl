{"custom_id": "scrapy#7196a11f5321d05b79c9dedc29398a200d00c911", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 5777 | Contributors (this commit): 63 | Commits (past 90d): 14 | Contributors (cumulative): 102 | DMM Complexity: None\n\nDIFF:\n@@ -339,7 +339,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n     @staticmethod\n     def _get_async_method_pair(\n         mw: Any, methodname: str\n-    ) -> None | Callable | tuple[Callable, Callable]:\n+    ) -> Callable | tuple[Callable, Callable] | None:\n         normal_method: Callable | None = getattr(mw, methodname, None)\n         methodname_async = methodname + \"_async\"\n         async_method: Callable | None = getattr(mw, methodname_async, None)\n\n@@ -57,7 +57,7 @@ class Crawler:\n     def __init__(\n         self,\n         spidercls: type[Spider],\n-        settings: None | dict[str, Any] | Settings = None,\n+        settings: dict[str, Any] | Settings | None = None,\n         init_reactor: bool = False,\n     ):\n         if isinstance(spidercls, Spider):\n\n@@ -197,7 +197,7 @@ def _get_inputs(\n \n def _value(\n     ele: InputElement | SelectElement | TextareaElement,\n-) -> tuple[str | None, None | str | MultipleSelectOptions]:\n+) -> tuple[str | None, str | MultipleSelectOptions | None]:\n     n = ele.name\n     v = ele.value\n     if ele.tag == \"select\":\n@@ -206,8 +206,8 @@ def _value(\n \n \n def _select_value(\n-    ele: SelectElement, n: str | None, v: None | str | MultipleSelectOptions\n-) -> tuple[str | None, None | str | MultipleSelectOptions]:\n+    ele: SelectElement, n: str | None, v: str | MultipleSelectOptions | None\n+) -> tuple[str | None, str | MultipleSelectOptions | None]:\n     multiple = ele.multiple\n     if v is None and not multiple:\n         # Match browser behaviour on simple select tag without options selected\n\n@@ -40,7 +40,7 @@ class MiddlewareManager:\n         self.middlewares = middlewares\n         # Only process_spider_output and process_spider_exception can be None.\n         # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.\n-        self.methods: dict[str, deque[None | Callable | tuple[Callable, Callable]]] = (\n+        self.methods: dict[str, deque[Callable | tuple[Callable, Callable] | None]] = (\n             defaultdict(deque)\n         )\n         for mw in middlewares:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e07d48cc5bfb4e07e1319334884ab420a2616c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 12 | Churn Cumulative: 562 | Contributors (this commit): 17 | Commits (past 90d): 6 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -97,10 +97,8 @@ def get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n \n def _get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n     policy = asyncio.get_event_loop_policy()\n-    if (\n-        sys.version_info >= (3, 8)\n-        and sys.platform == \"win32\"\n-        and not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy)\n+    if sys.platform == \"win32\" and not isinstance(\n+        policy, asyncio.WindowsSelectorEventLoopPolicy\n     ):\n         policy = asyncio.WindowsSelectorEventLoopPolicy()\n         asyncio.set_event_loop_policy(policy)\n\n@@ -3,7 +3,7 @@ import sys\n \n from twisted.internet import asyncioreactor\n \n-if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n+if sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n asyncioreactor.install(asyncio.get_event_loop())\n \n\n@@ -4,7 +4,7 @@ import sys\n from twisted.internet import asyncioreactor\n from twisted.python import log\n \n-if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n+if sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n asyncioreactor.install(asyncio.get_event_loop())\n \n\n@@ -4,7 +4,7 @@ import sys\n from twisted.internet import asyncioreactor\n from uvloop import Loop\n \n-if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n+if sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n asyncio.set_event_loop(Loop())\n asyncioreactor.install(asyncio.get_event_loop())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5759b3f0f2b0a45588e7ae7cd455ee5e7d4f531c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 56 | Files Changed: 3 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 57 | Churn Cumulative: 1333 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,9 +4,7 @@ import logging\n import sys\n from abc import ABCMeta, abstractmethod\n from typing import TYPE_CHECKING\n-from warnings import warn\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n@@ -90,24 +88,6 @@ class PythonRobotParser(RobotParser):\n         return self.rp.can_fetch(user_agent, url)\n \n \n-class ReppyRobotParser(RobotParser):\n-    def __init__(self, robotstxt_body: bytes, spider: Spider | None):\n-        warn(\"ReppyRobotParser is deprecated.\", ScrapyDeprecationWarning, stacklevel=2)\n-        from reppy.robots import Robots\n-\n-        self.spider: Spider | None = spider\n-        self.rp = Robots.parse(\"\", robotstxt_body)\n-\n-    @classmethod\n-    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n-        spider = None if not crawler else crawler.spider\n-        o = cls(robotstxt_body, spider)\n-        return o\n-\n-    def allowed(self, url: str | bytes, user_agent: str | bytes) -> bool:\n-        return self.rp.allowed(url, user_agent)\n-\n-\n class RerpRobotParser(RobotParser):\n     def __init__(self, robotstxt_body: bytes, spider: Spider | None):\n         from robotexclusionrulesparser import RobotExclusionRulesParser\n\n@@ -11,7 +11,7 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response, TextResponse\n from scrapy.http.request import NO_CALLBACK\n from scrapy.settings import Settings\n-from tests.test_robotstxt_interface import reppy_available, rerp_available\n+from tests.test_robotstxt_interface import rerp_available\n \n \n class RobotsTxtMiddlewareTest(unittest.TestCase):\n@@ -254,14 +254,3 @@ class RobotsTxtMiddlewareWithRerpTest(RobotsTxtMiddlewareTest):\n         self.crawler.settings.set(\n             \"ROBOTSTXT_PARSER\", \"scrapy.robotstxt.RerpRobotParser\"\n         )\n-\n-\n-class RobotsTxtMiddlewareWithReppyTest(RobotsTxtMiddlewareTest):\n-    if not reppy_available():\n-        skip = \"Reppy parser is not installed\"\n-\n-    def setUp(self):\n-        super().setUp()\n-        self.crawler.settings.set(\n-            \"ROBOTSTXT_PARSER\", \"scrapy.robotstxt.ReppyRobotParser\"\n-        )\n\n@@ -3,15 +3,6 @@ from twisted.trial import unittest\n from scrapy.robotstxt import decode_robotstxt\n \n \n-def reppy_available():\n-    # check if reppy parser is installed\n-    try:\n-        from reppy.robots import Robots  # noqa: F401\n-    except ImportError:\n-        return False\n-    return True\n-\n-\n def rerp_available():\n     # check if robotexclusionrulesparser is installed\n     try:\n@@ -169,21 +160,6 @@ class PythonRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n         raise unittest.SkipTest(\"RobotFileParser does not support wildcards.\")\n \n \n-class ReppyRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n-    if not reppy_available():\n-        skip = \"Reppy parser is not installed\"\n-\n-    def setUp(self):\n-        from scrapy.robotstxt import ReppyRobotParser\n-\n-        super()._setUp(ReppyRobotParser)\n-\n-    def test_order_based_precedence(self):\n-        raise unittest.SkipTest(\n-            \"Reppy does not support order based directives precedence.\"\n-        )\n-\n-\n class RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n     if not rerp_available():\n         skip = \"Rerp parser is not installed\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#04d0411bf7538ebe8e81771ecf9c6792c71c863b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 20 | Files Changed: 10 | Hunks: 28 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 7695 | Contributors (this commit): 49 | Commits (past 90d): 17 | Contributors (cumulative): 139 | DMM Complexity: 0.0\n\nDIFF:\n@@ -428,7 +428,7 @@ with multiples lines\n \n     @defer.inlineCallbacks\n     def test_crawl_multiple(self):\n-        runner = CrawlerRunner({\"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\"})\n+        runner = CrawlerRunner()\n         runner.crawl(\n             SimpleSpider,\n             self.mockserver.url(\"/status?n=200\"),\n\n@@ -6,6 +6,7 @@ import subprocess\n import sys\n import warnings\n from pathlib import Path\n+from typing import Any\n \n import pytest\n from packaging.version import parse as parse_version\n@@ -28,10 +29,7 @@ from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer, get_mockserver_env\n \n-# To prevent warnings.\n-BASE_SETTINGS = {\n-    \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n-}\n+BASE_SETTINGS: dict[str, Any] = {}\n \n \n def get_raw_crawler(spidercls=None, settings_dict=None):\n@@ -478,8 +476,6 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n             custom_settings = {\n                 \"LOG_LEVEL\": \"INFO\",\n                 \"LOG_FILE\": str(log_file),\n-                # settings to avoid extra warnings\n-                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             }\n \n         configure_logging()\n@@ -582,7 +578,7 @@ class NoRequestsSpider(scrapy.Spider):\n @mark.usefixtures(\"reactor_pytest\")\n class CrawlerRunnerHasSpider(unittest.TestCase):\n     def _runner(self):\n-        return CrawlerRunner({\"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\"})\n+        return CrawlerRunner()\n \n     @inlineCallbacks\n     def test_crawler_runner_bootstrap_successful(self):\n@@ -631,7 +627,6 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n             CrawlerRunner(\n                 settings={\n                     \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-                    \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n                 }\n             )\n         else:\n@@ -640,7 +635,6 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n                 runner = CrawlerRunner(\n                     settings={\n                         \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-                        \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n                     }\n                 )\n                 yield runner.crawl(NoRequestsSpider)\n\n@@ -1,3 +1,5 @@\n+import warnings\n+\n import pytest\n \n from scrapy import Request, Spider\n@@ -87,6 +89,8 @@ def test_process_request_invalid_domains():\n     allowed_domains = [\"a.example\", None, \"http:////b.example\", \"//c.example\"]\n     spider = crawler._create_spider(name=\"a\", allowed_domains=allowed_domains)\n     mw = OffsiteMiddleware.from_crawler(crawler)\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"ignore\", UserWarning)\n         mw.spider_opened(spider)\n     request = Request(\"https://a.example\")\n     assert mw.process_request(request, spider) is None\n@@ -175,6 +179,8 @@ def test_request_scheduled_invalid_domains():\n     allowed_domains = [\"a.example\", None, \"http:////b.example\", \"//c.example\"]\n     spider = crawler._create_spider(name=\"a\", allowed_domains=allowed_domains)\n     mw = OffsiteMiddleware.from_crawler(crawler)\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"ignore\", UserWarning)\n         mw.spider_opened(spider)\n     request = Request(\"https://a.example\")\n     assert mw.request_scheduled(request, spider) is None\n\n@@ -50,7 +50,6 @@ class RFPDupeFilterTest(unittest.TestCase):\n         settings = {\n             \"DUPEFILTER_DEBUG\": True,\n             \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n-            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n@@ -61,7 +60,6 @@ class RFPDupeFilterTest(unittest.TestCase):\n         settings = {\n             \"DUPEFILTER_DEBUG\": True,\n             \"DUPEFILTER_CLASS\": FromSettingsRFPDupeFilter,\n-            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n@@ -71,7 +69,6 @@ class RFPDupeFilterTest(unittest.TestCase):\n     def test_df_direct_scheduler(self):\n         settings = {\n             \"DUPEFILTER_CLASS\": DirectDupeFilter,\n-            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n@@ -176,7 +173,6 @@ class RFPDupeFilterTest(unittest.TestCase):\n             settings = {\n                 \"DUPEFILTER_DEBUG\": False,\n                 \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n-                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             }\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n@@ -205,7 +201,6 @@ class RFPDupeFilterTest(unittest.TestCase):\n             settings = {\n                 \"DUPEFILTER_DEBUG\": True,\n                 \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n-                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             }\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n@@ -243,7 +238,6 @@ class RFPDupeFilterTest(unittest.TestCase):\n         with LogCapture() as log:\n             settings = {\n                 \"DUPEFILTER_DEBUG\": True,\n-                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             }\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n\n@@ -71,7 +71,6 @@ class FileDownloadCrawlTestCase(TestCase):\n         # prepare a directory for storing files\n         self.tmpmediastore = Path(mkdtemp())\n         self.settings = {\n-            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             \"ITEM_PIPELINES\": {self.pipeline_class: 1},\n             self.store_setting_key: str(self.tmpmediastore),\n         }\n\n@@ -53,7 +53,6 @@ class MockCrawler(Crawler):\n             \"SCHEDULER_PRIORITY_QUEUE\": priority_queue_cls,\n             \"JOBDIR\": jobdir,\n             \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n-            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n         super().__init__(Spider, settings)\n         self.engine = MockEngine(downloader=MockDownloader())\n\n@@ -103,7 +103,6 @@ class SpiderLoaderTest(unittest.TestCase):\n         runner = CrawlerRunner(\n             {\n                 \"SPIDER_MODULES\": [module],\n-                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             }\n         )\n \n\n@@ -1,8 +1,8 @@\n import asyncio\n import warnings\n-from unittest import TestCase\n \n from pytest import mark\n+from twisted.trial.unittest import TestCase\n \n from scrapy.utils.defer import deferred_f_from_coro_f\n from scrapy.utils.reactor import (\n\n@@ -3,6 +3,8 @@ import unittest\n import warnings\n from collections.abc import Iterator, Mapping, MutableMapping\n \n+import pytest\n+\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.utils.datatypes import (\n@@ -90,12 +92,14 @@ class CaseInsensitiveDictMixin:\n         self.assertRaises(KeyError, d.__getitem__, \"key_LOWER\")\n         self.assertRaises(KeyError, d.__getitem__, \"key_lower\")\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_getdefault(self):\n         d = CaselessDict()\n         self.assertEqual(d.get(\"c\", 5), 5)\n         d[\"c\"] = 10\n         self.assertEqual(d.get(\"c\", 5), 10)\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_setdefault(self):\n         d = CaselessDict({\"a\": 1, \"b\": 2})\n \n@@ -212,11 +216,13 @@ class CaseInsensitiveDictTest(CaseInsensitiveDictMixin, unittest.TestCase):\n         self.assertEqual(list(iterkeys), [\"AsDf\", \"FoO\"])\n \n \n+@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n class CaselessDictTest(CaseInsensitiveDictMixin, unittest.TestCase):\n     dict_class = CaselessDict\n \n     def test_deprecation_message(self):\n         with warnings.catch_warnings(record=True) as caught:\n+            warnings.filterwarnings(\"always\", category=ScrapyDeprecationWarning)\n             self.dict_class({\"foo\": \"bar\"})\n \n             self.assertEqual(len(caught), 1)\n\n@@ -4,6 +4,8 @@ import unittest\n from pathlib import Path\n from unittest import mock\n \n+import pytest\n+\n from scrapy.item import Field, Item\n from scrapy.utils.misc import (\n     arg_to_iter,\n@@ -97,6 +99,7 @@ class UtilsMiscTestCase(unittest.TestCase):\n             list(arg_to_iter(TestItem(name=\"john\"))), [TestItem(name=\"john\")]\n         )\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_create_instance(self):\n         settings = mock.MagicMock()\n         crawler = mock.MagicMock(spec_set=[\"settings\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0523e1616d32182499a2dcd3fb98b38bd3c74041", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 430 | Contributors (this commit): 24 | Commits (past 90d): 1 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@\n #\n # All configuration values have a default; values that are commented out\n # serve to show the default.\n-\n+import os\n import sys\n from pathlib import Path\n \n@@ -186,6 +186,8 @@ html_css_files = [\n     \"custom.css\",\n ]\n \n+# Set canonical URL from the Read the Docs Domain\n+html_baseurl = os.environ.get(\"READTHEDOCS_CANONICAL_URL\", \"\")\n \n # Options for LaTeX output\n # ------------------------\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fcb5ab6cffa8cec7c731bbd81419635fa2f2ece0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 13 | Churn Cumulative: 998 | Contributors (this commit): 26 | Commits (past 90d): 4 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,10 +16,8 @@ from email.utils import formatdate\n from io import BytesIO\n from typing import IO, TYPE_CHECKING, Any\n \n-from twisted import version as twisted_version\n from twisted.internet import ssl\n from twisted.internet.defer import Deferred\n-from twisted.python.versions import Version\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n@@ -217,12 +215,9 @@ class MailSender:\n             \"heloFallback\": True,\n             \"requireAuthentication\": False,\n             \"requireTransportSecurity\": self.smtptls,\n+            \"hostname\": self.smtphost,\n         }\n \n-        # Newer versions of twisted require the hostname to use STARTTLS\n-        if twisted_version >= Version(\"twisted\", 21, 2, 0):\n-            factory_keywords[\"hostname\"] = self.smtphost\n-\n         factory = ESMTPSenderFactory(\n             self.smtpuser,\n             self.smtppass,\n\n@@ -2,11 +2,8 @@ import unittest\n from email.charset import Charset\n from io import BytesIO\n \n-from twisted import version as twisted_version\n from twisted.internet import defer\n from twisted.internet._sslverify import ClientTLSOptions\n-from twisted.internet.ssl import ClientContextFactory\n-from twisted.python.versions import Version\n \n from scrapy.mail import MailSender\n \n@@ -159,10 +156,7 @@ class MailSenderTest(unittest.TestCase):\n         )\n \n         context = factory.buildProtocol(\"test@scrapy.org\").context\n-        if twisted_version >= Version(\"twisted\", 21, 2, 0):\n         self.assertIsInstance(context, ClientTLSOptions)\n-        else:\n-            self.assertIsInstance(context, ClientContextFactory)\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5bbf8124ac6785b824b005ad1380039c963c2af1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1026 | Contributors (this commit): 40 | Commits (past 90d): 3 | Contributors (cumulative): 40 | DMM Complexity: 0.0\n\nDIFF:\n@@ -263,9 +263,7 @@ def is_generator_with_return_value(callable: Callable[..., Any]) -> bool:\n \n     def returns_none(return_node: ast.Return) -> bool:\n         value = return_node.value\n-        return (\n-            value is None or isinstance(value, ast.NameConstant) and value.value is None\n-        )\n+        return value is None or isinstance(value, ast.Constant) and value.value is None\n \n     if inspect.isgeneratorfunction(callable):\n         func = callable\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65ecd5d5287491cb0c44541252a127144438da01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 783 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,8 +15,10 @@ from typing import (\n     cast,\n )\n \n+from twisted import version as twisted_version\n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n+from twisted.python.versions import Version\n \n from scrapy.http.request import NO_CALLBACK, Request\n from scrapy.settings import Settings\n@@ -206,8 +208,8 @@ class MediaPipeline(ABC):\n             # minimize cached information for failure\n             result.cleanFailure()\n             result.frames = []\n-            result.stack = []\n-\n+            if twisted_version <= Version(\"twisted\", 24, 10, 0):\n+                result.stack = []  # type: ignore[method-assign]\n             # This code fixes a memory leak by avoiding to keep references to\n             # the Request and Response objects on the Media Pipeline cache.\n             #\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12b087b0f23d91a16c7382baeba96d5bf32ab946", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 419 | Contributors (this commit): 19 | Commits (past 90d): 5 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -33,8 +33,8 @@ class StackTraceDump:\n     def __init__(self, crawler: Crawler):\n         self.crawler: Crawler = crawler\n         try:\n-            signal.signal(signal.SIGUSR2, self.dump_stacktrace)\n-            signal.signal(signal.SIGQUIT, self.dump_stacktrace)\n+            signal.signal(signal.SIGUSR2, self.dump_stacktrace)  # type: ignore[attr-defined]\n+            signal.signal(signal.SIGQUIT, self.dump_stacktrace)  # type: ignore[attr-defined]\n         except AttributeError:\n             # win32 platforms don't support SIGUSR signals\n             pass\n@@ -70,7 +70,7 @@ class StackTraceDump:\n class Debugger:\n     def __init__(self) -> None:\n         try:\n-            signal.signal(signal.SIGUSR2, self._enter_debugger)\n+            signal.signal(signal.SIGUSR2, self._enter_debugger)  # type: ignore[attr-defined]\n         except AttributeError:\n             # win32 platforms don't support SIGUSR signals\n             pass\n\n@@ -82,7 +82,7 @@ def _embed_standard_shell(\n     else:\n         import rlcompleter  # noqa: F401\n \n-        readline.parse_and_bind(\"tab:complete\")\n+        readline.parse_and_bind(\"tab:complete\")  # type: ignore[attr-defined]\n \n     @wraps(_embed_standard_shell)\n     def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d2bdbad8c8cc5e5b4b9d3a79c94e2411a44e94be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 34 | Churn Cumulative: 473 | Contributors (this commit): 16 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,8 +6,6 @@ import pkgutil\n import sys\n import warnings\n \n-from twisted import version as _txv\n-\n # Declare top-level shortcuts\n from scrapy.http import FormRequest, Request\n from scrapy.item import Field, Item\n@@ -17,7 +15,6 @@ from scrapy.spiders import Spider\n __all__ = [\n     \"__version__\",\n     \"version_info\",\n-    \"twisted_version\",\n     \"Spider\",\n     \"Request\",\n     \"FormRequest\",\n@@ -30,7 +27,23 @@ __all__ = [\n # Scrapy and Twisted versions\n __version__ = (pkgutil.get_data(__package__, \"VERSION\") or b\"\").decode(\"ascii\").strip()\n version_info = tuple(int(v) if v.isdigit() else v for v in __version__.split(\".\"))\n-twisted_version = (_txv.major, _txv.minor, _txv.micro)\n+\n+\n+def __getattr__(name: str):\n+    if name == \"twisted_version\":\n+        import warnings\n+\n+        from twisted import version as _txv\n+\n+        from scrapy.exceptions import ScrapyDeprecationWarning\n+\n+        warnings.warn(\n+            \"The scrapy.twisted_version attribute is deprecated, use twisted.version instead\",\n+            ScrapyDeprecationWarning,\n+        )\n+        return _txv.major, _txv.minor, _txv.micro\n+\n+    raise AttributeError\n \n \n # Ignore noisy twisted deprecation warnings\n\n@@ -0,0 +1,13 @@\n+import warnings\n+\n+\n+def test_deprecated_twisted_version():\n+    with warnings.catch_warnings(record=True) as warns:\n+        from scrapy import twisted_version\n+\n+        assert twisted_version is not None\n+        assert isinstance(twisted_version, tuple)\n+        assert (\n+            \"The scrapy.twisted_version attribute is deprecated, use twisted.version instead\"\n+            in warns[0].message.args\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d85c39f5bcd728915fccece86f2b2e4ef37c0e53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 542 | Files Changed: 18 | Hunks: 100 | Methods Changed: 59 | Complexity Δ (Sum/Max): -50/6 | Churn Δ: 609 | Churn Cumulative: 29149 | Contributors (this commit): 129 | Commits (past 90d): 49 | Contributors (cumulative): 443 | DMM Complexity: 0.28763440860215056\n\nDIFF:\n@@ -89,6 +89,30 @@ def requires_uvloop(request):\n         pytest.skip(\"uvloop is not installed\")\n \n \n+@pytest.fixture(autouse=True)\n+def requires_botocore(request):\n+    if not request.node.get_closest_marker(\"requires_botocore\"):\n+        return\n+    try:\n+        import botocore\n+\n+        del botocore\n+    except ImportError:\n+        pytest.skip(\"botocore is not installed\")\n+\n+\n+@pytest.fixture(autouse=True)\n+def requires_boto3(request):\n+    if not request.node.get_closest_marker(\"requires_boto3\"):\n+        return\n+    try:\n+        import boto3\n+\n+        del boto3\n+    except ImportError:\n+        pytest.skip(\"boto3 is not installed\")\n+\n+\n def pytest_configure(config):\n     if config.getoption(\"--reactor\") == \"asyncio\":\n         install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\n@@ -162,12 +162,6 @@ class BaseRunSpiderCommand(ScrapyCommand):\n             help=\"dump scraped items into FILE, overwriting any existing file,\"\n             \" to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)\",\n         )\n-        parser.add_argument(\n-            \"-t\",\n-            \"--output-format\",\n-            metavar=\"FORMAT\",\n-            help=\"format to use for dumping items\",\n-        )\n \n     def process_options(self, args: list[str], opts: argparse.Namespace) -> None:\n         super().process_options(args, opts)\n@@ -179,8 +173,7 @@ class BaseRunSpiderCommand(ScrapyCommand):\n             feeds = feed_process_params_from_cli(\n                 self.settings,\n                 opts.output,\n-                opts.output_format,\n-                opts.overwrite_output,\n+                overwrite_output=opts.overwrite_output,\n             )\n             self.settings.set(\"FEEDS\", feeds, priority=\"cmdline\")\n \n\n@@ -3,7 +3,6 @@ from __future__ import annotations\n import logging\n import pprint\n import signal\n-import warnings\n from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n from twisted.internet.defer import (\n@@ -17,7 +16,6 @@ from zope.interface.verify import verifyClass\n from scrapy import Spider, signals\n from scrapy.addons import AddonManager\n from scrapy.core.engine import ExecutionEngine\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n from scrapy.interfaces import ISpiderLoader\n from scrapy.logformatter import LogFormatter\n@@ -142,10 +140,8 @@ class Crawler:\n         if self.crawling:\n             raise RuntimeError(\"Crawling already taking place\")\n         if self._started:\n-            warnings.warn(\n-                \"Running Crawler.crawl() more than once is deprecated.\",\n-                ScrapyDeprecationWarning,\n-                stacklevel=2,\n+            raise RuntimeError(\n+                \"Cannot run Crawler.crawl() more than once on the same instance.\"\n             )\n         self.crawling = self._started = True\n \n\n@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-import warnings\n from itertools import chain\n from logging import getLogger\n from typing import TYPE_CHECKING, Any\n@@ -15,7 +14,6 @@ from scrapy.utils._compression import (\n     _unbrotli,\n     _unzstd,\n )\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n if TYPE_CHECKING:\n@@ -72,21 +70,7 @@ class HttpCompressionMiddleware:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\n             raise NotConfigured\n-        try:\n         return cls(crawler=crawler)\n-        except TypeError:\n-            warnings.warn(\n-                \"HttpCompressionMiddleware subclasses must either modify \"\n-                \"their '__init__' method to support a 'crawler' parameter or \"\n-                \"reimplement their 'from_crawler' method.\",\n-                ScrapyDeprecationWarning,\n-            )\n-            mw = cls()\n-            mw.stats = crawler.stats\n-            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n-            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n-            crawler.signals.connect(mw.open_spider, signals.spider_opened)\n-            return mw\n \n     def open_spider(self, spider: Spider) -> None:\n         if hasattr(spider, \"download_maxsize\"):\n\n@@ -12,12 +12,10 @@ once the spider has finished crawling all regular (non-failed) pages.\n \n from __future__ import annotations\n \n-import warnings\n from logging import Logger, getLogger\n-from typing import TYPE_CHECKING, Any\n+from typing import TYPE_CHECKING\n \n-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n-from scrapy.settings import BaseSettings, Settings\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import global_object_name\n from scrapy.utils.response import response_status_message\n@@ -29,33 +27,13 @@ if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n     from scrapy.http import Response\n     from scrapy.http.request import Request\n+    from scrapy.settings import BaseSettings\n     from scrapy.spiders import Spider\n \n \n retry_logger = getLogger(__name__)\n \n \n-def backwards_compatibility_getattr(self: Any, name: str) -> tuple[Any, ...]:\n-    if name == \"EXCEPTIONS_TO_RETRY\":\n-        warnings.warn(\n-            \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n-            \"Use the RETRY_EXCEPTIONS setting instead.\",\n-            ScrapyDeprecationWarning,\n-            stacklevel=2,\n-        )\n-        return tuple(\n-            load_object(x) if isinstance(x, str) else x\n-            for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\n-        )\n-    raise AttributeError(\n-        f\"{self.__class__.__name__!r} object has no attribute {name!r}\"\n-    )\n-\n-\n-class BackwardsCompatibilityMetaclass(type):\n-    __getattr__ = backwards_compatibility_getattr\n-\n-\n def get_retry_request(\n     request: Request,\n     *,\n@@ -144,18 +122,13 @@ def get_retry_request(\n     return None\n \n \n-class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n+class RetryMiddleware:\n     def __init__(self, settings: BaseSettings):\n         if not settings.getbool(\"RETRY_ENABLED\"):\n             raise NotConfigured\n         self.max_retry_times = settings.getint(\"RETRY_TIMES\")\n         self.retry_http_codes = {int(x) for x in settings.getlist(\"RETRY_HTTP_CODES\")}\n         self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n-\n-        try:\n-            self.exceptions_to_retry = self.__getattribute__(\"EXCEPTIONS_TO_RETRY\")\n-        except AttributeError:\n-            # If EXCEPTIONS_TO_RETRY is not \"overridden\"\n         self.exceptions_to_retry = tuple(\n             load_object(x) if isinstance(x, str) else x\n             for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n@@ -199,5 +172,3 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n             max_retry_times=max_retry_times,\n             priority_adjust=priority_adjust,\n         )\n-\n-    __getattr__ = backwards_compatibility_getattr\n\n@@ -26,10 +26,8 @@ from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.extensions.postprocessing import PostProcessingManager\n from scrapy.settings import Settings\n-from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.defer import maybe_deferred_to_future\n-from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import build_from_crawler, load_object\n@@ -48,13 +46,6 @@ if TYPE_CHECKING:\n     from scrapy.exporters import BaseItemExporter\n     from scrapy.settings import BaseSettings\n \n-try:\n-    import boto3  # noqa: F401\n-\n-    IS_BOTO3_AVAILABLE = True\n-except ImportError:\n-    IS_BOTO3_AVAILABLE = False\n-\n \n logger = logging.getLogger(__name__)\n \n@@ -217,8 +208,10 @@ class S3FeedStorage(BlockingFeedStorage):\n         session_token: str | None = None,\n         region_name: str | None = None,\n     ):\n-        if not is_botocore_available():\n-            raise NotConfigured(\"missing botocore library\")\n+        try:\n+            import boto3.session\n+        except ImportError:\n+            raise NotConfigured(\"missing boto3 library\")\n         u = urlparse(uri)\n         assert u.hostname\n         self.bucketname: str = u.hostname\n@@ -229,15 +222,8 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.acl: str | None = acl\n         self.endpoint_url: str | None = endpoint_url\n         self.region_name: str | None = region_name\n-        # It can be either botocore.client.BaseClient or mypy_boto3_s3.S3Client,\n-        # there seems to be no good way to infer it statically.\n-        self.s3_client: Any\n-\n-        if IS_BOTO3_AVAILABLE:\n-            import boto3.session\n \n         boto3_session = boto3.session.Session()\n-\n         self.s3_client = boto3_session.client(\n             \"s3\",\n             aws_access_key_id=self.access_key,\n@@ -246,25 +232,6 @@ class S3FeedStorage(BlockingFeedStorage):\n             endpoint_url=self.endpoint_url,\n             region_name=self.region_name,\n         )\n-        else:\n-            warnings.warn(\n-                \"`botocore` usage has been deprecated for S3 feed \"\n-                \"export, please use `boto3` to avoid problems\",\n-                category=ScrapyDeprecationWarning,\n-            )\n-\n-            import botocore.session\n-\n-            botocore_session = botocore.session.get_session()\n-\n-            self.s3_client = botocore_session.create_client(\n-                \"s3\",\n-                aws_access_key_id=self.access_key,\n-                aws_secret_access_key=self.secret_key,\n-                aws_session_token=self.session_token,\n-                endpoint_url=self.endpoint_url,\n-                region_name=self.region_name,\n-            )\n \n         if feed_options and feed_options.get(\"overwrite\", True) is False:\n             logger.warning(\n@@ -295,17 +262,10 @@ class S3FeedStorage(BlockingFeedStorage):\n \n     def _store_in_thread(self, file: IO[bytes]) -> None:\n         file.seek(0)\n-        kwargs: dict[str, Any]\n-        if IS_BOTO3_AVAILABLE:\n-            kwargs = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n+        kwargs: dict[str, Any] = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n         self.s3_client.upload_fileobj(\n             Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs\n         )\n-        else:\n-            kwargs = {\"ACL\": self.acl} if self.acl else {}\n-            self.s3_client.put_object(\n-                Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n-            )\n         file.close()\n \n \n@@ -464,12 +424,6 @@ class FeedSlot:\n             self._exporting = False\n \n \n-_FeedSlot = create_deprecated_class(\n-    name=\"_FeedSlot\",\n-    new_class=FeedSlot,\n-)\n-\n-\n class FeedExporter:\n     _pending_deferreds: list[Deferred[None]] = []\n \n\n@@ -8,14 +8,13 @@ from __future__ import annotations\n \n import functools\n import hashlib\n-import warnings\n from contextlib import suppress\n from io import BytesIO\n from typing import TYPE_CHECKING, Any, cast\n \n from itemadapter import ItemAdapter\n \n-from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\n+from scrapy.exceptions import NotConfigured\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import (\n@@ -27,7 +26,7 @@ from scrapy.pipelines.files import (\n     _md5sum,\n )\n from scrapy.settings import Settings\n-from scrapy.utils.python import get_func_args, to_bytes\n+from scrapy.utils.python import to_bytes\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -42,18 +41,6 @@ if TYPE_CHECKING:\n     from scrapy.pipelines.media import FileInfoOrError, MediaPipeline\n \n \n-class NoimagesDrop(DropItem):\n-    \"\"\"Product with no images exception\"\"\"\n-\n-    def __init__(self, *args: Any, **kwargs: Any):\n-        warnings.warn(\n-            \"The NoimagesDrop class is deprecated\",\n-            category=ScrapyDeprecationWarning,\n-            stacklevel=2,\n-        )\n-        super().__init__(*args, **kwargs)\n-\n-\n class ImageException(FileException):\n     \"\"\"General image error exception\"\"\"\n \n@@ -120,8 +107,6 @@ class ImagesPipeline(FilesPipeline):\n             resolve(\"IMAGES_THUMBS\"), self.THUMBS\n         )\n \n-        self._deprecated_convert_image: bool | None = None\n-\n     @classmethod\n     def from_settings(cls, settings: Settings) -> Self:\n         s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n@@ -203,20 +188,6 @@ class ImagesPipeline(FilesPipeline):\n                 f\"{self.min_width}x{self.min_height})\"\n             )\n \n-        if self._deprecated_convert_image is None:\n-            self._deprecated_convert_image = \"response_body\" not in get_func_args(\n-                self.convert_image\n-            )\n-            if self._deprecated_convert_image:\n-                warnings.warn(\n-                    f\"{self.__class__.__name__}.convert_image() method overridden in a deprecated way, \"\n-                    \"overridden method does not accept response_body argument.\",\n-                    category=ScrapyDeprecationWarning,\n-                )\n-\n-        if self._deprecated_convert_image:\n-            image, buf = self.convert_image(orig_image)\n-        else:\n         image, buf = self.convert_image(\n             orig_image, response_body=BytesIO(response.body)\n         )\n@@ -226,26 +197,16 @@ class ImagesPipeline(FilesPipeline):\n             thumb_path = self.thumb_path(\n                 request, thumb_id, response=response, info=info, item=item\n             )\n-            if self._deprecated_convert_image:\n-                thumb_image, thumb_buf = self.convert_image(image, size)\n-            else:\n-                thumb_image, thumb_buf = self.convert_image(image, size, buf)\n+            thumb_image, thumb_buf = self.convert_image(image, size, response_body=buf)\n             yield thumb_path, thumb_image, thumb_buf\n \n     def convert_image(\n         self,\n         image: Image.Image,\n         size: tuple[int, int] | None = None,\n-        response_body: BytesIO | None = None,\n+        *,\n+        response_body: BytesIO,\n     ) -> tuple[Image.Image, BytesIO]:\n-        if response_body is None:\n-            warnings.warn(\n-                f\"{self.__class__.__name__}.convert_image() method called in a deprecated way, \"\n-                \"method called without response_body argument.\",\n-                category=ScrapyDeprecationWarning,\n-                stacklevel=2,\n-            )\n-\n         if image.format in (\"PNG\", \"WEBP\") and image.mode == \"RGBA\":\n             background = self._Image.new(\"RGBA\", image.size, (255, 255, 255))\n             background.paste(image, image)\n@@ -268,7 +229,7 @@ class ImagesPipeline(FilesPipeline):\n             except AttributeError:\n                 resampling_filter = self._Image.ANTIALIAS  # type: ignore[attr-defined]\n             image.thumbnail(size, resampling_filter)\n-        elif response_body is not None and image.format == \"JPEG\":\n+        elif image.format == \"JPEG\":\n             return image, response_body\n \n         buf = BytesIO()\n\n@@ -3,14 +3,13 @@ from __future__ import annotations\n import numbers\n import os\n import sys\n-import warnings\n from collections.abc import Iterable\n from configparser import ConfigParser\n from operator import itemgetter\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Callable, cast\n \n-from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n+from scrapy.exceptions import UsageError\n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n from scrapy.utils.python import without_none_values\n@@ -21,7 +20,7 @@ if TYPE_CHECKING:\n \n def build_component_list(\n     compdict: MutableMapping[Any, Any],\n-    custom: Any = None,\n+    *,\n     convert: Callable[[Any], Any] = update_classpath,\n ) -> list[Any]:\n     \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n@@ -60,19 +59,6 @@ def build_component_list(\n                     \"please provide a real number or None instead\"\n                 )\n \n-    if custom is not None:\n-        warnings.warn(\n-            \"The 'custom' attribute of build_component_list() is deprecated. \"\n-            \"Please merge its value into 'compdict' manually or change your \"\n-            \"code to use Settings.getwithbase().\",\n-            category=ScrapyDeprecationWarning,\n-            stacklevel=2,\n-        )\n-        if isinstance(custom, (list, tuple)):\n-            _check_components(custom)\n-            return type(custom)(convert(c) for c in custom)  # type: ignore[return-value]\n-        compdict.update(custom)\n-\n     _validate_values(compdict)\n     compdict = without_none_values(_map_keys(compdict))\n     return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]\n@@ -159,7 +145,7 @@ def feed_complete_default_values_from_settings(\n def feed_process_params_from_cli(\n     settings: BaseSettings,\n     output: list[str],\n-    output_format: str | None = None,\n+    *,\n     overwrite_output: list[str] | None = None,\n ) -> dict[str, dict[str, Any]]:\n     \"\"\"\n@@ -186,37 +172,9 @@ def feed_process_params_from_cli(\n             raise UsageError(\n                 \"Please use only one of -o/--output and -O/--overwrite-output\"\n             )\n-        if output_format:\n-            raise UsageError(\n-                \"-t/--output-format is a deprecated command line option\"\n-                \" and does not work in combination with -O/--overwrite-output.\"\n-                \" To specify a format please specify it after a colon at the end of the\"\n-                \" output URI (i.e. -O <URI>:<FORMAT>).\"\n-                \" Example working in the tutorial: \"\n-                \"scrapy crawl quotes -O quotes.json:json\"\n-            )\n         output = overwrite_output\n         overwrite = True\n \n-    if output_format:\n-        if len(output) == 1:\n-            check_valid_format(output_format)\n-            message = (\n-                \"The -t/--output-format command line option is deprecated in favor of \"\n-                \"specifying the output format within the output URI using the -o/--output or the\"\n-                \" -O/--overwrite-output option (i.e. -o/-O <URI>:<FORMAT>). See the documentation\"\n-                \" of the -o or -O option or the following examples for more information. \"\n-                \"Examples working in the tutorial: \"\n-                \"scrapy crawl quotes -o quotes.csv:csv   or   \"\n-                \"scrapy crawl quotes -O quotes.json:json\"\n-            )\n-            warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n-            return {output[0]: {\"format\": output_format}}\n-        raise UsageError(\n-            \"The -t command-line option cannot be used if multiple output \"\n-            \"URIs are specified\"\n-        )\n-\n     result: dict[str, dict[str, Any]] = {}\n     for element in output:\n         try:\n\n@@ -4,12 +4,11 @@ import asyncio\n import sys\n from contextlib import suppress\n from typing import TYPE_CHECKING, Any, Generic, TypeVar\n-from warnings import catch_warnings, filterwarnings, warn\n+from warnings import catch_warnings, filterwarnings\n \n from twisted.internet import asyncioreactor, error\n from twisted.internet.base import DelayedCall\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import load_object\n \n if TYPE_CHECKING:\n@@ -79,22 +78,6 @@ def set_asyncio_event_loop_policy() -> None:\n     _get_asyncio_event_loop_policy()\n \n \n-def get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n-    warn(\n-        \"Call to deprecated function \"\n-        \"scrapy.utils.reactor.get_asyncio_event_loop_policy().\\n\"\n-        \"\\n\"\n-        \"Please use get_event_loop, new_event_loop and set_event_loop\"\n-        \" from asyncio instead, as the corresponding policy methods may lead\"\n-        \" to unexpected behaviour.\\n\"\n-        \"This function is replaced by set_asyncio_event_loop_policy and\"\n-        \" is meant to be used only when the reactor is being installed.\",\n-        category=ScrapyDeprecationWarning,\n-        stacklevel=2,\n-    )\n-    return _get_asyncio_event_loop_policy()\n-\n-\n def _get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n     policy = asyncio.get_event_loop_policy()\n     if sys.platform == \"win32\" and not isinstance(\n\n@@ -30,17 +30,9 @@ if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n \n \n-def _serialize_headers(headers: Iterable[bytes], request: Request) -> Iterable[bytes]:\n-    for header in headers:\n-        if header in request.headers:\n-            yield header\n-            yield from request.headers.getlist(header)\n-\n-\n _fingerprint_cache: WeakKeyDictionary[\n     Request, dict[tuple[tuple[bytes, ...] | None, bool], bytes]\n-]\n-_fingerprint_cache = WeakKeyDictionary()\n+] = WeakKeyDictionary()\n \n \n def fingerprint(\n\n@@ -8,7 +8,6 @@ import warnings\n from pathlib import Path\n from typing import Any\n \n-import pytest\n from packaging.version import parse as parse_version\n from pexpect.popen_spawn import PopenSpawn\n from pytest import mark, raises\n@@ -82,13 +81,10 @@ class CrawlerTestCase(BaseCrawlerTest):\n             Crawler(DefaultSpider())\n \n     @inlineCallbacks\n-    def test_crawler_crawl_twice_deprecated(self):\n+    def test_crawler_crawl_twice_unsupported(self):\n         crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n         yield crawler.crawl()\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=r\"Running Crawler.crawl\\(\\) more than once is deprecated\",\n-        ):\n+        with raises(RuntimeError, match=\"more than once on the same instance\"):\n             yield crawler.crawl()\n \n     def test_get_addon(self):\n\n@@ -8,6 +8,7 @@ from pathlib import Path\n from tempfile import mkdtemp, mkstemp\n from unittest import SkipTest, mock\n \n+import pytest\n from testfixtures import LogCapture\n from twisted.cred import checkers, credentials, portal\n from twisted.internet import defer, error, reactor\n@@ -32,7 +33,7 @@ from scrapy.responsetypes import responsetypes\n from scrapy.spiders import Spider\n from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes\n-from scrapy.utils.test import get_crawler, skip_if_no_boto\n+from scrapy.utils.test import get_crawler\n from tests import NON_EXISTING_RESOLVABLE\n from tests.mockserver import (\n     Echo,\n@@ -824,9 +825,9 @@ class HttpDownloadHandlerMock:\n         return request\n \n \n+@pytest.mark.requires_botocore\n class S3AnonTestCase(unittest.TestCase):\n     def setUp(self):\n-        skip_if_no_boto()\n         crawler = get_crawler()\n         self.s3reqh = build_from_crawler(\n             S3DownloadHandler,\n@@ -845,6 +846,7 @@ class S3AnonTestCase(unittest.TestCase):\n         self.assertEqual(httpreq.url, \"http://aws-publicdatasets.s3.amazonaws.com/\")\n \n \n+@pytest.mark.requires_botocore\n class S3TestCase(unittest.TestCase):\n     download_handler_cls: type = S3DownloadHandler\n \n@@ -856,7 +858,6 @@ class S3TestCase(unittest.TestCase):\n     AWS_SECRET_ACCESS_KEY = \"uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o\"\n \n     def setUp(self):\n-        skip_if_no_boto()\n         crawler = get_crawler()\n         s3reqh = build_from_crawler(\n             S3DownloadHandler,\n\n@@ -3,7 +3,6 @@ from io import BytesIO\n from logging import WARNING\n from pathlib import Path\n from unittest import SkipTest, TestCase\n-from warnings import catch_warnings\n \n from testfixtures import LogCapture\n from w3lib.encoding import resolve_encoding\n@@ -12,7 +11,7 @@ from scrapy.downloadermiddlewares.httpcompression import (\n     ACCEPTED_ENCODINGS,\n     HttpCompressionMiddleware,\n )\n-from scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import HtmlResponse, Request, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.spiders import Spider\n@@ -700,29 +699,3 @@ class HttpCompressionTest(TestCase):\n         except ImportError:\n             raise SkipTest(\"no zstd support (zstandard)\")\n         self._test_download_warnsize_request_meta(\"zstd\")\n-\n-\n-class HttpCompressionSubclassTest(TestCase):\n-    def test_init_missing_stats(self):\n-        class HttpCompressionMiddlewareSubclass(HttpCompressionMiddleware):\n-            def __init__(self):\n-                super().__init__()\n-\n-        crawler = get_crawler(Spider)\n-        with catch_warnings(record=True) as caught_warnings:\n-            HttpCompressionMiddlewareSubclass.from_crawler(crawler)\n-        messages = tuple(\n-            str(warning.message)\n-            for warning in caught_warnings\n-            if warning.category is ScrapyDeprecationWarning\n-        )\n-        self.assertEqual(\n-            messages,\n-            (\n-                (\n-                    \"HttpCompressionMiddleware subclasses must either modify \"\n-                    \"their '__init__' method to support a 'crawler' parameter \"\n-                    \"or reimplement their 'from_crawler' method.\"\n-                ),\n-            ),\n-        )\n\n@@ -1,6 +1,5 @@\n import logging\n import unittest\n-import warnings\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -122,37 +121,6 @@ class RetryTest(unittest.TestCase):\n         req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n         self._test_retry_exception(req, exc(\"foo\"), mw)\n \n-    def test_exception_to_retry_custom_middleware(self):\n-        exc = ValueError\n-\n-        with warnings.catch_warnings(record=True) as warns:\n-\n-            class MyRetryMiddleware(RetryMiddleware):\n-                EXCEPTIONS_TO_RETRY = RetryMiddleware.EXCEPTIONS_TO_RETRY + (exc,)\n-\n-            self.assertEqual(len(warns), 1)\n-\n-        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n-        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n-        req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n-        assert isinstance(req, Request)\n-        self.assertEqual(req.meta[\"retry_times\"], 1)\n-\n-    def test_exception_to_retry_custom_middleware_self(self):\n-        class MyRetryMiddleware(RetryMiddleware):\n-            def process_exception(self, request, exception, spider):\n-                if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n-                    return self._retry(request, exception, spider)\n-\n-        exc = OSError\n-        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n-        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n-        with warnings.catch_warnings(record=True) as warns:\n-            req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n-        assert isinstance(req, Request)\n-        self.assertEqual(req.meta[\"retry_times\"], 1)\n-        self.assertEqual(len(warns), 1)\n-\n     def _test_retry_exception(self, req, exception, mw=None):\n         if mw is None:\n             mw = self.mw\n\n@@ -37,7 +37,6 @@ from scrapy import signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.exporters import CsvItemExporter, JsonItemExporter\n from scrapy.extensions.feedexport import (\n-    IS_BOTO3_AVAILABLE,\n     BlockingFeedStorage,\n     FeedExporter,\n     FeedSlot,\n@@ -50,7 +49,7 @@ from scrapy.extensions.feedexport import (\n )\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n-from scrapy.utils.test import get_crawler, mock_google_cloud_storage, skip_if_no_boto\n+from scrapy.utils.test import get_crawler, mock_google_cloud_storage\n from tests.mockserver import MockFTPServer, MockServer\n from tests.spiders import ItemSpider\n \n@@ -240,10 +239,8 @@ class BlockingFeedStorageTest(unittest.TestCase):\n         self.assertRaises(OSError, b.open, spider=spider)\n \n \n+@pytest.mark.requires_boto3\n class S3FeedStorageTest(unittest.TestCase):\n-    def setUp(self):\n-        skip_if_no_boto()\n-\n     def test_parse_credentials(self):\n         aws_credentials = {\n             \"AWS_ACCESS_KEY_ID\": \"settings_key\",\n@@ -292,38 +289,12 @@ class S3FeedStorageTest(unittest.TestCase):\n \n         file = mock.MagicMock()\n \n-        if IS_BOTO3_AVAILABLE:\n         storage.s3_client = mock.MagicMock()\n         yield storage.store(file)\n         self.assertEqual(\n             storage.s3_client.upload_fileobj.call_args,\n             mock.call(Bucket=bucket, Key=key, Fileobj=file),\n         )\n-        else:\n-            from botocore.stub import Stubber\n-\n-            with Stubber(storage.s3_client) as stub:\n-                stub.add_response(\n-                    \"put_object\",\n-                    expected_params={\n-                        \"Body\": file,\n-                        \"Bucket\": bucket,\n-                        \"Key\": key,\n-                    },\n-                    service_response={},\n-                )\n-\n-                yield storage.store(file)\n-\n-                stub.assert_no_pending_responses()\n-                self.assertEqual(\n-                    file.method_calls,\n-                    [\n-                        mock.call.seek(0),\n-                        # The call to read does not happen with Stubber\n-                        mock.call.close(),\n-                    ],\n-                )\n \n     def test_init_without_acl(self):\n         storage = S3FeedStorage(\"s3://mybucket/export.csv\", \"access_key\", \"secret_key\")\n@@ -459,14 +430,11 @@ class S3FeedStorageTest(unittest.TestCase):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        if IS_BOTO3_AVAILABLE:\n         acl = (\n             storage.s3_client.upload_fileobj.call_args[1]\n             .get(\"ExtraArgs\", {})\n             .get(\"ACL\")\n         )\n-        else:\n-            acl = storage.s3_client.put_object.call_args[1].get(\"ACL\")\n         self.assertIsNone(acl)\n \n     @defer.inlineCallbacks\n@@ -480,10 +448,7 @@ class S3FeedStorageTest(unittest.TestCase):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        if IS_BOTO3_AVAILABLE:\n         acl = storage.s3_client.upload_fileobj.call_args[1][\"ExtraArgs\"][\"ACL\"]\n-        else:\n-            acl = storage.s3_client.put_object.call_args[1][\"ACL\"]\n         self.assertEqual(acl, \"custom-acl\")\n \n     def test_overwrite_default(self):\n@@ -2647,9 +2612,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 12\n         )\n \n+    @pytest.mark.requires_boto3\n     @defer.inlineCallbacks\n     def test_s3_export(self):\n-        skip_if_no_boto()\n         bucket = \"mybucket\"\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n\n@@ -11,6 +11,7 @@ from unittest import mock\n from urllib.parse import urlparse\n \n import attr\n+import pytest\n from itemadapter import ItemAdapter\n from twisted.internet import defer\n from twisted.trial import unittest\n@@ -30,7 +31,6 @@ from scrapy.utils.test import (\n     get_crawler,\n     get_ftp_content_and_delete,\n     get_gcs_content_and_delete,\n-    skip_if_no_boto,\n )\n from tests.mockserver import MockFTPServer\n \n@@ -507,11 +507,10 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n         self.assertEqual(fs_store.basedir, str(path))\n \n \n+@pytest.mark.requires_botocore\n class TestS3FilesStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n-        skip_if_no_boto()\n-\n         bucket = \"mybucket\"\n         key = \"export.csv\"\n         uri = f\"s3://{bucket}/{key}\"\n@@ -557,8 +556,6 @@ class TestS3FilesStore(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_stat(self):\n-        skip_if_no_boto()\n-\n         bucket = \"mybucket\"\n         key = \"export.csv\"\n         uri = f\"s3://{bucket}/{key}\"\n\n@@ -1,24 +1,19 @@\n from __future__ import annotations\n \n import dataclasses\n-import hashlib\n import io\n import random\n-import warnings\n from shutil import rmtree\n from tempfile import mkdtemp\n-from unittest.mock import patch\n \n import attr\n from itemadapter import ItemAdapter\n from twisted.trial import unittest\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.item import Field, Item\n-from scrapy.pipelines.images import ImageException, ImagesPipeline, NoimagesDrop\n+from scrapy.pipelines.images import ImageException, ImagesPipeline\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_bytes\n \n skip_pillow: str | None\n try:\n@@ -159,7 +154,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         with self.assertRaises(ImageException):\n             next(self.pipeline.get_images(response=resp3, request=req, info=object()))\n \n-    def test_get_images_new(self):\n+    def test_get_images(self):\n         self.pipeline.min_width = 0\n         self.pipeline.min_height = 0\n         self.pipeline.thumbs = {\"small\": (20, 20)}\n@@ -185,101 +180,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         self.assertEqual(thumb_img, thumb_img)\n         self.assertEqual(orig_thumb_buf.getvalue(), thumb_buf.getvalue())\n \n-    def test_get_images_old(self):\n-        self.pipeline.thumbs = {\"small\": (20, 20)}\n-        orig_im, buf = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n-        resp = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf.getvalue())\n-        req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n-\n-        def overridden_convert_image(image, size=None):\n-            im, buf = _create_image(\"JPEG\", \"RGB\", (50, 50), (0, 0, 0))\n-            return im, buf\n-\n-        with patch.object(self.pipeline, \"convert_image\", overridden_convert_image):\n-            with warnings.catch_warnings(record=True) as w:\n-                warnings.simplefilter(\"always\")\n-                get_images_gen = self.pipeline.get_images(\n-                    response=resp, request=req, info=object()\n-                )\n-                path, new_im, new_buf = next(get_images_gen)\n-                self.assertEqual(\n-                    path, \"full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\"\n-                )\n-                self.assertEqual(orig_im.mode, new_im.mode)\n-                self.assertEqual(orig_im.getcolors(), new_im.getcolors())\n-                self.assertEqual(buf.getvalue(), new_buf.getvalue())\n-\n-                thumb_path, thumb_img, thumb_buf = next(get_images_gen)\n-                self.assertEqual(\n-                    thumb_path,\n-                    \"thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg\",\n-                )\n-                self.assertEqual(orig_im.mode, thumb_img.mode)\n-                self.assertEqual(orig_im.getcolors(), thumb_img.getcolors())\n-                self.assertEqual(buf.getvalue(), thumb_buf.getvalue())\n-\n-                expected_warning_msg = (\n-                    \".convert_image() method overridden in a deprecated way, \"\n-                    \"overridden method does not accept response_body argument.\"\n-                )\n-                self.assertEqual(\n-                    len(\n-                        [\n-                            warning\n-                            for warning in w\n-                            if expected_warning_msg in str(warning.message)\n-                        ]\n-                    ),\n-                    1,\n-                )\n-\n-    def test_convert_image_old(self):\n-        # tests for old API\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter(\"always\")\n-            SIZE = (100, 100)\n-            # straight forward case: RGB and JPEG\n-            COLOUR = (0, 127, 255)\n-            im, _ = _create_image(\"JPEG\", \"RGB\", SIZE, COLOUR)\n-            converted, _ = self.pipeline.convert_image(im)\n-            self.assertEqual(converted.mode, \"RGB\")\n-            self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n-\n-            # check that thumbnail keep image ratio\n-            thumbnail, _ = self.pipeline.convert_image(converted, size=(10, 25))\n-            self.assertEqual(thumbnail.mode, \"RGB\")\n-            self.assertEqual(thumbnail.size, (10, 10))\n-\n-            # transparency case: RGBA and PNG\n-            COLOUR = (0, 127, 255, 50)\n-            im, _ = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n-            converted, _ = self.pipeline.convert_image(im)\n-            self.assertEqual(converted.mode, \"RGB\")\n-            self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n-\n-            # transparency case with palette: P and PNG\n-            COLOUR = (0, 127, 255, 50)\n-            im, _ = _create_image(\"PNG\", \"RGBA\", SIZE, COLOUR)\n-            im = im.convert(\"P\")\n-            converted, _ = self.pipeline.convert_image(im)\n-            self.assertEqual(converted.mode, \"RGB\")\n-            self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n-\n-            # ensure that we received deprecation warnings\n-            expected_warning_msg = \".convert_image() method called in a deprecated way\"\n-            self.assertTrue(\n-                len(\n-                    [\n-                        warning\n-                        for warning in w\n-                        if expected_warning_msg in str(warning.message)\n-                    ]\n-                )\n-                == 4\n-            )\n-\n-    def test_convert_image_new(self):\n-        # tests for new API\n+    def test_convert_image(self):\n         SIZE = (100, 100)\n         # straight forward case: RGB and JPEG\n         COLOUR = (0, 127, 255)\n@@ -313,19 +214,6 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n \n-class DeprecatedImagesPipeline(ImagesPipeline):\n-    def file_key(self, url):\n-        return self.image_key(url)\n-\n-    def image_key(self, url):\n-        image_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n-        return f\"empty/{image_guid}.jpg\"\n-\n-    def thumb_key(self, url, thumb_id):\n-        thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n-        return f\"thumbsup/{thumb_id}/{thumb_guid}.jpg\"\n-\n-\n class ImagesPipelineTestCaseFieldsMixin:\n     skip = skip_pillow\n \n@@ -627,23 +515,6 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n             self.assertEqual(getattr(pipeline_cls, pipe_attr.lower()), expected_value)\n \n \n-class NoimagesDropTestCase(unittest.TestCase):\n-    def test_deprecation_warning(self):\n-        arg = \"\"\n-        with warnings.catch_warnings(record=True) as w:\n-            NoimagesDrop(arg)\n-            self.assertEqual(len(w), 1)\n-            self.assertEqual(w[0].category, ScrapyDeprecationWarning)\n-        with warnings.catch_warnings(record=True) as w:\n-\n-            class SubclassedNoimagesDrop(NoimagesDrop):\n-                pass\n-\n-            SubclassedNoimagesDrop(arg)\n-            self.assertEqual(len(w), 1)\n-            self.assertEqual(w[0].category, ScrapyDeprecationWarning)\n-\n-\n def _create_image(format, *a, **kw):\n     buf = io.BytesIO()\n     Image.new(*a, **kw).save(buf, format)\n\n@@ -1,9 +1,6 @@\n import unittest\n-import warnings\n \n-import pytest\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n+from scrapy.exceptions import UsageError\n from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.conf import (\n     arglist_to_dict,\n@@ -20,50 +17,6 @@ class BuildComponentListTest(unittest.TestCase):\n             build_component_list(d, convert=lambda x: x), [\"one\", \"four\", \"three\"]\n         )\n \n-    def test_backward_compatible_build_dict(self):\n-        base = {\"one\": 1, \"two\": 2, \"three\": 3, \"five\": 5, \"six\": None}\n-        custom = {\"two\": None, \"three\": 8, \"four\": 4}\n-        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n-            self.assertEqual(\n-                build_component_list(base, custom, convert=lambda x: x),\n-                [\"one\", \"four\", \"five\", \"three\"],\n-            )\n-\n-    def test_return_list(self):\n-        custom = [\"a\", \"b\", \"c\"]\n-        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n-            self.assertEqual(\n-                build_component_list(None, custom, convert=lambda x: x), custom\n-            )\n-\n-    def test_map_dict(self):\n-        custom = {\"one\": 1, \"two\": 2, \"three\": 3}\n-        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n-            self.assertEqual(\n-                build_component_list({}, custom, convert=lambda x: x.upper()),\n-                [\"ONE\", \"TWO\", \"THREE\"],\n-            )\n-\n-    def test_map_list(self):\n-        custom = [\"a\", \"b\", \"c\"]\n-        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n-            self.assertEqual(\n-                build_component_list(None, custom, lambda x: x.upper()), [\"A\", \"B\", \"C\"]\n-            )\n-\n-    def test_duplicate_components_in_dict(self):\n-        duplicate_dict = {\"one\": 1, \"two\": 2, \"ONE\": 4}\n-        with self.assertRaises(ValueError):\n-            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n-                build_component_list({}, duplicate_dict, convert=lambda x: x.lower())\n-\n-    def test_duplicate_components_in_list(self):\n-        duplicate_list = [\"a\", \"b\", \"a\"]\n-        with self.assertRaises(ValueError) as cm:\n-            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n-                build_component_list(None, duplicate_list, convert=lambda x: x)\n-        self.assertIn(str(duplicate_list), str(cm.exception))\n-\n     def test_duplicate_components_in_basesettings(self):\n         # Higher priority takes precedence\n         duplicate_bs = BaseSettings({\"one\": 1, \"two\": 2}, priority=0)\n@@ -92,11 +45,6 @@ class BuildComponentListTest(unittest.TestCase):\n             \"c\": 22222222222222222222,\n         }\n         self.assertEqual(build_component_list(d, convert=lambda x: x), [\"b\", \"c\", \"a\"])\n-        # raise exception for invalid values\n-        d = {\"one\": \"5\"}\n-        with self.assertRaises(ValueError):\n-            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n-                build_component_list({}, d, convert=lambda x: x)\n \n \n class UtilsConfTestCase(unittest.TestCase):\n@@ -115,7 +63,6 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             feed_process_params_from_cli,\n             settings,\n             [\"items.dat\"],\n-            \"noformat\",\n         )\n \n     def test_feed_export_config_mismatch(self):\n@@ -125,18 +72,8 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             feed_process_params_from_cli,\n             settings,\n             [\"items1.dat\", \"items2.dat\"],\n-            \"noformat\",\n         )\n \n-    def test_feed_export_config_backward_compatible(self):\n-        with warnings.catch_warnings(record=True) as cw:\n-            settings = Settings()\n-            self.assertEqual(\n-                {\"items.dat\": {\"format\": \"csv\"}},\n-                feed_process_params_from_cli(settings, [\"items.dat\"], \"csv\"),\n-            )\n-            self.assertEqual(cw[0].category, ScrapyDeprecationWarning)\n-\n     def test_feed_export_config_explicit_formats(self):\n         settings = Settings()\n         self.assertEqual(\n@@ -174,7 +111,9 @@ class FeedExportConfigTestCase(unittest.TestCase):\n         settings = Settings()\n         self.assertEqual(\n             {\"output.json\": {\"format\": \"json\", \"overwrite\": True}},\n-            feed_process_params_from_cli(settings, [], None, [\"output.json\"]),\n+            feed_process_params_from_cli(\n+                settings, [], overwrite_output=[\"output.json\"]\n+            ),\n         )\n \n     def test_output_and_overwrite_output(self):\n@@ -182,8 +121,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             feed_process_params_from_cli(\n                 Settings(),\n                 [\"output1.json\"],\n-                None,\n-                [\"output2.json\"],\n+                overwrite_output=[\"output2.json\"],\n             )\n \n     def test_feed_complete_default_values_from_settings_empty(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
