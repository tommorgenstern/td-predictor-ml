{"custom_id": "keras#088ab64c9ab6335fa56883f8ab2fcb764cbe4dcd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 202 | Lines Deleted: 24 | Files Changed: 2 | Hunks: 20 | Methods Changed: 18 | Complexity Δ (Sum/Max): 24/16 | Churn Δ: 226 | Churn Cumulative: 252 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,9 @@\n+import io\n+\n from keras.api_export import keras_export\n from keras.layers import Layer\n+from keras.ops import convert_to_numpy\n+from keras.ops import convert_to_tensor\n \n \n @keras_export(\"keras.layers.TorchModuleWrapper\")\n@@ -69,8 +73,8 @@ class TorchModuleWrapper(Layer):\n     ```\n     \"\"\"\n \n-    def __init__(self, module, name=None):\n-        super().__init__(name=name)\n+    def __init__(self, module, name=None, *args, **kwargs):\n+        super().__init__(name=name, *args, **kwargs)\n         import torch.nn as nn\n \n         if (\n@@ -104,3 +108,40 @@ class TorchModuleWrapper(Layer):\n \n     def call(self, *args, **kwargs):\n         return self.module.forward(*args, **kwargs)\n+\n+    def save_own_variables(self, store):\n+        \"\"\"Saves model's state from `state_dict`.\n+        `model.parameters` excludes some of model's state like\n+        `BatchNorm` mean and variance. So, use `state_dict` to obtain\n+        all of model's state.\n+        \"\"\"\n+        state_dict = self.module.state_dict()\n+        for key in state_dict.keys():\n+            store[key] = convert_to_numpy(state_dict[key])\n+\n+    def load_own_variables(self, store):\n+        \"\"\"Loads model's state via `state_dict`.\"\"\"\n+        state_dict = {}\n+        for key in store.keys():\n+            if isinstance(key, bytes):\n+                key = key.decode()\n+            state_dict[key] = convert_to_tensor(store[key])\n+        self.module.load_state_dict(state_dict)\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        import torch\n+\n+        buffer = io.BytesIO()\n+        torch.save(self.module, buffer)\n+        config = {\"module\": buffer.getvalue()}\n+        return {**base_config, **config}\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        import torch\n+\n+        if \"module\" in config:\n+            buffer = io.BytesIO(config[\"module\"])\n+            config[\"module\"] = torch.load(buffer)\n+        return cls(**config)\n\n@@ -1,56 +1,193 @@\n+import os\n+\n import numpy as np\n import pytest\n import torch\n+from absl.testing import parameterized\n \n from keras import backend\n from keras import layers\n from keras import models\n+from keras import saving\n from keras import testing\n from keras.utils.torch_utils import TorchModuleWrapper\n \n \n class Classifier(models.Model):\n-    def __init__(self, *args, **kwargs):\n+    def __init__(\n+        self, use_batch_norm=False, num_torch_layers=1, *args, **kwargs\n+    ):\n         super().__init__(*args, **kwargs)\n-        self.fc = TorchModuleWrapper(torch.nn.Linear(2, 4))\n+        self.use_batch_norm = use_batch_norm\n+        self.num_torch_layers = num_torch_layers\n+        self.torch_wrappers = []\n+        for _ in range(num_torch_layers):\n+            modules = [torch.nn.Linear(2, 2)]\n+            if use_batch_norm:\n+                modules.append(torch.nn.BatchNorm1d(2))\n+            torch_model = torch.nn.Sequential(*modules)\n+            self.torch_wrappers.append(TorchModuleWrapper(torch_model))\n+        self.fc = layers.Dense(1)\n \n     def call(self, x):\n+        for wrapper in self.torch_wrappers:\n+            x = wrapper(x)\n         return self.fc(x)\n \n+    def get_config(self):\n+        config = super().get_config()\n+        config[\"use_batch_norm\"] = self.use_batch_norm\n+        config[\"num_torch_layers\"] = self.num_torch_layers\n+        return config\n+\n \n class ClassifierWithNoSpecialCasing(models.Model):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n-        self.fc = torch.nn.Linear(2, 4)\n-        self.fc2 = layers.Dense(2)\n+        self.fc1 = torch.nn.Linear(2, 4)\n+        self.bn1 = torch.nn.BatchNorm1d(4)\n+        self.fc2 = torch.nn.Linear(4, 4)\n+        self.fc3 = layers.Dense(2)\n \n     def call(self, x):\n-        return self.fc(self.fc2(x))\n+        return self.fc3(self.fc2(self.bn1(self.fc1(x))))\n \n \n-class TorchUtilsTest(testing.TestCase):\n @pytest.mark.skipif(\n     backend.backend() != \"torch\", reason=\"Requires torch backend\"\n )\n-    def test_basic_usage(self):\n-        model = Classifier()\n-        self.assertEqual(len(model.layers), 1)\n-        self.assertEqual(len(model.trainable_weights), 2)\n+class TorchUtilsTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        {\"use_batch_norm\": False, \"num_torch_layers\": 1},\n+        {\"use_batch_norm\": True, \"num_torch_layers\": 1},\n+    )\n+    def test_basic_usage(self, use_batch_norm, num_torch_layers):\n+        model = Classifier(use_batch_norm, num_torch_layers)\n+        self.assertEqual(len(model.layers), 2)\n+        # Linear - Weights, bias, BN - beta, gamma\n+        torch_trainable_count = 0\n+        for i, layer in zip(range(num_torch_layers), model.torch_wrappers):\n+            layer_trainable_count = 2\n+            if use_batch_norm:\n+                layer_trainable_count += 2\n+            self.assertEqual(\n+                len(layer.trainable_weights), layer_trainable_count\n+            )\n+            torch_trainable_count += layer_trainable_count\n         model(np.random.random((3, 2)))\n-        model.compile(optimizer=\"sgd\", loss=\"mse\")\n-        model.fit(np.random.random((3, 2)), np.random.random((3, 4)))\n-\n-    @pytest.mark.skipif(\n-        backend.backend() != \"torch\", reason=\"Requires torch backend\"\n+        self.assertEqual(len(model.layers), 2 * num_torch_layers)\n+        self.assertEqual(\n+            len(model.trainable_weights), torch_trainable_count + 2\n         )\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(np.random.random((3, 2)), np.random.random((3, 1)))\n+\n     def test_module_autowrapping(self):\n         model = ClassifierWithNoSpecialCasing()\n-        self.assertIsInstance(model.fc, TorchModuleWrapper)\n-        self.assertFalse(isinstance(model.fc2, TorchModuleWrapper))\n-        self.assertEqual(len(model.fc.trainable_weights), 2)\n-        model(np.random.random((3, 2)))\n-        self.assertEqual(len(model._layers), 2)\n+        self.assertIsInstance(model.fc1, TorchModuleWrapper)\n+        self.assertIsInstance(model.bn1, TorchModuleWrapper)\n+        self.assertIsInstance(model.fc2, TorchModuleWrapper)\n+        self.assertFalse(isinstance(model.fc3, TorchModuleWrapper))\n+        self.assertEqual(len(model.fc1.trainable_weights), 2)\n+        self.assertEqual(len(model.bn1.trainable_weights), 2)\n         self.assertEqual(len(model.fc2.trainable_weights), 2)\n-        self.assertEqual(len(model.trainable_weights), 4)\n+        model(np.random.random((3, 2)))\n+        self.assertEqual(len(model.layers), 4)\n+        self.assertEqual(len(model.fc3.trainable_weights), 2)\n+        self.assertEqual(len(model.trainable_weights), 8)\n         model.compile(optimizer=\"sgd\", loss=\"mse\")\n-        model.fit(np.random.random((3, 2)), np.random.random((3, 4)))\n+        model.fit(np.random.random((3, 2)), np.random.random((3, 2)))\n+\n+    def test_load_weights_autowrapping(self):\n+        # Test loading weights\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.weights.h5\")\n+        model = ClassifierWithNoSpecialCasing()\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        x, y = np.random.random((3, 2)), np.random.random((3, 1))\n+        x_test, y_test = np.random.random((3, 2)), np.random.random((3, 1))\n+        model.fit(x, y)\n+        ref_loss = model.evaluate(x_test, y_test)\n+        model.save_weights(temp_filepath)\n+\n+        new_model = ClassifierWithNoSpecialCasing()\n+        new_model(np.random.random((3, 2)))\n+        new_model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        new_model.load_weights(temp_filepath)\n+        for ref_w, new_w in zip(model.get_weights(), new_model.get_weights()):\n+            self.assertAllClose(ref_w, new_w, atol=1e-5)\n+        loss = new_model.evaluate(x_test, y_test)\n+        self.assertAllClose(ref_loss, loss, atol=1e-5)\n+\n+    def test_serialize_model_autowrapping(self):\n+        # Test loading saved model\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n+        model = ClassifierWithNoSpecialCasing()\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        x, y = np.random.random((3, 2)), np.random.random((3, 1))\n+        x_test, y_test = np.random.random((3, 2)), np.random.random((3, 1))\n+        model.fit(x, y)\n+        ref_loss = model.evaluate(x_test, y_test)\n+        model.save(temp_filepath)\n+\n+        new_model = saving.load_model(temp_filepath)\n+        for ref_w, new_w in zip(model.get_weights(), new_model.get_weights()):\n+            self.assertAllClose(ref_w, new_w, atol=1e-5)\n+        loss = new_model.evaluate(x_test, y_test)\n+        self.assertAllClose(ref_loss, loss, atol=1e-5)\n+\n+    @parameterized.parameters(\n+        {\"use_batch_norm\": False, \"num_torch_layers\": 1},\n+        {\"use_batch_norm\": True, \"num_torch_layers\": 1},\n+        {\"use_batch_norm\": False, \"num_torch_layers\": 2},\n+        {\"use_batch_norm\": True, \"num_torch_layers\": 2},\n+    )\n+    def test_load_weights(self, use_batch_norm, num_torch_layers):\n+        # Test loading weights\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.weights.h5\")\n+        model = Classifier(use_batch_norm, num_torch_layers)\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        x, y = np.random.random((3, 2)), np.random.random((3, 1))\n+        x_test, y_test = np.random.random((3, 2)), np.random.random((3, 1))\n+        model.fit(x, y)\n+        ref_loss = model.evaluate(x_test, y_test)\n+        model.save_weights(temp_filepath)\n+\n+        new_model = Classifier(use_batch_norm, num_torch_layers)\n+        new_model(np.random.random((3, 2)))\n+        new_model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        new_model.load_weights(temp_filepath)\n+        for ref_w, new_w in zip(model.get_weights(), new_model.get_weights()):\n+            self.assertAllClose(ref_w, new_w, atol=1e-5)\n+        loss = new_model.evaluate(x_test, y_test)\n+        self.assertAllClose(ref_loss, loss, atol=1e-5)\n+\n+    @parameterized.parameters(\n+        {\"use_batch_norm\": False, \"num_torch_layers\": 1},\n+        {\"use_batch_norm\": True, \"num_torch_layers\": 1},\n+        {\"use_batch_norm\": False, \"num_torch_layers\": 2},\n+        {\"use_batch_norm\": True, \"num_torch_layers\": 2},\n+    )\n+    def test_serialize_model(self, use_batch_norm, num_torch_layers):\n+        # Test loading saved model\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n+        model = Classifier(use_batch_norm, num_torch_layers)\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        x, y = np.random.random((3, 2)), np.random.random((3, 1))\n+        x_test, y_test = np.random.random((3, 2)), np.random.random((3, 1))\n+        model.fit(x, y)\n+        ref_loss = model.evaluate(x_test, y_test)\n+        model.save(temp_filepath)\n+\n+        new_model = saving.load_model(temp_filepath)\n+        for ref_w, new_w in zip(model.get_weights(), new_model.get_weights()):\n+            self.assertAllClose(ref_w, new_w, atol=1e-5)\n+        loss = new_model.evaluate(x_test, y_test)\n+        self.assertAllClose(ref_loss, loss, atol=1e-5)\n+\n+    def test_from_config(self):\n+        module = torch.nn.Sequential(torch.nn.Linear(2, 4))\n+        mw = TorchModuleWrapper(module)\n+        config = mw.get_config()\n+        new_mw = TorchModuleWrapper.from_config(config)\n+        for ref_w, new_w in zip(mw.get_weights(), new_mw.get_weights()):\n+            self.assertAllClose(ref_w, new_w, atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11a4682274541c330b82e0135c2b4f3862343d9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 73 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -34,8 +34,8 @@ class TorchModuleWrapper(Layer):\n     from keras.layers import TorchModuleWrapper\n \n     class Classifier(keras.Model):\n-        def __init__(self, *args, **kwargs):\n-            super().__init__(*args, **kwargs)\n+        def __init__(self, **kwargs):\n+            super().__init__(**kwargs)\n             # Wrap `torch.nn.Module`s with `TorchModuleWrapper`\n             # if they contain parameters\n             self.conv1 = TorchModuleWrapper(\n@@ -73,10 +73,12 @@ class TorchModuleWrapper(Layer):\n     ```\n     \"\"\"\n \n-    def __init__(self, module, name=None, *args, **kwargs):\n-        super().__init__(name=name, *args, **kwargs)\n+    def __init__(self, module, name=None, **kwargs):\n+        super().__init__(name=name, **kwargs)\n         import torch.nn as nn\n \n+        from keras.backend.torch.core import get_device\n+\n         if (\n             isinstance(module, nn.modules.lazy.LazyModuleMixin)\n             and module.has_uninitialized_params()\n@@ -87,8 +89,6 @@ class TorchModuleWrapper(Layer):\n                 f\"Received uninitialized LazyModule: module={module}\"\n             )\n \n-        from keras.backend.torch.core import get_device\n-\n         self.module = module.to(get_device())\n         self._track_module_parameters()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1ed36d5ed432d4aa83ca1969badf88edfb76437", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 47 | Churn Cumulative: 573 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -346,10 +346,10 @@ class ModelParallel(Distribution):\n     # will be split across 4 devices. Any other variable that doesn't\n     # match any key in the layout map will be fully replicated.\n     layout_map = LayoutMap(device_mesh)\n-    layout_map['.*dense.*kernel'] = (None, 'model')\n-    layout_map['.*dense.*bias'] = ('model',)\n-    layout_map['.*conv2d.*kernel'] = (None, None, None, 'model')\n-    layout_map['.*conv2d.*bias'] = ('model',)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n \n     distribution = ModelParallel(device_mesh=device_mesh,\n                                  layout_map=layout_map,\n@@ -437,10 +437,10 @@ class LayoutMap(collections.abc.MutableMapping):\n \n     ```python\n     layout_map = LayoutMap(device_mesh=None)\n-    layout_map['.*dense.*kernel'] = (None, 'model')         # layout_2d\n-    layout_map['.*dense.*bias'] = ('model',)                # layout_1d\n-    layout_map['.*conv2d.*kernel'] = TensorLayout((None, None, None, 'model'))\n-    layout_map['.*conv2d.*bias'] = TensorLayout(('model',))  # layout_1d\n+    layout_map['dense.*kernel'] = (None, 'model')         # layout_2d\n+    layout_map['dense.*bias'] = ('model',)                # layout_1d\n+    layout_map['conv2d.*kernel'] = TensorLayout((None, None, None, 'model'))\n+    layout_map['conv2d.*bias'] = TensorLayout(('model',))  # layout_1d\n \n     layout_1 = layout_map['dense_1.kernel']             # layout_1 == layout_2d\n     layout_2 = layout_map['dense_1.bias']               # layout_2 == layout_1d\n@@ -465,9 +465,9 @@ class LayoutMap(collections.abc.MutableMapping):\n         \"\"\"Retrieves the corresponding layout by the string key.\n \n         When there isn't an exact match, all the existing keys in the layout map\n-        will be treated as a regex and map against the input key again. The\n-        first match will be returned, based on the key insertion order. Returns\n-        `None` if there isn't any match found.\n+        will be treated as a regex and map against the input key again. When\n+        there are multiple matches for the regex, an `ValueError` will be\n+        raised. Returns `None` if there isn't any match found.\n \n         Args:\n             key: String key to query a layout.\n@@ -478,9 +478,19 @@ class LayoutMap(collections.abc.MutableMapping):\n         if key in self._layout_map:\n             return self._layout_map[key]\n \n+        matching_keys = []\n         for k in self._layout_map:\n-            if re.match(k, key):\n-                return self._layout_map[k]\n+            if re.search(k, key):\n+                matching_keys.append(k)\n+        if len(matching_keys) > 1:\n+            raise ValueError(\n+                f\"Path '{key}' matches multiple layout \"\n+                f\"specification keys: {matching_keys}. Please make \"\n+                \"sure each tensor/variable path only matches at most \"\n+                \"one layout specification key in the LayoutMap.\"\n+            )\n+        elif len(matching_keys) == 1:\n+            return self._layout_map[matching_keys[0]]\n         return None\n \n     def __setitem__(self, key, layout):\n\n@@ -319,15 +319,18 @@ class LayoutMapTest(testing.TestCase):\n         layout_map[\"dense.*kernel\"] = self.replicated_2d\n         layout_map[\"dense.*bias\"] = self.replicated_1d\n \n-        layout_map[\".*bias\"] = self.sharded_1d\n+        layout_map[\"bias\"] = self.sharded_1d\n \n         self.assertEqual(layout_map[\"dense/kernel\"], self.sharded_2d)\n         self.assertEqual(layout_map[\"dense/bias\"], self.sharded_1d)\n \n-        # Map against the wildcard bias rule for dense, and based on the order\n-        # of insertion, it will not use .*bias.\n         self.assertEqual(layout_map[\"dense_2/kernel\"], self.replicated_2d)\n-        self.assertEqual(layout_map[\"dense_2/bias\"], self.replicated_1d)\n+        # Map against the wildcard bias rule for dense. This will cause a\n+        # ValueError\n+        with self.assertRaisesRegex(\n+            ValueError, \"Path 'dense_2/bias' matches multiple layout\"\n+        ):\n+            layout_map[\"dense_2/bias\"]\n \n         self.assertIsNone(layout_map[\"conv2d/kernel\"])\n         self.assertEqual(layout_map[\"conv2d/bias\"], self.sharded_1d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d7e5d2d4c79d710d9f15d89b38056978eb94912", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,12 +1,12 @@\n import numpy as np\n import tensorflow as tf\n \n+import keras\n from keras import layers\n from keras import losses\n from keras import metrics\n from keras import models\n from keras import optimizers\n-from keras.utils import rng_utils\n \n \n def test_model_fit():\n@@ -19,7 +19,7 @@ def test_model_fit():\n         ],\n     )\n \n-    rng_utils.set_random_seed(1337)\n+    keras.utils.set_random_seed(1337)\n \n     strategy = tf.distribute.MirroredStrategy([\"CPU:0\", \"CPU:1\"])\n     with strategy.scope():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23719ded54068622d5f46da3868f9c20ded6d27b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -197,7 +197,7 @@ class MeanMetricWrapper(Mean):\n \n     def get_config(self):\n         base_config = super().get_config()\n-        config = {\"fn\": serialization_lib.serialize_keras_object(self.fn)}\n+        config = {\"fn\": serialization_lib.serialize_keras_object(self._fn)}\n         config.update(serialization_lib.serialize_keras_object(self._fn_kwargs))\n         return {**base_config, **config}\n \n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras import testing\n from keras.metrics import reduction_metrics\n+from keras.saving import register_keras_serializable\n \n \n class SumTest(testing.TestCase):\n@@ -58,6 +59,8 @@ class MeanTest(testing.TestCase):\n         self.assertAllClose(result, 3.0, atol=1e-3)\n \n \n+# How users would register a custom function or class to use with MeanMetricWrapper.\n+@register_keras_serializable(package='test', name='mse')\n def mse(y_true, y_pred):\n     return (y_true - y_pred) ** 2\n \n@@ -70,7 +73,14 @@ class MetricWrapperTest(testing.TestCase):\n         self.assertEqual(mse_obj.name, \"mse\")\n         self.assertEqual(len(mse_obj.variables), 2)\n         self.assertEqual(mse_obj._dtype, \"float32\")\n-        # TODO: Check save and restore config\n+        # Check save and restore config\n+        mse_obj2 = reduction_metrics.MeanMetricWrapper.from_config(\n+            mse_obj.get_config()\n+        )\n+        self.assertEqual(mse_obj2.name, \"mse\")\n+        self.assertEqual(len(mse_obj2.variables), 2)\n+        self.assertEqual(mse_obj2._dtype, \"float32\")\n+        self.assertTrue(\"fn\" in mse_obj2.get_config())\n \n     def test_unweighted(self):\n         mse_obj = reduction_metrics.MeanMetricWrapper(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c0c8e76fd93e7c4819c90b2b1ddb483ca65dbee1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 18 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -60,7 +60,7 @@ class MeanTest(testing.TestCase):\n \n \n # How users would register a custom function or class to use with MeanMetricWrapper.\n-@register_keras_serializable(package='test', name='mse')\n+@register_keras_serializable(package=\"test\", name=\"mse\")\n def mse(y_true, y_pred):\n     return (y_true - y_pred) ** 2\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#817509cfd8ee8b604c7db236592ff0e1a0992210", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 165 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -810,7 +810,7 @@ def take(x, indices, axis=None):\n             )\n             return tfnp.take(x, tf.sparse.to_dense(indices), axis=axis)\n         return tf.nn.safe_embedding_lookup_sparse(\n-            embedding_weights=x,\n+            embedding_weights=tf.convert_to_tensor(x),\n             sparse_ids=tf.sparse.expand_dims(indices, axis=-1),\n             default_id=0,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6222bc9e535a197b2a1121427e40ea9ea3a4be94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 12 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ class CanaryLayer(layers.Layer):\n     def __init__(self):\n         super().__init__()\n         self.counter = self.add_weight(\n-            shape=(), initializer=\"zeros\", dtype=\"int32\", trainable=False\n+            shape=(), initializer=\"zeros\", dtype=\"float32\", trainable=False\n         )\n \n     def call(self, x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46b491ef728703f9a03b3522edb77dc2d946377e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 21 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,8 @@ class MeanTest(testing.TestCase):\n         self.assertAllClose(result, 3.0, atol=1e-3)\n \n \n-# How users would register a custom function or class to use with MeanMetricWrapper.\n+# How users would register a custom function or class to use with\n+# MeanMetricWrapper.\n @register_keras_serializable(package=\"test\", name=\"mse\")\n def mse(y_true, y_pred):\n     return (y_true - y_pred) ** 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a108358ec4cf25cd1e59b6dd5af8d252604be9a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 84 | Churn Cumulative: 597 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,3 +122,48 @@ class DenseTest(testing.TestCase):\n         self.assertIsInstance(\n             g.gradient(outputs, layer.kernel), tf.IndexedSlices\n         )\n+\n+    def test_dense_no_activation(self):\n+        layer = layers.Dense(units=2, use_bias=False, activation=None)\n+        layer.build((1, 2))\n+        layer.set_weights(\n+            [\n+                np.array([[1.0, -2.0], [3.0, -4.0]]),\n+            ]\n+        )\n+        inputs = np.array(\n+            [[-1.0, 2.0]],\n+        )\n+        self.assertEqual(layer.bias, None)\n+        self.assertAllClose(layer(inputs), [[5.0, -6.0]])\n+\n+    def test_dense_without_activation_set(self):\n+        layer = layers.Dense(units=2, use_bias=False)\n+        layer.build((1, 2))\n+        layer.set_weights(\n+            [\n+                np.array([[1.0, -2.0], [3.0, -4.0]]),\n+            ]\n+        )\n+        layer.activation = None\n+        inputs = np.array(\n+            [[-1.0, 2.0]],\n+        )\n+        self.assertEqual(layer.bias, None)\n+        self.assertAllClose(layer(inputs), [[5.0, -6.0]])\n+\n+    def test_dense_with_activation(self):\n+        layer = layers.Dense(units=2, use_bias=False, activation=\"relu\")\n+        layer.build((1, 2))\n+        layer.set_weights(\n+            [\n+                np.array([[1.0, -2.0], [3.0, -4.0]]),\n+            ]\n+        )\n+\n+        inputs = np.array(\n+            [[-1.0, 2.0]],\n+        )\n+        output = layer(inputs)\n+        expected_output = np.array([[5.0, 0.0]])\n+        self.assertAllClose(output, expected_output)\n\n@@ -88,3 +88,9 @@ class EmbeddingTest(test_case.TestCase):\n         layer.build()\n         out = layer.compute_mask(np.array(([2, 1, 0])))\n         self.assertAllClose(out, np.array([True, True, False]))\n+\n+    def test_compute_mask_no_masking(self):\n+        layer = layers.Embedding(input_dim=3, output_dim=2, mask_zero=False)\n+        input_data = np.array([2, 1, 0])\n+        mask = layer.compute_mask(input_data)\n+        self.assertIsNone(mask)\n\n@@ -108,3 +108,26 @@ class InputLayerTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(values.output, input_tensor)\n         self.assertEqual(values.output.ndim, input_tensor.ndim)\n         self.assertEqual(values.output.dtype, dtype)\n+\n+    def test_input_shape_deprecated(self):\n+        input_shape = (2, 3)\n+        batch_size = 4\n+        dtype = \"float32\"\n+\n+        with self.assertWarnsRegex(\n+            UserWarning,\n+            \"Argument `input_shape` is deprecated. Use `shape` instead.\",\n+        ):\n+            layer = InputLayer(\n+                input_shape=input_shape, batch_size=batch_size, dtype=dtype\n+            )\n+\n+        self.assertEqual(layer.batch_shape[0], batch_size)\n+        self.assertEqual(layer.batch_shape[1:], input_shape)\n+        self.assertEqual(layer.dtype, dtype)\n+        self.assertIsInstance(layer.output, KerasTensor)\n+\n+    def test_call_method(self):\n+        layer = InputLayer(shape=(32,))\n+        output = layer.call()\n+        self.assertIsNone(output)\n\n@@ -68,3 +68,13 @@ class WrapperTest(testing.TestCase):\n             expected_num_losses=0,\n             supports_masking=False,\n         )\n+\n+    def test_wrapper_invalid_layer(self):\n+        invalid_layer = \"This is not a valid Keras layer.\"\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Layer .* supplied to Wrapper isn't a supported layer type. \"\n+            \"Please ensure wrapped layer is a valid Keras layer.\",\n+        ):\n+            layers.Wrapper(invalid_layer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#81db804d3147a846b056db73fd02bc78cad8cde6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 53 | Churn Cumulative: 127 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,7 +19,7 @@ ARRAY_TYPES = (np.ndarray,)\n if backend.backend() == \"tensorflow\":\n     from keras.utils.module_utils import tensorflow as tf\n \n-    ARRAY_TYPES = ARRAY_TYPES + (np.ndarray, tf.RaggedTensor)\n+    ARRAY_TYPES = ARRAY_TYPES + (tf.Tensor, tf.RaggedTensor)\n if pandas:\n     ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n \n@@ -164,8 +164,8 @@ def train_validation_split(arrays, validation_split):\n     if unsplitable:\n         raise ValueError(\n             \"Argument `validation_split` is only supported \"\n-            \"for tensors or NumPy \"\n-            \"arrays. Found incompatible type in the input: {unsplitable}\"\n+            \"for tensors or NumPy arrays.\"\n+            f\"Found incompatible type in the input: {unsplitable}\"\n         )\n \n     if all(t is None for t in flat_arrays):\n\n@@ -198,6 +198,53 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n             atol=6.1051628e-1,\n         )\n     \n+    @parameterized.named_parameters(\n+        [\n+            (\"eager\", True, False, False),\n+            (\"graph_fn\", False, False, False),\n+            (\"jit\", False, True, False),\n+            (\"steps_per_epoch_eager\", True, False, True),\n+            (\"steps_per_epoch_graph_fn\", False, False, True),\n+            (\"steps_per_epoch_jit\", False, True, True),\n+        ]\n+    )\n+    @pytest.mark.requires_trainable_backend\n+    def test_fit_with_val_split(self, run_eagerly, \n+                                jit_compile, use_steps_per_epoch):\n+        if not run_eagerly and not jit_compile and use_steps_per_epoch:\n+            if backend.backend() == \"tensorflow\":\n+                self.skipTest(\n+                    \"TODO: Graph mode without XLA in TF backend leads to \"\n+                    \"unexpected logs, need further checks.\"\n+                )\n+\n+        model = ExampleModel(units=3)\n+        epochs = 3\n+        batch_size = 20\n+        steps_per_epoch = 7\n+        dataset_size = batch_size * (steps_per_epoch - 2)\n+        x = np.ones((dataset_size, 4))\n+        y = np.zeros((dataset_size, 3))\n+\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+            run_eagerly=run_eagerly,\n+            jit_compile=jit_compile,\n+        )\n+        history = model.fit(\n+            x,\n+            y,\n+            batch_size=batch_size,\n+            steps_per_epoch=steps_per_epoch if use_steps_per_epoch else None,\n+            epochs=epochs,\n+            validation_split=0.2,\n+        )\n+        history = history.history\n+        self.assertIn(\"loss\", history)\n+        self.assertIn(\"val_loss\", history)\n+\n     @parameterized.named_parameters(\n         [\n             (\"eager\", True, False),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#87abd8bf2c8432b2772919f4dc9b923db8b88380", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 116 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -209,8 +209,9 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         ]\n     )\n     @pytest.mark.requires_trainable_backend\n-    def test_fit_with_val_split(self, run_eagerly, \n-                                jit_compile, use_steps_per_epoch):\n+    def test_fit_with_val_split(\n+        self, run_eagerly, jit_compile, use_steps_per_epoch\n+    ):\n         if not run_eagerly and not jit_compile and use_steps_per_epoch:\n             if backend.backend() == \"tensorflow\":\n                 self.skipTest(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c3ebe77137898f31ace99db1fa7df053182459ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 119 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,9 @@\n import torch\n+from packaging.version import parse\n \n from keras import optimizers\n from keras.optimizers.base_optimizer import BaseOptimizer\n+from keras.utils import torch_utils\n \n \n class TorchOptimizer(BaseOptimizer):\n@@ -33,7 +35,7 @@ class TorchOptimizer(BaseOptimizer):\n             return OPTIMIZERS[cls](*args, **kwargs)\n         return super().__new__(cls)\n \n-    @torch.no_grad\n+    @torch_utils.no_grad\n     def _apply_weight_decay(self, variables):\n         if self.weight_decay is None:\n             return\n\n@@ -1,10 +1,11 @@\n import torch\n \n from keras.optimizers.base_optimizer import BaseOptimizer\n+from keras.utils import torch_utils\n \n \n class TorchParallelOptimizer(BaseOptimizer):\n-    @torch.no_grad\n+    @torch_utils.no_grad\n     def _internal_apply_gradients(self, grads_and_vars):\n         grads, trainable_variables = zip(*grads_and_vars)\n \n\n@@ -1,5 +1,7 @@\n import io\n \n+from packaging.version import parse\n+\n from keras.api_export import keras_export\n from keras.layers import Layer\n from keras.ops import convert_to_numpy\n@@ -145,3 +147,12 @@ class TorchModuleWrapper(Layer):\n             buffer = io.BytesIO(config[\"module\"])\n             config[\"module\"] = torch.load(buffer)\n         return cls(**config)\n+\n+\n+def no_grad(orig_func):\n+    import torch\n+\n+    if parse(torch.__version__) >= parse(\"2.1.0\"):\n+        return torch.no_grad(orig_func)\n+    else:\n+        return orig_func\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#033ac3e5398153d4066720abd0024617befbf2bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 38 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import torch\n-from packaging.version import parse\n \n from keras import optimizers\n from keras.optimizers.base_optimizer import BaseOptimizer\n\n@@ -1,5 +1,3 @@\n-import torch\n-\n from keras.optimizers.base_optimizer import BaseOptimizer\n from keras.utils import torch_utils\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#32e91543b5d622fa0efbbffc1cffddc29fce4e25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 790 | Lines Deleted: 609 | Files Changed: 4 | Hunks: 67 | Methods Changed: 46 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1399 | Churn Cumulative: 2623 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,276 @@ from keras.backend.common.backend_utils import (\n )\n \n \n+def np_conv1d_transpose(\n+    x,\n+    kernel_weights,\n+    bias_weights,\n+    strides,\n+    padding,\n+    output_padding,\n+    data_format,\n+    dilation_rate,\n+):\n+    if data_format == \"channels_first\":\n+        x = x.transpose((0, 2, 1))\n+    if isinstance(strides, (tuple, list)):\n+        h_stride = strides[0]\n+    else:\n+        h_stride = strides\n+    if isinstance(dilation_rate, (tuple, list)):\n+        h_dilation = dilation_rate[0]\n+    else:\n+        h_dilation = dilation_rate\n+\n+    h_kernel, ch_out, ch_in = kernel_weights.shape\n+    n_batch, h_x, _ = x.shape\n+    # Get output shape and padding\n+    _, h_out, _ = compute_conv_transpose_output_shape(\n+        x.shape,\n+        kernel_weights.shape,\n+        ch_out,\n+        strides,\n+        padding,\n+        output_padding,\n+        data_format,\n+        dilation_rate,\n+    )\n+    jax_padding = compute_conv_transpose_padding_args_for_jax(\n+        input_shape=x.shape,\n+        kernel_shape=kernel_weights.shape,\n+        strides=strides,\n+        padding=padding,\n+        output_padding=output_padding,\n+        dilation_rate=dilation_rate,\n+    )\n+    h_pad_side1 = h_kernel - 1 - jax_padding[0][0]\n+\n+    if h_dilation > 1:\n+        # Increase kernel size\n+        new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n+        new_kenel_size_tuple = (new_h_kernel,)\n+        new_kernel_weights = np.zeros(\n+            (*new_kenel_size_tuple, ch_out, ch_in),\n+            dtype=kernel_weights.dtype,\n+        )\n+        new_kernel_weights[::h_dilation] = kernel_weights\n+        kernel_weights = new_kernel_weights\n+        h_kernel = kernel_weights.shape[0]\n+\n+    # Compute output\n+    output = np.zeros([n_batch, h_out + h_kernel, ch_out])\n+    for nb in range(n_batch):\n+        for h_x_idx in range(h_x):\n+            h_out_idx = h_x_idx * h_stride  # Index in output\n+            output[nb, h_out_idx : h_out_idx + h_kernel, :] += np.sum(\n+                kernel_weights[:, :, :] * x[nb, h_x_idx, :], axis=-1\n+            )\n+    output = output + bias_weights\n+\n+    # Cut padding results from output\n+    output = output[:, h_pad_side1 : h_out + h_pad_side1]\n+    if data_format == \"channels_first\":\n+        output = output.transpose((0, 2, 1))\n+    return output\n+\n+\n+def np_conv2d_transpose(\n+    x,\n+    kernel_weights,\n+    bias_weights,\n+    strides,\n+    padding,\n+    output_padding,\n+    data_format,\n+    dilation_rate,\n+):\n+    if data_format == \"channels_first\":\n+        x = x.transpose((0, 2, 3, 1))\n+    if isinstance(strides, (tuple, list)):\n+        h_stride, w_stride = strides\n+    else:\n+        h_stride = strides\n+        w_stride = strides\n+    if isinstance(dilation_rate, (tuple, list)):\n+        h_dilation, w_dilation = dilation_rate\n+    else:\n+        h_dilation = dilation_rate\n+        w_dilation = dilation_rate\n+\n+    h_kernel, w_kernel, ch_out, ch_in = kernel_weights.shape\n+    n_batch, h_x, w_x, _ = x.shape\n+    # Get output shape and padding\n+    _, h_out, w_out, _ = compute_conv_transpose_output_shape(\n+        x.shape,\n+        kernel_weights.shape,\n+        ch_out,\n+        strides,\n+        padding,\n+        output_padding,\n+        data_format,\n+        dilation_rate,\n+    )\n+    jax_padding = compute_conv_transpose_padding_args_for_jax(\n+        input_shape=x.shape,\n+        kernel_shape=kernel_weights.shape,\n+        strides=strides,\n+        padding=padding,\n+        output_padding=output_padding,\n+        dilation_rate=dilation_rate,\n+    )\n+    h_pad_side1 = h_kernel - 1 - jax_padding[0][0]\n+    w_pad_side1 = w_kernel - 1 - jax_padding[1][0]\n+\n+    if h_dilation > 1 or w_dilation > 1:\n+        # Increase kernel size\n+        new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n+        new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n+        new_kenel_size_tuple = (new_h_kernel, new_w_kernel)\n+        new_kernel_weights = np.zeros(\n+            (*new_kenel_size_tuple, ch_out, ch_in),\n+            dtype=kernel_weights.dtype,\n+        )\n+        new_kernel_weights[::h_dilation, ::w_dilation] = kernel_weights\n+        kernel_weights = new_kernel_weights\n+        h_kernel, w_kernel = kernel_weights.shape[:2]\n+\n+    # Compute output\n+    output = np.zeros([n_batch, h_out + h_kernel, w_out + w_kernel, ch_out])\n+    for nb in range(n_batch):\n+        for h_x_idx in range(h_x):\n+            h_out_idx = h_x_idx * h_stride  # Index in output\n+            for w_x_idx in range(w_x):\n+                w_out_idx = w_x_idx * w_stride\n+                output[\n+                    nb,\n+                    h_out_idx : h_out_idx + h_kernel,\n+                    w_out_idx : w_out_idx + w_kernel,\n+                    :,\n+                ] += np.sum(\n+                    kernel_weights[:, :, :, :] * x[nb, h_x_idx, w_x_idx, :],\n+                    axis=-1,\n+                )\n+    output = output + bias_weights\n+\n+    # Cut padding results from output\n+    output = output[\n+        :,\n+        h_pad_side1 : h_out + h_pad_side1,\n+        w_pad_side1 : w_out + w_pad_side1,\n+    ]\n+    if data_format == \"channels_first\":\n+        output = output.transpose((0, 3, 1, 2))\n+    return output\n+\n+\n+def np_conv3d_transpose(\n+    x,\n+    kernel_weights,\n+    bias_weights,\n+    strides,\n+    padding,\n+    output_padding,\n+    data_format,\n+    dilation_rate,\n+):\n+    if data_format == \"channels_first\":\n+        x = x.transpose((0, 2, 3, 4, 1))\n+    if isinstance(strides, (tuple, list)):\n+        h_stride, w_stride, d_stride = strides\n+    else:\n+        h_stride = strides\n+        w_stride = strides\n+        d_stride = strides\n+    if isinstance(dilation_rate, (tuple, list)):\n+        h_dilation, w_dilation, d_dilation = dilation_rate\n+    else:\n+        h_dilation = dilation_rate\n+        w_dilation = dilation_rate\n+        d_dilation = dilation_rate\n+\n+    h_kernel, w_kernel, d_kernel, ch_out, ch_in = kernel_weights.shape\n+    n_batch, h_x, w_x, d_x, _ = x.shape\n+    # Get output shape and padding\n+    _, h_out, w_out, d_out, _ = compute_conv_transpose_output_shape(\n+        x.shape,\n+        kernel_weights.shape,\n+        ch_out,\n+        strides,\n+        padding,\n+        output_padding,\n+        data_format,\n+        dilation_rate,\n+    )\n+    jax_padding = compute_conv_transpose_padding_args_for_jax(\n+        input_shape=x.shape,\n+        kernel_shape=kernel_weights.shape,\n+        strides=strides,\n+        padding=padding,\n+        output_padding=output_padding,\n+        dilation_rate=dilation_rate,\n+    )\n+    h_pad_side1 = h_kernel - 1 - jax_padding[0][0]\n+    w_pad_side1 = w_kernel - 1 - jax_padding[1][0]\n+    d_pad_side1 = d_kernel - 1 - jax_padding[2][0]\n+\n+    if h_dilation > 1 or w_dilation > 1 or d_dilation > 1:\n+        # Increase kernel size\n+        new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n+        new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n+        new_d_kernel = d_kernel + (d_dilation - 1) * (d_kernel - 1)\n+        new_kenel_size_tuple = (new_h_kernel, new_w_kernel, new_d_kernel)\n+        new_kernel_weights = np.zeros(\n+            (*new_kenel_size_tuple, ch_out, ch_in),\n+            dtype=kernel_weights.dtype,\n+        )\n+        new_kernel_weights[\n+            ::h_dilation, ::w_dilation, ::d_dilation\n+        ] = kernel_weights\n+        kernel_weights = new_kernel_weights\n+        h_kernel, w_kernel, d_kernel = kernel_weights.shape[:3]\n+\n+    # Compute output\n+    output = np.zeros(\n+        [\n+            n_batch,\n+            h_out + h_kernel,\n+            w_out + w_kernel,\n+            d_out + d_kernel,\n+            ch_out,\n+        ]\n+    )\n+    for nb in range(n_batch):\n+        for h_x_idx in range(h_x):\n+            h_out_idx = h_x_idx * h_stride  # Index in output\n+            for w_x_idx in range(w_x):\n+                w_out_idx = w_x_idx * w_stride\n+                for d_x_idx in range(d_x):\n+                    d_out_idx = d_x_idx * d_stride\n+                    output[\n+                        nb,\n+                        h_out_idx : h_out_idx + h_kernel,\n+                        w_out_idx : w_out_idx + w_kernel,\n+                        d_out_idx : d_out_idx + d_kernel,\n+                        :,\n+                    ] += np.sum(\n+                        kernel_weights[:, :, :, :, :]\n+                        * x[nb, h_x_idx, w_x_idx, d_x_idx, :],\n+                        axis=-1,\n+                    )\n+    output = output + bias_weights\n+\n+    # Cut padding results from output\n+    output = output[\n+        :,\n+        h_pad_side1 : h_out + h_pad_side1,\n+        w_pad_side1 : w_out + w_pad_side1,\n+        d_pad_side1 : d_out + d_pad_side1,\n+    ]\n+    if data_format == \"channels_first\":\n+        output = output.transpose((0, 4, 1, 2, 3))\n+    return output\n+\n+\n class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters(\n         {\n@@ -258,276 +528,6 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n-    def np_conv1d_transpose(\n-        self,\n-        x,\n-        kernel_weights,\n-        bias_weights,\n-        strides,\n-        padding,\n-        output_padding,\n-        data_format,\n-        dilation_rate,\n-    ):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 1))\n-        if isinstance(strides, (tuple, list)):\n-            h_stride = strides[0]\n-        else:\n-            h_stride = strides\n-        if isinstance(dilation_rate, (tuple, list)):\n-            h_dilation = dilation_rate[0]\n-        else:\n-            h_dilation = dilation_rate\n-\n-        h_kernel, ch_out, ch_in = kernel_weights.shape\n-        n_batch, h_x, _ = x.shape\n-        # Get output shape and padding\n-        _, h_out, _ = compute_conv_transpose_output_shape(\n-            x.shape,\n-            kernel_weights.shape,\n-            ch_out,\n-            strides,\n-            padding,\n-            output_padding,\n-            data_format,\n-            dilation_rate,\n-        )\n-        jax_padding = compute_conv_transpose_padding_args_for_jax(\n-            input_shape=x.shape,\n-            kernel_shape=kernel_weights.shape,\n-            strides=strides,\n-            padding=padding,\n-            output_padding=output_padding,\n-            dilation_rate=dilation_rate,\n-        )\n-        h_pad_side1 = h_kernel - 1 - jax_padding[0][0]\n-\n-        if h_dilation > 1:\n-            # Increase kernel size\n-            new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n-            new_kenel_size_tuple = (new_h_kernel,)\n-            new_kernel_weights = np.zeros(\n-                (*new_kenel_size_tuple, ch_out, ch_in),\n-                dtype=kernel_weights.dtype,\n-            )\n-            new_kernel_weights[::h_dilation] = kernel_weights\n-            kernel_weights = new_kernel_weights\n-            h_kernel = kernel_weights.shape[0]\n-\n-        # Compute output\n-        output = np.zeros([n_batch, h_out + h_kernel, ch_out])\n-        for nb in range(n_batch):\n-            for h_x_idx in range(h_x):\n-                h_out_idx = h_x_idx * h_stride  # Index in output\n-                output[nb, h_out_idx : h_out_idx + h_kernel, :] += np.sum(\n-                    kernel_weights[:, :, :] * x[nb, h_x_idx, :], axis=-1\n-                )\n-        output = output + bias_weights\n-\n-        # Cut padding results from output\n-        output = output[:, h_pad_side1 : h_out + h_pad_side1]\n-        if data_format == \"channels_first\":\n-            output = output.transpose((0, 2, 1))\n-        return output\n-\n-    def np_conv2d_transpose(\n-        self,\n-        x,\n-        kernel_weights,\n-        bias_weights,\n-        strides,\n-        padding,\n-        output_padding,\n-        data_format,\n-        dilation_rate,\n-    ):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 1))\n-        if isinstance(strides, (tuple, list)):\n-            h_stride, w_stride = strides\n-        else:\n-            h_stride = strides\n-            w_stride = strides\n-        if isinstance(dilation_rate, (tuple, list)):\n-            h_dilation, w_dilation = dilation_rate\n-        else:\n-            h_dilation = dilation_rate\n-            w_dilation = dilation_rate\n-\n-        h_kernel, w_kernel, ch_out, ch_in = kernel_weights.shape\n-        n_batch, h_x, w_x, _ = x.shape\n-        # Get output shape and padding\n-        _, h_out, w_out, _ = compute_conv_transpose_output_shape(\n-            x.shape,\n-            kernel_weights.shape,\n-            ch_out,\n-            strides,\n-            padding,\n-            output_padding,\n-            data_format,\n-            dilation_rate,\n-        )\n-        jax_padding = compute_conv_transpose_padding_args_for_jax(\n-            input_shape=x.shape,\n-            kernel_shape=kernel_weights.shape,\n-            strides=strides,\n-            padding=padding,\n-            output_padding=output_padding,\n-            dilation_rate=dilation_rate,\n-        )\n-        h_pad_side1 = h_kernel - 1 - jax_padding[0][0]\n-        w_pad_side1 = w_kernel - 1 - jax_padding[1][0]\n-\n-        if h_dilation > 1 or w_dilation > 1:\n-            # Increase kernel size\n-            new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n-            new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n-            new_kenel_size_tuple = (new_h_kernel, new_w_kernel)\n-            new_kernel_weights = np.zeros(\n-                (*new_kenel_size_tuple, ch_out, ch_in),\n-                dtype=kernel_weights.dtype,\n-            )\n-            new_kernel_weights[::h_dilation, ::w_dilation] = kernel_weights\n-            kernel_weights = new_kernel_weights\n-            h_kernel, w_kernel = kernel_weights.shape[:2]\n-\n-        # Compute output\n-        output = np.zeros([n_batch, h_out + h_kernel, w_out + w_kernel, ch_out])\n-        for nb in range(n_batch):\n-            for h_x_idx in range(h_x):\n-                h_out_idx = h_x_idx * h_stride  # Index in output\n-                for w_x_idx in range(w_x):\n-                    w_out_idx = w_x_idx * w_stride\n-                    output[\n-                        nb,\n-                        h_out_idx : h_out_idx + h_kernel,\n-                        w_out_idx : w_out_idx + w_kernel,\n-                        :,\n-                    ] += np.sum(\n-                        kernel_weights[:, :, :, :] * x[nb, h_x_idx, w_x_idx, :],\n-                        axis=-1,\n-                    )\n-        output = output + bias_weights\n-\n-        # Cut padding results from output\n-        output = output[\n-            :,\n-            h_pad_side1 : h_out + h_pad_side1,\n-            w_pad_side1 : w_out + w_pad_side1,\n-        ]\n-        if data_format == \"channels_first\":\n-            output = output.transpose((0, 3, 1, 2))\n-        return output\n-\n-    def np_conv3d_transpose(\n-        self,\n-        x,\n-        kernel_weights,\n-        bias_weights,\n-        strides,\n-        padding,\n-        output_padding,\n-        data_format,\n-        dilation_rate,\n-    ):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 4, 1))\n-        if isinstance(strides, (tuple, list)):\n-            h_stride, w_stride, d_stride = strides\n-        else:\n-            h_stride = strides\n-            w_stride = strides\n-            d_stride = strides\n-        if isinstance(dilation_rate, (tuple, list)):\n-            h_dilation, w_dilation, d_dilation = dilation_rate\n-        else:\n-            h_dilation = dilation_rate\n-            w_dilation = dilation_rate\n-            d_dilation = dilation_rate\n-\n-        h_kernel, w_kernel, d_kernel, ch_out, ch_in = kernel_weights.shape\n-        n_batch, h_x, w_x, d_x, _ = x.shape\n-        # Get output shape and padding\n-        _, h_out, w_out, d_out, _ = compute_conv_transpose_output_shape(\n-            x.shape,\n-            kernel_weights.shape,\n-            ch_out,\n-            strides,\n-            padding,\n-            output_padding,\n-            data_format,\n-            dilation_rate,\n-        )\n-        jax_padding = compute_conv_transpose_padding_args_for_jax(\n-            input_shape=x.shape,\n-            kernel_shape=kernel_weights.shape,\n-            strides=strides,\n-            padding=padding,\n-            output_padding=output_padding,\n-            dilation_rate=dilation_rate,\n-        )\n-        h_pad_side1 = h_kernel - 1 - jax_padding[0][0]\n-        w_pad_side1 = w_kernel - 1 - jax_padding[1][0]\n-        d_pad_side1 = d_kernel - 1 - jax_padding[2][0]\n-\n-        if h_dilation > 1 or w_dilation > 1 or d_dilation > 1:\n-            # Increase kernel size\n-            new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n-            new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n-            new_d_kernel = d_kernel + (d_dilation - 1) * (d_kernel - 1)\n-            new_kenel_size_tuple = (new_h_kernel, new_w_kernel, new_d_kernel)\n-            new_kernel_weights = np.zeros(\n-                (*new_kenel_size_tuple, ch_out, ch_in),\n-                dtype=kernel_weights.dtype,\n-            )\n-            new_kernel_weights[\n-                ::h_dilation, ::w_dilation, ::d_dilation\n-            ] = kernel_weights\n-            kernel_weights = new_kernel_weights\n-            h_kernel, w_kernel, d_kernel = kernel_weights.shape[:3]\n-\n-        # Compute output\n-        output = np.zeros(\n-            [\n-                n_batch,\n-                h_out + h_kernel,\n-                w_out + w_kernel,\n-                d_out + d_kernel,\n-                ch_out,\n-            ]\n-        )\n-        for nb in range(n_batch):\n-            for h_x_idx in range(h_x):\n-                h_out_idx = h_x_idx * h_stride  # Index in output\n-                for w_x_idx in range(w_x):\n-                    w_out_idx = w_x_idx * w_stride\n-                    for d_x_idx in range(d_x):\n-                        d_out_idx = d_x_idx * d_stride\n-                        output[\n-                            nb,\n-                            h_out_idx : h_out_idx + h_kernel,\n-                            w_out_idx : w_out_idx + w_kernel,\n-                            d_out_idx : d_out_idx + d_kernel,\n-                            :,\n-                        ] += np.sum(\n-                            kernel_weights[:, :, :, :, :]\n-                            * x[nb, h_x_idx, w_x_idx, d_x_idx, :],\n-                            axis=-1,\n-                        )\n-        output = output + bias_weights\n-\n-        # Cut padding results from output\n-        output = output[\n-            :,\n-            h_pad_side1 : h_out + h_pad_side1,\n-            w_pad_side1 : w_out + w_pad_side1,\n-            d_pad_side1 : d_out + d_pad_side1,\n-        ]\n-        if data_format == \"channels_first\":\n-            output = output.transpose((0, 4, 1, 2, 3))\n-        return output\n-\n     @parameterized.parameters(\n         {\n             \"filters\": 5,\n@@ -587,7 +587,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         layer.bias.assign(bias_weights)\n \n         outputs = layer(inputs)\n-        expected = self.np_conv1d_transpose(\n+        expected = np_conv1d_transpose(\n             inputs,\n             kernel_weights,\n             bias_weights,\n@@ -667,7 +667,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         layer.bias.assign(bias_weights)\n \n         outputs = layer(inputs)\n-        expected = self.np_conv2d_transpose(\n+        expected = np_conv2d_transpose(\n             inputs,\n             kernel_weights,\n             bias_weights,\n@@ -738,7 +738,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         layer.bias.assign(bias_weights)\n \n         outputs = layer(inputs)\n-        expected = self.np_conv3d_transpose(\n+        expected = np_conv3d_transpose(\n             inputs,\n             kernel_weights,\n             bias_weights,\n@@ -776,7 +776,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         )\n \n         # Exepected result\n-        expected_res = self.np_conv1d_transpose(\n+        expected_res = np_conv1d_transpose(\n             x=input,\n             kernel_weights=kernel_weights,\n             bias_weights=np.zeros(shape=(1,)),\n\n@@ -8,6 +8,132 @@ from keras import layers\n from keras import testing\n \n \n+def _same_padding(input_size, pool_size, stride):\n+    if input_size % stride == 0:\n+        return max(pool_size - stride, 0)\n+    else:\n+        return max(pool_size - (input_size % stride), 0)\n+\n+\n+def np_avgpool1d(x, pool_size, strides, padding, data_format):\n+    if data_format == \"channels_first\":\n+        x = x.swapaxes(1, 2)\n+    if isinstance(pool_size, (tuple, list)):\n+        pool_size = pool_size[0]\n+    if isinstance(strides, (tuple, list)):\n+        h_stride = strides[0]\n+    else:\n+        h_stride = strides\n+\n+    if padding == \"same\":\n+        n_batch, h_x, ch_x = x.shape\n+        pad_value = _same_padding(h_x, pool_size, h_stride)\n+        npad = [(0, 0)] * x.ndim\n+        npad[1] = (0, pad_value)\n+        x = np.pad(x, pad_width=npad, mode=\"edge\")\n+\n+    n_batch, h_x, ch_x = x.shape\n+    out_h = int((h_x - pool_size) / h_stride) + 1\n+\n+    stride_shape = (n_batch, out_h, ch_x, pool_size)\n+    strides = (\n+        x.strides[0],\n+        h_stride * x.strides[1],\n+        x.strides[2],\n+        x.strides[1],\n+    )\n+    windows = as_strided(x, shape=stride_shape, strides=strides)\n+    out = np.mean(windows, axis=(3,))\n+    if data_format == \"channels_first\":\n+        out = out.swapaxes(1, 2)\n+    return out\n+\n+\n+def np_avgpool2d(x, pool_size, strides, padding, data_format):\n+    if data_format == \"channels_first\":\n+        x = x.transpose((0, 2, 3, 1))\n+    if isinstance(pool_size, int):\n+        pool_size = (pool_size, pool_size)\n+    if isinstance(strides, int):\n+        strides = (strides, strides)\n+\n+    h_pool_size, w_pool_size = pool_size\n+    h_stride, w_stride = strides\n+    if padding == \"same\":\n+        n_batch, h_x, w_x, ch_x = x.shape\n+        h_padding = _same_padding(h_x, h_pool_size, h_stride)\n+        w_padding = _same_padding(w_x, w_pool_size, w_stride)\n+        npad = [(0, 0)] * x.ndim\n+        npad[1] = (0, h_padding)\n+        npad[2] = (0, w_padding)\n+        x = np.pad(x, pad_width=npad, mode=\"edge\")\n+\n+    n_batch, h_x, w_x, ch_x = x.shape\n+    out_h = int((h_x - h_pool_size) / h_stride) + 1\n+    out_w = int((w_x - w_pool_size) / w_stride) + 1\n+\n+    stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n+    strides = (\n+        x.strides[0],\n+        h_stride * x.strides[1],\n+        w_stride * x.strides[2],\n+        x.strides[3],\n+        x.strides[1],\n+        x.strides[2],\n+    )\n+    windows = as_strided(x, shape=stride_shape, strides=strides)\n+    out = np.mean(windows, axis=(4, 5))\n+    if data_format == \"channels_first\":\n+        out = out.transpose((0, 3, 1, 2))\n+    return out\n+\n+\n+def np_avgpool3d(x, pool_size, strides, padding, data_format):\n+    if data_format == \"channels_first\":\n+        x = x.transpose((0, 2, 3, 4, 1))\n+\n+    if isinstance(pool_size, int):\n+        pool_size = (pool_size, pool_size, pool_size)\n+    if isinstance(strides, int):\n+        strides = (strides, strides, strides)\n+\n+    h_pool_size, w_pool_size, d_pool_size = pool_size\n+    h_stride, w_stride, d_stride = strides\n+\n+    if padding == \"same\":\n+        n_batch, h_x, w_x, d_x, ch_x = x.shape\n+        h_padding = _same_padding(h_x, h_pool_size, h_stride)\n+        w_padding = _same_padding(w_x, w_pool_size, w_stride)\n+        d_padding = _same_padding(d_x, d_pool_size, d_stride)\n+        npad = [(0, 0)] * x.ndim\n+        npad[1] = (0, h_padding)\n+        npad[2] = (0, w_padding)\n+        npad[3] = (0, d_padding)\n+        x = np.pad(x, pad_width=npad, mode=\"symmetric\")\n+\n+    n_batch, h_x, w_x, d_x, ch_x = x.shape\n+    out_h = int((h_x - h_pool_size) / h_stride) + 1\n+    out_w = int((w_x - w_pool_size) / w_stride) + 1\n+    out_d = int((d_x - d_pool_size) / d_stride) + 1\n+\n+    stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n+    strides = (\n+        x.strides[0],\n+        h_stride * x.strides[1],\n+        w_stride * x.strides[2],\n+        d_stride * x.strides[3],\n+        x.strides[4],\n+        x.strides[1],\n+        x.strides[2],\n+        x.strides[3],\n+    )\n+    windows = as_strided(x, shape=stride_shape, strides=strides)\n+    out = np.mean(windows, axis=(5, 6, 7))\n+    if data_format == \"channels_first\":\n+        out = out.transpose((0, 4, 1, 2, 3))\n+    return out\n+\n+\n @pytest.mark.requires_trainable_backend\n class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters(\n@@ -111,128 +237,6 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n-    def _same_padding(self, input_size, pool_size, stride):\n-        if input_size % stride == 0:\n-            return max(pool_size - stride, 0)\n-        else:\n-            return max(pool_size - (input_size % stride), 0)\n-\n-    def _np_avgpool1d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.swapaxes(1, 2)\n-        if isinstance(pool_size, (tuple, list)):\n-            pool_size = pool_size[0]\n-        if isinstance(strides, (tuple, list)):\n-            h_stride = strides[0]\n-        else:\n-            h_stride = strides\n-\n-        if padding == \"same\":\n-            n_batch, h_x, ch_x = x.shape\n-            pad_value = self._same_padding(h_x, pool_size, h_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, pad_value)\n-            x = np.pad(x, pad_width=npad, mode=\"edge\")\n-\n-        n_batch, h_x, ch_x = x.shape\n-        out_h = int((h_x - pool_size) / h_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, ch_x, pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            x.strides[2],\n-            x.strides[1],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.mean(windows, axis=(3,))\n-        if data_format == \"channels_first\":\n-            out = out.swapaxes(1, 2)\n-        return out\n-\n-    def _np_avgpool2d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 1))\n-        if isinstance(pool_size, int):\n-            pool_size = (pool_size, pool_size)\n-        if isinstance(strides, int):\n-            strides = (strides, strides)\n-\n-        h_pool_size, w_pool_size = pool_size\n-        h_stride, w_stride = strides\n-        if padding == \"same\":\n-            n_batch, h_x, w_x, ch_x = x.shape\n-            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n-            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, h_padding)\n-            npad[2] = (0, w_padding)\n-            x = np.pad(x, pad_width=npad, mode=\"edge\")\n-\n-        n_batch, h_x, w_x, ch_x = x.shape\n-        out_h = int((h_x - h_pool_size) / h_stride) + 1\n-        out_w = int((w_x - w_pool_size) / w_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            w_stride * x.strides[2],\n-            x.strides[3],\n-            x.strides[1],\n-            x.strides[2],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.mean(windows, axis=(4, 5))\n-        if data_format == \"channels_first\":\n-            out = out.transpose((0, 3, 1, 2))\n-        return out\n-\n-    def _np_avgpool3d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 4, 1))\n-\n-        if isinstance(pool_size, int):\n-            pool_size = (pool_size, pool_size, pool_size)\n-        if isinstance(strides, int):\n-            strides = (strides, strides, strides)\n-\n-        h_pool_size, w_pool_size, d_pool_size = pool_size\n-        h_stride, w_stride, d_stride = strides\n-\n-        if padding == \"same\":\n-            n_batch, h_x, w_x, d_x, ch_x = x.shape\n-            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n-            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n-            d_padding = self._same_padding(d_x, d_pool_size, d_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, h_padding)\n-            npad[2] = (0, w_padding)\n-            npad[3] = (0, d_padding)\n-            x = np.pad(x, pad_width=npad, mode=\"symmetric\")\n-\n-        n_batch, h_x, w_x, d_x, ch_x = x.shape\n-        out_h = int((h_x - h_pool_size) / h_stride) + 1\n-        out_w = int((w_x - w_pool_size) / w_stride) + 1\n-        out_d = int((d_x - d_pool_size) / d_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            w_stride * x.strides[2],\n-            d_stride * x.strides[3],\n-            x.strides[4],\n-            x.strides[1],\n-            x.strides[2],\n-            x.strides[3],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.mean(windows, axis=(5, 6, 7))\n-        if data_format == \"channels_first\":\n-            out = out.transpose((0, 4, 1, 2, 3))\n-        return out\n-\n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n         (2, 1, \"valid\", \"channels_first\"),\n@@ -249,7 +253,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n         )\n         outputs = layer(inputs)\n-        expected = self._np_avgpool1d(\n+        expected = np_avgpool1d(\n             inputs, pool_size, strides, padding, data_format\n         )\n         self.assertAllClose(outputs, expected)\n@@ -276,7 +280,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n         )\n         outputs = layer(inputs)\n-        expected = self._np_avgpool1d(\n+        expected = np_avgpool1d(\n             inputs, pool_size, strides, padding, data_format\n         )\n         self.assertAllClose(outputs, expected)\n@@ -294,7 +298,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n         )\n         outputs = layer(inputs)\n-        expected = self._np_avgpool2d(\n+        expected = np_avgpool2d(\n             inputs, pool_size, strides, padding, data_format\n         )\n         self.assertAllClose(outputs, expected)\n@@ -320,7 +324,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n         )\n         outputs = layer(inputs)\n-        expected = self._np_avgpool2d(\n+        expected = np_avgpool2d(\n             inputs, pool_size, strides, padding, data_format\n         )\n         self.assertAllClose(outputs, expected)\n@@ -341,7 +345,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n         )\n         outputs = layer(inputs)\n-        expected = self._np_avgpool3d(\n+        expected = np_avgpool3d(\n             inputs, pool_size, strides, padding, data_format\n         )\n         self.assertAllClose(outputs, expected)\n@@ -368,7 +372,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n         )\n         outputs = layer(inputs)\n-        expected = self._np_avgpool3d(\n+        expected = np_avgpool3d(\n             inputs, pool_size, strides, padding, data_format\n         )\n         self.assertAllClose(outputs, expected)\n\n@@ -7,6 +7,132 @@ from keras import layers\n from keras import testing\n \n \n+def _same_padding(input_size, pool_size, stride):\n+    if input_size % stride == 0:\n+        return max(pool_size - stride, 0)\n+    else:\n+        return max(pool_size - (input_size % stride), 0)\n+\n+\n+def np_maxpool1d(x, pool_size, strides, padding, data_format):\n+    if data_format == \"channels_first\":\n+        x = x.swapaxes(1, 2)\n+    if isinstance(pool_size, (tuple, list)):\n+        pool_size = pool_size[0]\n+    if isinstance(strides, (tuple, list)):\n+        h_stride = strides[0]\n+    else:\n+        h_stride = strides\n+\n+    if padding == \"same\":\n+        n_batch, h_x, ch_x = x.shape\n+        pad_value = _same_padding(h_x, pool_size, h_stride)\n+        npad = [(0, 0)] * x.ndim\n+        npad[1] = (0, pad_value)\n+        x = np.pad(x, pad_width=npad, mode=\"constant\", constant_values=-np.inf)\n+\n+    n_batch, h_x, ch_x = x.shape\n+    out_h = int((h_x - pool_size) / h_stride) + 1\n+\n+    stride_shape = (n_batch, out_h, ch_x, pool_size)\n+    strides = (\n+        x.strides[0],\n+        h_stride * x.strides[1],\n+        x.strides[2],\n+        x.strides[1],\n+    )\n+    windows = as_strided(x, shape=stride_shape, strides=strides)\n+    out = np.max(windows, axis=(3,))\n+    if data_format == \"channels_first\":\n+        out = out.swapaxes(1, 2)\n+    return out\n+\n+\n+def np_maxpool2d(x, pool_size, strides, padding, data_format):\n+    if data_format == \"channels_first\":\n+        x = x.transpose((0, 2, 3, 1))\n+    if isinstance(pool_size, int):\n+        pool_size = (pool_size, pool_size)\n+    if isinstance(strides, int):\n+        strides = (strides, strides)\n+\n+    h_pool_size, w_pool_size = pool_size\n+    h_stride, w_stride = strides\n+    if padding == \"same\":\n+        n_batch, h_x, w_x, ch_x = x.shape\n+        h_padding = _same_padding(h_x, h_pool_size, h_stride)\n+        w_padding = _same_padding(w_x, w_pool_size, w_stride)\n+        npad = [(0, 0)] * x.ndim\n+        npad[1] = (0, h_padding)\n+        npad[2] = (0, w_padding)\n+        x = np.pad(x, pad_width=npad, mode=\"constant\", constant_values=-np.inf)\n+\n+    n_batch, h_x, w_x, ch_x = x.shape\n+    out_h = int((h_x - h_pool_size) / h_stride) + 1\n+    out_w = int((w_x - w_pool_size) / w_stride) + 1\n+\n+    stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n+    strides = (\n+        x.strides[0],\n+        h_stride * x.strides[1],\n+        w_stride * x.strides[2],\n+        x.strides[3],\n+        x.strides[1],\n+        x.strides[2],\n+    )\n+    windows = as_strided(x, shape=stride_shape, strides=strides)\n+    out = np.max(windows, axis=(4, 5))\n+    if data_format == \"channels_first\":\n+        out = out.transpose((0, 3, 1, 2))\n+    return out\n+\n+\n+def np_maxpool3d(x, pool_size, strides, padding, data_format):\n+    if data_format == \"channels_first\":\n+        x = x.transpose((0, 2, 3, 4, 1))\n+\n+    if isinstance(pool_size, int):\n+        pool_size = (pool_size, pool_size, pool_size)\n+    if isinstance(strides, int):\n+        strides = (strides, strides, strides)\n+\n+    h_pool_size, w_pool_size, d_pool_size = pool_size\n+    h_stride, w_stride, d_stride = strides\n+\n+    if padding == \"same\":\n+        n_batch, h_x, w_x, d_x, ch_x = x.shape\n+        h_padding = _same_padding(h_x, h_pool_size, h_stride)\n+        w_padding = _same_padding(w_x, w_pool_size, w_stride)\n+        d_padding = _same_padding(d_x, d_pool_size, d_stride)\n+        npad = [(0, 0)] * x.ndim\n+        npad[1] = (0, h_padding)\n+        npad[2] = (0, w_padding)\n+        npad[3] = (0, d_padding)\n+        x = np.pad(x, pad_width=npad, mode=\"constant\", constant_values=-np.inf)\n+\n+    n_batch, h_x, w_x, d_x, ch_x = x.shape\n+    out_h = int((h_x - h_pool_size) / h_stride) + 1\n+    out_w = int((w_x - w_pool_size) / w_stride) + 1\n+    out_d = int((d_x - d_pool_size) / d_stride) + 1\n+\n+    stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n+    strides = (\n+        x.strides[0],\n+        h_stride * x.strides[1],\n+        w_stride * x.strides[2],\n+        d_stride * x.strides[3],\n+        x.strides[4],\n+        x.strides[1],\n+        x.strides[2],\n+        x.strides[3],\n+    )\n+    windows = as_strided(x, shape=stride_shape, strides=strides)\n+    out = np.max(windows, axis=(5, 6, 7))\n+    if data_format == \"channels_first\":\n+        out = out.transpose((0, 4, 1, 2, 3))\n+    return out\n+\n+\n @pytest.mark.requires_trainable_backend\n class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters(\n@@ -110,134 +236,6 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n-    def _same_padding(self, input_size, pool_size, stride):\n-        if input_size % stride == 0:\n-            return max(pool_size - stride, 0)\n-        else:\n-            return max(pool_size - (input_size % stride), 0)\n-\n-    def _np_maxpool1d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.swapaxes(1, 2)\n-        if isinstance(pool_size, (tuple, list)):\n-            pool_size = pool_size[0]\n-        if isinstance(strides, (tuple, list)):\n-            h_stride = strides[0]\n-        else:\n-            h_stride = strides\n-\n-        if padding == \"same\":\n-            n_batch, h_x, ch_x = x.shape\n-            pad_value = self._same_padding(h_x, pool_size, h_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, pad_value)\n-            x = np.pad(\n-                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n-            )\n-\n-        n_batch, h_x, ch_x = x.shape\n-        out_h = int((h_x - pool_size) / h_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, ch_x, pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            x.strides[2],\n-            x.strides[1],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.max(windows, axis=(3,))\n-        if data_format == \"channels_first\":\n-            out = out.swapaxes(1, 2)\n-        return out\n-\n-    def _np_maxpool2d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 1))\n-        if isinstance(pool_size, int):\n-            pool_size = (pool_size, pool_size)\n-        if isinstance(strides, int):\n-            strides = (strides, strides)\n-\n-        h_pool_size, w_pool_size = pool_size\n-        h_stride, w_stride = strides\n-        if padding == \"same\":\n-            n_batch, h_x, w_x, ch_x = x.shape\n-            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n-            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, h_padding)\n-            npad[2] = (0, w_padding)\n-            x = np.pad(\n-                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n-            )\n-\n-        n_batch, h_x, w_x, ch_x = x.shape\n-        out_h = int((h_x - h_pool_size) / h_stride) + 1\n-        out_w = int((w_x - w_pool_size) / w_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            w_stride * x.strides[2],\n-            x.strides[3],\n-            x.strides[1],\n-            x.strides[2],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.max(windows, axis=(4, 5))\n-        if data_format == \"channels_first\":\n-            out = out.transpose((0, 3, 1, 2))\n-        return out\n-\n-    def _np_maxpool3d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 4, 1))\n-\n-        if isinstance(pool_size, int):\n-            pool_size = (pool_size, pool_size, pool_size)\n-        if isinstance(strides, int):\n-            strides = (strides, strides, strides)\n-\n-        h_pool_size, w_pool_size, d_pool_size = pool_size\n-        h_stride, w_stride, d_stride = strides\n-\n-        if padding == \"same\":\n-            n_batch, h_x, w_x, d_x, ch_x = x.shape\n-            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n-            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n-            d_padding = self._same_padding(d_x, d_pool_size, d_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, h_padding)\n-            npad[2] = (0, w_padding)\n-            npad[3] = (0, d_padding)\n-            x = np.pad(\n-                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n-            )\n-\n-        n_batch, h_x, w_x, d_x, ch_x = x.shape\n-        out_h = int((h_x - h_pool_size) / h_stride) + 1\n-        out_w = int((w_x - w_pool_size) / w_stride) + 1\n-        out_d = int((d_x - d_pool_size) / d_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            w_stride * x.strides[2],\n-            d_stride * x.strides[3],\n-            x.strides[4],\n-            x.strides[1],\n-            x.strides[2],\n-            x.strides[3],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.max(windows, axis=(5, 6, 7))\n-        if data_format == \"channels_first\":\n-            out = out.transpose((0, 4, 1, 2, 3))\n-        return out\n-\n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n         (2, 1, \"valid\", \"channels_first\"),\n@@ -256,7 +254,7 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n         )\n         outputs = layer(inputs)\n-        expected = self._np_maxpool1d(\n+        expected = np_maxpool1d(\n             inputs, pool_size, strides, padding, data_format\n         )\n         self.assertAllClose(outputs, expected)\n@@ -278,7 +276,7 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n         )\n         outputs = layer(inputs)\n-        expected = self._np_maxpool2d(\n+        expected = np_maxpool2d(\n             inputs, pool_size, strides, padding, data_format\n         )\n         self.assertAllClose(outputs, expected)\n@@ -299,7 +297,7 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             data_format=data_format,\n         )\n         outputs = layer(inputs)\n-        expected = self._np_maxpool3d(\n+        expected = np_maxpool3d(\n             inputs, pool_size, strides, padding, data_format\n         )\n         self.assertAllClose(outputs, expected)\n\n@@ -1,11 +1,20 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n \n from keras import backend\n from keras import testing\n from keras.backend.common.keras_tensor import KerasTensor\n+from keras.layers.convolutional.conv_test import np_conv1d\n+from keras.layers.convolutional.conv_test import np_conv2d\n+from keras.layers.convolutional.conv_test import np_conv3d\n+from keras.layers.convolutional.conv_transpose_test import np_conv1d_transpose\n+from keras.layers.convolutional.conv_transpose_test import np_conv2d_transpose\n+from keras.layers.convolutional.depthwise_conv_test import np_depthwise_conv2d\n+from keras.layers.pooling.average_pooling_test import np_avgpool1d\n+from keras.layers.pooling.average_pooling_test import np_avgpool2d\n+from keras.layers.pooling.max_pooling_test import np_maxpool1d\n+from keras.layers.pooling.max_pooling_test import np_maxpool2d\n from keras.ops import nn as knn\n \n \n@@ -788,22 +797,24 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         x = np.arange(120, dtype=float).reshape([2, 20, 3])\n         self.assertAllClose(\n             knn.max_pool(x, 2, 1, padding=\"valid\"),\n-            tf.nn.max_pool1d(x, 2, 1, padding=\"VALID\"),\n+            np_maxpool1d(x, 2, 1, padding=\"valid\", data_format=\"channels_last\"),\n         )\n         self.assertAllClose(\n             knn.max_pool(x, 2, 2, padding=\"same\"),\n-            tf.nn.max_pool1d(x, 2, 2, padding=\"SAME\"),\n+            np_maxpool1d(x, 2, 2, padding=\"same\", data_format=\"channels_last\"),\n         )\n \n         # Test 2D max pooling.\n         x = np.arange(540, dtype=float).reshape([2, 10, 9, 3])\n         self.assertAllClose(\n             knn.max_pool(x, 2, 1, padding=\"valid\"),\n-            tf.nn.max_pool2d(x, 2, 1, padding=\"VALID\"),\n+            np_maxpool2d(x, 2, 1, padding=\"valid\", data_format=\"channels_last\"),\n         )\n         self.assertAllClose(\n             knn.max_pool(x, 2, (2, 1), padding=\"same\"),\n-            tf.nn.max_pool2d(x, 2, (2, 1), padding=\"SAME\"),\n+            np_maxpool2d(\n+                x, 2, (2, 1), padding=\"same\", data_format=\"channels_last\"\n+            ),\n         )\n \n     def test_average_pool_valid_padding(self):\n@@ -811,14 +822,14 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         x = np.arange(120, dtype=float).reshape([2, 20, 3])\n         self.assertAllClose(\n             knn.average_pool(x, 2, 1, padding=\"valid\"),\n-            tf.nn.avg_pool1d(x, 2, 1, padding=\"VALID\"),\n+            np_avgpool1d(x, 2, 1, padding=\"valid\", data_format=\"channels_last\"),\n         )\n \n         # Test 2D max pooling.\n         x = np.arange(540, dtype=float).reshape([2, 10, 9, 3])\n         self.assertAllClose(\n             knn.average_pool(x, 2, 1, padding=\"valid\"),\n-            tf.nn.avg_pool2d(x, 2, 1, padding=\"VALID\"),\n+            np_avgpool2d(x, 2, 1, padding=\"valid\", data_format=\"channels_last\"),\n         )\n \n     @pytest.mark.skipif(\n@@ -830,14 +841,16 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         x = np.arange(120, dtype=float).reshape([2, 20, 3])\n         self.assertAllClose(\n             knn.average_pool(x, 2, 2, padding=\"same\"),\n-            tf.nn.avg_pool1d(x, 2, 2, padding=\"SAME\"),\n+            np_avgpool1d(x, 2, 2, padding=\"same\", data_format=\"channels_last\"),\n         )\n \n         # Test 2D max pooling.\n         x = np.arange(540, dtype=float).reshape([2, 10, 9, 3])\n         self.assertAllClose(\n             knn.average_pool(x, 2, (2, 1), padding=\"same\"),\n-            tf.nn.avg_pool2d(x, 2, (2, 1), padding=\"SAME\"),\n+            np_avgpool2d(\n+                x, 2, (2, 1), padding=\"same\", data_format=\"channels_last\"\n+            ),\n         )\n \n     @parameterized.product(\n@@ -859,12 +872,15 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             dilation_rate=dilation_rate,\n         )\n-        expected = tf.nn.conv1d(\n+        expected = np_conv1d(\n             inputs_1d,\n             kernel,\n-            strides,\n-            padding=padding.upper(),\n-            dilations=dilation_rate,\n+            bias_weights=np.zeros((2,)),\n+            strides=strides,\n+            padding=padding.lower(),\n+            data_format=\"channels_last\",\n+            dilation_rate=dilation_rate,\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n@@ -873,19 +889,55 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         kernel = np.arange(24, dtype=float).reshape([2, 2, 3, 2])\n \n         outputs = knn.conv(inputs_2d, kernel, 1, padding=\"valid\")\n-        expected = tf.nn.conv2d(inputs_2d, kernel, 1, padding=\"VALID\")\n+        expected = np_conv2d(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros((2,)),\n+            strides=1,\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n+        )\n         self.assertAllClose(outputs, expected)\n \n         outputs = knn.conv(inputs_2d, kernel, (1, 2), padding=\"valid\")\n-        expected = tf.nn.conv2d(inputs_2d, kernel, (1, 2), padding=\"VALID\")\n+        expected = np_conv2d(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros((2,)),\n+            strides=(1, 2),\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n+        )\n         self.assertAllClose(outputs, expected)\n \n         outputs = knn.conv(inputs_2d, kernel, (1, 2), padding=\"same\")\n-        expected = tf.nn.conv2d(inputs_2d, kernel, (1, 2), padding=\"SAME\")\n+        expected = np_conv2d(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros((2,)),\n+            strides=(1, 2),\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n+        )\n         self.assertAllClose(outputs, expected)\n \n         outputs = knn.conv(inputs_2d, kernel, 2, padding=\"same\")\n-        expected = tf.nn.conv2d(inputs_2d, kernel, 2, padding=\"SAME\")\n+        expected = np_conv2d(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros((2,)),\n+            strides=2,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n+        )\n         self.assertAllClose(outputs, expected)\n \n         # Test group > 1.\n@@ -894,8 +946,15 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         outputs = knn.conv(\n             inputs_2d, kernel, 2, padding=\"same\", dilation_rate=1\n         )\n-        expected = tf.nn.conv2d(\n-            inputs_2d, kernel, 2, padding=\"SAME\", dilations=1\n+        expected = np_conv2d(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros((6,)),\n+            strides=2,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n@@ -906,12 +965,15 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=\"same\",\n             dilation_rate=(2, 1),\n         )\n-        expected = tf.nn.conv2d(\n+        expected = np_conv2d(\n             inputs_2d,\n             kernel,\n-            1,\n-            padding=\"SAME\",\n-            dilations=(2, 1),\n+            bias_weights=np.zeros((6,)),\n+            strides=1,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=(2, 1),\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n@@ -920,8 +982,15 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         kernel = np.arange(162, dtype=float).reshape([3, 3, 3, 3, 2])\n \n         outputs = knn.conv(inputs_3d, kernel, 1, padding=\"valid\")\n-        expected = tf.nn.conv3d(\n-            inputs_3d, kernel, (1, 1, 1, 1, 1), padding=\"VALID\"\n+        expected = np_conv3d(\n+            inputs_3d,\n+            kernel,\n+            bias_weights=np.zeros((2,)),\n+            strides=(1, 1, 1),\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n@@ -932,24 +1001,41 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=\"valid\",\n             dilation_rate=(1, 1, 1),\n         )\n-        expected = tf.nn.conv3d(\n+        expected = np_conv3d(\n             inputs_3d,\n             kernel,\n-            (1, 1, 1, 1, 1),\n-            padding=\"VALID\",\n-            dilations=(1, 1, 1, 1, 1),\n+            bias_weights=np.zeros((2,)),\n+            strides=(1, 1, 1),\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=(1, 1, 1),\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n         outputs = knn.conv(inputs_3d, kernel, 2, padding=\"valid\")\n-        expected = tf.nn.conv3d(\n-            inputs_3d, kernel, (1, 2, 2, 2, 1), padding=\"VALID\"\n+        expected = np_conv3d(\n+            inputs_3d,\n+            kernel,\n+            bias_weights=np.zeros((2,)),\n+            strides=2,\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n         outputs = knn.conv(inputs_3d, kernel, 2, padding=\"same\")\n-        expected = tf.nn.conv3d(\n-            inputs_3d, kernel, (1, 2, 2, 2, 1), padding=\"SAME\"\n+        expected = np_conv3d(\n+            inputs_3d,\n+            kernel,\n+            bias_weights=np.zeros((2,)),\n+            strides=2,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n \n@@ -958,28 +1044,52 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         kernel = np.arange(24, dtype=float).reshape([2, 2, 3, 2])\n \n         outputs = knn.depthwise_conv(inputs_2d, kernel, 1, padding=\"valid\")\n-        expected = tf.nn.depthwise_conv2d(\n-            inputs_2d, kernel, (1, 1, 1, 1), padding=\"VALID\"\n+        expected = np_depthwise_conv2d(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros((6,)),\n+            strides=1,\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n         outputs = knn.depthwise_conv(inputs_2d, kernel, (1, 1), padding=\"valid\")\n-        expected = tf.nn.depthwise_conv2d(\n-            inputs_2d, kernel, (1, 1, 1, 1), padding=\"VALID\"\n+        expected = np_depthwise_conv2d(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros((6,)),\n+            strides=(1, 1),\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n         outputs = knn.depthwise_conv(inputs_2d, kernel, (2, 2), padding=\"same\")\n-        expected = tf.nn.depthwise_conv2d(\n-            inputs_2d, kernel, (1, 2, 2, 1), padding=\"SAME\"\n+        expected = np_depthwise_conv2d(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros((6,)),\n+            strides=(2, 2),\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n         outputs = knn.depthwise_conv(\n             inputs_2d, kernel, 1, padding=\"same\", dilation_rate=(2, 2)\n         )\n-        expected = tf.nn.depthwise_conv2d(\n-            inputs_2d, kernel, (1, 1, 1, 1), padding=\"SAME\", dilations=(2, 2)\n+        expected = np_depthwise_conv2d(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros((6,)),\n+            strides=1,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=(2, 2),\n         )\n         self.assertAllClose(outputs, expected)\n \n@@ -992,12 +1102,25 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         outputs = knn.separable_conv(\n             inputs_2d, depthwise_kernel, pointwise_kernel, 1, padding=\"valid\"\n         )\n-        expected = tf.nn.separable_conv2d(\n+        # Depthwise followed by pointwise conv\n+        expected_depthwise = np_depthwise_conv2d(\n             inputs_2d,\n             depthwise_kernel,\n+            np.zeros(6),\n+            strides=1,\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+        )\n+        expected = np_conv2d(\n+            expected_depthwise,\n             pointwise_kernel,\n-            (1, 1, 1, 1),\n-            padding=\"VALID\",\n+            np.zeros(6 * 12),\n+            strides=1,\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n@@ -1008,24 +1131,30 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             (1, 1),\n             padding=\"valid\",\n         )\n-        expected = tf.nn.separable_conv2d(\n-            inputs_2d,\n-            depthwise_kernel,\n-            pointwise_kernel,\n-            (1, 1, 1, 1),\n-            padding=\"VALID\",\n-        )\n         self.assertAllClose(outputs, expected)\n \n         outputs = knn.separable_conv(\n             inputs_2d, depthwise_kernel, pointwise_kernel, 2, padding=\"same\"\n         )\n-        expected = tf.nn.separable_conv2d(\n+        # Depthwise followed by pointwise conv\n+        expected_depthwise = np_depthwise_conv2d(\n             inputs_2d,\n             depthwise_kernel,\n+            np.zeros(6),\n+            strides=2,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+        )\n+        expected = np_conv2d(\n+            expected_depthwise,\n             pointwise_kernel,\n-            (1, 2, 2, 1),\n-            padding=\"SAME\",\n+            np.zeros(6 * 12),\n+            strides=1,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n@@ -1037,13 +1166,25 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=\"same\",\n             dilation_rate=(2, 2),\n         )\n-        expected = tf.nn.separable_conv2d(\n+        # Depthwise followed by pointwise conv\n+        expected_depthwise = np_depthwise_conv2d(\n             inputs_2d,\n             depthwise_kernel,\n+            np.zeros(6),\n+            strides=1,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=(2, 2),\n+        )\n+        expected = np_conv2d(\n+            expected_depthwise,\n             pointwise_kernel,\n-            (1, 1, 1, 1),\n-            padding=\"SAME\",\n-            dilations=(2, 2),\n+            np.zeros(6 * 12),\n+            strides=1,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n+            groups=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n@@ -1051,14 +1192,28 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         inputs_1d = np.arange(24, dtype=float).reshape([2, 4, 3])\n         kernel = np.arange(30, dtype=float).reshape([2, 5, 3])\n         outputs = knn.conv_transpose(inputs_1d, kernel, 2, padding=\"valid\")\n-        expected = tf.nn.conv_transpose(\n-            inputs_1d, kernel, [2, 8, 5], 2, padding=\"VALID\"\n+        expected = np_conv1d_transpose(\n+            inputs_1d,\n+            kernel,\n+            bias_weights=np.zeros(5),\n+            strides=2,\n+            output_padding=None,\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n         outputs = knn.conv_transpose(inputs_1d, kernel, 2, padding=\"same\")\n-        expected = tf.nn.conv_transpose(\n-            inputs_1d, kernel, [2, 8, 5], 2, padding=\"SAME\"\n+        expected = np_conv1d_transpose(\n+            inputs_1d,\n+            kernel,\n+            bias_weights=np.zeros(5),\n+            strides=2,\n+            output_padding=None,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n@@ -1067,46 +1222,70 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         kernel = np.arange(60, dtype=float).reshape([2, 2, 5, 3])\n \n         outputs = knn.conv_transpose(inputs_2d, kernel, (2, 2), padding=\"valid\")\n-        expected = tf.nn.conv_transpose(\n-            inputs_2d, kernel, [2, 8, 8, 5], (2, 2), padding=\"VALID\"\n+        expected = np_conv2d_transpose(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros(5),\n+            strides=(2, 2),\n+            output_padding=None,\n+            padding=\"valid\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n         outputs = knn.conv_transpose(inputs_2d, kernel, 2, padding=\"same\")\n-        expected = tf.nn.conv_transpose(\n-            inputs_2d, kernel, [2, 8, 8, 5], 2, padding=\"SAME\"\n+        expected = np_conv2d_transpose(\n+            inputs_2d,\n+            kernel,\n+            bias_weights=np.zeros(5),\n+            strides=2,\n+            output_padding=None,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            dilation_rate=1,\n         )\n         self.assertAllClose(outputs, expected)\n \n     def test_one_hot(self):\n         # Test 1D one-hot.\n         indices_1d = np.array([0, 1, 2, 3])\n-        self.assertAllClose(\n-            knn.one_hot(indices_1d, 4), tf.one_hot(indices_1d, 4)\n-        )\n+        self.assertAllClose(knn.one_hot(indices_1d, 4), np.eye(4)[indices_1d])\n         self.assertAllClose(\n             knn.one_hot(indices_1d, 4, axis=0),\n-            tf.one_hot(indices_1d, 4, axis=0),\n+            np.eye(4)[indices_1d],\n         )\n \n         # Test 2D one-hot.\n         indices_2d = np.array([[0, 1], [2, 3]])\n-        self.assertAllClose(\n-            knn.one_hot(indices_2d, 4), tf.one_hot(indices_2d, 4)\n-        )\n+        self.assertAllClose(knn.one_hot(indices_2d, 4), np.eye(4)[indices_2d])\n         self.assertAllClose(\n             knn.one_hot(indices_2d, 4, axis=2),\n-            tf.one_hot(indices_2d, 4, axis=2),\n+            np.eye(4)[indices_2d],\n         )\n         self.assertAllClose(\n             knn.one_hot(indices_2d, 4, axis=1),\n-            tf.one_hot(indices_2d, 4, axis=1),\n+            np.transpose(np.eye(4)[indices_2d], (0, 2, 1)),\n         )\n \n         # Test 1D one-hot with negative inputs\n         indices_1d = np.array([0, -1, -1, 3])\n         self.assertAllClose(\n-            knn.one_hot(indices_1d, 4), tf.one_hot(indices_1d, 4)\n+            knn.one_hot(indices_1d, 4),\n+            np.array(\n+                [\n+                    [1, 0, 0, 0],\n+                    [0, 0, 0, 0],\n+                    [0, 0, 0, 0],\n+                    [\n+                        0,\n+                        0,\n+                        0,\n+                        1,\n+                    ],\n+                ],\n+                dtype=np.float32,\n+            ),\n         )\n \n     def test_binary_crossentropy(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
