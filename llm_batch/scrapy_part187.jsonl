{"custom_id": "scrapy#fee55657094fa8ecf871258953d461e9b1fa0e26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 158 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -72,7 +72,6 @@ class TelnetConsole(protocol.ServerFactory):\n             'crawler': self.crawler,\n             'extensions': self.crawler.extensions,\n             'stats': self.crawler.stats,\n-            'spiders': self.crawler.spiders,\n             'settings': self.crawler.settings,\n             'est': lambda: print_engine_status(self.crawler.engine),\n             'p': pprint.pprint,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad587ea792d4f401abc69d010b350dcafe84eab6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 41 | Files Changed: 12 | Hunks: 38 | Methods Changed: 22 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 98 | Churn Cumulative: 2834 | Contributors (this commit): 14 | Commits (past 90d): 29 | Contributors (cumulative): 60 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,10 +69,10 @@ class Command(ScrapyCommand):\n         # contract requests\n         contract_reqs = defaultdict(list)\n \n-        spiders = self.crawler_process.spiders\n+        spider_loader = self.crawler_process.spider_loader\n \n-        for spidername in args or spiders.list():\n-            spidercls = spiders.load(spidername)\n+        for spidername in args or spider_loader.list():\n+            spidercls = spider_loader.load(spidername)\n             spidercls.start_requests = lambda s: conman.from_spider(s, result)\n \n             tested_methods = conman.tested_methods_from_spidercls(spidercls)\n\n@@ -27,7 +27,7 @@ class Command(ScrapyCommand):\n \n         editor = self.settings['EDITOR']\n         try:\n-            spidercls = self.crawler_process.spiders.load(args[0])\n+            spidercls = self.crawler_process.spider_loader.load(args[0])\n         except KeyError:\n             return self._err(\"Spider not found: %s\" % args[0])\n \n\n@@ -48,10 +48,10 @@ class Command(ScrapyCommand):\n         request.meta['handle_httpstatus_all'] = True\n \n         spidercls = DefaultSpider\n-        spiders = self.crawler_process.spiders\n+        spider_loader = self.crawler_process.spider_loader\n         if opts.spider:\n-            spidercls = spiders.load(opts.spider)\n+            spidercls = spider_loader.load(opts.spider)\n         else:\n-            spidercls = spidercls_for_request(spiders, request, spidercls)\n+            spidercls = spidercls_for_request(spider_loader, request, spidercls)\n         self.crawler_process.crawl(spidercls, start_requests=lambda: [request])\n         self.crawler_process.start()\n\n@@ -65,7 +65,7 @@ class Command(ScrapyCommand):\n             return\n \n         try:\n-            spidercls = self.crawler_process.spiders.load(name)\n+            spidercls = self.crawler_process.spider_loader.load(name)\n         except KeyError:\n             pass\n         else:\n\n@@ -10,5 +10,5 @@ class Command(ScrapyCommand):\n         return \"List available spiders\"\n \n     def run(self, args, opts):\n-        for s in sorted(self.crawler_process.spiders.list()):\n+        for s in sorted(self.crawler_process.spider_loader.list()):\n             print(s)\n\n@@ -124,15 +124,15 @@ class Command(ScrapyCommand):\n                     level=log.ERROR, spider=spider.name)\n \n     def set_spidercls(self, url, opts):\n-        spiders = self.crawler_process.spiders\n+        spider_loader = self.crawler_process.spider_loader\n         if opts.spider:\n             try:\n-                self.spidercls = spiders.load(opts.spider)\n+                self.spidercls = spider_loader.load(opts.spider)\n             except KeyError:\n                 log.msg(format='Unable to find spider: %(spider)s',\n                         level=log.ERROR, spider=opts.spider)\n         else:\n-            self.spidercls = spidercls_for_request(spiders, Request(url))\n+            self.spidercls = spidercls_for_request(spider_loader, Request(url))\n             if not self.spidercls:\n                 log.msg(format='Unable to find spider for: %(url)s',\n                         level=log.ERROR, url=url)\n\n@@ -42,13 +42,13 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         url = args[0] if args else None\n-        spiders = self.crawler_process.spiders\n+        spider_loader = self.crawler_process.spider_loader\n \n         spidercls = DefaultSpider\n         if opts.spider:\n-            spidercls = spiders.load(opts.spider)\n+            spidercls = spider_loader.load(opts.spider)\n         elif url:\n-            spidercls = spidercls_for_request(spiders, Request(url),\n+            spidercls = spidercls_for_request(spider_loader, Request(url),\n                                               spidercls, log_multiple=True)\n \n         # The crawler is created this way since the Shell manually handles the\n\n@@ -43,7 +43,7 @@ class Crawler(object):\n     def spiders(self):\n         if not hasattr(self, '_spiders'):\n             warnings.warn(\"Crawler.spiders is deprecated, use \"\n-                          \"CrawlerRunner.spiders or instantiate \"\n+                          \"CrawlerRunner.spider_loader or instantiate \"\n                           \"scrapy.spiderloader.SpiderLoader with your \"\n                           \"settings.\",\n                           category=ScrapyDeprecationWarning, stacklevel=2)\n@@ -84,10 +84,17 @@ class CrawlerRunner(object):\n         if isinstance(settings, dict):\n             settings = Settings(settings)\n         self.settings = settings\n-        self.spiders = _get_spider_loader(settings)\n+        self.spider_loader = _get_spider_loader(settings)\n         self.crawlers = set()\n         self._active = set()\n \n+    @property\n+    def spiders(self):\n+        warnings.warn(\"CrawlerRunner.spiders attribute is renamed to \"\n+                      \"CrawlerRunner.spider_loader.\",\n+                      category=ScrapyDeprecationWarning, stacklevel=2)\n+        return self.spider_loader\n+\n     def crawl(self, crawler_or_spidercls, *args, **kwargs):\n         crawler = crawler_or_spidercls\n         if not isinstance(crawler_or_spidercls, Crawler):\n@@ -107,7 +114,7 @@ class CrawlerRunner(object):\n \n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, six.string_types):\n-            spidercls = self.spiders.load(spidercls)\n+            spidercls = self.spider_loader.load(spidercls)\n         return Crawler(spidercls, self.settings)\n \n     def _setup_crawler_logging(self, crawler):\n\n@@ -41,7 +41,7 @@ class SpiderLoader(object):\n \n     def find_by_request(self, request):\n         \"\"\"\n-        Return the list of spiders names that can handle the given request.\n+        Return the list of spider names that can handle the given request.\n         \"\"\"\n         return [name for name, cls in self._spiders.items()\n                 if cls.handles_request(request)]\n\n@@ -26,7 +26,7 @@ def iter_spider_classes(module):\n            getattr(obj, 'name', None):\n             yield obj\n \n-def spidercls_for_request(spiderloader, request, default_spidercls=None,\n+def spidercls_for_request(spider_loader, request, default_spidercls=None,\n                           log_none=False, log_multiple=False):\n     \"\"\"Return a spider class that handles the given Request.\n \n@@ -38,9 +38,9 @@ def spidercls_for_request(spiderloader, request, default_spidercls=None,\n     default_spidercls passed. It can optionally log if multiple or no spiders\n     are found.\n     \"\"\"\n-    snames = spiderloader.find_by_request(request)\n+    snames = spider_loader.find_by_request(request)\n     if len(snames) == 1:\n-        return spiderloader.load(snames[0])\n+        return spider_loader.load(snames[0])\n \n     if len(snames) > 1 and log_multiple:\n         log.msg(format='More than one spider can handle: %(request)s - %(snames)s',\n\n@@ -19,8 +19,8 @@ class CrawlerTestCase(unittest.TestCase):\n             spiders = self.crawler.spiders\n             self.assertEqual(len(w), 1)\n             self.assertIn(\"Crawler.spiders\", str(w[0].message))\n-            sm_cls = load_object(self.crawler.settings['SPIDER_LOADER_CLASS'])\n-            self.assertIsInstance(spiders, sm_cls)\n+            sl_cls = load_object(self.crawler.settings['SPIDER_LOADER_CLASS'])\n+            self.assertIsInstance(spiders, sl_cls)\n \n             self.crawler.spiders\n             self.assertEqual(len(w), 1, \"Warn deprecated access only once\")\n@@ -78,3 +78,12 @@ class CrawlerRunnerTestCase(unittest.TestCase):\n         )\n         self.assertIsInstance(runner.settings, Settings)\n \n+    def test_deprecated_attribute_spiders(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            runner = CrawlerRunner(Settings())\n+            spiders = runner.spiders\n+            self.assertEqual(len(w), 1)\n+            self.assertIn(\"CrawlerRunner.spiders\", str(w[0].message))\n+            self.assertIn(\"CrawlerRunner.spider_loader\", str(w[0].message))\n+            sl_cls = load_object(runner.settings['SPIDER_LOADER_CLASS'])\n+            self.assertIsInstance(spiders, sl_cls)\n\n@@ -26,53 +26,53 @@ class SpiderLoaderTest(unittest.TestCase):\n         shutil.copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(self.tmpdir)\n         settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n-        self.spiderloader = SpiderLoader.from_settings(settings)\n+        self.spider_loader = SpiderLoader.from_settings(settings)\n \n     def tearDown(self):\n-        del self.spiderloader\n+        del self.spider_loader\n         del sys.modules['test_spiders_xxx']\n         sys.path.remove(self.tmpdir)\n \n     def test_interface(self):\n-        verifyObject(ISpiderLoader, self.spiderloader)\n+        verifyObject(ISpiderLoader, self.spider_loader)\n \n     def test_list(self):\n-        self.assertEqual(set(self.spiderloader.list()),\n+        self.assertEqual(set(self.spider_loader.list()),\n             set(['spider1', 'spider2', 'spider3']))\n \n     def test_load(self):\n-        spider1 = self.spiderloader.load(\"spider1\")\n+        spider1 = self.spider_loader.load(\"spider1\")\n         self.assertEqual(spider1.__name__, 'Spider1')\n \n     def test_find_by_request(self):\n-        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy1.org/test')),\n+        self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy1.org/test')),\n             ['spider1'])\n-        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy2.org/test')),\n+        self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy2.org/test')),\n             ['spider2'])\n-        self.assertEqual(set(self.spiderloader.find_by_request(Request('http://scrapy3.org/test'))),\n+        self.assertEqual(set(self.spider_loader.find_by_request(Request('http://scrapy3.org/test'))),\n             set(['spider1', 'spider2']))\n-        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy999.org/test')),\n+        self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy999.org/test')),\n             [])\n-        self.assertEqual(self.spiderloader.find_by_request(Request('http://spider3.com')),\n+        self.assertEqual(self.spider_loader.find_by_request(Request('http://spider3.com')),\n             [])\n-        self.assertEqual(self.spiderloader.find_by_request(Request('http://spider3.com/onlythis')),\n+        self.assertEqual(self.spider_loader.find_by_request(Request('http://spider3.com/onlythis')),\n             ['spider3'])\n \n     def test_load_spider_module(self):\n         module = 'tests.test_spiderloader.test_spiders.spider1'\n         settings = Settings({'SPIDER_MODULES': [module]})\n-        self.spiderloader = SpiderLoader.from_settings(settings)\n-        assert len(self.spiderloader._spiders) == 1\n+        self.spider_loader = SpiderLoader.from_settings(settings)\n+        assert len(self.spider_loader._spiders) == 1\n \n     def test_load_spider_module(self):\n         prefix = 'tests.test_spiderloader.test_spiders.'\n         module = ','.join(prefix + s for s in ('spider1', 'spider2'))\n         settings = Settings({'SPIDER_MODULES': module})\n-        self.spiderloader = SpiderLoader.from_settings(settings)\n-        assert len(self.spiderloader._spiders) == 2\n+        self.spider_loader = SpiderLoader.from_settings(settings)\n+        assert len(self.spider_loader._spiders) == 2\n \n     def test_load_base_spider(self):\n         module = 'tests.test_spiderloader.test_spiders.spider0'\n         settings = Settings({'SPIDER_MODULES': [module]})\n-        self.spiderloader = SpiderLoader.from_settings(settings)\n-        assert len(self.spiderloader._spiders) == 0\n+        self.spider_loader = SpiderLoader.from_settings(settings)\n+        assert len(self.spider_loader._spiders) == 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ffc60910aadb8b804c440695baa4dd4e0805df4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 214 | Files Changed: 6 | Hunks: 7 | Methods Changed: 17 | Complexity Δ (Sum/Max): -30/0 | Churn Δ: 214 | Churn Cumulative: 595 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,11 +2,7 @@ import six\n import pytest\n from twisted.python import log\n \n-from scrapy import optional_features\n-\n collect_ignore = [\"scrapy/stats.py\", \"scrapy/project.py\"]\n-if 'django' not in optional_features:\n-    collect_ignore.append(\"tests/test_djangoitem/models.py\")\n \n if six.PY3:\n     for line in open('tests/py3-ignores.txt'):\n\n@@ -38,13 +38,6 @@ except ImportError:\n     pass\n else:\n     optional_features.add('boto')\n-try:\n-    import django\n-    del django\n-except ImportError:\n-    pass\n-else:\n-    optional_features.add('django')\n \n from twisted import version as _txv\n twisted_version = (_txv.major, _txv.minor, _txv.micro)\n\n@@ -1,75 +0,0 @@\n-from scrapy.item import Field, Item, ItemMeta\n-from scrapy import optional_features\n-if 'django' in optional_features:\n-    from django.core.exceptions import ValidationError\n-\n-\n-class DjangoItemMeta(ItemMeta):\n-\n-    def  __new__(mcs, class_name, bases, attrs):\n-        cls = super(DjangoItemMeta, mcs).__new__(mcs, class_name, bases, attrs)\n-        cls.fields = cls.fields.copy()\n-\n-        if cls.django_model:\n-            cls._model_fields = []\n-            cls._model_meta = cls.django_model._meta\n-            for model_field in cls._model_meta.fields:\n-                if not model_field.auto_created:\n-                    if model_field.name not in cls.fields:\n-                        cls.fields[model_field.name] = Field()\n-                    cls._model_fields.append(model_field.name)\n-        return cls\n-\n-\n-class DjangoItem(Item):\n-\n-    __metaclass__ = DjangoItemMeta\n-\n-    django_model = None\n-\n-    def __init__(self, *args, **kwargs):\n-        super(DjangoItem, self).__init__(*args, **kwargs)\n-        self._instance = None\n-        self._errors = None\n-\n-    def save(self, commit=True):\n-        if commit:\n-            self.instance.save()\n-        return self.instance\n-\n-    def is_valid(self, exclude=None):\n-        self._get_errors(exclude)\n-        return not bool(self._errors)\n-\n-    def _get_errors(self, exclude=None):\n-        if self._errors is not None:\n-            return self._errors\n-\n-        self._errors = {}\n-        if exclude is None:\n-            exclude = []\n-\n-        try:\n-            self.instance.clean_fields(exclude=exclude)\n-        except ValidationError as e:\n-            self._errors = e.update_error_dict(self._errors)\n-\n-        try:\n-            self.instance.clean()\n-        except ValidationError as e:\n-            self._errors = e.update_error_dict(self._errors)\n-\n-        # uniqueness is not checked, because it is faster to check it when\n-        # saving object to database. Just beware, that failed save()\n-        # raises IntegrityError instead of ValidationError.\n-\n-        return self._errors\n-    errors = property(_get_errors)\n-\n-    @property\n-    def instance(self):\n-        if self._instance is None:\n-            modelargs = dict((k, self.get(k)) for k in self._values\n-                             if k in self._model_fields)\n-            self._instance = self.django_model(**modelargs)\n-        return self._instance\n\n@@ -1,103 +0,0 @@\n-import os\n-from twisted.trial import unittest\n-\n-from scrapy.contrib.djangoitem import DjangoItem, Field\n-from scrapy import optional_features\n-\n-os.environ['DJANGO_SETTINGS_MODULE'] = 'tests.test_djangoitem.settings'\n-\n-if 'django' in optional_features:\n-    from .models import Person, IdentifiedPerson\n-\n-    class BasePersonItem(DjangoItem):\n-        django_model = Person\n-\n-    class NewFieldPersonItem(BasePersonItem):\n-        other = Field()\n-\n-    class OverrideFieldPersonItem(BasePersonItem):\n-        age = Field()\n-\n-    class IdentifiedPersonItem(DjangoItem):\n-        django_model = IdentifiedPerson\n-\n-\n-class DjangoItemTest(unittest.TestCase):\n-\n-    def assertSortedEqual(self, first, second, msg=None):\n-        return self.assertEqual(sorted(first), sorted(second), msg)\n-\n-    def setUp(self):\n-        if 'django' not in optional_features:\n-            raise unittest.SkipTest(\"Django is not available\")\n-\n-    def test_base(self):\n-        i = BasePersonItem()\n-        self.assertSortedEqual(i.fields.keys(), ['age', 'name'])\n-\n-    def test_new_fields(self):\n-        i = NewFieldPersonItem()\n-        self.assertSortedEqual(i.fields.keys(), ['age', 'other', 'name'])\n-\n-    def test_override_field(self):\n-        i = OverrideFieldPersonItem()\n-        self.assertSortedEqual(i.fields.keys(), ['age', 'name'])\n-\n-    def test_custom_primary_key_field(self):\n-        \"\"\"\n-        Test that if a custom primary key exists, it is\n-        in the field list.\n-        \"\"\"\n-        i = IdentifiedPersonItem()\n-        self.assertSortedEqual(i.fields.keys(), ['age', 'identifier', 'name'])\n-\n-    def test_save(self):\n-        i = BasePersonItem()\n-        self.assertSortedEqual(i.fields.keys(), ['age', 'name'])\n-\n-        i['name'] = 'John'\n-        i['age'] = '22'\n-        person = i.save(commit=False)\n-\n-        self.assertEqual(person.name, 'John')\n-        self.assertEqual(person.age, '22')\n-\n-    def test_override_save(self):\n-        i = OverrideFieldPersonItem()\n-\n-        i['name'] = 'John'\n-        # it is not obvious that \"age\" should be saved also, since it was\n-        # redefined in child class\n-        i['age'] = '22'\n-        person = i.save(commit=False)\n-\n-        self.assertEqual(person.name, 'John')\n-        self.assertEqual(person.age, '22')\n-\n-    def test_validation(self):\n-        long_name = 'z' * 300\n-        i = BasePersonItem(name=long_name)\n-        self.assertFalse(i.is_valid())\n-        self.assertEqual(set(i.errors), set(['age', 'name']))\n-        i = BasePersonItem(name='John')\n-        self.assertTrue(i.is_valid(exclude=['age']))\n-        self.assertEqual({}, i.errors)\n-\n-        # once the item is validated, it does not validate again\n-        i['name'] = long_name\n-        self.assertTrue(i.is_valid())\n-\n-    def test_override_validation(self):\n-        i = OverrideFieldPersonItem()\n-        i['name'] = 'John'\n-        self.assertFalse(i.is_valid())\n-\n-        i = i = OverrideFieldPersonItem()\n-        i['name'] = 'John'\n-        i['age'] = '22'\n-        self.assertTrue(i.is_valid())\n-\n-    def test_default_field_values(self):\n-        i = BasePersonItem()\n-        person = i.save(commit=False)\n-        self.assertEqual(person.name, 'Robot')\n\n@@ -1,17 +0,0 @@\n-from django.db import models\n-\n-\n-class Person(models.Model):\n-    name = models.CharField(max_length=255, default='Robot')\n-    age = models.IntegerField()\n-\n-    class Meta:\n-        app_label = 'test_djangoitem'\n-\n-class IdentifiedPerson(models.Model):\n-    identifier = models.PositiveIntegerField(primary_key=True)\n-    name = models.CharField(max_length=255)\n-    age = models.IntegerField()\n-\n-    class Meta:\n-        app_label = 'test_djangoitem'\n\n@@ -1,8 +0,0 @@\n-DATABASES = {\n-    'default': {\n-        'ENGINE': 'django.db.backends.sqlite3',\n-        'NAME': ':memory:',\n-    }\n-}\n-\n-SECRET_KEY = 'top-secret'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#017fb25beef63639060485dae393ed6e244d3f70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 21 | Churn Cumulative: 49 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -85,6 +85,27 @@ class BasicItemLoaderTest(unittest.TestCase):\n         il.replace_value('sku', [valid_fragment], re=sku_re)\n         self.assertEqual(il.load_item()['sku'], u'1234')\n \n+    def test_self_referencing_loader(self):\n+        class MyLoader(ItemLoader):\n+            url_out = TakeFirst()\n+\n+            def img_url_out(self, values):\n+                return (self.get_output_value('url') or '') + values[0]\n+\n+        il = MyLoader(item={})\n+        il.add_value('url', 'http://example.com/')\n+        il.add_value('img_url', '1234.png')\n+        self.assertEqual(il.load_item(), {\n+            'url': 'http://example.com/',\n+            'img_url': 'http://example.com/1234.png',\n+        })\n+\n+        il = MyLoader(item={})\n+        il.add_value('img_url', '1234.png')\n+        self.assertEqual(il.load_item(), {\n+            'img_url': '1234.png',\n+        })\n+\n     def test_add_value(self):\n         il = TestItemLoader()\n         il.add_value('name', u'marta')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d39722f6a37bc1b1558e75b2c77542925437d3db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 279 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -80,7 +80,7 @@ class ItemLoader(object):\n \n     def load_item(self):\n         item = self.item\n-        for field_name in self._values:\n+        for field_name in tuple(self._values):\n             value = self.get_output_value(field_name)\n             if value is not None:\n                 item[field_name] = value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ae37d61f9aa4f0244ba3d7bf1bfaca3e2ac3749", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 259 | Files Changed: 1 | Hunks: 1 | Methods Changed: 19 | Complexity Δ (Sum/Max): -52/0 | Churn Δ: 259 | Churn Cumulative: 692 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 0.21243523316062177\n\nDIFF:\n@@ -1,259 +0,0 @@\n-from __future__ import print_function\n-import sys\n-import os\n-import glob\n-import tempfile\n-import shutil\n-import time\n-import netrc\n-import json\n-from six.moves.urllib import request\n-from six.moves.urllib.parse import urlparse, urljoin\n-from six.moves.urllib.error import HTTPError, URLError\n-from subprocess import Popen, PIPE, check_call\n-\n-from w3lib.form import encode_multipart\n-\n-from scrapy.command import ScrapyCommand\n-from scrapy.exceptions import UsageError\n-from scrapy.utils.http import basic_auth_header\n-from scrapy.utils.python import retry_on_eintr\n-from scrapy.utils.conf import get_config, closest_scrapy_cfg\n-\n-_SETUP_PY_TEMPLATE = \\\n-\"\"\"# Automatically created by: scrapy deploy\n-\n-from setuptools import setup, find_packages\n-\n-setup(\n-    name         = 'project',\n-    version      = '1.0',\n-    packages     = find_packages(),\n-    entry_points = {'scrapy': ['settings = %(settings)s']},\n-)\n-\"\"\"\n-\n-class Command(ScrapyCommand):\n-\n-    requires_project = True\n-\n-    def syntax(self):\n-        return \"[options] [ [target] | -l | -L <target> ]\"\n-\n-    def short_desc(self):\n-        return \"Deploy project in Scrapyd target\"\n-\n-    def long_desc(self):\n-        return \"Deploy the current project into the given Scrapyd server \" \\\n-            \"(known as target)\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-p\", \"--project\",\n-            help=\"the project name in the target\")\n-        parser.add_option(\"-v\", \"--version\",\n-            help=\"the version to deploy. Defaults to current timestamp\")\n-        parser.add_option(\"-l\", \"--list-targets\", action=\"store_true\", \\\n-            help=\"list available targets\")\n-        parser.add_option(\"-d\", \"--debug\", action=\"store_true\",\n-            help=\"debug mode (do not remove build dir)\")\n-        parser.add_option(\"-L\", \"--list-projects\", metavar=\"TARGET\", \\\n-            help=\"list available projects on TARGET\")\n-        parser.add_option(\"--egg\", metavar=\"FILE\",\n-            help=\"use the given egg, instead of building it\")\n-        parser.add_option(\"--build-egg\", metavar=\"FILE\",\n-            help=\"only build the egg, don't deploy it\")\n-\n-    def run(self, args, opts):\n-        try:\n-            import setuptools\n-        except ImportError:\n-            raise UsageError(\"setuptools not installed\")\n-\n-        request.install_opener(request.build_opener(HTTPRedirectHandler))\n-\n-        if opts.list_targets:\n-            for name, target in _get_targets().items():\n-                print(\"%-20s %s\" % (name, target['url']))\n-            return\n-\n-        if opts.list_projects:\n-            target = _get_target(opts.list_projects)\n-            req = request.Request(_url(target, 'listprojects.json'))\n-            _add_auth_header(req, target)\n-            f = request.urlopen(req)\n-            projects = json.loads(f.read())['projects']\n-            print(os.linesep.join(projects))\n-            return\n-\n-        tmpdir = None\n-\n-        if opts.build_egg: # build egg only\n-            egg, tmpdir = _build_egg()\n-            _log(\"Writing egg to %s\" % opts.build_egg)\n-            shutil.copyfile(egg, opts.build_egg)\n-        else: # buld egg and deploy\n-            target_name = _get_target_name(args)\n-            target = _get_target(target_name)\n-            project = _get_project(target, opts)\n-            version = _get_version(target, opts)\n-            if opts.egg:\n-                _log(\"Using egg: %s\" % opts.egg)\n-                egg = opts.egg\n-            else:\n-                _log(\"Packing version %s\" % version)\n-                egg, tmpdir = _build_egg()\n-            if not _upload_egg(target, egg, project, version):\n-                self.exitcode = 1\n-\n-        if tmpdir:\n-            if opts.debug:\n-                _log(\"Output dir not removed: %s\" % tmpdir)\n-            else:\n-                shutil.rmtree(tmpdir)\n-\n-def _log(message):\n-    sys.stderr.write(message + os.linesep)\n-\n-def _get_target_name(args):\n-    if len(args) > 1:\n-        raise UsageError(\"Too many arguments: %s\" % ' '.join(args))\n-    elif args:\n-        return args[0]\n-    elif len(args) < 1:\n-        return 'default'\n-\n-def _get_project(target, opts):\n-    project = opts.project or target.get('project')\n-    if not project:\n-        raise UsageError(\"Missing project\")\n-    return project\n-\n-def _get_option(section, option, default=None):\n-    cfg = get_config()\n-    return cfg.get(section, option) if cfg.has_option(section, option) \\\n-        else default\n-\n-def _get_targets():\n-    cfg = get_config()\n-    baset = dict(cfg.items('deploy')) if cfg.has_section('deploy') else {}\n-    targets = {}\n-    if 'url' in baset:\n-        targets['default'] = baset\n-    for x in cfg.sections():\n-        if x.startswith('deploy:'):\n-            t = baset.copy()\n-            t.update(cfg.items(x))\n-            targets[x[7:]] = t\n-    return targets\n-\n-def _get_target(name):\n-    try:\n-        return _get_targets()[name]\n-    except KeyError:\n-        raise UsageError(\"Unknown target: %s\" % name)\n-\n-def _url(target, action):\n-    return urljoin(target['url'], action)\n-\n-def _get_version(target, opts):\n-    version = opts.version or target.get('version')\n-    if version == 'HG':\n-        p = Popen(['hg', 'tip', '--template', '{rev}'], stdout=PIPE)\n-        d = 'r%s' % p.communicate()[0]\n-        p = Popen(['hg', 'branch'], stdout=PIPE)\n-        b = p.communicate()[0].strip('\\n')\n-        return '%s-%s' % (d, b)\n-    elif version == 'GIT':\n-        p = Popen(['git', 'describe', '--always'], stdout=PIPE)\n-        d = p.communicate()[0].strip('\\n')\n-        p = Popen(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], stdout=PIPE)\n-        b = p.communicate()[0].strip('\\n')\n-        return '%s-%s' % (d, b)\n-    elif version:\n-        return version\n-    else:\n-        return str(int(time.time()))\n-\n-def _upload_egg(target, eggpath, project, version):\n-    with open(eggpath, 'rb') as f:\n-        eggdata = f.read()\n-    data = {\n-        'project': project,\n-        'version': version,\n-        'egg': ('project.egg', eggdata),\n-    }\n-    body, boundary = encode_multipart(data)\n-    url = _url(target, 'addversion.json')\n-    headers = {\n-        'Content-Type': 'multipart/form-data; boundary=%s' % boundary,\n-        'Content-Length': str(len(body)),\n-    }\n-    req = request.Request(url, body, headers)\n-    _add_auth_header(req, target)\n-    _log('Deploying to project \"%s\" in %s' % (project, url))\n-    return _http_post(req)\n-\n-def _add_auth_header(req, target):\n-    if 'username' in target:\n-        u, p = target.get('username'), target.get('password', '')\n-        req.add_header('Authorization', basic_auth_header(u, p))\n-    else: # try netrc\n-        try:\n-            host = urlparse(target['url']).hostname\n-            a = netrc.netrc().authenticators(host)\n-            req.add_header('Authorization', basic_auth_header(a[0], a[2]))\n-        except (netrc.NetrcParseError, IOError, TypeError):\n-            pass\n-\n-def _http_post(req):\n-    try:\n-        f = request.urlopen(req)\n-        _log(\"Server response (%s):\" % f.code)\n-        print(f.read())\n-        return True\n-    except HTTPError as e:\n-        _log(\"Deploy failed (%s):\" % e.code)\n-        print(e.read())\n-    except URLError as e:\n-        _log(\"Deploy failed: %s\" % e)\n-\n-def _build_egg():\n-    closest = closest_scrapy_cfg()\n-    os.chdir(os.path.dirname(closest))\n-    if not os.path.exists('setup.py'):\n-        settings = get_config().get('settings', 'default')\n-        _create_default_setup_py(settings=settings)\n-    d = tempfile.mkdtemp(prefix=\"scrapydeploy-\")\n-    o = open(os.path.join(d, \"stdout\"), \"wb\")\n-    e = open(os.path.join(d, \"stderr\"), \"wb\")\n-    retry_on_eintr(check_call, [sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=o, stderr=e)\n-    o.close()\n-    e.close()\n-    egg = glob.glob(os.path.join(d, '*.egg'))[0]\n-    return egg, d\n-\n-def _create_default_setup_py(**kwargs):\n-    with open('setup.py', 'w') as f:\n-        f.write(_SETUP_PY_TEMPLATE % kwargs)\n-\n-\n-class HTTPRedirectHandler(request.HTTPRedirectHandler):\n-\n-    def redirect_request(self, req, fp, code, msg, headers, newurl):\n-        newurl = newurl.replace(' ', '%20')\n-        if code in (301, 307):\n-            return request.Request(newurl,\n-                                   data=req.get_data(),\n-                                   headers=req.headers,\n-                                   origin_req_host=req.get_origin_req_host(),\n-                                   unverifiable=True)\n-        elif code in (302, 303):\n-            newheaders = dict((k, v) for k, v in req.headers.items()\n-                              if k.lower() not in (\"content-length\", \"content-type\"))\n-            return request.Request(newurl,\n-                                   headers=newheaders,\n-                                   origin_req_host=req.get_origin_req_host(),\n-                                   unverifiable=True)\n-        else:\n-            raise HTTPError(req.get_full_url(), code, msg, headers, fp)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3d3a9491412d2a91b0927a05908593dcd329e4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 1315 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -192,8 +192,8 @@ def _get_spider_loader(settings):\n             'Please use SPIDER_LOADER_CLASS.',\n             category=ScrapyDeprecationWarning, stacklevel=2\n         )\n-    cls_path = settings.get('SPIDER_LOADER_CLASS',\n-                            settings.get('SPIDER_MANAGER_CLASS'))\n+    cls_path = settings.get('SPIDER_MANAGER_CLASS',\n+                            settings.get('SPIDER_LOADER_CLASS'))\n     loader_cls = load_object(cls_path)\n     verifyClass(ISpiderLoader, loader_cls)\n     return loader_cls.from_settings(settings.frozencopy())\n\n@@ -5,6 +5,7 @@ from zope.interface.verify import DoesNotImplement\n \n from scrapy.crawler import Crawler, CrawlerRunner\n from scrapy.settings import Settings, default_settings\n+from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n \n@@ -60,6 +61,10 @@ class SpiderLoaderWithWrongInterface(object):\n         pass\n \n \n+class CustomSpiderLoader(SpiderLoader):\n+    pass\n+\n+\n class CrawlerRunnerTestCase(unittest.TestCase):\n \n     def test_spider_manager_verify_interface(self):\n@@ -87,3 +92,13 @@ class CrawlerRunnerTestCase(unittest.TestCase):\n             self.assertIn(\"CrawlerRunner.spider_loader\", str(w[0].message))\n             sl_cls = load_object(runner.settings['SPIDER_LOADER_CLASS'])\n             self.assertIsInstance(spiders, sl_cls)\n+\n+    def test_spidermanager_deprecation(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            runner = CrawlerRunner({\n+                'SPIDER_MANAGER_CLASS': 'tests.test_crawler.CustomSpiderLoader'\n+            })\n+            self.assertIsInstance(runner.spider_loader, CustomSpiderLoader)\n+            self.assertEqual(len(w), 1)\n+            self.assertIn('Please use SPIDER_LOADER_CLASS', str(w[0].message))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7871acd66553dca4d787d2d6fbff174851a10acd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 22/19 | Churn Δ: 87 | Churn Cumulative: 290 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 0.7083333333333334\n\nDIFF:\n@@ -24,18 +24,21 @@ class Field(dict):\n class ItemMeta(ABCMeta):\n \n     def __new__(mcs, class_name, bases, attrs):\n+        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n+        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n+\n         fields = {}\n         new_attrs = {}\n-        for n, v in six.iteritems(attrs):\n+        for n in dir(_class):\n+            v = getattr(_class, n)\n             if isinstance(v, Field):\n                 fields[n] = v\n-            else:\n-                new_attrs[n] = v\n+            elif n in attrs:\n+                new_attrs[n] = attrs[n]\n \n-        cls = super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)\n-        cls.fields = cls.fields.copy()\n-        cls.fields.update(fields)\n-        return cls\n+        new_attrs['fields'] = fields\n+        new_attrs['_class'] = _class\n+        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)\n \n \n class DictItem(MutableMapping, BaseItem):\n\n@@ -134,6 +134,76 @@ class ItemTest(unittest.TestCase):\n         self.assertEqual(list(i.keys()), ['keys'])\n         self.assertEqual(list(i.values()), [3])\n \n+    def test_metaclass_multiple_inheritance_simple(self):\n+        class A(Item):\n+            save = Field(default='A')\n+\n+        class B(A): pass\n+\n+        class C(Item):\n+            save = Field(default='C')\n+\n+        class D(B, C): pass\n+\n+        self.assertEqual(D(save='X')['save'], 'X')\n+        self.assertEqual(D.fields, {'save': {'default': 'A'}})\n+\n+        # D class inverted\n+        class E(C, B): pass\n+\n+        self.assertEqual(E(save='X')['save'], 'X')\n+        self.assertEqual(E.fields, {'save': {'default': 'C'}})\n+\n+    def test_metaclass_multiple_inheritance_diamond(self):\n+        class A(Item):\n+            save = Field(default='A')\n+            load = Field(default='A')\n+\n+        class B(A): pass\n+\n+        class C(A):\n+            save = Field(default='C')\n+\n+        class D(B, C):\n+            load = Field(default='D')\n+\n+        self.assertEqual(D(save='X')['save'], 'X')\n+        self.assertEqual(D(load='X')['load'], 'X')\n+        self.assertEqual(D.fields, {'save': {'default': 'C'},\n+            'load': {'default': 'D'}})\n+\n+        # D class inverted\n+        class E(C, B):\n+            load = Field(default='E')\n+\n+        self.assertEqual(E(save='X')['save'], 'X')\n+        self.assertEqual(E(load='X')['load'], 'X')\n+        self.assertEqual(E.fields, {'save': {'default': 'C'},\n+            'load': {'default': 'E'}})\n+\n+    def test_metaclass_multiple_inheritance_without_metaclass(self):\n+        class A(Item):\n+            save = Field(default='A')\n+\n+        class B(A): pass\n+\n+        class C(object):\n+            not_allowed = Field(default='not_allowed')\n+            save = Field(default='C')\n+\n+        class D(B, C): pass\n+\n+        self.assertRaises(KeyError, D, not_allowed='value')\n+        self.assertEqual(D(save='X')['save'], 'X')\n+        self.assertEqual(D.fields, {'save': {'default': 'A'}})\n+\n+        # D class inverted\n+        class E(C, B): pass\n+\n+        self.assertRaises(KeyError, E, not_allowed='value')\n+        self.assertEqual(E(save='X')['save'], 'X')\n+        self.assertEqual(E.fields, {'save': {'default': 'A'}})\n+\n     def test_to_dict(self):\n         class TestItem(Item):\n             name = Field()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7a958f90bef3e6a1ab51bfb04260ed6186f38924", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 395 | Lines Deleted: 216 | Files Changed: 34 | Hunks: 181 | Methods Changed: 70 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 611 | Churn Cumulative: 11582 | Contributors (this commit): 51 | Commits (past 90d): 67 | Contributors (cumulative): 215 | DMM Complexity: 0.6785714285714286\n\nDIFF:\n@@ -1,5 +1,8 @@\n from __future__ import print_function\n+import logging\n+\n from w3lib.url import is_url\n+\n from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n from scrapy.item import BaseItem\n@@ -7,7 +10,9 @@ from scrapy.utils import display\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n from scrapy.exceptions import UsageError\n-from scrapy import log\n+\n+logger = logging.getLogger('scrapy')\n+\n \n class Command(ScrapyCommand):\n \n@@ -119,9 +124,9 @@ class Command(ScrapyCommand):\n                 if rule.link_extractor.matches(response.url) and rule.callback:\n                     return rule.callback\n         else:\n-            log.msg(format='No CrawlSpider rules found in spider %(spider)r, '\n+            logger.error('No CrawlSpider rules found in spider %(spider)r, '\n                          'please specify a callback to use for parsing',\n-                    level=log.ERROR, spider=spider.name)\n+                         {'spider': spider.name})\n \n     def set_spidercls(self, url, opts):\n         spider_loader = self.crawler_process.spider_loader\n@@ -129,13 +134,13 @@ class Command(ScrapyCommand):\n             try:\n                 self.spidercls = spider_loader.load(opts.spider)\n             except KeyError:\n-                log.msg(format='Unable to find spider: %(spider)s',\n-                        level=log.ERROR, spider=opts.spider)\n+                logger.error('Unable to find spider: %(spider)s',\n+                             {'spider': opts.spider})\n         else:\n             self.spidercls = spidercls_for_request(spider_loader, Request(url))\n             if not self.spidercls:\n-                log.msg(format='Unable to find spider for: %(url)s',\n-                        level=log.ERROR, url=url)\n+                logger.error('Unable to find spider for: %(url)s',\n+                             {'url': url})\n \n         request = Request(url, opts.callback)\n         _start_requests = lambda s: [self.prepare_request(s, request, opts)]\n@@ -148,8 +153,8 @@ class Command(ScrapyCommand):\n         self.crawler_process.start()\n \n         if not self.first_response:\n-            log.msg(format='No response downloaded for: %(url)s',\n-                    level=log.ERROR, url=url)\n+            logger.error('No response downloaded for: %(url)s',\n+                         {'url': url})\n \n     def prepare_request(self, spider, request, opts):\n         def callback(response):\n@@ -170,8 +175,8 @@ class Command(ScrapyCommand):\n                 if callable(cb_method):\n                     cb = cb_method\n                 else:\n-                    log.msg(format='Cannot find callback %(callback)r in spider: %(spider)s',\n-                            callback=callback, spider=spider.name, level=log.ERROR)\n+                    logger.error('Cannot find callback %(callback)r in spider: %(spider)s',\n+                                 {'callback': callback, 'spider': spider.name})\n                     return\n \n             # parse items and requests\n\n@@ -9,7 +9,6 @@ from threading import Thread\n from scrapy.command import ScrapyCommand\n from scrapy.shell import Shell\n from scrapy.http import Request\n-from scrapy import log\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n \n \n\n@@ -6,13 +6,15 @@ See documentation in docs/topics/extensions.rst\n \n import sys\n import signal\n+import logging\n import traceback\n import threading\n from pdb import Pdb\n \n from scrapy.utils.engine import format_engine_status\n from scrapy.utils.trackref import format_live_refs\n-from scrapy import log\n+\n+logger = logging.getLogger('scrapy')\n \n \n class StackTraceDump(object):\n@@ -31,12 +33,14 @@ class StackTraceDump(object):\n         return cls(crawler)\n \n     def dump_stacktrace(self, signum, frame):\n-        stackdumps = self._thread_stacks()\n-        enginestatus = format_engine_status(self.crawler.engine)\n-        liverefs = format_live_refs()\n-        msg = \"Dumping stack trace and engine status\" \\\n-            \"\\n{0}\\n{1}\\n{2}\".format(enginestatus, liverefs, stackdumps)\n-        log.msg(msg)\n+        log_args = {\n+            'stackdumps': self._thread_stacks(),\n+            'enginestatus': format_engine_status(self.crawler.engine),\n+            'liverefs': format_live_refs(),\n+        }\n+        logger.info(\"Dumping stack trace and engine status\\n\"\n+                    \"%(enginestatus)s\\n%(liverefs)s\\n%(stackdumps)s\",\n+                    log_args, extra={'crawler': self.crawler})\n \n     def _thread_stacks(self):\n         id2name = dict((th.ident, th.name) for th in threading.enumerate())\n\n@@ -1,14 +1,19 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n import re\n+import logging\n+\n import six\n from w3lib import html\n-from scrapy import log\n+\n from scrapy.exceptions import NotConfigured\n from scrapy.http import HtmlResponse\n from scrapy.utils.response import _noscript_re, _script_re\n \n \n+logger = logging.getLogger('scrapy')\n+\n+\n class AjaxCrawlMiddleware(object):\n     \"\"\"\n     Handle 'AJAX crawlable' pages marked as crawlable via meta tag.\n@@ -46,9 +51,9 @@ class AjaxCrawlMiddleware(object):\n \n         # scrapy already handles #! links properly\n         ajax_crawl_request = request.replace(url=request.url+'#!')\n-        log.msg(format=\"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n-                level=log.DEBUG, spider=spider,\n-                ajax_crawl_request=ajax_crawl_request, request=request)\n+        logger.debug(\"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n+                     {'ajax_crawl_request': ajax_crawl_request, 'request': request},\n+                     extra={'spider': spider})\n \n         ajax_crawl_request.meta['ajax_crawlable'] = True\n         return ajax_crawl_request\n\n@@ -1,11 +1,13 @@\n import os\n import six\n+import logging\n from collections import defaultdict\n \n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n-from scrapy import log\n+\n+logger = logging.getLogger('scrapy')\n \n \n class CookiesMiddleware(object):\n@@ -54,7 +56,7 @@ class CookiesMiddleware(object):\n             if cl:\n                 msg = \"Sending cookies to: %s\" % request + os.linesep\n                 msg += os.linesep.join(\"Cookie: %s\" % c for c in cl)\n-                log.msg(msg, spider=spider, level=log.DEBUG)\n+                logger.debug(msg, extra={'spider': spider})\n \n     def _debug_set_cookie(self, response, spider):\n         if self.debug:\n@@ -62,7 +64,7 @@ class CookiesMiddleware(object):\n             if cl:\n                 msg = \"Received cookies from: %s\" % response + os.linesep\n                 msg += os.linesep.join(\"Set-Cookie: %s\" % c for c in cl)\n-                log.msg(msg, spider=spider, level=log.DEBUG)\n+                logger.debug(msg, extra={'spider': spider})\n \n     def _format_cookie(self, cookie):\n         # build cookie string\n\n@@ -6,6 +6,7 @@ import bz2\n import gzip\n import zipfile\n import tarfile\n+import logging\n from tempfile import mktemp\n \n import six\n@@ -15,9 +16,10 @@ try:\n except ImportError:\n     from io import BytesIO\n \n-from scrapy import log\n from scrapy.responsetypes import responsetypes\n \n+logger = logging.getLogger('scrapy')\n+\n \n class DecompressionMiddleware(object):\n     \"\"\" This middleware tries to recognise and extract the possibly compressed\n@@ -80,7 +82,7 @@ class DecompressionMiddleware(object):\n         for fmt, func in six.iteritems(self._formats):\n             new_response = func(response)\n             if new_response:\n-                log.msg(format='Decompressed response with format: %(responsefmt)s',\n-                        level=log.DEBUG, spider=spider, responsefmt=fmt)\n+                logger.debug('Decompressed response with format: %(responsefmt)s',\n+                             {'responsefmt': fmt}, extra={'spider': spider})\n                 return new_response\n         return response\n\n@@ -1,10 +1,12 @@\n+import logging\n from six.moves.urllib.parse import urljoin\n \n-from scrapy import log\n from scrapy.http import HtmlResponse\n from scrapy.utils.response import get_meta_refresh\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n+logger = logging.getLogger('scrapy')\n+\n \n class BaseRedirectMiddleware(object):\n \n@@ -32,13 +34,13 @@ class BaseRedirectMiddleware(object):\n                 [request.url]\n             redirected.dont_filter = request.dont_filter\n             redirected.priority = request.priority + self.priority_adjust\n-            log.msg(format=\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n-                    level=log.DEBUG, spider=spider, request=request,\n-                    redirected=redirected, reason=reason)\n+            logger.debug(\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n+                         {'reason': reason, 'redirected': redirected, 'request': request},\n+                         extra={'spider': spider})\n             return redirected\n         else:\n-            log.msg(format=\"Discarding %(request)s: max redirections reached\",\n-                    level=log.DEBUG, spider=spider, request=request)\n+            logger.debug(\"Discarding %(request)s: max redirections reached\",\n+                         {'request': request}, extra={'spider': spider})\n             raise IgnoreRequest(\"max redirections reached\")\n \n     def _redirect_request_using_get(self, request, redirect_url):\n\n@@ -17,17 +17,19 @@ About HTTP errors to consider:\n   protocol. It's included by default because it's a common code used to\n   indicate server overload, which would be something we want to retry\n \"\"\"\n+import logging\n \n from twisted.internet import defer\n from twisted.internet.error import TimeoutError, DNSLookupError, \\\n         ConnectionRefusedError, ConnectionDone, ConnectError, \\\n         ConnectionLost, TCPTimedOutError\n \n-from scrapy import log\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.response import response_status_message\n from scrapy.xlib.tx import ResponseFailed\n \n+logger = logging.getLogger('scrapy')\n+\n \n class RetryMiddleware(object):\n \n@@ -66,13 +68,15 @@ class RetryMiddleware(object):\n         retries = request.meta.get('retry_times', 0) + 1\n \n         if retries <= self.max_retry_times:\n-            log.msg(format=\"Retrying %(request)s (failed %(retries)d times): %(reason)s\",\n-                    level=log.DEBUG, spider=spider, request=request, retries=retries, reason=reason)\n+            logger.debug(\"Retrying %(request)s (failed %(retries)d times): %(reason)s\",\n+                         {'request': request, 'retries': retries, 'reason': reason},\n+                         extra={'spider': spider})\n             retryreq = request.copy()\n             retryreq.meta['retry_times'] = retries\n             retryreq.dont_filter = True\n             retryreq.priority = request.priority + self.priority_adjust\n             return retryreq\n         else:\n-            log.msg(format=\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n-                    level=log.DEBUG, spider=spider, request=request, retries=retries, reason=reason)\n+            logger.debug(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n+                         {'request': request, 'retries': retries, 'reason': reason},\n+                         extra={'spider': spider})\n\n@@ -4,13 +4,16 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n \n \"\"\"\n \n+import logging\n+\n from six.moves.urllib import robotparser\n \n-from scrapy import signals, log\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n \n+logger = logging.getLogger('scrapy')\n+\n \n class RobotsTxtMiddleware(object):\n     DOWNLOAD_PRIORITY = 1000\n@@ -32,8 +35,8 @@ class RobotsTxtMiddleware(object):\n             return\n         rp = self.robot_parser(request, spider)\n         if rp and not rp.can_fetch(self._useragent, request.url):\n-            log.msg(format=\"Forbidden by robots.txt: %(request)s\",\n-                    level=log.DEBUG, request=request)\n+            logger.debug(\"Forbidden by robots.txt: %(request)s\",\n+                         {'request': request}, extra={'spider': spider})\n             raise IgnoreRequest\n \n     def robot_parser(self, request, spider):\n@@ -54,8 +57,9 @@ class RobotsTxtMiddleware(object):\n \n     def _logerror(self, failure, request, spider):\n         if failure.type is not IgnoreRequest:\n-            log.msg(format=\"Error downloading %%(request)s: %s\" % failure.value,\n-                    level=log.ERROR, request=request, spider=spider)\n+            logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n+                         {'request': request, 'f_exception': failure.value},\n+                         extra={'spider': spider, 'failure': failure})\n \n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)\n\n@@ -4,7 +4,10 @@ Feed Exports extension\n See documentation in docs/topics/feed-exports.rst\n \"\"\"\n \n-import sys, os, posixpath\n+import os\n+import sys\n+import logging\n+import posixpath\n from tempfile import TemporaryFile\n from datetime import datetime\n from six.moves.urllib.parse import urlparse\n@@ -14,12 +17,14 @@ from zope.interface import Interface, implementer\n from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n \n-from scrapy import log, signals\n+from scrapy import signals\n from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import get_func_args\n \n+logger = logging.getLogger('scrapy')\n+\n \n class IFeedStorage(Interface):\n     \"\"\"Interface that all Feed Storages must implement\"\"\"\n@@ -171,11 +176,15 @@ class FeedExporter(object):\n         if not slot.itemcount and not self.store_empty:\n             return\n         slot.exporter.finish_exporting()\n-        logfmt = \"%%s %s feed (%d items) in: %s\" % (self.format, \\\n-            slot.itemcount, slot.uri)\n+        logfmt = \"%%s %(format)s feed (%(itemcount)d items) in: %(uri)s\"\n+        log_args = {'format': self.format,\n+                    'itemcount': slot.itemcount,\n+                    'uri': slot.uri}\n         d = defer.maybeDeferred(slot.storage.store, slot.file)\n-        d.addCallback(lambda _: log.msg(logfmt % \"Stored\", spider=spider))\n-        d.addErrback(log.err, logfmt % \"Error storing\", spider=spider)\n+        d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n+                                            extra={'spider': spider}))\n+        d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n+                                            extra={'spider': spider, 'failure': f}))\n         return d\n \n     def item_scraped(self, item, spider):\n@@ -198,7 +207,7 @@ class FeedExporter(object):\n     def _exporter_supported(self, format):\n         if format in self.exporters:\n             return True\n-        log.msg(\"Unknown feed format: %s\" % format, log.ERROR)\n+        logger.error(\"Unknown feed format: %(format)s\", {'format': format})\n \n     def _storage_supported(self, uri):\n         scheme = urlparse(uri).scheme\n@@ -207,9 +216,11 @@ class FeedExporter(object):\n                 self._get_storage(uri)\n                 return True\n             except NotConfigured:\n-                log.msg(\"Disabled feed storage scheme: %s\" % scheme, log.ERROR)\n+                logger.error(\"Disabled feed storage scheme: %(scheme)s\",\n+                             {'scheme': scheme})\n         else:\n-            log.msg(\"Unknown feed storage scheme: %s\" % scheme, log.ERROR)\n+            logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n+                         {'scheme': scheme})\n \n     def _get_exporter(self, *args, **kwargs):\n         return self.exporters[self.format](*args, **kwargs)\n\n@@ -1,7 +1,11 @@\n+import logging\n+\n from twisted.internet import task\n \n from scrapy.exceptions import NotConfigured\n-from scrapy import log, signals\n+from scrapy import signals\n+\n+logger = logging.getLogger('scrapy')\n \n \n class LogStats(object):\n@@ -35,9 +39,12 @@ class LogStats(object):\n         irate = (items - self.itemsprev) * self.multiplier\n         prate = (pages - self.pagesprev) * self.multiplier\n         self.pagesprev, self.itemsprev = pages, items\n-        msg = \"Crawled %d pages (at %d pages/min), scraped %d items (at %d items/min)\" \\\n-            % (pages, prate, items, irate)\n-        log.msg(msg, spider=spider)\n+\n+        msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n+               \"scraped %(items)d items (at %(itemrate)d items/min)\")\n+        log_args = {'pages': pages, 'pagerate': prate,\n+                    'items': items, 'itemrate': irate}\n+        logger.info(msg, log_args, extra={'spider': spider})\n \n     def spider_closed(self, spider, reason):\n         if self.task.running:\n\n@@ -5,16 +5,20 @@ See documentation in docs/topics/extensions.rst\n \"\"\"\n import sys\n import socket\n+import logging\n from pprint import pformat\n from importlib import import_module\n \n from twisted.internet import task\n \n-from scrapy import signals, log\n+from scrapy import signals\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.utils.engine import get_engine_status\n \n+logger = logging.getLogger('scrapy')\n+\n+\n class MemoryUsage(object):\n \n     def __init__(self, crawler):\n@@ -74,8 +78,8 @@ class MemoryUsage(object):\n         if self.get_virtual_size() > self.limit:\n             self.crawler.stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit/1024/1024\n-            log.msg(format=\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n-                    level=log.ERROR, memusage=mem)\n+            logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n+                         {'memusage': mem}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n                 subj = \"%s terminated: memory usage exceeded %dM at %s\" % \\\n                         (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n@@ -95,8 +99,8 @@ class MemoryUsage(object):\n         if self.get_virtual_size() > self.warning:\n             self.crawler.stats.set_value('memusage/warning_reached', 1)\n             mem = self.warning/1024/1024\n-            log.msg(format=\"Memory usage reached %(memusage)dM\",\n-                    level=log.WARNING, memusage=mem)\n+            logger.warning(\"Memory usage reached %(memusage)dM\",\n+                           {'memusage': mem}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n                 subj = \"%s warning: memory usage reached %dM at %s\" % \\\n                         (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n\n@@ -9,6 +9,7 @@ import os\n import os.path\n import rfc822\n import time\n+import logging\n from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n import six\n@@ -20,12 +21,13 @@ except ImportError:\n \n from twisted.internet import defer, threads\n \n-from scrapy import log\n from scrapy.contrib.pipeline.media import MediaPipeline\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\n \n+logger = logging.getLogger('scrapy')\n+\n \n class FileException(Exception):\n     \"\"\"General media error exception\"\"\"\n@@ -192,9 +194,13 @@ class FilesPipeline(MediaPipeline):\n                 return  # returning None force download\n \n             referer = request.headers.get('Referer')\n-            log.msg(format='File (uptodate): Downloaded %(medianame)s from %(request)s referred in <%(referer)s>',\n-                    level=log.DEBUG, spider=info.spider,\n-                    medianame=self.MEDIA_NAME, request=request, referer=referer)\n+            logger.debug(\n+                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n+                'referred in <%(referer)s>',\n+                {'medianame': self.MEDIA_NAME, 'request': request,\n+                 'referer': referer},\n+                extra={'spider': info.spider}\n+            )\n             self.inc_stats(info.spider, 'uptodate')\n \n             checksum = result.get('checksum', None)\n@@ -203,17 +209,23 @@ class FilesPipeline(MediaPipeline):\n         path = self.file_path(request, info=info)\n         dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n         dfd.addCallbacks(_onsuccess, lambda _: None)\n-        dfd.addErrback(log.err, self.__class__.__name__ + '.store.stat_file')\n+        dfd.addErrback(\n+            lambda f:\n+            logger.error(self.__class__.__name__ + '.store.stat_file',\n+                         extra={'spider': info.spider, 'failure': f})\n+        )\n         return dfd\n \n     def media_failed(self, failure, request, info):\n         if not isinstance(failure.value, IgnoreRequest):\n             referer = request.headers.get('Referer')\n-            log.msg(format='File (unknown-error): Error downloading '\n-                           '%(medianame)s from %(request)s referred in '\n-                           '<%(referer)s>: %(exception)s',\n-                    level=log.WARNING, spider=info.spider, exception=failure.value,\n-                    medianame=self.MEDIA_NAME, request=request, referer=referer)\n+            logger.warning(\n+                'File (unknown-error): Error downloading %(medianame)s from '\n+                '%(request)s referred in <%(referer)s>: %(exception)s',\n+                {'medianame': self.MEDIA_NAME, 'request': request,\n+                 'referer': referer, 'exception': failure.value},\n+                extra={'spider': info.spider}\n+            )\n \n         raise FileException\n \n@@ -221,34 +233,51 @@ class FilesPipeline(MediaPipeline):\n         referer = request.headers.get('Referer')\n \n         if response.status != 200:\n-            log.msg(format='File (code: %(status)s): Error downloading file from %(request)s referred in <%(referer)s>',\n-                    level=log.WARNING, spider=info.spider,\n-                    status=response.status, request=request, referer=referer)\n+            logger.warning(\n+                'File (code: %(status)s): Error downloading file from '\n+                '%(request)s referred in <%(referer)s>',\n+                {'status': response.status,\n+                 'request': request, 'referer': referer},\n+                extra={'spider': info.spider}\n+            )\n             raise FileException('download-error')\n \n         if not response.body:\n-            log.msg(format='File (empty-content): Empty file from %(request)s referred in <%(referer)s>: no-content',\n-                    level=log.WARNING, spider=info.spider,\n-                    request=request, referer=referer)\n+            logger.warning(\n+                'File (empty-content): Empty file from %(request)s referred '\n+                'in <%(referer)s>: no-content',\n+                {'request': request, 'referer': referer},\n+                extra={'spider': info.spider}\n+            )\n             raise FileException('empty-content')\n \n         status = 'cached' if 'cached' in response.flags else 'downloaded'\n-        log.msg(format='File (%(status)s): Downloaded file from %(request)s referred in <%(referer)s>',\n-                level=log.DEBUG, spider=info.spider,\n-                status=status, request=request, referer=referer)\n+        logger.debug(\n+            'File (%(status)s): Downloaded file from %(request)s referred in '\n+            '<%(referer)s>',\n+            {'status': status, 'request': request, 'referer': referer},\n+            extra={'spider': info.spider}\n+        )\n         self.inc_stats(info.spider, status)\n \n         try:\n             path = self.file_path(request, response=response, info=info)\n             checksum = self.file_downloaded(response, request, info)\n         except FileException as exc:\n-            whyfmt = 'File (error): Error processing file from %(request)s referred in <%(referer)s>: %(errormsg)s'\n-            log.msg(format=whyfmt, level=log.WARNING, spider=info.spider,\n-                    request=request, referer=referer, errormsg=str(exc))\n+            logger.warning(\n+                'File (error): Error processing file from %(request)s '\n+                'referred in <%(referer)s>: %(errormsg)s',\n+                {'request': request, 'referer': referer, 'errormsg': str(exc)},\n+                extra={'spider': info.spider}, exc_info=True\n+            )\n             raise\n         except Exception as exc:\n-            whyfmt = 'File (unknown-error): Error processing file from %(request)s referred in <%(referer)s>'\n-            log.err(None, whyfmt % {'request': request, 'referer': referer}, spider=info.spider)\n+            logger.exception(\n+                'File (unknown-error): Error processing file from %(request)s '\n+                'referred in <%(referer)s>',\n+                {'request': request, 'referer': referer},\n+                extra={'spider': info.spider}\n+            )\n             raise FileException(str(exc))\n \n         return {'url': request.url, 'path': path, 'checksum': checksum}\n\n@@ -1,13 +1,16 @@\n from __future__ import print_function\n+\n+import logging\n from collections import defaultdict\n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n-from scrapy import log\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n \n+logger = logging.getLogger('scrapy')\n+\n \n class MediaPipeline(object):\n \n@@ -66,7 +69,9 @@ class MediaPipeline(object):\n         dfd = mustbe_deferred(self.media_to_download, request, info)\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n-        dfd.addErrback(log.err, spider=info.spider)\n+        dfd.addErrback(lambda f: logger.error(\n+            f.value, extra={'spider': info.spider, 'failure': f})\n+        )\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n     def _check_media_to_download(self, result, request, info):\n@@ -117,8 +122,11 @@ class MediaPipeline(object):\n     def item_completed(self, results, item, info):\n         \"\"\"Called per item when all media requests has been processed\"\"\"\n         if self.LOG_FAILED_RESULTS:\n-            msg = '%s found errors processing %s' % (self.__class__.__name__, item)\n             for ok, value in results:\n                 if not ok:\n-                    log.err(value, msg, spider=info.spider)\n+                    logger.error(\n+                        '%(class)s found errors processing %(item)s',\n+                        {'class': self.__class__.__name__, 'item': item},\n+                        extra={'spider': info.spider, 'failure': value}\n+                    )\n         return item\n\n@@ -4,9 +4,13 @@ Depth Spider Middleware\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n-from scrapy import log\n+import logging\n+\n from scrapy.http import Request\n \n+logger = logging.getLogger('scrapy')\n+\n+\n class DepthMiddleware(object):\n \n     def __init__(self, maxdepth, stats=None, verbose_stats=False, prio=1):\n@@ -31,9 +35,9 @@ class DepthMiddleware(object):\n                 if self.prio:\n                     request.priority -= depth * self.prio\n                 if self.maxdepth and depth > self.maxdepth:\n-                    log.msg(format=\"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n-                            level=log.DEBUG, spider=spider,\n-                            maxdepth=self.maxdepth, requrl=request.url)\n+                    logger.debug(\"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n+                                 {'maxdepth': self.maxdepth, 'requrl': request.url},\n+                                 extra={'spider': spider})\n                     return False\n                 elif self.stats:\n                     if self.verbose_stats:\n\n@@ -3,8 +3,12 @@ HttpError Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+import logging\n+\n from scrapy.exceptions import IgnoreRequest\n-from scrapy import log\n+\n+logger = logging.getLogger('scrapy')\n+\n \n class HttpError(IgnoreRequest):\n     \"\"\"A non-200 response was filtered\"\"\"\n@@ -42,10 +46,8 @@ class HttpErrorMiddleware(object):\n \n     def process_spider_exception(self, response, exception, spider):\n         if isinstance(exception, HttpError):\n-            log.msg(\n-                format=\"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n-                level=log.DEBUG,\n-                spider=spider,\n-                response=response\n+            logger.debug(\n+                \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n+                {'response': response}, extra={'spider': spider},\n             )\n             return []\n\n@@ -5,11 +5,13 @@ See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n import re\n+import logging\n \n from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy import log\n+\n+logger = logging.getLogger('scrapy')\n \n class OffsiteMiddleware(object):\n \n@@ -31,8 +33,8 @@ class OffsiteMiddleware(object):\n                     domain = urlparse_cached(x).hostname\n                     if domain and domain not in self.domains_seen:\n                         self.domains_seen.add(domain)\n-                        log.msg(format=\"Filtered offsite request to %(domain)r: %(request)s\",\n-                                level=log.DEBUG, spider=spider, domain=domain, request=x)\n+                        logger.debug(\"Filtered offsite request to %(domain)r: %(request)s\",\n+                                     {'domain': domain, 'request': x}, extra={'spider': spider})\n                         self.stats.inc_value('offsite/domains', spider=spider)\n                     self.stats.inc_value('offsite/filtered', spider=spider)\n             else:\n\n@@ -4,10 +4,14 @@ Url Length Spider Middleware\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n-from scrapy import log\n+import logging\n+\n from scrapy.http import Request\n from scrapy.exceptions import NotConfigured\n \n+logger = logging.getLogger('scrapy')\n+\n+\n class UrlLengthMiddleware(object):\n \n     def __init__(self, maxlength):\n@@ -23,9 +27,9 @@ class UrlLengthMiddleware(object):\n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n             if isinstance(request, Request) and len(request.url) > self.maxlength:\n-                log.msg(format=\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n-                        level=log.DEBUG, spider=spider,\n-                        maxlength=self.maxlength, url=request.url)\n+                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n+                             {'maxlength': self.maxlength, 'url': request.url},\n+                             extra={'spider': spider})\n                 return False\n             else:\n                 return True\n\n@@ -1,10 +1,13 @@\n import re\n+import logging\n \n from scrapy.spider import Spider\n from scrapy.http import Request, XmlResponse\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n from scrapy.utils.gz import gunzip, is_gzipped\n-from scrapy import log\n+\n+logger = logging.getLogger('scrapy')\n+\n \n class SitemapSpider(Spider):\n \n@@ -32,8 +35,8 @@ class SitemapSpider(Spider):\n         else:\n             body = self._get_sitemap_body(response)\n             if body is None:\n-                log.msg(format=\"Ignoring invalid sitemap: %(response)s\",\n-                        level=log.WARNING, spider=self, response=response)\n+                logger.warning(\"Ignoring invalid sitemap: %(response)s\",\n+                               {'response': response}, extra={'spider': self})\n                 return\n \n             s = Sitemap(body)\n\n@@ -1,7 +1,10 @@\n import logging\n+\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n \n+logger = logging.getLogger('scrapy')\n+\n \n class AutoThrottle(object):\n \n@@ -47,9 +50,17 @@ class AutoThrottle(object):\n             diff = slot.delay - olddelay\n             size = len(response.body)\n             conc = len(slot.transferring)\n-            msg = \"slot: %s | conc:%2d | delay:%5d ms (%+d) | latency:%5d ms | size:%6d bytes\" % \\\n-                  (key, conc, slot.delay * 1000, diff * 1000, latency * 1000, size)\n-            spider.log(msg, level=logging.INFO)\n+            logger.info(\n+                \"slot: %(slot)s | conc:%(concurrency)2d | \"\n+                \"delay:%(delay)5d ms (%(delaydiff)+d) | \"\n+                \"latency:%(latency)5d ms | size:%(size)6d bytes\",\n+                {\n+                    'slot': key, 'concurrency': conc,\n+                    'delay': slot.delay * 1000, 'delaydiff': diff * 1000,\n+                    'latency': latency * 1000, 'size': size\n+                },\n+                extra={'spider': spider}\n+            )\n \n     def _get_slot(self, request, spider):\n         key = request.meta.get('download_slot')\n\n@@ -1,7 +1,7 @@\n \"\"\"Download handlers for http and https schemes\"\"\"\n \n import re\n-\n+import logging\n from io import BytesIO\n from time import time\n from six.moves.urllib.parse import urldefrag\n@@ -19,7 +19,9 @@ from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n from scrapy.utils.misc import load_object\n-from scrapy import log, twisted_version\n+from scrapy import twisted_version\n+\n+logger = logging.getLogger('scrapy')\n \n \n class HTTP11DownloadHandler(object):\n@@ -237,14 +239,16 @@ class ScrapyAgent(object):\n         expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n \n         if maxsize and expected_size > maxsize:\n-            log.msg(\"Expected response size (%s) larger than download max size (%s).\" % (expected_size, maxsize),\n-                    logLevel=log.ERROR)\n+            logger.error(\"Expected response size (%(size)s) larger than \"\n+                         \"download max size (%(maxsize)s).\",\n+                         {'size': expected_size, 'maxsize': maxsize})\n             txresponse._transport._producer.loseConnection()\n             raise defer.CancelledError()\n \n         if warnsize and expected_size > warnsize:\n-            log.msg(\"Expected response size (%s) larger than downlod warn size (%s).\" % (expected_size, warnsize),\n-                    logLevel=log.WARNING)\n+            logger.warning(\"Expected response size (%(size)s) larger than \"\n+                           \"download warn size (%(warnsize)s).\",\n+                           {'size': expected_size, 'warnsize': warnsize})\n \n         def _cancel(_):\n             txresponse._transport._producer.loseConnection()\n@@ -295,13 +299,17 @@ class _ResponseReader(protocol.Protocol):\n         self._bytes_received += len(bodyBytes)\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n-            log.msg(\"Received (%s) bytes larger than download max size (%s).\" % (self._bytes_received, self._maxsize),\n-                    logLevel=log.ERROR)\n+            logger.error(\"Received (%(bytes)s) bytes larger than download \"\n+                         \"max size (%(maxsize)s).\",\n+                         {'bytes': self._bytes_received,\n+                          'maxsize': self._maxsize})\n             self._finished.cancel()\n \n         if self._warnsize and self._bytes_received > self._warnsize:\n-            log.msg(\"Received (%s) bytes larger than download warn size (%s).\" % (self._bytes_received, self._warnsize),\n-                    logLevel=log.WARNING)\n+            logger.warning(\"Received (%(bytes)s) bytes larger than download \"\n+                           \"warn size (%(warnsize)s).\",\n+                           {'bytes': self._bytes_received,\n+                            'warnsize': self._warnsize})\n \n     def connectionLost(self, reason):\n         if self._finished.called:\n\n@@ -4,6 +4,7 @@ This is the Scrapy engine which controls the Scheduler, Downloader and Spiders.\n For more information see docs/topics/architecture.rst\n \n \"\"\"\n+import logging\n from time import time\n \n from twisted.internet import defer\n@@ -16,6 +17,8 @@ from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\n \n+logger = logging.getLogger('scrapy')\n+\n \n class Slot(object):\n \n@@ -106,10 +109,10 @@ class ExecutionEngine(object):\n                 request = next(slot.start_requests)\n             except StopIteration:\n                 slot.start_requests = None\n-            except Exception as exc:\n+            except Exception:\n                 slot.start_requests = None\n-                log.err(None, 'Obtaining request from start requests', \\\n-                        spider=spider)\n+                logger.exception('Error while obtaining start requests',\n+                                 extra={'spider': spider})\n             else:\n                 self.crawl(request, spider)\n \n@@ -130,11 +133,14 @@ class ExecutionEngine(object):\n             return\n         d = self._download(request, spider)\n         d.addBoth(self._handle_downloader_output, request, spider)\n-        d.addErrback(log.msg, spider=spider)\n+        d.addErrback(lambda f: logger.info('Error while handling downloader output',\n+                                           extra={'spider': spider, 'failure': f}))\n         d.addBoth(lambda _: slot.remove_request(request))\n-        d.addErrback(log.msg, spider=spider)\n+        d.addErrback(lambda f: logger.info('Error while removing request from slot',\n+                                           extra={'spider': spider, 'failure': f}))\n         d.addBoth(lambda _: slot.nextcall.schedule())\n-        d.addErrback(log.msg, spider=spider)\n+        d.addErrback(lambda f: logger.info('Error while scheduling new request',\n+                                           extra={'spider': spider, 'failure': f}))\n         return d\n \n     def _handle_downloader_output(self, response, request, spider):\n@@ -145,7 +151,8 @@ class ExecutionEngine(object):\n             return\n         # response is a Response or Failure\n         d = self.scraper.enqueue_scrape(response, request, spider)\n-        d.addErrback(log.err, spider=spider)\n+        d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',\n+                                            extra={'spider': spider, 'failure': f}))\n         return d\n \n     def spider_is_idle(self, spider):\n@@ -215,7 +222,7 @@ class ExecutionEngine(object):\n     def open_spider(self, spider, start_requests=(), close_if_idle=True):\n         assert self.has_capacity(), \"No free spider slot when opening %r\" % \\\n             spider.name\n-        log.msg(\"Spider opened\", spider=spider)\n+        logger.info(\"Spider opened\", extra={'spider': spider})\n         nextcall = CallLaterOnce(self._next_request, spider)\n         scheduler = self.scheduler_cls.from_crawler(self.crawler)\n         start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n@@ -252,33 +259,42 @@ class ExecutionEngine(object):\n         slot = self.slot\n         if slot.closing:\n             return slot.closing\n-        log.msg(format=\"Closing spider (%(reason)s)\", reason=reason, spider=spider)\n+        logger.info(\"Closing spider (%(reason)s)\",\n+                    {'reason': reason},\n+                    extra={'spider': spider})\n \n         dfd = slot.close()\n \n+        def log_failure(msg):\n+            def errback(failure):\n+                logger.error(msg, extra={'spider': spider, 'failure': failure})\n+            return errback\n+\n         dfd.addBoth(lambda _: self.downloader.close())\n-        dfd.addErrback(log.err, spider=spider)\n+        dfd.addErrback(log_failure('Downloader close failure'))\n \n         dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n-        dfd.addErrback(log.err, spider=spider)\n+        dfd.addErrback(log_failure('Scraper close failure'))\n \n         dfd.addBoth(lambda _: slot.scheduler.close(reason))\n-        dfd.addErrback(log.err, spider=spider)\n+        dfd.addErrback(log_failure('Scheduler close failure'))\n \n         dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n             signal=signals.spider_closed, spider=spider, reason=reason))\n-        dfd.addErrback(log.err, spider=spider)\n+        dfd.addErrback(log_failure('Error while sending spider_close signal'))\n \n         dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n-        dfd.addErrback(log.err, spider=spider)\n+        dfd.addErrback(log_failure('Stats close failure'))\n \n-        dfd.addBoth(lambda _: log.msg(format=\"Spider closed (%(reason)s)\", reason=reason, spider=spider))\n+        dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\",\n+                                          {'reason': reason},\n+                                          extra={'spider': spider}))\n \n         dfd.addBoth(lambda _: setattr(self, 'slot', None))\n-        dfd.addErrback(log.err, spider=spider)\n+        dfd.addErrback(log_failure('Error while unassigning slot'))\n \n         dfd.addBoth(lambda _: setattr(self, 'spider', None))\n-        dfd.addErrback(log.err, spider=spider)\n+        dfd.addErrback(log_failure('Error while unassigning spider'))\n \n         dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n \n\n@@ -1,12 +1,15 @@\n import os\n import json\n+import logging\n from os.path import join, exists\n \n from queuelib import PriorityQueue\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n from scrapy.utils.misc import load_object\n from scrapy.utils.job import job_dir\n-from scrapy import log\n+\n+logger = logging.getLogger('scrapy')\n+\n \n class Scheduler(object):\n \n@@ -80,9 +83,9 @@ class Scheduler(object):\n             self.dqs.push(reqd, -request.priority)\n         except ValueError as e: # non serializable request\n             if self.logunser:\n-                log.msg(format=\"Unable to serialize request: %(request)s - reason: %(reason)s\",\n-                        level=log.ERROR, spider=self.spider,\n-                        request=request, reason=e)\n+                logger.exception(\"Unable to serialize request: %(request)s - reason: %(reason)s\",\n+                                 {'request': request, 'reason': e},\n+                                 extra={'spider': self.spider})\n             return\n         else:\n             return True\n@@ -111,8 +114,8 @@ class Scheduler(object):\n             prios = ()\n         q = PriorityQueue(self._newdq, startprios=prios)\n         if q:\n-            log.msg(format=\"Resuming crawl (%(queuesize)d requests scheduled)\",\n-                    spider=self.spider, queuesize=len(q))\n+            logger.info(\"Resuming crawl (%(queuesize)d requests scheduled)\",\n+                        {'queuesize': len(q)}, extra={'spider': self.spider})\n         return q\n \n     def _dqdir(self, jobdir):\n\n@@ -1,6 +1,7 @@\n \"\"\"This module implements the Scraper component which parses responses and\n extracts information from them\"\"\"\n \n+import logging\n from collections import deque\n \n from twisted.python.failure import Failure\n@@ -16,6 +17,8 @@ from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy import log\n \n+logger = logging.getLogger('scrapy')\n+\n \n class Slot(object):\n     \"\"\"Scraper slot (one per running spider)\"\"\"\n@@ -102,7 +105,9 @@ class Scraper(object):\n             return _\n         dfd.addBoth(finish_scraping)\n         dfd.addErrback(\n-            log.err, 'Scraper bug processing %s' % request, spider=spider)\n+            lambda f: logger.error('Scraper bug processing %(request)s',\n+                                   {'request': request},\n+                                   extra={'spider': spider, 'failure': f}))\n         self._scrape_next(spider, slot)\n         return dfd\n \n@@ -145,10 +150,10 @@ class Scraper(object):\n             self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n             return\n         referer = request.headers.get('Referer')\n-        log.err(\n-            _failure,\n-            \"Spider error processing %s (referer: %s)\" % (request, referer),\n-            spider=spider\n+        logger.error(\n+            \"Spider error processing %(request)s (referer: %(referer)s)\",\n+            {'request': request, 'referer': referer},\n+            extra={'spider': spider, 'failure': _failure}\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,\n@@ -183,9 +188,10 @@ class Scraper(object):\n             pass\n         else:\n             typename = type(output).__name__\n-            log.msg(format='Spider must return Request, BaseItem, dict or None, '\n+            logger.error('Spider must return Request, BaseItem, dict or None, '\n                          'got %(typename)r in %(request)s',\n-                    level=log.ERROR, spider=spider, request=request, typename=typename)\n+                         {'request': request, 'typename': typename},\n+                         extra={'spider': spider})\n \n     def _log_download_errors(self, spider_failure, download_failure, request, spider):\n         \"\"\"Log and silence errors that come from the engine (typically download\n@@ -194,14 +200,15 @@ class Scraper(object):\n         if (isinstance(download_failure, Failure) and\n                 not download_failure.check(IgnoreRequest)):\n             if download_failure.frames:\n-                log.err(download_failure, 'Error downloading %s' % request,\n-                        spider=spider)\n+                logger.error('Error downloading %(request)s',\n+                             {'request': request},\n+                             extra={'spider': spider, 'failure': download_failure})\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:\n-                    log.msg(format='Error downloading %(request)s: %(errmsg)s',\n-                            level=log.ERROR, spider=spider, request=request,\n-                            errmsg=errmsg)\n+                    logger.error('Error downloading %(request)s: %(errmsg)s',\n+                                 {'request': request, 'errmsg': errmsg},\n+                                 extra={'spider': spider})\n \n         if spider_failure is not download_failure:\n             return spider_failure\n@@ -219,7 +226,8 @@ class Scraper(object):\n                     signal=signals.item_dropped, item=item, response=response,\n                     spider=spider, exception=output.value)\n             else:\n-                log.err(output, 'Error processing %s' % item, spider=spider)\n+                logger.error('Error processing %(item)s', {'item': item},\n+                             extra={'spider': spider, 'failure': output})\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n             log.msg(spider=spider, **logkws)\n\n@@ -1,5 +1,6 @@\n import six\n import signal\n+import logging\n import warnings\n \n from twisted.internet import reactor, defer\n@@ -14,7 +15,9 @@ from scrapy.signalmanager import SignalManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n-from scrapy import log, signals\n+from scrapy import signals\n+\n+logger = logging.getLogger('scrapy')\n \n \n class Crawler(object):\n@@ -145,15 +148,15 @@ class CrawlerProcess(CrawlerRunner):\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)\n         signame = signal_names[signum]\n-        log.msg(format=\"Received %(signame)s, shutting down gracefully. Send again to force \",\n-                level=log.INFO, signame=signame)\n+        logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n+                    {'signame': signame})\n         reactor.callFromThread(self.stop)\n \n     def _signal_kill(self, signum, _):\n         install_shutdown_handlers(signal.SIG_IGN)\n         signame = signal_names[signum]\n-        log.msg(format='Received %(signame)s twice, forcing unclean shutdown',\n-                level=log.INFO, signame=signame)\n+        logger.info('Received %(signame)s twice, forcing unclean shutdown',\n+                    {'signame': signame})\n         self._stop_logging()\n         reactor.callFromThread(self._stop_reactor)\n \n\n@@ -1,7 +1,7 @@\n from __future__ import print_function\n import os\n+import logging\n \n-from scrapy import log\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import request_fingerprint\n \n@@ -33,6 +33,7 @@ class RFPDupeFilter(BaseDupeFilter):\n         self.fingerprints = set()\n         self.logdupes = True\n         self.debug = debug\n+        self.logger = logging.getLogger('scrapy')\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n@@ -59,13 +60,13 @@ class RFPDupeFilter(BaseDupeFilter):\n \n     def log(self, request, spider):\n         if self.debug:\n-            fmt = \"Filtered duplicate request: %(request)s\"\n-            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)\n+            msg = \"Filtered duplicate request: %(request)s\"\n+            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n         elif self.logdupes:\n-            fmt = (\"Filtered duplicate request: %(request)s\"\n+            msg = (\"Filtered duplicate request: %(request)s\"\n                    \" - no more duplicates will be shown\"\n                    \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n-            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)\n+            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n             self.logdupes = False\n \n         spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider)\n\n@@ -3,6 +3,8 @@ Mail sending helpers\n \n See documentation in docs/topics/email.rst\n \"\"\"\n+import logging\n+\n from six.moves import cStringIO as StringIO\n import six\n \n@@ -20,7 +22,8 @@ else:\n from twisted.internet import defer, reactor, ssl\n from twisted.mail.smtp import ESMTPSenderFactory\n \n-from scrapy import log\n+logger = logging.getLogger('scrapy')\n+\n \n class MailSender(object):\n \n@@ -71,8 +74,10 @@ class MailSender(object):\n             _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n \n         if self.debug:\n-            log.msg(format='Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n-                    level=log.DEBUG, mailto=to, mailcc=cc, mailsubject=subject, mailattachs=len(attachs))\n+            logger.debug('Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n+                         'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n+                         {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n+                          'mailattachs': len(attachs)})\n             return\n \n         dfd = self._sendmail(rcpts, msg.as_string())\n@@ -83,17 +88,18 @@ class MailSender(object):\n         return dfd\n \n     def _sent_ok(self, result, to, cc, subject, nattachs):\n-        log.msg(format='Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n+        logger.info('Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n                     'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n-                mailto=to, mailcc=cc, mailsubject=subject, mailattachs=nattachs)\n+                    {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n+                     'mailattachs': nattachs})\n \n     def _sent_failed(self, failure, to, cc, subject, nattachs):\n         errstr = str(failure.value)\n-        log.msg(format='Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '\n+        logger.error('Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '\n                      'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d'\n                      '- %(mailerr)s',\n-                level=log.ERROR, mailto=to, mailcc=cc, mailsubject=subject,\n-                mailattachs=nattachs, mailerr=errstr)\n+                     {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n+                      'mailattachs': nattachs, 'mailerr': errstr})\n \n     def _sendmail(self, to_addrs, msg):\n         msg = StringIO(msg)\n\n@@ -1,10 +1,13 @@\n+import logging\n from collections import defaultdict\n \n-from scrapy import log\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import process_parallel, process_chain, process_chain_both\n \n+logger = logging.getLogger('scrapy')\n+\n+\n class MiddlewareManager(object):\n     \"\"\"Base class for implementing middleware managers\"\"\"\n \n@@ -37,12 +40,15 @@ class MiddlewareManager(object):\n             except NotConfigured as e:\n                 if e.args:\n                     clsname = clspath.split('.')[-1]\n-                    log.msg(format=\"Disabled %(clsname)s: %(eargs)s\",\n-                            level=log.WARNING, clsname=clsname, eargs=e.args[0])\n+                    logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n+                                   {'clsname': clsname, 'eargs': e.args[0]},\n+                                   extra={'crawler': crawler})\n \n         enabled = [x.__class__.__name__ for x in middlewares]\n-        log.msg(format=\"Enabled %(componentname)ss: %(enabledlist)s\", level=log.INFO,\n-                componentname=cls.component_name, enabledlist=', '.join(enabled))\n+        logger.info(\"Enabled %(componentname)ss: %(enabledlist)s\",\n+                    {'componentname': cls.component_name,\n+                     'enabledlist': ', '.join(enabled)},\n+                    extra={'crawler': crawler})\n         return cls(*middlewares)\n \n     @classmethod\n\n@@ -2,8 +2,10 @@\n Scrapy extension for collecting scraping stats\n \"\"\"\n import pprint\n+import logging\n+\n+logger = logging.getLogger('scrapy')\n \n-from scrapy import log\n \n class StatsCollector(object):\n \n@@ -41,8 +43,8 @@ class StatsCollector(object):\n \n     def close_spider(self, spider, reason):\n         if self._dump:\n-            log.msg(\"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats), \\\n-                spider=spider)\n+            logger.info(\"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n+                        extra={'spider': spider})\n         self._persist_stats(self._stats, spider)\n \n     def _persist_stats(self, stats, spider):\n\n@@ -5,6 +5,7 @@ See documentation in docs/topics/telnetconsole.rst\n \"\"\"\n \n import pprint\n+import logging\n \n from twisted.internet import protocol\n try:\n@@ -15,7 +16,7 @@ except ImportError:\n     TWISTED_CONCH_AVAILABLE = False\n \n from scrapy.exceptions import NotConfigured\n-from scrapy import log, signals\n+from scrapy import signals\n from scrapy.utils.trackref import print_live_refs\n from scrapy.utils.engine import print_engine_status\n from scrapy.utils.reactor import listen_tcp\n@@ -26,6 +27,8 @@ try:\n except ImportError:\n     hpy = None\n \n+logger = logging.getLogger('scrapy')\n+\n # signal to update telnet variables\n # args: telnet_vars\n update_telnet_vars = object()\n@@ -52,8 +55,9 @@ class TelnetConsole(protocol.ServerFactory):\n     def start_listening(self):\n         self.port = listen_tcp(self.portrange, self.host, self)\n         h = self.port.getHost()\n-        log.msg(format=\"Telnet console listening on %(host)s:%(port)d\",\n-                level=log.DEBUG, host=h.host, port=h.port)\n+        logger.debug(\"Telnet console listening on %(host)s:%(port)d\",\n+                     {'host': h.host, 'port': h.port},\n+                     extra={'crawler': self.crawler})\n \n     def stop_listening(self):\n         self.port.stopListening()\n\n@@ -1,15 +1,20 @@\n-import re, csv, six\n+import re\n+import csv\n+import logging\n \n try:\n     from cStringIO import StringIO as BytesIO\n except ImportError:\n     from io import BytesIO\n \n+import six\n+\n from scrapy.http import TextResponse, Response\n from scrapy.selector import Selector\n-from scrapy import log\n from scrapy.utils.python import re_rsearch, str_to_unicode\n \n+logger = logging.getLogger('scrapy')\n+\n \n def xmliter(obj, nodename):\n     \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n@@ -108,8 +113,10 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     while True:\n         row = _getrow(csv_r)\n         if len(row) != len(headers):\n-            log.msg(format=\"ignoring row %(csvlnum)d (length: %(csvrow)d, should be: %(csvheader)d)\",\n-                    level=log.WARNING, csvlnum=csv_r.line_num, csvrow=len(row), csvheader=len(headers))\n+            logger.warning(\"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n+                           \"should be: %(csvheader)d)\",\n+                           {'csvlnum': csv_r.line_num, 'csvrow': len(row),\n+                            'csvheader': len(headers)})\n             continue\n         else:\n             yield dict(zip(headers, row))\n\n@@ -1,5 +1,7 @@\n \"\"\"Helper functions for working with signals\"\"\"\n \n+import logging\n+\n from twisted.internet.defer import maybeDeferred, DeferredList, Deferred\n from twisted.python.failure import Failure\n \n@@ -7,7 +9,8 @@ from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \\\n     getAllReceivers, disconnect\n from scrapy.xlib.pydispatch.robustapply import robustApply\n \n-from scrapy import log\n+logger = logging.getLogger('scrapy')\n+\n \n def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n     \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n@@ -21,14 +24,14 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n             response = robustApply(receiver, signal=signal, sender=sender,\n                 *arguments, **named)\n             if isinstance(response, Deferred):\n-                log.msg(format=\"Cannot return deferreds from signal handler: %(receiver)s\",\n-                        level=log.ERROR, spider=spider, receiver=receiver)\n+                logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n+                             {'receiver': receiver}, extra={'spider': spider})\n         except dont_log:\n             result = Failure()\n         except Exception:\n             result = Failure()\n-            log.err(result, \"Error caught on signal handler: %s\" % receiver, \\\n-                spider=spider)\n+            logger.exception(\"Error caught on signal handler: %(receiver)s\",\n+                             {'receiver': receiver}, extra={'spider': spider})\n         else:\n             result = response\n         responses.append((receiver, result))\n@@ -41,8 +44,9 @@ def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n     \"\"\"\n     def logerror(failure, recv):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n-            log.err(failure, \"Error caught on signal handler: %s\" % recv, \\\n-                spider=spider)\n+            logger.error(\"Error caught on signal handler: %(receiver)s\",\n+                         {'receiver': recv},\n+                         extra={'spider': spider, 'failure': failure})\n         return failure\n \n     dont_log = named.pop('dont_log', None)\n\n@@ -1,11 +1,13 @@\n+import logging\n import inspect\n \n import six\n \n-from scrapy import log\n from scrapy.spider import Spider\n from scrapy.utils.misc import  arg_to_iter\n \n+logger = logging.getLogger('scrapy')\n+\n \n def iterate_spider_output(result):\n     return arg_to_iter(result)\n@@ -43,12 +45,12 @@ def spidercls_for_request(spider_loader, request, default_spidercls=None,\n         return spider_loader.load(snames[0])\n \n     if len(snames) > 1 and log_multiple:\n-        log.msg(format='More than one spider can handle: %(request)s - %(snames)s',\n-                level=log.ERROR, request=request, snames=', '.join(snames))\n+        logger.error('More than one spider can handle: %(request)s - %(snames)s',\n+                     {'request': request, 'snames': ', '.join(snames)})\n \n     if len(snames) == 0 and log_none:\n-        log.msg(format='Unable to find spider that handles: %(request)s',\n-                level=log.ERROR, request=request)\n+        logger.error('Unable to find spider that handles: %(request)s',\n+                     {'request': request})\n \n     return default_spidercls\n \n\n@@ -137,7 +137,6 @@ class RunSpiderCommandTest(CommandTest):\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n import scrapy\n-from scrapy import log\n \n class MySpider(scrapy.Spider):\n     name = 'myspider'\n@@ -148,10 +147,10 @@ class MySpider(scrapy.Spider):\n \"\"\")\n         p = self.proc('runspider', fname)\n         log = p.stderr.read()\n-        self.assertIn(\"[myspider] DEBUG: It Works!\", log)\n-        self.assertIn(\"[myspider] INFO: Spider opened\", log)\n-        self.assertIn(\"[myspider] INFO: Closing spider (finished)\", log)\n-        self.assertIn(\"[myspider] INFO: Spider closed (finished)\", log)\n+        self.assertIn(\"DEBUG: It Works!\", log)\n+        self.assertIn(\"INFO: Spider opened\", log)\n+        self.assertIn(\"INFO: Closing spider (finished)\", log)\n+        self.assertIn(\"INFO: Spider closed (finished)\", log)\n \n     def test_runspider_no_spider_found(self):\n         tmpdir = self.mktemp()\n@@ -159,7 +158,6 @@ class MySpider(scrapy.Spider):\n         fname = abspath(join(tmpdir, 'myspider.py'))\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n-from scrapy import log\n from scrapy.spider import Spider\n \"\"\")\n         p = self.proc('runspider', fname)\n@@ -192,7 +190,6 @@ class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n         fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n-from scrapy import log\n import scrapy\n \n class MySpider(scrapy.Spider):\n@@ -207,13 +204,13 @@ class MySpider(scrapy.Spider):\n         fname = abspath(join(self.proj_mod_path, 'pipelines.py'))\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n-from scrapy import log\n+import logging\n \n class MyPipeline(object):\n     component_name = 'my_pipeline'\n \n     def process_item(self, item, spider):\n-        log.msg('It Works!')\n+        logging.info('It Works!')\n         return item\n \"\"\")\n \n@@ -229,7 +226,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            '-a', 'test_arg=1',\n                                            '-c', 'parse',\n                                            self.url('/html')])\n-        self.assertIn(\"[parse_spider] DEBUG: It Works!\", stderr)\n+        self.assertIn(\"DEBUG: It Works!\", stderr)\n \n     @defer.inlineCallbacks\n     def test_pipelines(self):\n@@ -237,7 +234,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            '--pipelines',\n                                            '-c', 'parse',\n                                            self.url('/html')])\n-        self.assertIn(\"[scrapy] INFO: It Works!\", stderr)\n+        self.assertIn(\"INFO: It Works!\", stderr)\n \n     @defer.inlineCallbacks\n     def test_parse_items(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
