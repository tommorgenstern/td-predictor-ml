{"custom_id": "keras#63dc12d51565d0077eb2239f6219d4b51e117853", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3606 | Contributors (this commit): 15 | Commits (past 90d): 1 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -233,9 +233,9 @@ class LSTMCell(Layer, DropoutRNNCell):\n         rec_dp_mask = self.get_recurrent_dropout_mask(h_tm1)\n \n         if training and 0.0 < self.dropout < 1.0:\n-            inputs *= dp_mask\n+            inputs = inputs * dp_mask\n         if training and 0.0 < self.recurrent_dropout < 1.0:\n-            h_tm1 *= rec_dp_mask\n+            h_tm1 = h_tm1 * rec_dp_mask\n \n         if self.implementation == 1:\n             inputs_i = inputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3553fa49146f3cd62be1482e2195e71adfc3807b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 224 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,7 +10,6 @@ from packaging.version import parse\n from keras import backend\n from keras import callbacks as callbacks_module\n from keras import optimizers as optimizers_module\n-from keras.backend.torch.core import get_device\n from keras.trainers import data_adapters\n from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import data_adapter_utils\n@@ -508,10 +507,7 @@ class TorchEpochIterator(EpochIterator):\n         return super()._get_iterator(return_type)\n \n     def _prefetch_numpy_data(self, data):\n-        def to_device(d):\n-            return torch.as_tensor(d, device=get_device())\n-\n-        return tree.map_structure(to_device, data)\n+        return tree.map_structure(backend.convert_to_tensor, data)\n \n     def _prefetch_numpy_iterator(self, numpy_iterator):\n         \"\"\"Prefetch batches on device.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f35f2efcb14599cc0c982cf39d51df3d56dceb9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 38 | Files Changed: 2 | Hunks: 12 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 85 | Churn Cumulative: 1743 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import os\n \n import numpy as np\n+from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n from keras import backend\n@@ -10,7 +11,7 @@ from keras import testing\n from keras.saving import saving_api\n \n \n-class DicretizationTest(testing.TestCase):\n+class DicretizationTest(testing.TestCase, parameterized.TestCase):\n     def test_discretization_basics(self):\n         self.run_layer_test(\n             layers.Discretization,\n@@ -35,38 +36,39 @@ class DicretizationTest(testing.TestCase):\n         output = layer(np.array([[0.0, 0.1, 0.3]]))\n         self.assertTrue(output.dtype, \"int32\")\n \n-    def test_correctness(self):\n-        # int mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"int\"\n+    @parameterized.parameters(\n+        [\n+            (\"int\", [[-1.0, 0.0, 0.1, 0.8, 1.2]], [[0, 1, 1, 2, 3]]),\n+            (\"one_hot\", [0.1, 0.8], [[0, 1, 0, 0], [0, 0, 1, 0]]),\n+            (\"multi_hot\", [[0.1, 0.8]], [[0, 1, 1, 0]]),\n+            (\n+                \"one_hot\",\n+                [[[0.15, 0.75], [0.85, 0.45]]],\n+                [\n+                    [\n+                        [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]],\n+                        [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0]],\n+                    ]\n+                ],\n+            ),\n+            (\n+                \"multi_hot\",\n+                [[[0.15, 0.75], [0.85, 0.45]]],\n+                [[[0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0]]],\n+            ),\n+            (\"count\", [[0.1, 0.8, 0.9]], [[0, 1, 2, 0]]),\n+        ]\n     )\n-        output = layer(np.array([[-1.0, 0.0, 0.1, 0.8, 1.2]]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 1, 2, 3]]))\n+    def test_correctness(self, output_mode, input_array, expected_output):\n+        input_array = np.array(input_array)\n+        expected_output = np.array(expected_output)\n \n-        # one_hot mode\n         layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"one_hot\"\n+            bin_boundaries=[0.0, 0.5, 1.0], output_mode=output_mode\n         )\n-        output = layer(np.array([0.1, 0.8]))\n+        output = layer(input_array)\n         self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 0, 0], [0, 0, 1, 0]]))\n-\n-        # multi_hot mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"multi_hot\"\n-        )\n-        output = layer(np.array([[0.1, 0.8]]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 1, 0]]))\n-\n-        # count mode\n-        layer = layers.Discretization(\n-            bin_boundaries=[0.0, 0.5, 1.0], output_mode=\"count\"\n-        )\n-        output = layer(np.array([[0.1, 0.8, 0.9]]))\n-        self.assertTrue(backend.is_tensor(output))\n-        self.assertAllClose(output, np.array([[0, 1, 2, 0]]))\n+        self.assertAllClose(output, expected_output)\n \n     def test_tf_data_compatibility(self):\n         # With fixed bins\n\n@@ -120,20 +120,27 @@ def encode_categorical_inputs(\n     if output_mode == \"int\":\n         return backend_module.cast(inputs, dtype=dtype)\n \n-    original_shape = inputs.shape\n-    # In all cases, we should uprank scalar input to a single sample.\n-    if len(backend_module.shape(inputs)) == 0:\n-        inputs = backend_module.numpy.expand_dims(inputs, -1)\n+    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n+    original_shape = backend_module.shape(inputs)\n+    rank_of_inputs = len(original_shape)\n \n-    if len(backend_module.shape(inputs)) > 2:\n+    # In all cases, we should uprank scalar input to a single sample.\n+    if rank_of_inputs == 0:\n+        # We need to update `rank_of_inputs`\n+        # If necessary.\n+        inputs = backend_module.numpy.expand_dims(inputs, -1)\n+    elif rank_of_inputs > 2:\n+        # The `count` mode does not support inputs with a rank greater than 2.\n+        if not binary_output:\n             raise ValueError(\n-            \"When output_mode is not `'int'`, maximum supported output rank \"\n-            f\"is 2. Received output_mode {output_mode} and input shape \"\n-            f\"{original_shape}, \"\n-            f\"which would result in output rank {inputs.shape.rank}.\"\n+                \"When output_mode is anything other than \"\n+                \"`'multi_hot', 'one_hot', or 'int'`, \"\n+                \"the rank must be 2 or less. \"\n+                f\"Received output_mode: {output_mode} \"\n+                f\"and input shape: {original_shape}, \"\n+                f\"which would result in output rank {rank_of_inputs}.\"\n             )\n \n-    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n     if binary_output:\n         if output_mode == \"one_hot\":\n             bincounts = backend_module.nn.one_hot(inputs, depth)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c71ae2fef2c786ee97942f76b8c00af779309acc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 53 | Files Changed: 8 | Hunks: 17 | Methods Changed: 11 | Complexity Δ (Sum/Max): -7/3 | Churn Δ: 119 | Churn Cumulative: 16756 | Contributors (this commit): 28 | Commits (past 90d): 11 | Contributors (cumulative): 103 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-from keras import backend\n from keras.api_export import keras_export\n from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras.utils import backend_utils\n@@ -150,10 +149,4 @@ class CategoryEncoding(TFDataLayer):\n \n     def call(self, inputs):\n         outputs = self._encode(inputs)\n-\n-        if (\n-            self.backend._backend != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n-        return outputs\n+        return backend_utils.convert_tf_tensor(outputs)\n\n@@ -170,12 +170,7 @@ class HashedCrossing(Layer):\n             sparse=self.sparse,\n             dtype=self.compute_dtype,\n         )\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs, dtype=self.dtype)\n-        return outputs\n+        return backend_utils.convert_tf_tensor(outputs, dtype=self.dtype)\n \n     def get_config(self):\n         return {\n\n@@ -226,12 +226,7 @@ class Hashing(Layer):\n             sparse=self.sparse,\n             dtype=self.dtype,\n         )\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n-        return outputs\n+        return backend_utils.convert_tf_tensor(outputs)\n \n     def _hash_values_to_bins(self, values):\n         \"\"\"Converts a non-sparse tensor of values to bin indices.\"\"\"\n\n@@ -403,9 +403,4 @@ class IntegerLookup(IndexLookup):\n         ):\n             inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         outputs = super().call(inputs)\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n-        return outputs\n+        return backend_utils.convert_tf_tensor(outputs)\n\n@@ -390,10 +390,6 @@ class StringLookup(IndexLookup):\n             if not isinstance(inputs, (np.ndarray, list, tuple)):\n                 inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         outputs = super().call(inputs)\n-        if (\n-            not tf_inputs\n-            and backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n+        if not tf_inputs:\n+            outputs = backend_utils.convert_tf_tensor(outputs)\n         return outputs\n\n@@ -123,7 +123,9 @@ class TextVectorization(Layer):\n             have its time dimension padded or truncated to exactly\n             `output_sequence_length` values, resulting in a tensor of shape\n             `(batch_size, output_sequence_length)` regardless of how many tokens\n-            resulted from the splitting step. Defaults to `None`.\n+            resulted from the splitting step. Defaults to `None`. If `ragged`\n+            is `True` then `output_sequence_length` may still truncate the\n+            output.\n         pad_to_max_tokens: Only valid in  `\"multi_hot\"`, `\"count\"`,\n             and `\"tf_idf\"` modes. If `True`, the output will have\n             its feature axis padded to `max_tokens` even if the number\n@@ -315,13 +317,6 @@ class TextVectorization(Layer):\n                 f\"output_mode={output_mode}\"\n             )\n \n-        if ragged and output_sequence_length is not None:\n-            raise ValueError(\n-                \"`output_sequence_length` must not be set if ragged \"\n-                f\"is True. Received: ragged={ragged} and \"\n-                f\"output_sequence_length={output_sequence_length}\"\n-            )\n-\n         self._max_tokens = max_tokens\n         self._standardize = standardize\n         self._split = split\n@@ -582,21 +577,19 @@ class TextVectorization(Layer):\n \n         lookup_data = self._lookup_layer.call(inputs)\n \n-        # For any non-int output, we can return directly from the underlying\n-        # layer.\n+        # For non-int output, we can return directly from the underlying layer.\n         if self._output_mode != \"int\":\n-            outputs = lookup_data\n-\n-        if self._ragged:\n-            outputs = lookup_data\n+            return backend_utils.convert_tf_tensor(lookup_data)\n \n         # If we have a ragged tensor, we can pad during the conversion to dense.\n-        if isinstance(lookup_data, tf.RaggedTensor):\n+        if isinstance(lookup_data, tf.RaggedTensor) and not self._ragged:\n             shape = lookup_data.shape.as_list()\n             # If output sequence length is None, to_tensor will pad the last\n             # dimension to the bounding shape of the ragged dimension.\n             shape[-1] = self._output_sequence_length\n             outputs = lookup_data.to_tensor(default_value=0, shape=shape)\n+        else:\n+            outputs = lookup_data\n \n         # If we have a dense tensor, we need to pad/trim directly.\n         if self._output_sequence_length is not None:\n@@ -606,6 +599,7 @@ class TextVectorization(Layer):\n             # Maybe pad the output. We need to be careful to use dynamic shape\n             # here as required_space_to_batch_paddings requires a fully known\n             # shape.\n+            if not self._ragged:\n                 shape = tf.shape(outputs)\n                 padded_shape = tf.concat(\n                     (shape[:-1], [self._output_sequence_length]), 0\n@@ -615,12 +609,7 @@ class TextVectorization(Layer):\n                 )\n                 outputs = tf.pad(outputs, padding)\n \n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n-        return outputs\n+        return backend_utils.convert_tf_tensor(outputs)\n \n     def save_own_variables(self, store):\n         self._lookup_layer.save_own_variables(store)\n\n@@ -1,5 +1,6 @@\n import numpy as np\n import pytest\n+import tensorflow as tf\n from tensorflow import data as tf_data\n \n from keras import backend\n@@ -104,3 +105,46 @@ class TextVectorizationTest(testing.TestCase):\n             ]\n         )\n         model(backend.convert_to_tensor([[\"foo qux bar\"], [\"qux baz\"]]))\n+\n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"Requires ragged tensors.\"\n+    )\n+    def test_ragged_tensor(self):\n+        layer = layers.TextVectorization(\n+            output_mode=\"int\",\n+            vocabulary=[\"baz\", \"bar\", \"foo\"],\n+            ragged=True,\n+        )\n+        input_data = [[\"foo qux bar\"], [\"qux baz\"], [\"foo\"]]\n+        output = layer(input_data)\n+        self.assertIsInstance(output, tf.RaggedTensor)\n+        self.assertEqual(output.shape, (3, None))\n+        self.assertEqual(output.to_list(), [[4, 1, 3], [1, 2], [4]])\n+\n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"Requires ragged tensors.\"\n+    )\n+    def test_ragged_tensor_output_length(self):\n+        layer = layers.TextVectorization(\n+            output_mode=\"int\",\n+            vocabulary=[\"baz\", \"bar\", \"foo\"],\n+            ragged=True,\n+            output_sequence_length=2,\n+        )\n+        input_data = [[\"foo qux bar\"], [\"qux baz\"], [\"foo\"]]\n+        output = layer(input_data)\n+        self.assertIsInstance(output, tf.RaggedTensor)\n+        self.assertEqual(output.shape, (3, None))\n+        self.assertEqual(output.to_list(), [[4, 1], [1, 2], [4]])\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"tensorflow\",\n+        reason=\"Verify raises exception for non-TF backends\",\n+    )\n+    def test_raises_exception_ragged_tensor(self):\n+        with self.assertRaises(ValueError):\n+            _ = layers.TextVectorization(\n+                output_mode=\"int\",\n+                vocabulary=[\"baz\", \"bar\", \"foo\"],\n+                ragged=True,\n+            )\n\n@@ -15,6 +15,12 @@ def in_tf_graph():\n     return False\n \n \n+def convert_tf_tensor(outputs, dtype=None):\n+    if backend_module.backend() != \"tensorflow\" and not in_tf_graph():\n+        outputs = backend_module.convert_to_tensor(outputs, dtype=dtype)\n+    return outputs\n+\n+\n class TFGraphScope:\n     def __init__(self):\n         self._original_value = global_state.get_global_attribute(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8677c5190e264613551beaee182bddf4bfa25b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 598 | Lines Deleted: 6 | Files Changed: 8 | Hunks: 42 | Methods Changed: 42 | Complexity Δ (Sum/Max): 62/15 | Churn Δ: 604 | Churn Cumulative: 7107 | Contributors (this commit): 26 | Commits (past 90d): 18 | Contributors (cumulative): 67 | DMM Complexity: 1.0\n\nDIFF:\n@@ -137,6 +137,34 @@ class MultiHeadAttention(Layer):\n             )\n         self._attention_axes = attention_axes\n \n+    @property\n+    def num_heads(self):\n+        return self._num_heads\n+\n+    @property\n+    def key_dim(self):\n+        return self._key_dim\n+\n+    @property\n+    def value_dim(self):\n+        return self._value_dim\n+\n+    @property\n+    def dropout(self):\n+        return self._dropout\n+\n+    @property\n+    def use_bias(self):\n+        return self._use_bias\n+\n+    @property\n+    def output_shape(self):\n+        return self._output_shape\n+\n+    @property\n+    def attention_axes(self):\n+        return self._attention_axes\n+\n     def get_config(self):\n         base_config = super().get_config()\n         config = {\n@@ -236,6 +264,22 @@ class MultiHeadAttention(Layer):\n         self._output_dense.build(tuple(output_dense_input_shape))\n         self.built = True\n \n+    @property\n+    def query_dense(self):\n+        return self._query_dense\n+\n+    @property\n+    def key_dense(self):\n+        return self._key_dense\n+\n+    @property\n+    def value_dense(self):\n+        return self._value_dense\n+\n+    @property\n+    def output_dense(self):\n+        return self._output_dense\n+\n     def _get_common_kwargs_for_sublayer(self):\n         common_kwargs = dict(\n             kernel_regularizer=self._kernel_regularizer,\n@@ -313,6 +357,7 @@ class MultiHeadAttention(Layer):\n             )\n         )\n         self._softmax = Softmax(axis=norm_axes, dtype=self.dtype_policy)\n+        if self.dropout:\n             self._dropout_layer = Dropout(\n                 rate=self._dropout, dtype=self.dtype_policy\n             )\n@@ -375,13 +420,16 @@ class MultiHeadAttention(Layer):\n \n         # This is actually dropping out entire tokens to attend to, which might\n         # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_scores_dropout = self._dropout_layer(\n+        if self.dropout:\n+            final_attn_scores = self._dropout_layer(\n                 attention_scores, training=training\n             )\n+        else:\n+            final_attn_scores = attention_scores\n \n         # `context_layer` = [B, T, N, H]\n         attention_output = ops.einsum(\n-            self._combine_equation, attention_scores_dropout, value\n+            self._combine_equation, final_attn_scores, value\n         )\n         return attention_output, attention_scores\n \n\n@@ -1,3 +1,5 @@\n+import os\n+\n import numpy as np\n import pytest\n from absl.testing import parameterized\n@@ -6,6 +8,8 @@ from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import layers\n+from keras import models\n+from keras import saving\n from keras import testing\n \n \n@@ -287,3 +291,75 @@ class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n         self.assertIsInstance(\n             layer._key_dense.bias.constraint, constraints.NonNeg\n         )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lora(self):\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n+        value = np.array([[[1.0, 2.0], [3.0, 4.0]]])\n+        layer = layers.MultiHeadAttention(\n+            num_heads=3,\n+            key_dim=8,\n+            use_bias=False,\n+        )\n+        layer.build(query.shape, key.shape, value.shape)\n+        layer.query_dense.enable_lora(2)\n+        layer.key_dense.enable_lora(2)\n+        layer.value_dense.enable_lora(2)\n+\n+        self.assertLen(layer.trainable_variables, 7)\n+        self.assertLen(layer.non_trainable_variables, 3)\n+\n+        # Try eager call\n+        x = {\n+            \"query\": query,\n+            \"key\": key,\n+            \"value\": value,\n+        }\n+        y = np.random.random((1, 2, 2))\n+        _ = layer(**x)\n+\n+        # Try calling fit()\n+        inputs = {\n+            \"query\": layers.Input((2, 2)),\n+            \"key\": layers.Input((2, 2)),\n+            \"value\": layers.Input((2, 2)),\n+        }\n+        outputs = layer(inputs[\"query\"], inputs[\"key\"], inputs[\"value\"])\n+        model = models.Model(inputs, outputs)\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"lora_model.keras\")\n+        model.save(temp_filepath)\n+\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"lora_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+\n+        # Load the file into a fresh, non-lora model\n+        inputs = {\n+            \"query\": layers.Input((2, 2)),\n+            \"key\": layers.Input((2, 2)),\n+            \"value\": layers.Input((2, 2)),\n+        }\n+        outputs = layers.MultiHeadAttention(\n+            num_heads=3,\n+            key_dim=8,\n+            use_bias=False,\n+        )(inputs[\"query\"], inputs[\"key\"], inputs[\"value\"])\n+        new_model = models.Model(inputs, outputs)\n+\n+        new_model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try loading a normal checkpoint into a lora model\n+        new_model.save_weights(temp_filepath)\n+        model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n\n@@ -1,3 +1,5 @@\n+import numpy as np\n+\n from keras import activations\n from keras import constraints\n from keras import initializers\n@@ -44,6 +46,15 @@ class Dense(Layer):\n         kernel_constraint: Constraint function applied to\n             the `kernel` weights matrix.\n         bias_constraint: Constraint function applied to the bias vector.\n+        lora_rank: Optional integer. If set, the layer's forward pass\n+            will implement LoRA (Low-Rank Adaptation)\n+            with the provided rank. LoRA sets the layer's kernel\n+            to non-trainable and replaces it with a delta over the\n+            original kernel, obtained via multiplying two lower-rank\n+            trainable matrices. This can be useful to reduce the\n+            computation cost of fine-tuning large dense layers.\n+            You can also enable LoRA on an existing\n+            `Dense` layer by calling `layer.enable_lora(rank)`.\n \n     Input shape:\n         N-D tensor with shape: `(batch_size, ..., input_dim)`.\n@@ -68,6 +79,7 @@ class Dense(Layer):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n+        lora_rank=None,\n         **kwargs,\n     ):\n         super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n@@ -80,13 +92,14 @@ class Dense(Layer):\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.bias_constraint = constraints.get(bias_constraint)\n-\n+        self.lora_rank = lora_rank\n+        self.lora_enabled = False\n         self.input_spec = InputSpec(min_ndim=2)\n         self.supports_masking = True\n \n     def build(self, input_shape):\n         input_dim = input_shape[-1]\n-        self.kernel = self.add_weight(\n+        self._kernel = self.add_weight(\n             name=\"kernel\",\n             shape=(input_dim, self.units),\n             initializer=self.kernel_initializer,\n@@ -105,6 +118,20 @@ class Dense(Layer):\n             self.bias = None\n         self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n         self.built = True\n+        if self.lora_rank:\n+            self.enable_lora(self.lora_rank)\n+\n+    @property\n+    def kernel(self):\n+        if not self.built:\n+            raise AttributeError(\n+                \"You must build the layer before accessing `kernel`.\"\n+            )\n+        if self.lora_enabled:\n+            return self._kernel + ops.matmul(\n+                self.lora_kernel_a, self.lora_kernel_b\n+            )\n+        return self._kernel\n \n     def call(self, inputs):\n         x = ops.matmul(inputs, self.kernel)\n@@ -119,6 +146,59 @@ class Dense(Layer):\n         output_shape[-1] = self.units\n         return tuple(output_shape)\n \n+    def enable_lora(self, rank):\n+        if self.kernel_constraint:\n+            raise ValueError(\n+                \"Lora is incompatible with kernel constraints. \"\n+                \"In order to enable lora on this layer, remove the \"\n+                \"`kernel_constraint` argument.\"\n+            )\n+        if not self.built:\n+            raise ValueError(\n+                \"Cannot enable lora on a layer that isn't yet built.\"\n+            )\n+        if self.lora_enabled:\n+            raise ValueError(\n+                \"lora is already enabled. \"\n+                \"This can only be done once per layer.\"\n+            )\n+        self._tracker.locked = False\n+        self.lora_kernel_a = self.add_weight(\n+            name=\"lora_kernel_a\",\n+            shape=(self.kernel.shape[0], rank),\n+            initializer=\"zeros\",\n+            regularizer=self.kernel_regularizer,\n+        )\n+        self.lora_kernel_b = self.add_weight(\n+            name=\"lora_kernel_b\",\n+            shape=(rank, self.kernel.shape[1]),\n+            initializer=\"zeros\",\n+            regularizer=self.kernel_regularizer,\n+        )\n+        self.kernel.trainable = False\n+        self._tracker.locked = True\n+        self.lora_enabled = True\n+\n+    def save_own_variables(self, store):\n+        if not self.lora_enabled:\n+            return super().save_own_variables(store)\n+\n+        kernel_value = ops.convert_to_numpy(\n+            self.kernel + ops.matmul(self.lora_kernel_a, self.lora_kernel_b)\n+        )\n+        store[\"0\"] = kernel_value\n+        if self.use_bias:\n+            store[\"1\"] = self.bias.numpy()\n+\n+    def load_own_variables(self, store):\n+        if not self.lora_enabled:\n+            return super().load_own_variables(store)\n+        self._kernel.assign(store[\"0\"])\n+        if self.use_bias:\n+            self.bias.assign(store[\"1\"])\n+        self.lora_kernel_a.assign(np.zeros(self.lora_kernel_a.shape))\n+        self.lora_kernel_b.assign(np.zeros(self.lora_kernel_b.shape))\n+\n     def get_config(self):\n         base_config = super().get_config()\n         config = {\n@@ -136,4 +216,6 @@ class Dense(Layer):\n             \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n             \"bias_constraint\": constraints.serialize(self.bias_constraint),\n         }\n+        if self.lora_rank:\n+            config[\"lora_rank\"] = self.lora_rank\n         return {**base_config, **config}\n\n@@ -1,9 +1,13 @@\n+import os\n+\n import numpy as np\n import pytest\n \n from keras import backend\n from keras import constraints\n from keras import layers\n+from keras import models\n+from keras import saving\n from keras import testing\n from keras.backend.common import keras_tensor\n \n@@ -176,3 +180,72 @@ class DenseTest(testing.TestCase):\n         layer = layers.Dense(units=2, bias_constraint=\"non_neg\")\n         layer.build((None, 2))\n         self.assertIsInstance(layer.bias.constraint, constraints.NonNeg)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_enable_lora(self):\n+        layer = layers.Dense(units=16)\n+        layer.build((None, 8))\n+        layer.enable_lora(4)\n+        self.assertLen(layer.trainable_weights, 3)\n+        self.assertLen(layer.non_trainable_weights, 1)\n+        # Try eager call\n+        x = np.random.random((64, 8))\n+        y = np.random.random((64, 16))\n+        _ = layer(x[:2])\n+\n+        # Try calling fit()\n+        model = models.Sequential(\n+            [\n+                layer,\n+            ]\n+        )\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"lora_model.keras\")\n+        model.save(temp_filepath)\n+\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertFalse(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"lora_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+\n+        # Load the file into a fresh, non-lora model\n+        new_model = models.Sequential(\n+            [\n+                layers.Dense(units=16),\n+            ]\n+        )\n+        new_model.build((None, 8))\n+        new_model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try loading a normal checkpoint into a lora model\n+        new_model.save_weights(temp_filepath)\n+        model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lora_rank_argument(self):\n+        self.run_layer_test(\n+            layers.Dense,\n+            init_kwargs={\n+                \"units\": 5,\n+                \"activation\": \"sigmoid\",\n+                \"kernel_regularizer\": \"l2\",\n+                \"lora_rank\": 2,\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 3, 5),\n+            expected_num_trainable_weights=3,\n+            expected_num_non_trainable_weights=1,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=2,  # we have 2 regularizers.\n+            supports_masking=True,\n+        )\n\n@@ -1,5 +1,7 @@\n import re\n \n+import numpy as np\n+\n from keras import activations\n from keras import constraints\n from keras import initializers\n@@ -40,6 +42,17 @@ class EinsumDense(Layer):\n         kernel_constraint: Constraint function applied to the `kernel` weights\n             matrix.\n         bias_constraint: Constraint function applied to the bias vector.\n+        lora_rank: Optional integer. If set, the layer's forward pass\n+            will implement LoRA (Low-Rank Adaptation)\n+            with the provided rank. LoRA sets the layer's kernel\n+            to non-trainable and replaces it with a delta over the\n+            original kernel, obtained via multiplying two lower-rank\n+            trainable matrices\n+            (the factorization happens on the last dimension).\n+            This can be useful to reduce the\n+            computation cost of fine-tuning large dense layers.\n+            You can also enable LoRA on an existing\n+            `EinsumDense` layer by calling `layer.enable_lora(rank)`.\n         **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n \n     Examples:\n@@ -106,6 +119,7 @@ class EinsumDense(Layer):\n         bias_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n+        lora_rank=None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -122,6 +136,8 @@ class EinsumDense(Layer):\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.bias_constraint = constraints.get(bias_constraint)\n+        self.lora_rank = lora_rank\n+        self.lora_enabled = False\n \n     def build(self, input_shape):\n         shape_data = _analyze_einsum_string(\n@@ -132,7 +148,7 @@ class EinsumDense(Layer):\n         )\n         kernel_shape, bias_shape, full_output_shape = shape_data\n         self.full_output_shape = tuple(full_output_shape)\n-        self.kernel = self.add_weight(\n+        self._kernel = self.add_weight(\n             name=\"kernel\",\n             shape=tuple(kernel_shape),\n             initializer=self.kernel_initializer,\n@@ -155,6 +171,20 @@ class EinsumDense(Layer):\n         else:\n             self.bias = None\n         super().build(input_shape)\n+        if self.lora_rank:\n+            self.enable_lora(self.lora_rank)\n+\n+    @property\n+    def kernel(self):\n+        if not self.built:\n+            raise AttributeError(\n+                \"You must build the layer before accessing `kernel`.\"\n+            )\n+        if self.lora_enabled:\n+            return self._kernel + ops.matmul(\n+                self.lora_kernel_a, self.lora_kernel_b\n+            )\n+        return self._kernel\n \n     def compute_output_shape(self, _):\n         return self.full_output_shape\n@@ -180,6 +210,8 @@ class EinsumDense(Layer):\n             \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n             \"bias_constraint\": constraints.serialize(self.bias_constraint),\n         }\n+        if self.lora_rank:\n+            config[\"lora_rank\"] = self.lora_rank\n         return {**base_config, **config}\n \n     def call(self, inputs):\n@@ -190,6 +222,60 @@ class EinsumDense(Layer):\n             x = self.activation(x)\n         return x\n \n+    def enable_lora(self, rank):\n+        if self.kernel_constraint:\n+            raise ValueError(\n+                \"Lora is incompatible with kernel constraints. \"\n+                \"In order to enable lora on this layer, remove the \"\n+                \"`kernel_constraint` argument.\"\n+            )\n+        if not self.built:\n+            raise ValueError(\n+                \"Cannot enable lora on a layer that isn't yet built.\"\n+            )\n+        if self.lora_enabled:\n+            raise ValueError(\n+                \"lora is already enabled. \"\n+                \"This can only be done once per layer.\"\n+            )\n+\n+        self._tracker.locked = False\n+        self.lora_kernel_a = self.add_weight(\n+            name=\"lora_kernel_a\",\n+            shape=(self.kernel.shape[:-1] + (rank,)),\n+            initializer=\"zeros\",\n+            regularizer=self.kernel_regularizer,\n+        )\n+        self.lora_kernel_b = self.add_weight(\n+            name=\"lora_kernel_b\",\n+            shape=(rank, self.kernel.shape[-1]),\n+            initializer=\"zeros\",\n+            regularizer=self.kernel_regularizer,\n+        )\n+        self.kernel.trainable = False\n+        self._tracker.locked = True\n+        self.lora_enabled = True\n+\n+    def save_own_variables(self, store):\n+        if not self.lora_enabled:\n+            return super().save_own_variables(store)\n+\n+        kernel_value = ops.convert_to_numpy(\n+            self.kernel + ops.matmul(self.lora_kernel_a, self.lora_kernel_b)\n+        )\n+        store[\"0\"] = kernel_value\n+        if self.bias is not None:\n+            store[\"1\"] = self.bias.numpy()\n+\n+    def load_own_variables(self, store):\n+        if not self.lora_enabled:\n+            return super().load_own_variables(store)\n+        self._kernel.assign(store[\"0\"])\n+        if self.bias is not None:\n+            self.bias.assign(store[\"1\"])\n+        self.lora_kernel_a.assign(np.zeros(self.lora_kernel_a.shape))\n+        self.lora_kernel_b.assign(np.zeros(self.lora_kernel_b.shape))\n+\n \n def _analyze_einsum_string(equation, bias_axes, input_shape, output_shape):\n     \"\"\"Analyzes an einsum string to determine the required weight shape.\"\"\"\n\n@@ -1,8 +1,13 @@\n+import os\n+\n+import numpy as np\n import pytest\n from absl.testing import parameterized\n \n from keras import constraints\n from keras import layers\n+from keras import models\n+from keras import saving\n from keras import testing\n \n \n@@ -276,3 +281,80 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         )\n         layer.build((2, 1, 2))\n         self.assertIsInstance(layer.bias.constraint, constraints.NonNeg)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_enable_lora(self):\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 32),\n+            bias_axes=None,\n+        )\n+        layer.build((None, 3))\n+        layer.enable_lora(2)\n+        self.assertLen(layer.trainable_weights, 2)\n+        self.assertLen(layer.non_trainable_weights, 1)\n+        # Try eager call\n+        x = np.random.random((64, 3))\n+        y = np.random.random((64, 8, 32))\n+        _ = layer(x[:2])\n+\n+        # Try calling fit()\n+        model = models.Sequential(\n+            [\n+                layer,\n+            ]\n+        )\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"lora_model.keras\")\n+        model.save(temp_filepath)\n+\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertFalse(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"lora_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+\n+        # Load the file into a fresh, non-lora model\n+        new_model = models.Sequential(\n+            [\n+                layers.EinsumDense(\n+                    equation=\"ab,bcd->acd\",\n+                    output_shape=(8, 32),\n+                    bias_axes=None,\n+                ),\n+            ]\n+        )\n+        new_model.build((None, 3))\n+        new_model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try loading a normal checkpoint into a lora model\n+        new_model.save_weights(temp_filepath)\n+        model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lora_rank_argument(self):\n+        self.run_layer_test(\n+            layers.EinsumDense,\n+            init_kwargs={\n+                \"equation\": \"ab,bcd->acd\",\n+                \"output_shape\": (8, 32),\n+                \"bias_axes\": None,\n+                \"lora_rank\": 2,\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 8, 32),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=1,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n\n@@ -1,3 +1,5 @@\n+import numpy as np\n+\n from keras import constraints\n from keras import initializers\n from keras import ops\n@@ -48,6 +50,15 @@ class Embedding(Layer):\n             If mask_zero is set to True, as a consequence,\n             index 0 cannot be used in the vocabulary (input_dim should\n             equal size of vocabulary + 1).\n+        lora_rank: Optional integer. If set, the layer's forward pass\n+            will implement LoRA (Low-Rank Adaptation)\n+            with the provided rank. LoRA sets the layer's embeddings\n+            matrix to non-trainable and replaces it with a delta over the\n+            original matrix, obtained via multiplying two lower-rank\n+            trainable matrices. This can be useful to reduce the\n+            computation cost of fine-tuning large embedding layers.\n+            You can also enable LoRA on an existing\n+            `Embedding` layer by calling `layer.enable_lora(rank)`.\n \n     Input shape:\n         2D tensor with shape: `(batch_size, input_length)`.\n@@ -64,6 +75,7 @@ class Embedding(Layer):\n         embeddings_regularizer=None,\n         embeddings_constraint=None,\n         mask_zero=False,\n+        lora_rank=None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -75,9 +87,11 @@ class Embedding(Layer):\n         self.mask_zero = mask_zero\n         self.supports_masking = mask_zero\n         self.autocast = False\n+        self.lora_rank = lora_rank\n+        self.lora_enabled = False\n \n     def build(self, input_shape=None):\n-        self.embeddings = self.add_weight(\n+        self._embeddings = self.add_weight(\n             shape=(self.input_dim, self.output_dim),\n             initializer=self.embeddings_initializer,\n             name=\"embeddings\",\n@@ -86,6 +100,16 @@ class Embedding(Layer):\n             trainable=True,\n         )\n         self.built = True\n+        if self.lora_rank:\n+            self.enable_lora(self.lora_rank)\n+\n+    @property\n+    def embeddings(self):\n+        if self.lora_enabled:\n+            return self._embeddings + ops.matmul(\n+                self.lora_embeddings_a, self.lora_embeddings_b\n+            )\n+        return self._embeddings\n \n     def call(self, inputs):\n         if inputs.dtype != \"int32\" and inputs.dtype != \"int64\":\n@@ -101,6 +125,56 @@ class Embedding(Layer):\n     def compute_output_shape(self, input_shape):\n         return input_shape + (self.output_dim,)\n \n+    def enable_lora(self, rank):\n+        if self.embeddings_constraint:\n+            raise ValueError(\n+                \"Lora is incompatible with embedding constraints. \"\n+                \"In order to enable lora on this layer, remove the \"\n+                \"`embeddings_constraint` argument.\"\n+            )\n+        if not self.built:\n+            raise ValueError(\n+                \"Cannot enable lora on a layer that isn't yet built.\"\n+            )\n+        if self.lora_enabled:\n+            raise ValueError(\n+                \"lora is already enabled. \"\n+                \"This can only be done once per layer.\"\n+            )\n+        self._tracker.locked = False\n+        self.lora_embeddings_a = self.add_weight(\n+            name=\"lora_embeddings_a\",\n+            shape=(self.embeddings.shape[0], rank),\n+            initializer=\"zeros\",\n+            regularizer=self.embeddings_regularizer,\n+        )\n+        self.lora_embeddings_b = self.add_weight(\n+            name=\"lora_embeddings_b\",\n+            shape=(rank, self.embeddings.shape[1]),\n+            initializer=\"zeros\",\n+            regularizer=self.embeddings_regularizer,\n+        )\n+        self.embeddings.trainable = False\n+        self._tracker.locked = True\n+        self.lora_enabled = True\n+\n+    def save_own_variables(self, store):\n+        if not self.lora_enabled:\n+            return super().save_own_variables(store)\n+\n+        embeddings_value = ops.convert_to_numpy(\n+            self.embeddings\n+            + ops.matmul(self.lora_embeddings_a, self.lora_embeddings_b)\n+        )\n+        store[\"0\"] = embeddings_value\n+\n+    def load_own_variables(self, store):\n+        if not self.lora_enabled:\n+            return super().load_own_variables(store)\n+        self._embeddings.assign(store[\"0\"])\n+        self.lora_embeddings_a.assign(np.zeros(self.lora_embeddings_a.shape))\n+        self.lora_embeddings_b.assign(np.zeros(self.lora_embeddings_b.shape))\n+\n     def get_config(self):\n         base_config = super().get_config()\n         config = {\n@@ -120,4 +194,6 @@ class Embedding(Layer):\n             ),\n             \"mask_zero\": self.mask_zero,\n         }\n+        if self.lora_rank:\n+            config[\"lora_rank\"] = self.lora_rank\n         return {**base_config, **config}\n\n@@ -1,9 +1,13 @@\n+import os\n+\n import numpy as np\n import pytest\n \n from keras import backend\n from keras import constraints\n from keras import layers\n+from keras import models\n+from keras import saving\n from keras.testing import test_case\n \n \n@@ -100,3 +104,68 @@ class EmbeddingTest(test_case.TestCase):\n         layer = layers.Embedding(3, 2, embeddings_constraint=\"non_neg\")\n         layer.build((None, 2))\n         self.assertIsInstance(layer.embeddings.constraint, constraints.NonNeg)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_enable_lora(self):\n+        layer = layers.Embedding(10, 16)\n+        layer.build()\n+        layer.enable_lora(4)\n+        self.assertLen(layer.trainable_weights, 2)\n+        self.assertLen(layer.non_trainable_weights, 1)\n+        # Try eager call\n+        x = np.random.randint(0, 9, size=(64, 3))\n+        y = np.random.random((64, 3, 16))\n+        _ = layer(x[:2])\n+\n+        # Try calling fit()\n+        model = models.Sequential(\n+            [\n+                layer,\n+            ]\n+        )\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"lora_model.keras\")\n+        model.save(temp_filepath)\n+\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertFalse(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"lora_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+\n+        # Load the file into a fresh, non-lora model\n+        new_model = models.Sequential(\n+            [\n+                layers.Input((3,), dtype=\"int32\"),\n+                layers.Embedding(10, 16),\n+            ]\n+        )\n+        new_model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try loading a normal checkpoint into a lora model\n+        new_model.save_weights(temp_filepath)\n+        model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lora_rank_argument(self):\n+        self.run_layer_test(\n+            layers.Embedding,\n+            init_kwargs={\"input_dim\": 5, \"output_dim\": 4, \"lora_rank\": 2},\n+            input_shape=(2, 3),\n+            input_dtype=\"int32\",\n+            expected_output_shape=(2, 3, 4),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=1,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#488f33301fa579328dfbdcaa14284bd1a6a07d6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 121 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from keras.backend.common import global_state\n from keras.backend.common.name_scope import current_path\n from keras.backend.common.stateless_scope import get_stateless_scope\n from keras.backend.common.stateless_scope import in_stateless_scope\n+from keras.utils.module_utils import tensorflow as tf\n from keras.utils.naming import auto_name\n \n \n@@ -403,6 +404,11 @@ def standardize_shape(shape):\n             raise ValueError(\"Undefined shapes are not supported.\")\n         if not hasattr(shape, \"__iter__\"):\n             raise ValueError(f\"Cannot convert '{shape}' to a shape.\")\n+        if config.backend() == \"tensorflow\":\n+            if isinstance(shape, tf.TensorShape):\n+                # `tf.TensorShape` may contain `Dimension` objects.\n+                # We need to convert the items in it to either int or `None`\n+                shape = shape.as_list()\n         shape = tuple(shape)\n \n     if config.backend() == \"torch\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
