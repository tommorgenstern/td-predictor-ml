{"custom_id": "scrapy#a80ed769d9da533c518cc25f2e67a28a181942ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 24 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,5 +13,8 @@ class JsonResource(resource.Resource):\n     def render_object(self, obj, txrequest):\n         r = self.json_encoder.encode(obj) + \"\\n\"\n         txrequest.setHeader('Content-Type', 'application/json')\n+        txrequest.setHeader('Access-Control-Allow-Origin', '*')\n+        txrequest.setHeader('Access-Control-Allow-Methods', 'GET, POST, PATCH, PUT, DELETE')\n+        txrequest.setHeader('Access-Control-Allow-Headers',' X-Requested-With')\n         txrequest.setHeader('Content-Length', len(r))\n         return r\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf480015f40b3d539c0591ef6aed02e47ffae267", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 18 | Churn Cumulative: 234 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import json\n from . import default_settings\n \n \n@@ -37,6 +38,15 @@ class Settings(object):\n         else:\n             return str(value).split(',')\n \n+    def getdict(self, name, default=None):\n+        value = self.get(name)\n+        if value is None:\n+            return default or {}\n+        if isinstance(value, basestring):\n+            value = json.loads(value)\n+        if isinstance(value, dict):\n+            return value\n+        raise ValueError(\"Cannot convert value for setting '%s' to dict: '%s'\" % (name, value))\n \n class CrawlerSettings(Settings):\n \n\n@@ -21,6 +21,8 @@ class SettingsTest(unittest.TestCase):\n             'TEST_LIST1': ['one', 'two'],\n             'TEST_LIST2': 'one,two',\n             'TEST_STR': 'value',\n+            'TEST_DICT1': {'key1': 'val1', 'ke2': 3},\n+            'TEST_DICT2': '{\"key1\": \"val1\", \"ke2\": 3}',\n         })\n         assert settings.getbool('TEST_ENABLED1') is True\n         assert settings.getbool('TEST_ENABLED2') is True\n@@ -47,6 +49,12 @@ class SettingsTest(unittest.TestCase):\n         self.assertEqual(settings['TEST_STRx'], None)\n         self.assertEqual(settings.get('TEST_STRx'), None)\n         self.assertEqual(settings.get('TEST_STRx', 'default'), 'default')\n+        self.assertEqual(settings.getdict('TEST_DICT1'), {'key1': 'val1', 'ke2': 3})\n+        self.assertEqual(settings.getdict('TEST_DICT2'), {'key1': 'val1', 'ke2': 3})\n+        self.assertEqual(settings.getdict('TEST_DICT3'), {})\n+        self.assertEqual(settings.getdict('TEST_DICT3', {'key1': 5}), {'key1': 5})\n+        self.assertRaises(ValueError, settings.getdict, 'TEST_LIST1')\n+\n \n class CrawlerSettingsTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2a5c7ed4dae4d8adc404f12e92d351d738446517", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2002 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,6 +74,8 @@ class ExecutionEngine(object):\n         self.start_time = time()\n         yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n         self.running = True\n+        self._closewait = defer.Deferred()\n+        yield self._closewait\n \n     def stop(self):\n         \"\"\"Stop the execution engine gracefully\"\"\"\n@@ -284,3 +286,4 @@ class ExecutionEngine(object):\n     @defer.inlineCallbacks\n     def _finish_stopping_engine(self):\n         yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n+        self._closewait.callback(None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b4d14c8f635b28d5f72d37f924c4e71d71520ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 346 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -19,6 +19,7 @@ from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import get_func_args\n+from scrapy.utils.project import get_project_settings\n \n \n class IFeedStorage(Interface):\n@@ -81,7 +82,7 @@ class FileFeedStorage(object):\n class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri):\n-        from scrapy.conf import settings\n+        settings = get_project_settings()\n         try:\n             import boto\n         except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#21c8b89422429cf4cdf8054e1c8d398d6b1b82eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 349 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,6 @@ from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import get_func_args\n-from scrapy.utils.project import get_project_settings\n \n \n class IFeedStorage(Interface):\n@@ -82,7 +81,7 @@ class FileFeedStorage(object):\n class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri):\n-        settings = get_project_settings()\n+        from scrapy.conf import settings\n         try:\n             import boto\n         except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df19693ed28b83ba1e2bc3402c001c7babb8e212", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 37 | Churn Cumulative: 812 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n from scrapy.item import BaseItem\n from scrapy.utils import display\n+from scrapy.utils.conf import arglist_to_dict\n from scrapy.utils.spider import iterate_spider_output, create_spider_for_request\n from scrapy.exceptions import UsageError\n from scrapy import log\n@@ -27,6 +28,8 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"--spider\", dest=\"spider\", default=None, \\\n             help=\"use this spider without looking for one\")\n+        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n+            help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", \\\n             help=\"don't show links to follow (extracted requests)\")\n         parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", \\\n@@ -120,12 +123,12 @@ class Command(ScrapyCommand):\n     def set_spider(self, url, opts):\n         if opts.spider:\n             try:\n-                self.spider = self.crawler.spiders.create(opts.spider)\n+                self.spider = self.crawler.spiders.create(opts.spider, **opts.spargs)\n             except KeyError:\n                 log.msg(format='Unable to find spider: %(spider)s',\n                         level=log.ERROR, spider=opts.spider)\n         else:\n-            self.spider = create_spider_for_request(self.crawler.spiders, Request(url))\n+            self.spider = create_spider_for_request(self.crawler.spiders, Request(url), **opts.spargs)\n             if not self.spider:\n                 log.msg(format='Unable to find spider for: %(url)s',\n                         level=log.ERROR, url=url)\n@@ -183,6 +186,13 @@ class Command(ScrapyCommand):\n         request.callback = callback\n         return request\n \n+    def process_options(self, args, opts):\n+        ScrapyCommand.process_options(self, args, opts)\n+        try:\n+            opts.spargs = arglist_to_dict(opts.spargs)\n+        except ValueError:\n+            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n+\n     def run(self, args, opts):\n         # parse arguments\n         if not len(args) == 1 or not is_url(args[0]):\n\n@@ -175,3 +175,26 @@ from scrapy.spider import BaseSpider\n         log = p.stderr.read()\n         self.assert_(\"Unable to load\" in log)\n \n+\n+class ParseCommandTest(CommandTest):\n+\n+    def test_spider_arguments(self):\n+        spider_name = 'parse_spider'\n+        fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n+        with open(fname, 'w') as f:\n+            f.write(\"\"\"\n+from scrapy import log\n+from scrapy.spider import BaseSpider\n+\n+class MySpider(BaseSpider):\n+    name = '{0}'\n+\n+    def parse(self, response):\n+        if self.test_arg:\n+            self.log('It Works!')\n+        return []\n+\"\"\".format(spider_name))\n+\n+        p = self.proc('parse', '--spider', spider_name, '-a', 'test_arg=1', '-c', 'parse', 'http://scrapinghub.com')\n+        log = p.stderr.read()\n+        self.assert_(\"[parse_spider] DEBUG: It Works!\" in log, log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b39527f72dc619c45a25ff6c6b0ae29f2b519d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 47 | Churn Cumulative: 859 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 0.875\n\nDIFF:\n@@ -30,6 +30,8 @@ class Command(ScrapyCommand):\n             help=\"use this spider without looking for one\")\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set spider argument (may be repeated)\")\n+        parser.add_option(\"--pipelines\", action=\"store_true\", \\\n+            help=\"process items through pipelines\")\n         parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", \\\n             help=\"don't show links to follow (extracted requests)\")\n         parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", \\\n@@ -171,6 +173,10 @@ class Command(ScrapyCommand):\n             depth = response.meta['_depth']\n \n             items, requests = self.run_callback(response, cb)\n+            if opts.pipelines:\n+                itemproc = self.crawler.engine.scraper.itemproc\n+                for item in items:\n+                    itemproc.process_item(item, self.spider)\n             self.add_items(depth, items)\n             self.add_requests(depth, requests)\n \n\n@@ -178,23 +178,52 @@ from scrapy.spider import BaseSpider\n \n class ParseCommandTest(CommandTest):\n \n-    def test_spider_arguments(self):\n-        spider_name = 'parse_spider'\n+    def setUp(self):\n+        super(ParseCommandTest, self).setUp()\n+        self.spider_name = 'parse_spider'\n         fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n from scrapy import log\n from scrapy.spider import BaseSpider\n+from scrapy.item import Item\n \n class MySpider(BaseSpider):\n     name = '{0}'\n \n     def parse(self, response):\n-        if self.test_arg:\n+        if getattr(self, 'test_arg', None):\n             self.log('It Works!')\n-        return []\n-\"\"\".format(spider_name))\n+        return [Item()]\n+\"\"\".format(self.spider_name))\n \n-        p = self.proc('parse', '--spider', spider_name, '-a', 'test_arg=1', '-c', 'parse', 'http://scrapinghub.com')\n+        fname = abspath(join(self.proj_mod_path, 'pipelines.py'))\n+        with open(fname, 'w') as f:\n+            f.write(\"\"\"\n+from scrapy import log\n+\n+class MyPipeline(object):\n+    component_name = 'my_pipeline'\n+\n+    def process_item(self, item, spider):\n+        log.msg('It Works!')\n+        return item\n+\"\"\")\n+\n+        fname = abspath(join(self.proj_mod_path, 'settings.py'))\n+        with open(fname, 'a') as f:\n+            f.write(\"\"\"\n+ITEM_PIPELINES = ['{0}.pipelines.MyPipeline']\n+\"\"\".format(self.project_name))\n+\n+    def test_spider_arguments(self):\n+        p = self.proc('parse', '--spider', self.spider_name, '-a', 'test_arg=1',\n+                '-c', 'parse', 'http://scrapinghub.com')\n         log = p.stderr.read()\n         self.assert_(\"[parse_spider] DEBUG: It Works!\" in log, log)\n+\n+    def test_pipelines(self):\n+        p = self.proc('parse', '--spider', self.spider_name, '--pipelines',\n+                '-c', 'parse', 'http://scrapinghub.com')\n+        log = p.stderr.read()\n+        print log\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce177fa5bcd311b35f29faf7e50a65281a174975", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 62 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -33,7 +33,7 @@ class CrawlTestCase(TestCase):\n \n     def setUp(self):\n         self.proc = Popen([sys.executable, '-m', 'scrapy.tests.mockserver'])\n-        time.sleep(0.2)\n+        time.sleep(0.5)\n \n     def tearDown(self):\n         self.proc.kill()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0c88f2d93b9f9c313aca06594d8d68ea9c9a34c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 411 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -226,4 +226,4 @@ ITEM_PIPELINES = ['{0}.pipelines.MyPipeline']\n         p = self.proc('parse', '--spider', self.spider_name, '--pipelines',\n                 '-c', 'parse', 'http://scrapinghub.com')\n         log = p.stderr.read()\n-        print log\n+        self.assert_(\"[scrapy] INFO: It Works!\" in log, log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#626331b8650a3b12d596630f07974870bdfb6d06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 158 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -77,7 +77,12 @@ class Root(Resource):\n         return 'Scrapy mock HTTP server\\n'\n \n \n+if __name__ == \"__main__\":\n     root = Root()\n     factory = Site(root)\n-reactor.listenTCP(8998, factory)\n+    port = reactor.listenTCP(8998, factory, interface=\"127.0.0.1\")\n+    def print_listening():\n+        h = port.getHost()\n+        print \"Mock server running at http://%s:%d\" % (h.host, h.port)\n+    reactor.callWhenRunning(print_listening)\n     reactor.run()\n\n@@ -1,7 +1,7 @@\n import sys, time\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase, SkipTest\n-from subprocess import Popen\n+from subprocess import Popen, PIPE\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n@@ -32,8 +32,8 @@ def docrawl(spider, settings=None):\n class CrawlTestCase(TestCase):\n \n     def setUp(self):\n-        self.proc = Popen([sys.executable, '-m', 'scrapy.tests.mockserver'])\n-        time.sleep(0.5)\n+        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'], stdout=PIPE)\n+        self.proc.stdout.readline()\n \n     def tearDown(self):\n         self.proc.kill()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6df274bba535468e9eb2892687df9847b7f69d42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 13 | Churn Cumulative: 342 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ Scrapy Item\n See documentation in docs/topics/item.rst\n \"\"\"\n \n+from copy import deepcopy\n from pprint import pformat\n from UserDict import DictMixin\n \n@@ -76,6 +77,9 @@ class DictItem(DictMixin, BaseItem):\n     def __repr__(self):\n         return pformat(dict(self))\n \n+    def copy(self):\n+        return self.__class__(self)\n+\n \n class Item(DictItem):\n \n\n@@ -133,6 +133,15 @@ class ItemTest(unittest.TestCase):\n         i['name'] = u'John'\n         self.assertEqual(dict(i), {'name': u'John'})\n \n+    def test_copy(self):\n+        class TestItem(Item):\n+            name = Field()\n+        item = TestItem({'name':'lower'})\n+        copied_item = item.copy()\n+        self.assertNotEqual(id(item), id(copied_item))\n+        copied_item['name'] = copied_item['name'].upper()\n+        self.assertNotEqual(item['name'], copied_item['name'])\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc592e9958eaf374547d7d51d6e990eb86bd0253", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 159 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,6 @@ Scrapy Item\n See documentation in docs/topics/item.rst\n \"\"\"\n \n-from copy import deepcopy\n from pprint import pformat\n from UserDict import DictMixin\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d02da2f31faff3f507ab5374370a8c6e531daa02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 639 | Files Changed: 8 | Hunks: 8 | Methods Changed: 41 | Complexity Δ (Sum/Max): -106/0 | Churn Δ: 643 | Churn Cumulative: 2230 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 15 | DMM Complexity: 0.024017467248908297\n\nDIFF:\n@@ -2,7 +2,7 @@ import os\n import json\n from os.path import join, exists\n \n-from scrapy.utils.pqueue import PriorityQueue\n+from queuelib import PriorityQueue\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n from scrapy.utils.misc import load_object\n from scrapy.utils.job import job_dir\n\n@@ -4,7 +4,7 @@ Scheduler queues\n \n import marshal, cPickle as pickle\n \n-from scrapy.utils import queue\n+from queuelib import queue\n \n def _serializable_queue(queue_class, serialize, deserialize):\n \n\n@@ -1,4 +1,4 @@\n-from scrapy.tests import test_utils_queue as t\n+from queuelib.tests import test_queue as t\n from scrapy.squeue import MarshalFifoDiskQueue, MarshalLifoDiskQueue, PickleFifoDiskQueue, PickleLifoDiskQueue\n from scrapy.item import Item, Field\n from scrapy.http import Request\n\n@@ -1,183 +0,0 @@\n-from twisted.trial import unittest\n-\n-from scrapy.utils.pqueue import PriorityQueue\n-from scrapy.utils.queue import FifoMemoryQueue, LifoMemoryQueue, FifoDiskQueue, LifoDiskQueue\n-\n-\n-def track_closed(cls):\n-    \"\"\"Wraps a queue class to track down if close() method was called\"\"\"\n-\n-    class TrackingClosed(cls):\n-\n-        def __init__(self, *a, **kw):\n-            super(TrackingClosed, self).__init__(*a, **kw)\n-            self.closed = False\n-\n-        def close(self):\n-            super(TrackingClosed, self).close()\n-            self.closed = True\n-\n-    return TrackingClosed\n-\n-\n-class FifoMemoryPriorityQueueTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        self.q = PriorityQueue(self.qfactory)\n-\n-    def qfactory(self, prio):\n-        return track_closed(FifoMemoryQueue)()\n-\n-    def test_push_pop_noprio(self):\n-        self.q.push('a')\n-        self.q.push('b')\n-        self.q.push('c')\n-        self.assertEqual(self.q.pop(), 'a')\n-        self.assertEqual(self.q.pop(), 'b')\n-        self.assertEqual(self.q.pop(), 'c')\n-        self.assertEqual(self.q.pop(), None)\n-\n-    def test_push_pop_prio(self):\n-        self.q.push('a', 3)\n-        self.q.push('b', 1)\n-        self.q.push('c', 2)\n-        self.q.push('d', 1)\n-        self.assertEqual(self.q.pop(), 'b')\n-        self.assertEqual(self.q.pop(), 'd')\n-        self.assertEqual(self.q.pop(), 'c')\n-        self.assertEqual(self.q.pop(), 'a')\n-        self.assertEqual(self.q.pop(), None)\n-\n-    def test_len_nonzero(self):\n-        assert not self.q\n-        self.assertEqual(len(self.q), 0)\n-        self.q.push('a', 3)\n-        assert self.q\n-        self.q.push('b', 1)\n-        self.q.push('c', 2)\n-        self.q.push('d', 1)\n-        self.assertEqual(len(self.q), 4)\n-        self.q.pop()\n-        self.q.pop()\n-        self.q.pop()\n-        self.q.pop()\n-        assert not self.q\n-        self.assertEqual(len(self.q), 0)\n-\n-    def test_close(self):\n-        self.q.push('a', 3)\n-        self.q.push('b', 1)\n-        self.q.push('c', 2)\n-        self.q.push('d', 1)\n-        iqueues = self.q.queues.values()\n-        self.assertEqual(sorted(self.q.close()), [1, 2, 3])\n-        assert all(q.closed for q in iqueues)\n-\n-    def test_close_return_active(self):\n-        self.q.push('b', 1)\n-        self.q.push('c', 2)\n-        self.q.push('a', 3)\n-        self.q.pop()\n-        self.assertEqual(sorted(self.q.close()), [2, 3])\n-\n-    def test_popped_internal_queues_closed(self):\n-        self.q.push('a', 3)\n-        self.q.push('b', 1)\n-        self.q.push('c', 2)\n-        p1queue = self.q.queues[1]\n-        self.assertEqual(self.q.pop(), 'b')\n-        self.q.close()\n-        assert p1queue.closed\n-\n-\n-class LifoMemoryPriorityQueueTest(FifoMemoryPriorityQueueTest):\n-\n-    def qfactory(self, prio):\n-        return track_closed(LifoMemoryQueue)()\n-\n-    def test_push_pop_noprio(self):\n-        self.q.push('a')\n-        self.q.push('b')\n-        self.q.push('c')\n-        self.assertEqual(self.q.pop(), 'c')\n-        self.assertEqual(self.q.pop(), 'b')\n-        self.assertEqual(self.q.pop(), 'a')\n-        self.assertEqual(self.q.pop(), None)\n-\n-    def test_push_pop_prio(self):\n-        self.q.push('a', 3)\n-        self.q.push('b', 1)\n-        self.q.push('c', 2)\n-        self.q.push('d', 1)\n-        self.assertEqual(self.q.pop(), 'd')\n-        self.assertEqual(self.q.pop(), 'b')\n-        self.assertEqual(self.q.pop(), 'c')\n-        self.assertEqual(self.q.pop(), 'a')\n-        self.assertEqual(self.q.pop(), None)\n-\n-\n-class FifoDiskPriorityQueueTest(FifoMemoryPriorityQueueTest):\n-\n-    def setUp(self):\n-        self.q = PriorityQueue(self.qfactory)\n-\n-    def qfactory(self, prio):\n-        return track_closed(FifoDiskQueue)(self.mktemp())\n-\n-    def test_nonserializable_object_one(self):\n-        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n-        self.assertEqual(self.q.close(), [])\n-\n-    def test_nonserializable_object_many_close(self):\n-        self.q.push('a', 3)\n-        self.q.push('b', 1)\n-        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n-        self.q.push('c', 2)\n-        self.assertEqual(self.q.pop(), 'b')\n-        self.assertEqual(sorted(self.q.close()), [2, 3])\n-\n-    def test_nonserializable_object_many_pop(self):\n-        self.q.push('a', 3)\n-        self.q.push('b', 1)\n-        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n-        self.q.push('c', 2)\n-        self.assertEqual(self.q.pop(), 'b')\n-        self.assertEqual(self.q.pop(), 'c')\n-        self.assertEqual(self.q.pop(), 'a')\n-        self.assertEqual(self.q.pop(), None)\n-        self.assertEqual(self.q.close(), [])\n-\n-\n-class FifoDiskPriorityQueueTest(FifoMemoryPriorityQueueTest):\n-\n-    def qfactory(self, prio):\n-        return track_closed(FifoDiskQueue)(self.mktemp())\n-\n-    def test_nonserializable_object_one(self):\n-        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n-        self.assertEqual(self.q.close(), [])\n-\n-    def test_nonserializable_object_many_close(self):\n-        self.q.push('a', 3)\n-        self.q.push('b', 1)\n-        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n-        self.q.push('c', 2)\n-        self.assertEqual(self.q.pop(), 'b')\n-        self.assertEqual(sorted(self.q.close()), [2, 3])\n-\n-    def test_nonserializable_object_many_pop(self):\n-        self.q.push('a', 3)\n-        self.q.push('b', 1)\n-        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n-        self.q.push('c', 2)\n-        self.assertEqual(self.q.pop(), 'b')\n-        self.assertEqual(self.q.pop(), 'c')\n-        self.assertEqual(self.q.pop(), 'a')\n-        self.assertEqual(self.q.pop(), None)\n-        self.assertEqual(self.q.close(), [])\n-\n-\n-class LifoDiskPriorityQueueTest(LifoMemoryPriorityQueueTest):\n-\n-    def qfactory(self, prio):\n-        return track_closed(LifoDiskQueue)(self.mktemp())\n\n@@ -1,224 +0,0 @@\n-import os, glob\n-from twisted.trial import unittest\n-\n-from scrapy.utils.queue import FifoMemoryQueue, LifoMemoryQueue, FifoDiskQueue, LifoDiskQueue\n-\n-class FifoMemoryQueueTest(unittest.TestCase):\n-\n-    def queue(self):\n-        return FifoMemoryQueue()\n-\n-    def test_empty(self):\n-        \"\"\"Empty queue test\"\"\"\n-        q = self.queue()\n-        assert q.pop() is None\n-\n-    def test_push_pop1(self):\n-        \"\"\"Basic push/pop test\"\"\"\n-        q = self.queue()\n-        q.push('a')\n-        q.push('b')\n-        q.push('c')\n-        self.assertEqual(q.pop(), 'a')\n-        self.assertEqual(q.pop(), 'b')\n-        self.assertEqual(q.pop(), 'c')\n-        self.assertEqual(q.pop(), None)\n-\n-    def test_push_pop2(self):\n-        \"\"\"Test interleaved push and pops\"\"\"\n-        q = self.queue()\n-        q.push('a')\n-        q.push('b')\n-        q.push('c')\n-        q.push('d')\n-        self.assertEqual(q.pop(), 'a')\n-        self.assertEqual(q.pop(), 'b')\n-        q.push('e')\n-        self.assertEqual(q.pop(), 'c')\n-        self.assertEqual(q.pop(), 'd')\n-        self.assertEqual(q.pop(), 'e')\n-\n-    def test_len(self):\n-        q = self.queue()\n-        self.assertEqual(len(q), 0)\n-        q.push('a')\n-        self.assertEqual(len(q), 1)\n-        q.push('b')\n-        q.push('c')\n-        self.assertEqual(len(q), 3)\n-        q.pop()\n-        q.pop()\n-        q.pop()\n-        self.assertEqual(len(q), 0)\n-\n-\n-class LifoMemoryQueueTest(unittest.TestCase):\n-\n-    def queue(self):\n-        return LifoMemoryQueue()\n-\n-    def test_empty(self):\n-        \"\"\"Empty queue test\"\"\"\n-        q = self.queue()\n-        assert q.pop() is None\n-\n-    def test_push_pop1(self):\n-        \"\"\"Basic push/pop test\"\"\"\n-        q = self.queue()\n-        q.push('a')\n-        q.push('b')\n-        q.push('c')\n-        self.assertEqual(q.pop(), 'c')\n-        self.assertEqual(q.pop(), 'b')\n-        self.assertEqual(q.pop(), 'a')\n-        self.assertEqual(q.pop(), None)\n-\n-    def test_push_pop2(self):\n-        \"\"\"Test interleaved push and pops\"\"\"\n-        q = self.queue()\n-        q.push('a')\n-        q.push('b')\n-        q.push('c')\n-        q.push('d')\n-        self.assertEqual(q.pop(), 'd')\n-        self.assertEqual(q.pop(), 'c')\n-        q.push('e')\n-        self.assertEqual(q.pop(), 'e')\n-        self.assertEqual(q.pop(), 'b')\n-        self.assertEqual(q.pop(), 'a')\n-\n-    def test_len(self):\n-        q = self.queue()\n-        self.assertEqual(len(q), 0)\n-        q.push('a')\n-        self.assertEqual(len(q), 1)\n-        q.push('b')\n-        q.push('c')\n-        self.assertEqual(len(q), 3)\n-        q.pop()\n-        q.pop()\n-        q.pop()\n-        self.assertEqual(len(q), 0)\n-\n-\n-class FifoDiskQueueTest(FifoMemoryQueueTest):\n-\n-    chunksize = 100000\n-\n-    def setUp(self):\n-        self.qdir = self.mktemp()\n-\n-    def queue(self):\n-        return FifoDiskQueue(self.qdir, chunksize=self.chunksize)\n-\n-    def test_close_open(self):\n-        \"\"\"Test closing and re-opening keeps state\"\"\"\n-        q = self.queue()\n-        q.push('a')\n-        q.push('b')\n-        q.push('c')\n-        q.push('d')\n-        self.assertEqual(q.pop(), 'a')\n-        self.assertEqual(q.pop(), 'b')\n-        q.close()\n-        del q\n-        q = self.queue()\n-        self.assertEqual(len(q), 2)\n-        q.push('e')\n-        self.assertEqual(q.pop(), 'c')\n-        self.assertEqual(q.pop(), 'd')\n-        q.close()\n-        del q\n-        q = self.queue()\n-        self.assertEqual(q.pop(), 'e')\n-        self.assertEqual(len(q), 0)\n-\n-    def test_chunks(self):\n-        \"\"\"Test chunks are created and removed\"\"\"\n-        q = self.queue()\n-        for x in range(5):\n-            q.push(str(x))\n-        chunks = glob.glob(os.path.join(self.qdir, 'q*'))\n-        self.assertEqual(len(chunks), 5/self.chunksize + 1)\n-        for x in range(5):\n-            q.pop()\n-        chunks = glob.glob(os.path.join(self.qdir, 'q*'))\n-        self.assertEqual(len(chunks), 1)\n-\n-    def test_cleanup(self):\n-        \"\"\"Test queue dir is removed if queue is empty\"\"\"\n-        q = self.queue()\n-        assert os.path.exists(self.qdir)\n-        for x in range(5):\n-            q.push(str(x))\n-        for x in range(5):\n-            q.pop()\n-        q.close()\n-        assert not os.path.exists(self.qdir)\n-\n-\n-class ChunkSize1FifoDiskQueueTest(FifoDiskQueueTest):\n-    chunksize = 1\n-\n-class ChunkSize2FifoDiskQueueTest(FifoDiskQueueTest):\n-    chunksize = 2\n-\n-class ChunkSize3FifoDiskQueueTest(FifoDiskQueueTest):\n-    chunksize = 3\n-\n-class ChunkSize4FifoDiskQueueTest(FifoDiskQueueTest):\n-    chunksize = 4\n-\n-\n-class LifoDiskQueueTest(LifoMemoryQueueTest):\n-\n-    def setUp(self):\n-        self.path = self.mktemp()\n-\n-    def queue(self):\n-        return LifoDiskQueue(self.path)\n-\n-    def test_close_open(self):\n-        \"\"\"Test closing and re-opening keeps state\"\"\"\n-        q = self.queue()\n-        q.push('a')\n-        q.push('b')\n-        q.push('c')\n-        q.push('d')\n-        self.assertEqual(q.pop(), 'd')\n-        self.assertEqual(q.pop(), 'c')\n-        q.close()\n-        del q\n-        q = self.queue()\n-        self.assertEqual(len(q), 2)\n-        q.push('e')\n-        self.assertEqual(q.pop(), 'e')\n-        self.assertEqual(q.pop(), 'b')\n-        q.close()\n-        del q\n-        q = self.queue()\n-        self.assertEqual(q.pop(), 'a')\n-        self.assertEqual(len(q), 0)\n-\n-    def test_cleanup(self):\n-        \"\"\"Test queue file is removed if queue is empty\"\"\"\n-        q = self.queue()\n-        assert os.path.exists(self.path)\n-        for x in range(5):\n-            q.push(str(x))\n-        for x in range(5):\n-            q.pop()\n-        q.close()\n-        assert not os.path.exists(self.path)\n-\n-    def test_file_size_shrinks(self):\n-        \"\"\"Test size of queue file shrinks when popping items\"\"\"\n-        q = self.queue()\n-        q.push('a')\n-        q.push('b')\n-        q.close()\n-        size = os.path.getsize(self.path)\n-        q = self.queue()\n-        q.pop()\n-        q.close()\n-        assert os.path.getsize(self.path), size\n\n@@ -1,55 +0,0 @@\n-class PriorityQueue(object):\n-    \"\"\"A priority queue implemented using multiple internal queues (typically,\n-    FIFO queues). The internal queue must implement the following methods:\n-\n-        * push(obj)\n-        * pop()\n-        * close()\n-        * __len__()\n-\n-    The constructor receives a qfactory argument, which is a callable used to\n-    instantiate a new (internal) queue when a new priority is allocated. The\n-    qfactory function is called with the priority number as first and only\n-    argument.\n-\n-    Only integer priorities should be used. Lower numbers are higher\n-    priorities.\n-    \"\"\"\n-\n-    def __init__(self, qfactory, startprios=()):\n-        self.queues = {}\n-        self.qfactory = qfactory\n-        for p in startprios:\n-            self.queues[p] = self.qfactory(p)\n-        self.curprio = min(startprios) if startprios else None\n-\n-    def push(self, obj, priority=0):\n-        if priority not in self.queues:\n-            self.queues[priority] = self.qfactory(priority)\n-        q = self.queues[priority]\n-        q.push(obj) # this may fail (eg. serialization error)\n-        if priority < self.curprio or self.curprio is None:\n-            self.curprio = priority\n-\n-    def pop(self):\n-        if self.curprio is None:\n-            return\n-        q = self.queues[self.curprio]\n-        m = q.pop()\n-        if len(q) == 0:\n-            del self.queues[self.curprio]\n-            q.close()\n-            prios = [p for p, q in self.queues.items() if len(q) > 0]\n-            self.curprio = min(prios) if prios else None\n-        return m\n-\n-    def close(self):\n-        active = []\n-        for p, q in self.queues.items():\n-            if len(q):\n-                active.append(p)\n-            q.close()\n-        return active\n-\n-    def __len__(self):\n-        return sum(len(x) for x in self.queues.values()) if self.queues else 0\n\n@@ -1,173 +0,0 @@\n-import os\n-import struct\n-import glob\n-import json\n-from collections import deque\n-\n-\n-class FifoMemoryQueue(object):\n-    \"\"\"Memory FIFO queue.\"\"\"\n-\n-    def __init__(self):\n-        self.q = deque()\n-        self.push = self.q.append\n-\n-    def pop(self):\n-        q = self.q\n-        return q.popleft() if q else None\n-\n-    def close(self):\n-        pass\n-\n-    def __len__(self):\n-        return len(self.q)\n-\n-\n-class LifoMemoryQueue(FifoMemoryQueue):\n-    \"\"\"Memory LIFO queue.\"\"\"\n-\n-    def pop(self):\n-        q = self.q\n-        return q.pop() if q else None\n-\n-\n-class FifoDiskQueue(object):\n-    \"\"\"Persistent FIFO queue.\"\"\"\n-\n-    szhdr_format = \">L\"\n-    szhdr_size = struct.calcsize(szhdr_format)\n-\n-    def __init__(self, path, chunksize=100000):\n-        self.path = path\n-        if not os.path.exists(path):\n-            os.makedirs(path)\n-        self.info = self._loadinfo(chunksize)\n-        self.chunksize = self.info['chunksize']\n-        self.headf = self._openchunk(self.info['head'][0], 'ab+')\n-        self.tailf = self._openchunk(self.info['tail'][0])\n-        os.lseek(self.tailf.fileno(), self.info['tail'][2], os.SEEK_SET)\n-\n-    def push(self, string):\n-        hnum, hpos = self.info['head']\n-        hpos += 1\n-        szhdr = struct.pack(self.szhdr_format, len(string))\n-        os.write(self.headf.fileno(), szhdr + string)\n-        if hpos == self.chunksize:\n-            hpos = 0\n-            hnum += 1\n-            self.headf.close()\n-            self.headf = self._openchunk(hnum, 'ab+')\n-        self.info['size'] += 1\n-        self.info['head'] = hnum, hpos\n-\n-    def _openchunk(self, number, mode='r'):\n-        return open(os.path.join(self.path, 'q%05d' % number), mode)\n-\n-    def pop(self):\n-        tnum, tcnt, toffset = self.info['tail']\n-        if [tnum, tcnt] >= self.info['head']:\n-            return\n-        tfd = self.tailf.fileno()\n-        szhdr = os.read(tfd, self.szhdr_size)\n-        if not szhdr:\n-            return\n-        size, = struct.unpack(self.szhdr_format, szhdr)\n-        data = os.read(tfd, size)\n-        tcnt += 1\n-        toffset += self.szhdr_size + size\n-        if tcnt == self.chunksize and tnum <= self.info['head'][0]:\n-            tcnt = toffset = 0\n-            tnum += 1\n-            self.tailf.close()\n-            os.remove(self.tailf.name)\n-            self.tailf = self._openchunk(tnum)\n-        self.info['size'] -= 1\n-        self.info['tail'] = tnum, tcnt, toffset\n-        return data\n-\n-    def close(self):\n-        self.headf.close()\n-        self.tailf.close()\n-        self._saveinfo(self.info)\n-        if len(self) == 0:\n-            self._cleanup()\n-\n-    def __len__(self):\n-        return self.info['size']\n-\n-    def _loadinfo(self, chunksize):\n-        infopath = self._infopath()\n-        if os.path.exists(infopath):\n-            with open(infopath) as f:\n-                info = json.load(f)\n-        else:\n-            info = {\n-                'chunksize': chunksize,\n-                'size': 0,\n-                'tail': [0, 0, 0],\n-                'head': [0, 0],\n-            }\n-        return info\n-\n-    def _saveinfo(self, info):\n-        with open(self._infopath(), 'w') as f:\n-            json.dump(info, f)\n-\n-    def _infopath(self):\n-        return os.path.join(self.path, 'info.json')\n-\n-    def _cleanup(self):\n-        for x in glob.glob(os.path.join(self.path, 'q*')):\n-            os.remove(x)\n-        os.remove(os.path.join(self.path, 'info.json'))\n-        if not os.listdir(self.path):\n-            os.rmdir(self.path)\n-\n-\n-\n-class LifoDiskQueue(object):\n-    \"\"\"Persistent LIFO queue.\"\"\"\n-\n-    SIZE_FORMAT = \">L\"\n-    SIZE_SIZE = struct.calcsize(SIZE_FORMAT)\n-\n-    def __init__(self, path):\n-        self.path = path\n-        if os.path.exists(path):\n-            self.f = open(path, 'rb+')\n-            qsize = self.f.read(self.SIZE_SIZE)\n-            self.size, = struct.unpack(self.SIZE_FORMAT, qsize)\n-            self.f.seek(0, os.SEEK_END)\n-        else:\n-            self.f = open(path, 'wb+')\n-            self.f.write(struct.pack(self.SIZE_FORMAT, 0))\n-            self.size = 0\n-\n-    def push(self, string):\n-        self.f.write(string)\n-        ssize = struct.pack(self.SIZE_FORMAT, len(string))\n-        self.f.write(ssize)\n-        self.size += 1\n-\n-    def pop(self):\n-        if not self.size:\n-            return\n-        self.f.seek(-self.SIZE_SIZE, os.SEEK_END)\n-        size, = struct.unpack(self.SIZE_FORMAT, self.f.read())\n-        self.f.seek(-size-self.SIZE_SIZE, os.SEEK_END)\n-        data = self.f.read(size)\n-        self.f.seek(-size, os.SEEK_CUR)\n-        self.f.truncate()\n-        self.size -= 1\n-        return data\n-\n-    def close(self):\n-        if self.size:\n-            self.f.seek(0)\n-            self.f.write(struct.pack(self.SIZE_FORMAT, self.size))\n-        self.f.close()\n-        if not self.size:\n-            os.remove(self.path)\n-\n-    def __len__(self):\n-        return self.size\n\n@@ -122,6 +122,6 @@ try:\n except ImportError:\n     from distutils.core import setup\n else:\n-    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.2', 'lxml', 'pyOpenSSL']\n+    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL']\n \n setup(**setup_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31bec08a39db8ca8232a34b123d4d2f52c4b68a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 11 | Churn Cumulative: 84 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,8 @@\n import time\n from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE\n-\n from scrapy.utils.httpobj import urlparse_cached\n \n+\n class CookieJar(object):\n     def __init__(self, policy=None, check_expired_frequency=10000):\n         self.policy = policy or DefaultCookiePolicy()\n@@ -22,7 +22,6 @@ class CookieJar(object):\n \n         # the cookiejar implementation iterates through all domains\n         # instead we restrict to potential matches on the domain\n-        \n         req_host = urlparse_cached(request).netloc\n         if not IPV4_RE.search(req_host):\n             hosts = potential_domain_matches(req_host)\n@@ -41,14 +40,6 @@ class CookieJar(object):\n             if not wreq.has_header(\"Cookie\"):\n                 wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n \n-        # if necessary, advertise that we know RFC 2965\n-        if (self.policy.rfc2965 and not self.policy.hide_cookie2 and\n-            not request.has_header(\"Cookie2\")):\n-            for cookie in cookies:\n-                if cookie.version != 1:\n-                    request.add_unredirected_header(\"Cookie2\", '$Version=\"1\"')\n-                    break\n-        \n         self.processed += 1\n         if self.processed % self.check_expired_frequency == 0:\n             # This is still quite inefficient for large number of cookies\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32b781a1b82dcc21914f92db480e61b119a3cb1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 86 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -92,7 +92,7 @@ def potential_domain_matches(domain):\n             start = domain.index('.', start) + 1\n     except ValueError:\n         pass\n-    return matches\n+    return matches + ['.' + d for d in matches]\n \n class _DummyLock(object):\n     def acquire(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ba2b60a4b9f991e8a99a8dfcefaee0fe452a75f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 779 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -288,9 +288,9 @@ def parse_cachecontrol(header):\n \n     http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n \n-    >>> cachecontrol_directives('public, max-age=3600')\n+    >>> parse_cachecontrol('public, max-age=3600')\n     {'public': None, 'max-age': '3600'}\n-    >>> cachecontrol_directives('')\n+    >>> parse_cachecontrol('')\n     {}\n \n     \"\"\"\n\n@@ -80,7 +80,7 @@ def potential_domain_matches(domain):\n     \"\"\"Potential domain matches for a cookie\n \n     >>> potential_domain_matches('www.example.com')\n-    ['www.example.com', 'example.com']\n+    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n \n     \"\"\"\n     matches = [domain]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36ee36000e2d65d33a6b6732b58dd2f1109c8780", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 92 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -80,7 +80,7 @@ class Root(Resource):\n if __name__ == \"__main__\":\n     root = Root()\n     factory = Site(root)\n-    port = reactor.listenTCP(8998, factory, interface=\"127.0.0.1\")\n+    port = reactor.listenTCP(8998, factory)\n     def print_listening():\n         h = port.getHost()\n         print \"Mock server running at http://%s:%d\" % (h.host, h.port)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af5c13fa1453791b3fec2283dca872ba9de2ed7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 41 | Churn Cumulative: 2804 | Contributors (this commit): 9 | Commits (past 90d): 10 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ import warnings\n from time import time\n from collections import deque\n \n-from twisted.internet import reactor, defer\n+from twisted.internet import reactor, defer, task\n \n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.httpobj import urlparse_cached\n@@ -35,6 +35,10 @@ class Slot(object):\n             return random.uniform(0.5 * self.delay, 1.5 * self.delay)\n         return self.delay\n \n+    def close(self):\n+        if self.latercall and self.latercall.active():\n+            self.latercall.cancel()\n+\n \n def _get_concurrency_delay(concurrency, spider, settings):\n     delay = settings.getfloat('DOWNLOAD_DELAY')\n@@ -71,6 +75,8 @@ class Downloader(object):\n         self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n         self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n+        self._slot_gc_loop = task.LoopingCall(self._slot_gc)\n+        self._slot_gc_loop.start(60)\n \n     def fetch(self, request, spider):\n         def _deactivate(response):\n@@ -172,3 +178,14 @@ class Downloader(object):\n \n     def is_idle(self):\n         return not self.slots\n+\n+    def close(self):\n+        self._slot_gc_loop.stop()\n+        for slot in self.slots.itervalues():\n+            slot.close()\n+\n+    def _slot_gc(self, age=60):\n+        mintime = time() - age\n+        for key, slot in self.slots.items():\n+            if not slot.active and slot.lastseen + slot.delay < mintime:\n+                self.slots.pop(key).close()\n\n@@ -254,6 +254,9 @@ class ExecutionEngine(object):\n \n         dfd = slot.close()\n \n+        dfd.addBoth(lambda _: self.downloader.close())\n+        dfd.addErrback(log.err, spider=spider)\n+\n         dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n         dfd.addErrback(log.err, spider=spider)\n \n\n@@ -1,6 +1,6 @@\n import sys, time\n from twisted.internet import defer\n-from twisted.trial.unittest import TestCase, SkipTest\n+from twisted.trial.unittest import TestCase\n from subprocess import Popen, PIPE\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n@@ -10,12 +10,13 @@ from scrapy.utils.test import get_crawler\n class FollowAllSpider(BaseSpider):\n \n     name = 'follow'\n-    start_urls = [\"http://localhost:8998/follow?total=10&show=5&order=rand\"]\n     link_extractor = SgmlLinkExtractor()\n \n-    def __init__(self):\n+    def __init__(self, total=10, show=20, order=\"rand\"):\n         self.urls_visited = []\n         self.times = []\n+        url = \"http://localhost:8998/follow?total=%s&show=%s&order=%s\" % (total, show, order)\n+        self.start_urls = [url]\n \n     def parse(self, response):\n         self.urls_visited.append(response.url)\n@@ -48,13 +49,9 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_delay(self):\n-        # FIXME: this test fails because Scrapy leaves the reactor dirty with\n-        # callLater calls when download delays are used. This test should be\n-        # enabled after this bug is fixed.\n-        raise SkipTest(\"disabled due to a reactor leak in the scrapy downloader\")\n-\n         spider = FollowAllSpider()\n-        yield docrawl(spider)\n+        yield docrawl(spider, {\"DOWNLOAD_DELAY\": 0.3})\n         t = spider.times[0]\n-        for y in spider.times[1:]:\n-            self.assertTrue(y-t > 0.5, \"download delay too small: %s\" % (y-t))\n+        for t2 in spider.times[1:]:\n+            self.assertTrue(t2-t > 0.15, \"download delay too small: %s\" % (t2-t))\n+            t = t2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
