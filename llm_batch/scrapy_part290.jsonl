{"custom_id": "scrapy#b1ddd7bd7b84d8d8417228aa7392d418463c9728", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 118 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -47,12 +47,7 @@ def nonserializable_object_test(self):\n     self.assertRaises(ValueError, q.push, sel)\n \n \n-class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n-\n-    chunksize = 100000\n-\n-    def queue(self):\n-        return MarshalFifoDiskQueue(self.qpath, chunksize=self.chunksize)\n+class FifoDiskQueueTestMixin:\n \n     def test_serialize(self):\n         q = self.queue()\n@@ -66,6 +61,13 @@ class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n     test_nonserializable_object = nonserializable_object_test\n \n \n+class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n+    chunksize = 100000\n+\n+    def queue(self):\n+        return MarshalFifoDiskQueue(self.qpath, chunksize=self.chunksize)\n+\n+\n class ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 1\n \n@@ -82,7 +84,7 @@ class ChunkSize4MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 4\n \n \n-class PickleFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n+class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n \n     chunksize = 100000\n \n@@ -133,10 +135,7 @@ class ChunkSize4PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 4\n \n \n-class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest):\n-\n-    def queue(self):\n-        return MarshalLifoDiskQueue(self.qpath)\n+class LifoDiskQueueTestMixin:\n \n     def test_serialize(self):\n         q = self.queue()\n@@ -150,7 +149,13 @@ class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest):\n     test_nonserializable_object = nonserializable_object_test\n \n \n-class PickleLifoDiskQueueTest(MarshalLifoDiskQueueTest):\n+class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n+\n+    def queue(self):\n+        return MarshalLifoDiskQueue(self.qpath)\n+\n+\n+class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n \n     def queue(self):\n         return PickleLifoDiskQueue(self.qpath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93436f9d3a67cd8abe3b321321c2d36d94f75b8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 33 | Churn Cumulative: 279 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,11 +82,10 @@ def _scrapy_non_serialization_queue(queue_class):\n def _pickle_serialize(obj):\n     try:\n         return pickle.dumps(obj, protocol=4)\n-    # Python <= 3.4 raises pickle.PicklingError here while\n-    # 3.5 <= Python < 3.6 raises AttributeError and\n-    # Python >= 3.6 raises TypeError\n+    # Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s)\n+    # TypeError is raised from parsel.Selector\n     except (pickle.PicklingError, AttributeError, TypeError) as e:\n-        raise ValueError(str(e))\n+        raise ValueError(str(e)) from e\n \n \n PickleFifoDiskQueueNonRequest = _serializable_queue(\n\n@@ -28,20 +28,7 @@ class TestLoader(ItemLoader):\n \n def nonserializable_object_test(self):\n     q = self.queue()\n-    try:\n-        pickle.dumps(lambda x: x)\n-    except Exception:\n-        # Trigger Twisted bug #7989\n-        import twisted.persisted.styles  # NOQA\n     self.assertRaises(ValueError, q.push, lambda x: x)\n-    else:\n-        # Use a different unpickleable object\n-        class A:\n-            pass\n-\n-        a = A()\n-        a.__reduce__ = a.__reduce_ex__ = None\n-        self.assertRaises(ValueError, q.push, a)\n     # Selectors should fail (lxml.html.HtmlElement objects can't be pickled)\n     sel = Selector(text='<html><body><p>some text</p></body></html>')\n     self.assertRaises(ValueError, q.push, sel)\n@@ -118,6 +105,19 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n         self.assertEqual(r.url, r2.url)\n         assert r2.meta['request'] is r2\n \n+    def test_non_pickable_object(self):\n+        q = self.queue()\n+        try:\n+            q.push(lambda x: x)\n+        except ValueError as exc:\n+            self.assertIsInstance(exc.__context__, AttributeError)\n+\n+        sel = Selector(text='<html><body><p>some text</p></body></html>')\n+        try:\n+            q.push(sel)\n+        except ValueError as exc:\n+            self.assertIsInstance(exc.__context__, TypeError)\n+\n \n class ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e382c816024baffca05b0da29def95f723d27fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 146 | Contributors (this commit): 9 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import pickle\n-\n from queuelib.tests import test_queue as t\n from scrapy.squeues import (\n     MarshalFifoDiskQueueNonRequest as MarshalFifoDiskQueue,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc23d1cb580795f6fde6e27f92f39ce4e3b8b558", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 56 | Files Changed: 4 | Hunks: 16 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 140 | Churn Cumulative: 1441 | Contributors (this commit): 15 | Commits (past 90d): 13 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -197,8 +197,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n             r1 = Request('http://scrapytest.org/index.html')\n             r2 = Request('http://scrapytest.org/index.html',\n-                headers={'Referer': 'http://scrapytest.org/INDEX.html'}\n-            )\n+                         headers={'Referer': 'http://scrapytest.org/INDEX.html'})\n \n             dupefilter.log(r1, spider)\n             dupefilter.log(r2, spider)\n\n@@ -342,20 +342,22 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         i2 = dict(name=u'bar', age=i1)\n         i3 = TestItem(name=u'buz', age=i2)\n \n-        self.assertExportResult(i3,\n-            b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n-            b'<items>'\n-                b'<item>'\n-                    b'<age>'\n-                        b'<age>'\n-                            b'<age>22</age>'\n-                            b'<name>foo\\xc2\\xa3hoo</name>'\n-                        b'</age>'\n-                        b'<name>bar</name>'\n-                    b'</age>'\n-                    b'<name>buz</name>'\n-                b'</item>'\n-            b'</items>'\n+        self.assertExportResult(\n+            i3,\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n+                <items>\n+                    <item>\n+                        <age>\n+                            <age>\n+                                <age>22</age>\n+                                <name>foo\\xc2\\xa3hoo</name>\n+                            </age>\n+                            <name>bar</name>\n+                        </age>\n+                        <name>buz</name>\n+                    </item>\n+                </items>\n+            \"\"\"\n         )\n \n     def test_nested_list_item(self):\n@@ -363,31 +365,35 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         i2 = dict(name=u'bar', v2={\"egg\": [\"spam\"]})\n         i3 = TestItem(name=u'buz', age=[i1, i2])\n \n-        self.assertExportResult(i3,\n-            b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n-            b'<items>'\n-                b'<item>'\n-                    b'<age>'\n-                        b'<value><name>foo</name></value>'\n-                        b'<value><name>bar</name><v2><egg><value>spam</value></egg></v2></value>'\n-                    b'</age>'\n-                    b'<name>buz</name>'\n-                b'</item>'\n-            b'</items>'\n+        self.assertExportResult(\n+            i3,\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n+                <items>\n+                    <item>\n+                        <age>\n+                            <value><name>foo</name></value>\n+                            <value><name>bar</name><v2><egg><value>spam</value></egg></v2></value>\n+                        </age>\n+                        <name>buz</name>\n+                    </item>\n+                </items>\n+            \"\"\"\n         )\n \n     def test_nonstring_types_item(self):\n         item = self._get_nonstring_types_item()\n-        self.assertExportResult(item,\n-            b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n-            b'<items>'\n-               b'<item>'\n-                   b'<float>3.14</float>'\n-                   b'<boolean>False</boolean>'\n-                   b'<number>22</number>'\n-                   b'<time>2015-01-01 01:01:01</time>'\n-               b'</item>'\n-            b'</items>'\n+        self.assertExportResult(\n+            item,\n+            b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n\n+                <items>\n+                   <item>\n+                       <float>3.14</float>\n+                       <boolean>False</boolean>\n+                       <number>22</number>\n+                       <time>2015-01-01 01:01:01</time>\n+                   </item>\n+                </items>\n+            \"\"\"\n         )\n \n \n\n@@ -413,23 +413,29 @@ class Base:\n             response = HtmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n \n             lx = self.extractor_cls()\n-            self.assertEqual(lx.extract_links(response),\n-                             [Link(url='http://example.com/about.html', text=u'About us', fragment='', nofollow=False),\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(url='http://example.com/about.html', text=u'About us', fragment='', nofollow=False),\n                     Link(url='http://example.com/follow.html', text=u'Follow this link', fragment='', nofollow=False),\n                     Link(url='http://example.com/nofollow.html', text=u'Dont follow this one', fragment='', nofollow=True),\n                     Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False),\n-                              Link(url='http://google.com/something', text=u'External link not to follow', nofollow=True)]\n+                    Link(url='http://google.com/something', text=u'External link not to follow', nofollow=True),\n+                ]\n             )\n \n             response = XmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n \n             lx = self.extractor_cls()\n-            self.assertEqual(lx.extract_links(response),\n-                             [Link(url='http://example.com/about.html', text=u'About us', fragment='', nofollow=False),\n+            self.assertEqual(\n+                lx.extract_links(response),\n+                [\n+                    Link(url='http://example.com/about.html', text=u'About us', fragment='', nofollow=False),\n                     Link(url='http://example.com/follow.html', text=u'Follow this link', fragment='', nofollow=False),\n                     Link(url='http://example.com/nofollow.html', text=u'Dont follow this one', fragment='', nofollow=True),\n                     Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False),\n-                              Link(url='http://google.com/something', text=u'External link not to follow', nofollow=True)]\n+                    Link(url='http://google.com/something', text=u'External link not to follow', nofollow=True),\n+                ]\n             )\n \n         def test_link_wrong_href(self):\n\n@@ -58,10 +58,13 @@ class SitemapTest(unittest.TestCase):\n   </url>\n </urlset>\n \"\"\")\n-        self.assertEqual(list(s),\n-            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n+        self.assertEqual(\n+            list(s),\n+            [\n+                {'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n                 {'loc': 'http://www.example.com/2', 'lastmod': ''},\n-            ])\n+            ]\n+        )\n \n     def test_sitemap_wrong_ns(self):\n         \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n@@ -80,10 +83,13 @@ class SitemapTest(unittest.TestCase):\n   </url>\n </urlset>\n \"\"\")\n-        self.assertEqual(list(s),\n-            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n+        self.assertEqual(\n+            list(s),\n+            [\n+                {'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n                 {'loc': 'http://www.example.com/2', 'lastmod': ''},\n-            ])\n+            ]\n+        )\n \n     def test_sitemap_wrong_ns2(self):\n         \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n@@ -103,10 +109,13 @@ class SitemapTest(unittest.TestCase):\n </urlset>\n \"\"\")\n         assert s.type == 'urlset'\n-        self.assertEqual(list(s),\n-            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n+        self.assertEqual(\n+            list(s),\n+            [\n+                {'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n                 {'loc': 'http://www.example.com/2', 'lastmod': ''},\n-            ])\n+            ]\n+        )\n \n     def test_sitemap_urls_from_robots(self):\n         robots = \"\"\"User-agent: *\n@@ -195,11 +204,19 @@ Disallow: /forum/active/\n         </url>\n     </urlset>\"\"\")\n \n-        self.assertEqual(list(s), [\n-            {'loc': 'http://www.example.com/english/',\n-             'alternate': ['http://www.example.com/deutsch/', 'http://www.example.com/schweiz-deutsch/', 'http://www.example.com/english/']\n+        self.assertEqual(\n+            list(s),\n+            [\n+                {\n+                    'loc': 'http://www.example.com/english/',\n+                    'alternate': [\n+                        'http://www.example.com/deutsch/',\n+                        'http://www.example.com/schweiz-deutsch/',\n+                        'http://www.example.com/english/',\n+                    ],\n                 }\n-        ])\n+            ]\n+        )\n \n     def test_xml_entity_expansion(self):\n         s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4c12a234ae65d49678a9840708ff5e7b9d6dcecc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 38 | Files Changed: 10 | Hunks: 20 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 99 | Churn Cumulative: 3889 | Contributors (this commit): 46 | Commits (past 90d): 23 | Contributors (cumulative): 100 | DMM Complexity: 1.0\n\nDIFF:\n@@ -86,8 +86,8 @@ class BrowserLikeContextFactory(ScrapyClientContextFactory):\n         #\n         # This means that a website like https://www.cacert.org will be rejected\n         # by default, since CAcert.org CA certificate is seldom shipped.\n-        return optionsForClientTLS(hostname.decode(\"ascii\"),\n+        return optionsForClientTLS(\n+            hostname=hostname.decode(\"ascii\"),\n             trustRoot=platformTrust(),\n-                                   extraCertificateOptions={\n-                                        'method': self._ssl_method,\n-                                   })\n+            extraCertificateOptions={'method': self._ssl_method},\n+        )\n\n@@ -105,6 +105,7 @@ class S3DownloadHandler:\n                 key=unquote(p.path),\n                 query_args=unquote(p.query),\n                 headers=request.headers,\n-                    data=request.body)\n+                data=request.body,\n+            )\n             request = request.replace(url=url, headers=signed_headers)\n         return self._download_http(request, spider)\n\n@@ -88,8 +88,8 @@ class ScrapyHTTPPageGetter(HTTPClient):\n             self.transport.stopProducing()\n \n         self.factory.noPage(\n-                defer.TimeoutError(\"Getting %s took longer than %s seconds.\" %\n-                                   (self.factory.url, self.factory.timeout)))\n+            defer.TimeoutError(\"Getting %s took longer than %s seconds.\"\n+                               % (self.factory.url, self.factory.timeout)))\n \n \n class ScrapyHTTPClientFactory(HTTPClientFactory):\n\n@@ -12,9 +12,15 @@ once the spider has finished crawling all regular (non failed) pages.\n import logging\n \n from twisted.internet import defer\n-from twisted.internet.error import TimeoutError, DNSLookupError, \\\n-        ConnectionRefusedError, ConnectionDone, ConnectError, \\\n-        ConnectionLost, TCPTimedOutError\n+from twisted.internet.error import (\n+    ConnectError,\n+    ConnectionDone,\n+    ConnectionLost,\n+    ConnectionRefusedError,\n+    DNSLookupError,\n+    TCPTimedOutError,\n+    TimeoutError,\n+)\n from twisted.web.client import ResponseFailed\n \n from scrapy.exceptions import NotConfigured\n\n@@ -24,15 +24,17 @@ class SpiderLoader:\n         self._load_all_spiders()\n \n     def _check_name_duplicates(self):\n-        dupes = [\"\\n\".join(\"  {cls} named {name!r} (in {module})\".format(\n-                                module=mod, cls=cls, name=name)\n-                           for (mod, cls) in locations)\n-                 for name, locations in self._found.items()\n-                 if len(locations) > 1]\n+        dupes = []\n+        for name, locations in self._found.items():\n+            dupes.extend([\n+                \"  {cls} named {name!r} (in {module})\".format(module=mod, cls=cls, name=name)\n+                for mod, cls in locations\n+            ])\n+\n         if dupes:\n+            dupes_string = \"\\n\\n\".join(dupes)\n             msg = (\"There are several spiders with the same name:\\n\\n\"\n-                   \"{}\\n\\n  This can cause unexpected behavior.\".format(\n-                        \"\\n\\n\".join(dupes)))\n+                   \"{}\\n\\n  This can cause unexpected behavior.\".format(dupes_string))\n             warnings.warn(msg, UserWarning)\n \n     def _load_spiders(self, module):\n@@ -45,11 +47,12 @@ class SpiderLoader:\n             try:\n                 for module in walk_modules(name):\n                     self._load_spiders(module)\n-            except ImportError as e:\n+            except ImportError:\n                 if self.warn_only:\n-                    msg = (\"\\n{tb}Could not load spiders from module '{modname}'. \"\n-                           \"See above traceback for details.\".format(\n-                                modname=name, tb=traceback.format_exc()))\n+                    msg = (\n+                        \"\\n{tb}Could not load spiders from module '{modname}'. \"\n+                        \"See above traceback for details.\".format(modname=name, tb=traceback.format_exc())\n+                    )\n                     warnings.warn(msg, RuntimeWarning)\n                 else:\n                     raise\n\n@@ -139,10 +139,12 @@ class CookiesMiddlewareTest(TestCase):\n \n     def test_complex_cookies(self):\n         # merge some cookies into jar\n-        cookies = [{'name': 'C1', 'value': 'value1', 'path': '/foo', 'domain': 'scrapytest.org'},\n+        cookies = [\n+            {'name': 'C1', 'value': 'value1', 'path': '/foo', 'domain': 'scrapytest.org'},\n             {'name': 'C2', 'value': 'value2', 'path': '/bar', 'domain': 'scrapytest.org'},\n             {'name': 'C3', 'value': 'value3', 'path': '/foo', 'domain': 'scrapytest.org'},\n-                {'name': 'C4', 'value': 'value4', 'path': '/foo', 'domain': 'scrapy.org'}]\n+            {'name': 'C4', 'value': 'value4', 'path': '/foo', 'domain': 'scrapy.org'},\n+        ]\n \n         req = Request('http://scrapytest.org/', cookies=cookies)\n         self.mw.process_request(req, self.spider)\n\n@@ -1,8 +1,14 @@\n import unittest\n from twisted.internet import defer\n-from twisted.internet.error import TimeoutError, DNSLookupError, \\\n-        ConnectionRefusedError, ConnectionDone, ConnectError, \\\n-        ConnectionLost, TCPTimedOutError\n+from twisted.internet.error import (\n+    ConnectError,\n+    ConnectionDone,\n+    ConnectionLost,\n+    ConnectionRefusedError,\n+    DNSLookupError,\n+    TCPTimedOutError,\n+    TimeoutError,\n+)\n from twisted.web.client import ResponseFailed\n \n from scrapy.downloadermiddlewares.retry import RetryMiddleware\n\n@@ -504,9 +504,11 @@ class FormRequestTest(RequestTest):\n         response = _buildresponse(\n             '<form></form>',\n             url='http://www.example.com')\n-        req = self.request_class.from_response(response,\n+        req = self.request_class.from_response(\n+            response=response,\n             method='GET',\n-                formdata=(('foo', 'bar'), ('foo', 'baz')))\n+            formdata=(('foo', 'bar'), ('foo', 'baz')),\n+        )\n         self.assertEqual(urlparse(req.url).hostname, 'www.example.com')\n         self.assertEqual(urlparse(req.url).query, 'foo=bar&foo=baz')\n \n@@ -530,9 +532,11 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test\" value=\"val2\">\n             <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n             </form>\"\"\")\n-        req = self.request_class.from_response(response,\n+        req = self.request_class.from_response(\n+            response=response,\n             formdata={'one': ['two', 'three'], 'six': 'seven'},\n-                headers={\"Accept-Encoding\": \"gzip,deflate\"})\n+            headers={\"Accept-Encoding\": \"gzip,deflate\"},\n+        )\n         self.assertEqual(req.method, 'POST')\n         self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')\n         self.assertEqual(req.headers['Accept-Encoding'], b'gzip,deflate')\n\n@@ -44,9 +44,7 @@ class RedirectedMediaDownloadSpider(MediaDownloadSpider):\n     name = 'redirectedmedia'\n \n     def _process_url(self, url):\n-        return add_or_replace_parameter(\n-                    self.mockserver.url('/redirect-to'),\n-                    'goto', url)\n+        return add_or_replace_parameter(self.mockserver.url('/redirect-to'), 'goto', url)\n \n \n class FileDownloadCrawlTestCase(TestCase):\n\n@@ -18,6 +18,14 @@ except ImportError:\n from twisted.python.filepath import FilePath\n from twisted.protocols.policies import WrappingFactory\n from twisted.internet.defer import inlineCallbacks\n+from twisted.web.test.test_webclient import (\n+    ForeverTakingResource,\n+    ErrorResource,\n+    NoLengthResource,\n+    HostHeaderResource,\n+    PayloadResource,\n+    BrokenDownloadResource,\n+)\n \n from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n@@ -202,11 +210,6 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             Headers({'Hello': ['World'], 'Foo': ['Bar']}))\n \n \n-from twisted.web.test.test_webclient import ForeverTakingResource, \\\n-        ErrorResource, NoLengthResource, HostHeaderResource, \\\n-        PayloadResource, BrokenDownloadResource\n-\n-\n class EncodingResource(resource.Resource):\n     out_encoding = 'cp1251'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#88efc988473fb0db8ca8fb512b2aab834c5aa7af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 319 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -93,8 +93,8 @@ class XmliterTestCase(unittest.TestCase):\n             # with bytes\n             XmlResponse(url=\"http://example.com\", body=body.encode('utf-8')),\n             # Unicode body needs encoding information\n-            XmlResponse(url=\"http://example.com\", body=body, encoding='utf-8')):\n-\n+            XmlResponse(url=\"http://example.com\", body=body, encoding='utf-8'),\n+        ):\n             attrs = []\n             for x in self.xmliter(r, u'þingflokkur'):\n                 attrs.append((x.attrib['id'],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2af34873cc385e47cc962903ff3d6a13c9847615", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 689 | Contributors (this commit): 22 | Commits (past 90d): 5 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,8 +27,8 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--spider\", dest=\"spider\", help=\"use this spider\")\n         parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\",\n                           help=\"print response HTTP headers instead of body\")\n-        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\",\n-                          default=False, help=\"do not handle HTTP 3xx status codes and print response as-is\")\n+        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n+                          help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def _print_headers(self, headers, prefix):\n         for key, values in headers.items():\n\n@@ -37,8 +37,8 @@ class Command(ScrapyCommand):\n                           help=\"evaluate the code in the shell, print the result and exit\")\n         parser.add_option(\"--spider\", dest=\"spider\",\n                           help=\"use this spider\")\n-        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\",\n-            default=False, help=\"do not handle HTTP 3xx status codes and print response as-is\")\n+        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n+                          help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def update_vars(self, vars):\n         \"\"\"You can use this function to update the Scrapy objects that will be\n\n@@ -102,10 +102,8 @@ class Command(ScrapyCommand):\n         move(join(project_dir, 'module'), join(project_dir, project_name))\n         for paths in TEMPLATES_TO_RENDER:\n             path = join(*paths)\n-            tplfile = join(project_dir,\n-                string.Template(path).substitute(project_name=project_name))\n-            render_templatefile(tplfile, project_name=project_name,\n-                ProjectName=string_camelcase(project_name))\n+            tplfile = join(project_dir, string.Template(path).substitute(project_name=project_name))\n+            render_templatefile(tplfile, project_name=project_name, ProjectName=string_camelcase(project_name))\n         print(\"New Scrapy project '%s', using template directory '%s', \"\n               \"created in:\" % (project_name, self.templates_dir))\n         print(\"    %s\\n\" % abspath(project_dir))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7383b2b428b08e84d6e3e40e6229c892e5802f7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 2827 | Contributors (this commit): 30 | Commits (past 90d): 11 | Contributors (cumulative): 38 | DMM Complexity: 0.0\n\nDIFF:\n@@ -46,11 +46,12 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         #\n         # * getattr() for `_ssl_method` attribute for context factories\n         #   not calling super(..., self).__init__\n-        return CertificateOptions(verify=False,\n-                    method=getattr(self, 'method',\n-                                   getattr(self, '_ssl_method', None)),\n+        return CertificateOptions(\n+            verify=False,\n+            method=getattr(self, 'method', getattr(self, '_ssl_method', None)),\n             fixBrokenPeers=True,\n-                    acceptableCiphers=self.tls_ciphers)\n+            acceptableCiphers=self.tls_ciphers,\n+        )\n \n     # kept for old-style HTTP/1.0 downloader context twisted calls,\n     # e.g. connectSSL()\n\n@@ -86,10 +86,9 @@ class FTPDownloadHandler:\n         password = request.meta.get(\"ftp_password\", self.default_password)\n         passive_mode = 1 if bool(request.meta.get(\"ftp_passive\",\n                                                   self.passive_mode)) else 0\n-        creator = ClientCreator(reactor, FTPClient, user, password,\n-            passive=passive_mode)\n-        return creator.connectTCP(parsed_url.hostname, parsed_url.port or 21).addCallback(self.gotClient,\n-                                request, unquote(parsed_url.path))\n+        creator = ClientCreator(reactor, FTPClient, user, password, passive=passive_mode)\n+        dfd = creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)\n+        return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))\n \n     def gotClient(self, client, request, filepath):\n         self.client = client\n\n@@ -217,11 +217,9 @@ class ExecutionEngine:\n         self.slot.nextcall.schedule()\n \n     def schedule(self, request, spider):\n-        self.signals.send_catch_log(signal=signals.request_scheduled,\n-                request=request, spider=spider)\n+        self.signals.send_catch_log(signals.request_scheduled, request=request, spider=spider)\n         if not self.slot.scheduler.enqueue_request(request):\n-            self.signals.send_catch_log(signal=signals.request_dropped,\n-                                        request=request, spider=spider)\n+            self.signals.send_catch_log(signals.request_dropped, request=request, spider=spider)\n \n     def download(self, request, spider):\n         d = self._download(request, spider)\n@@ -247,7 +245,7 @@ class ExecutionEngine:\n                 logkws = self.logformatter.crawled(request, response, spider)\n                 if logkws is not None:\n                     logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n-                self.signals.send_catch_log(signal=signals.response_received,\n+                self.signals.send_catch_log(signals.response_received,\n                                             response=response, request=request, spider=spider)\n             return response\n \n@@ -286,8 +284,7 @@ class ExecutionEngine:\n         next loop and this function is guaranteed to be called (at least) once\n         again for this spider.\n         \"\"\"\n-        res = self.signals.send_catch_log(signal=signals.spider_idle,\n-            spider=spider, dont_log=DontCloseSpider)\n+        res = self.signals.send_catch_log(signals.spider_idle, spider=spider, dont_log=DontCloseSpider)\n         if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res):\n             return\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a72f5aadcf0b7ee8ef77f43602aecf34ffea7fd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 31 | Churn Cumulative: 2356 | Contributors (this commit): 30 | Commits (past 90d): 7 | Contributors (cumulative): 61 | DMM Complexity: 1.0\n\nDIFF:\n@@ -88,8 +88,11 @@ def process_chain_both(callbacks, errbacks, input, *a, **kw):\n     \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n     d = defer.Deferred()\n     for cb, eb in zip(callbacks, errbacks):\n-        d.addCallbacks(cb, eb, callbackArgs=a, callbackKeywords=kw,\n-            errbackArgs=a, errbackKeywords=kw)\n+        d.addCallbacks(\n+            callback=cb, errback=eb,\n+            callbackArgs=a, callbackKeywords=kw,\n+            errbackArgs=a, errbackKeywords=kw,\n+        )\n     if isinstance(input, failure.Failure):\n         d.errback(input)\n     else:\n\n@@ -144,10 +144,12 @@ def _get_handler(settings):\n def log_scrapy_info(settings):\n     logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                 {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n-    logger.info(\"Versions: %(versions)s\",\n-                {'versions': \", \".join(\"%s %s\" % (name, version)\n+    versions = [\n+        \"%s %s\" % (name, version)\n         for name, version in scrapy_components_versions()\n-                    if name != \"Scrapy\")})\n+        if name != \"Scrapy\"\n+    ]\n+    logger.info(\"Versions: %(versions)s\", {'versions': \", \".join(versions)})\n     from twisted.internet import reactor\n     logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n \n\n@@ -19,8 +19,7 @@ def get_base_url(response):\n     \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n     if response not in _baseurl_cache:\n         text = response.text[0:4096]\n-        _baseurl_cache[response] = html.get_base_url(text, response.url,\n-            response.encoding)\n+        _baseurl_cache[response] = html.get_base_url(text, response.url, response.encoding)\n     return _baseurl_cache[response]\n \n \n@@ -31,8 +30,8 @@ def get_meta_refresh(response, ignore_tags=('script', 'noscript')):\n     \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.text[0:4096]\n-        _metaref_cache[response] = html.get_meta_refresh(text, response.url,\n-            response.encoding, ignore_tags=ignore_tags)\n+        _metaref_cache[response] = html.get_meta_refresh(\n+            text, response.url, response.encoding, ignore_tags=ignore_tags)\n     return _metaref_cache[response]\n \n \n\n@@ -28,8 +28,7 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n     responses = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n         try:\n-            response = robustApply(receiver, signal=signal, sender=sender,\n-                *arguments, **named)\n+            response = robustApply(receiver, signal=signal, sender=sender, *arguments, **named)\n             if isinstance(response, Deferred):\n                 logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n                              {'receiver': receiver}, extra={'spider': spider})\n@@ -63,8 +62,7 @@ def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n     spider = named.get('spider', None)\n     dfds = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n-        d = maybeDeferred_coro(robustApply, receiver, signal=signal, sender=sender,\n-                *arguments, **named)\n+        d = maybeDeferred_coro(robustApply, receiver, signal=signal, sender=sender, *arguments, **named)\n         d.addErrback(logerror, receiver)\n         d.addBoth(lambda result: (receiver, result))\n         dfds.append(d)\n\n@@ -27,8 +27,7 @@ def url_is_from_any_domain(url, domains):\n \n def url_is_from_spider(url, spider):\n     \"\"\"Return True if the url belongs to the given spider\"\"\"\n-    return url_is_from_any_domain(url,\n-        [spider.name] + list(getattr(spider, 'allowed_domains', [])))\n+    return url_is_from_any_domain(url, [spider.name] + list(getattr(spider, 'allowed_domains', [])))\n \n \n def url_has_any_extension(url, extensions):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4171b3f663aea5706ba6acf15f64e92be0e34f2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 292 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,9 +46,10 @@ class RFC2616Policy:\n     def __init__(self, settings):\n         self.always_store = settings.getbool('HTTPCACHE_ALWAYS_STORE')\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n-        self.ignore_response_cache_controls = [to_bytes(cc) for cc in\n-            settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')]\n         self._cc_parsed = WeakKeyDictionary()\n+        self.ignore_response_cache_controls = [\n+            to_bytes(cc) for cc in settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')\n+        ]\n \n     def _parse_cachecontrol(self, r):\n         if r not in self._cc_parsed:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2851f641f2c105350948a34be55aa00dfed9933a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 44 | Churn Cumulative: 830 | Contributors (this commit): 26 | Commits (past 90d): 12 | Contributors (cumulative): 41 | DMM Complexity: 0.7\n\nDIFF:\n@@ -86,8 +86,7 @@ class DictItem(MutableMapping, BaseItem):\n         if key in self.fields:\n             self._values[key] = value\n         else:\n-            raise KeyError(\"%s does not support field: %s\" %\n-                (self.__class__.__name__, key))\n+            raise KeyError(\"%s does not support field: %s\" % (self.__class__.__name__, key))\n \n     def __delitem__(self, key):\n         del self._values[key]\n@@ -99,8 +98,7 @@ class DictItem(MutableMapping, BaseItem):\n \n     def __setattr__(self, name, value):\n         if not name.startswith('_'):\n-            raise AttributeError(\"Use item[%r] = %r to set field value\" %\n-                (name, value))\n+            raise AttributeError(\"Use item[%r] = %r to set field value\" % (name, value))\n         super(DictItem, self).__setattr__(name, value)\n \n     def __len__(self):\n\n@@ -28,8 +28,10 @@ def _to_bytes_or_none(text):\n \n \n class MailSender:\n-    def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',\n-            smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n+    def __init__(\n+        self, smtphost='localhost', mailfrom='scrapy@localhost', smtpuser=None,\n+        smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False\n+    ):\n         self.smtphost = smtphost\n         self.smtpport = smtpport\n         self.smtpuser = _to_bytes_or_none(smtpuser)\n@@ -41,9 +43,15 @@ class MailSender:\n \n     @classmethod\n     def from_settings(cls, settings):\n-        return cls(settings['MAIL_HOST'], settings['MAIL_FROM'], settings['MAIL_USER'],\n-            settings['MAIL_PASS'], settings.getint('MAIL_PORT'),\n-            settings.getbool('MAIL_TLS'), settings.getbool('MAIL_SSL'))\n+        return cls(\n+            smtphost=settings['MAIL_HOST'],\n+            mailfrom=settings['MAIL_FROM'],\n+            smtpuser=settings['MAIL_USER'],\n+            smtppass=settings['MAIL_PASS'],\n+            smtpport=settings.getint('MAIL_PORT'),\n+            smtptls=settings.getbool('MAIL_TLS'),\n+            smtpssl=settings.getbool('MAIL_SSL'),\n+        )\n \n     def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):\n         from twisted.internet import reactor\n@@ -89,9 +97,12 @@ class MailSender:\n             return\n \n         dfd = self._sendmail(rcpts, msg.as_string().encode(charset or 'utf-8'))\n-        dfd.addCallbacks(self._sent_ok, self._sent_failed,\n+        dfd.addCallbacks(\n+            callback=self._sent_ok,\n+            errback=self._sent_failed,\n             callbackArgs=[to, cc, subject, len(attachs)],\n-            errbackArgs=[to, cc, subject, len(attachs)])\n+            errbackArgs=[to, cc, subject, len(attachs)],\n+        )\n         reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)\n         return dfd\n \n@@ -115,9 +126,10 @@ class MailSender:\n         from twisted.mail.smtp import ESMTPSenderFactory\n         msg = BytesIO(msg)\n         d = defer.Deferred()\n-        factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom,\n-            to_addrs, msg, d, heloFallback=True, requireAuthentication=False,\n-            requireTransportSecurity=self.smtptls)\n+        factory = ESMTPSenderFactory(\n+            self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d,\n+            heloFallback=True, requireAuthentication=False, requireTransportSecurity=self.smtptls,\n+        )\n         factory.noisy = False\n \n         if self.smtpssl:\n\n@@ -58,9 +58,9 @@ class ResponseTypes:\n \n     def from_content_disposition(self, content_disposition):\n         try:\n-            filename = to_unicode(content_disposition,\n-                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n-            filename = filename.strip('\"\\'')\n+            filename = to_unicode(\n+                content_disposition, encoding='latin-1', errors='replace'\n+            ).split(';')[1].split('=')[1].strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:\n             return Response\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c6d13075a2ab06c0d670e19457f9d9f219ceeee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 39 | Files Changed: 8 | Hunks: 18 | Methods Changed: 17 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 97 | Churn Cumulative: 5499 | Contributors (this commit): 47 | Commits (past 90d): 26 | Contributors (cumulative): 97 | DMM Complexity: 1.0\n\nDIFF:\n@@ -101,15 +101,13 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_local_nofile(self):\n         filepath = 'file:///tests/sample_data/test_site/nothinghere.html'\n-        errcode, out, err = yield self.execute([filepath, '-c', 'item'],\n-                                       check_code=False)\n+        errcode, out, err = yield self.execute([filepath, '-c', 'item'], check_code=False)\n         self.assertEqual(errcode, 1, out or err)\n         self.assertIn(b'No such file or directory', err)\n \n     @defer.inlineCallbacks\n     def test_dns_failures(self):\n         url = 'www.somedomainthatdoesntexi.st'\n-        errcode, out, err = yield self.execute([url, '-c', 'item'],\n-                                       check_code=False)\n+        errcode, out, err = yield self.execute([url, '-c', 'item'], check_code=False)\n         self.assertEqual(errcode, 1, out or err)\n         self.assertIn(b'DNS lookup failed', err)\n\n@@ -232,7 +232,8 @@ class ContractsManagerTest(unittest.TestCase):\n         # extract contracts correctly\n         contracts = self.conman.extract_contracts(spider.returns_request)\n         self.assertEqual(len(contracts), 2)\n-        self.assertEqual(frozenset(type(x) for x in contracts),\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n             frozenset([UrlContract, ReturnsContract]))\n \n         # returns request for valid method\n\n@@ -853,8 +853,7 @@ class S3TestCase(unittest.TestCase):\n     def test_request_signing4(self):\n         # fetches the access control policy sub-resource for the 'johnsmith' bucket.\n         date = 'Tue, 27 Mar 2007 19:44:46 +0000'\n-        req = Request('s3://johnsmith/?acl',\n-            method='GET', headers={'Date': date})\n+        req = Request('s3://johnsmith/?acl', method='GET', headers={'Date': date})\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         self.assertEqual(httpreq.headers['Authorization'],\n@@ -879,7 +878,8 @@ class S3TestCase(unittest.TestCase):\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n         # botocore does not override Date with x-amz-date\n-        self.assertEqual(httpreq.headers['Authorization'],\n+        self.assertEqual(\n+            httpreq.headers['Authorization'],\n             b'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')\n \n     def test_request_signing6(self):\n\n@@ -43,8 +43,11 @@ class TestHttpProxyMiddleware(TestCase):\n         os.environ.pop('file_proxy', None)\n         mw = HttpProxyMiddleware()\n \n-        for url, proxy in [('http://e.com', http_proxy),\n-                ('https://e.com', https_proxy), ('file://tmp/a', None)]:\n+        for url, proxy in [\n+            ('http://e.com', http_proxy),\n+            ('https://e.com', https_proxy),\n+            ('file://tmp/a', None),\n+        ]:\n             req = Request(url)\n             assert mw.process_request(req, spider) is None\n             self.assertEqual(req.url, url)\n\n@@ -81,9 +81,17 @@ class RetryTest(unittest.TestCase):\n         assert self.crawler.stats.get_value('retry/count') == 2\n \n     def test_twistederrors(self):\n-        exceptions = [defer.TimeoutError, TCPTimedOutError, TimeoutError,\n-                DNSLookupError, ConnectionRefusedError, ConnectionDone,\n-                ConnectError, ConnectionLost, ResponseFailed]\n+        exceptions = [\n+            ConnectError,\n+            ConnectionDone,\n+            ConnectionLost,\n+            ConnectionRefusedError,\n+            defer.TimeoutError,\n+            DNSLookupError,\n+            ResponseFailed,\n+            TCPTimedOutError,\n+            TimeoutError,\n+        ]\n \n         for exc in exceptions:\n             req = Request('http://www.scrapytest.org/%s' % exc.__name__)\n\n@@ -177,10 +177,13 @@ class RFPDupeFilterTest(unittest.TestCase):\n             dupefilter.log(r2, spider)\n \n             assert crawler.stats.get_value('dupefilter/filtered') == 2\n-            l.check_present(('scrapy.dupefilters', 'DEBUG',\n-                ('Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n-                ' - no more duplicates will be shown'\n-                ' (see DUPEFILTER_DEBUG to show all duplicates)')))\n+            l.check_present(\n+                (\n+                    'scrapy.dupefilters',\n+                    'DEBUG',\n+                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)'\n+                )\n+            )\n \n             dupefilter.close('finished')\n \n@@ -203,11 +206,19 @@ class RFPDupeFilterTest(unittest.TestCase):\n             dupefilter.log(r2, spider)\n \n             assert crawler.stats.get_value('dupefilter/filtered') == 2\n-            l.check_present(('scrapy.dupefilters', 'DEBUG',\n-                ('Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n-                ' (referer: None)')))\n-            l.check_present(('scrapy.dupefilters', 'DEBUG',\n-                ('Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n-                ' (referer: http://scrapytest.org/INDEX.html)')))\n+            l.check_present(\n+                (\n+                    'scrapy.dupefilters',\n+                    'DEBUG',\n+                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)'\n+                )\n+            )\n+            l.check_present(\n+                (\n+                    'scrapy.dupefilters',\n+                    'DEBUG',\n+                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: http://scrapytest.org/INDEX.html)'\n+                )\n+            )\n \n             dupefilter.close('finished')\n\n@@ -415,8 +415,7 @@ class FormRequestTest(RequestTest):\n         # using multiples values for a single key\n         data = {'price': u'\\xa3 100', 'colours': ['red', 'blue', 'green']}\n         r3 = self.request_class(\"http://www.example.com\", formdata=data)\n-        self.assertQueryEqual(r3.body,\n-            b'colours=red&colours=blue&colours=green&price=%C2%A3+100')\n+        self.assertQueryEqual(r3.body, b'colours=red&colours=blue&colours=green&price=%C2%A3+100')\n \n     def test_from_response_post(self):\n         response = _buildresponse(\n@@ -426,8 +425,7 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n             </form>\"\"\",\n             url=\"http://www.example.com/this/list.html\")\n-        req = self.request_class.from_response(response,\n-                formdata={'one': ['two', 'three'], 'six': 'seven'})\n+        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n \n         self.assertEqual(req.method, 'POST')\n         self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n@@ -446,8 +444,7 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test2\" value=\"xxx \\xc2\\xb5\">\n             </form>\"\"\",\n             url=\"http://www.example.com/this/list.html\")\n-        req = self.request_class.from_response(response,\n-                formdata={'one': ['two', 'three'], 'six': 'seven'})\n+        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n \n         self.assertEqual(req.method, 'POST')\n         self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n@@ -468,8 +465,7 @@ class FormRequestTest(RequestTest):\n             url=\"http://www.example.com/this/list.html\",\n             encoding='latin1',\n         )\n-        req = self.request_class.from_response(response,\n-                formdata={'one': ['two', 'three'], 'six': 'seven'})\n+        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n \n         self.assertEqual(req.method, 'POST')\n         self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n@@ -488,8 +484,7 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test2\" value=\"xxx µ\">\n             </form>\"\"\",\n             url=\"http://www.example.com/this/list.html\")\n-        req = self.request_class.from_response(response,\n-                formdata={'one': ['two', 'three'], 'six': 'seven'})\n+        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n \n         self.assertEqual(req.method, 'POST')\n         self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')\n@@ -549,8 +544,7 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"test2\" value=\"xxx\">\n             </form>\"\"\",\n             url=\"http://www.example.com/this/list.html\")\n-        r1 = self.request_class.from_response(response,\n-                formdata={'one': ['two', 'three'], 'six': 'seven'})\n+        r1 = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})\n         self.assertEqual(r1.method, 'GET')\n         self.assertEqual(urlparse(r1.url).hostname, \"www.example.com\")\n         self.assertEqual(urlparse(r1.url).path, \"/this/get.php\")\n\n@@ -485,8 +485,10 @@ class TextResponseTest(BaseResponseTest):\n             response.xpath(\"normalize-space(//p[@class=\\\"content\\\"])\").getall(),\n         )\n         self.assertEqual(\n-            response.xpath(\"//title[count(following::p[@class=$pclass])=$pcount]/text()\",\n-                pclass=\"content\", pcount=1).getall(),\n+            response.xpath(\n+                \"//title[count(following::p[@class=$pclass])=$pcount]/text()\",\n+                pclass=\"content\", pcount=1,\n+            ).getall(),\n             response.xpath(\"//title[count(following::p[@class=\\\"content\\\"])=1]/text()\").getall(),\n         )\n \n@@ -566,10 +568,12 @@ class TextResponseTest(BaseResponseTest):\n             'http://example.com',\n             body=b'''<html><body><a href=\" foo\\n\">click me</a></body></html>'''\n         )\n-        self._assert_followed_url(resp.css('a')[0],\n+        self._assert_followed_url(\n+            resp.css('a')[0],\n             'http://example.com/foo',\n             response=resp)\n-        self._assert_followed_url(resp.css('a::attr(href)')[0],\n+        self._assert_followed_url(\n+            resp.css('a::attr(href)')[0],\n             'http://example.com/foo',\n             response=resp)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
