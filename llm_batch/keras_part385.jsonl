{"custom_id": "keras#a4c20bc51aeed79dc210bed2b2c1c54801d10f9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 50 | Churn Cumulative: 10764 | Contributors (this commit): 10 | Commits (past 90d): 17 | Contributors (cumulative): 17 | DMM Complexity: 0.8695652173913043\n\nDIFF:\n@@ -1719,6 +1719,15 @@ def identity(x, name=None):\n # tf.random.uniform.\n _USE_GENERATOR_FOR_RNG = False\n \n+# The global generator to create the seed when initializing the\n+# tf.random.Genrator used by RandomGenerator. When tf.random.Generator becomes\n+# the default solution, we would like the it to be initialized in a controlable\n+# way, so that each client of the program could start with same seed. This is\n+# very important for certain use case that requires all the client to have their\n+# state in sync. This instance will be set when user call\n+# `tf.keras.util.set_random_seed()`\n+_SEED_GENERATOR = threading.local()\n+\n \n def use_generator_for_rng():\n   return _USE_GENERATOR_FOR_RNG\n@@ -1770,8 +1779,11 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n         if self._seed is not None:\n           self._generator = tf.random.Generator.from_seed(self._seed)\n         else:\n-          self._generator = tf.random.Generator.from_seed(\n-              random.randint(1, 1e9))\n+          if getattr(_SEED_GENERATOR, 'generator', None):\n+            seed = _SEED_GENERATOR.generator.randint(1, 1e9)\n+          else:\n+            seed = random.randint(1, 1e9)\n+          self._generator = tf.random.Generator.from_seed(seed)\n     else:\n       # In the v1 case, we use stateful op, regardless whether user provide a\n       # seed or not. Seeded stateful op will ensure generating same sequences.\n\n@@ -32,6 +32,7 @@ from keras.engine import input_layer\n from keras.layers import advanced_activations\n from keras.layers.normalization import batch_normalization_v1\n from keras.utils import tf_inspect\n+from keras.utils import tf_utils\n \n \n def compare_single_input_op_to_numpy(keras_op,\n@@ -2312,6 +2313,38 @@ class RandomGeneratorTest(tf.test.TestCase):\n       self.assertIsNone(seeded._generator)\n       self.assertIsNone(unseeded._generator)\n \n+  def test_unseeded_with_utils_set_random_seed(self):\n+    keras_seed = 1337\n+    tf_utils.set_random_seed(keras_seed)\n+    gen1 = backend.RandomGenerator(seed=None, force_generator=True)\n+    output1 = gen1.random_normal(shape=[2, 3])\n+    output2 = gen1.random_normal(shape=[2, 3])\n+\n+    self.assertNotAllClose(output1, output2)\n+\n+    # Make sure even with unseeded backend generator, as long as we set the\n+    # keras random seed, it will make the generator to produce the same\n+    # sequence. This will ensure all the client are in sync in the multi-client\n+    # setting, when they all set the keras seed.\n+    tf_utils.set_random_seed(keras_seed)\n+    gen2 = backend.RandomGenerator(seed=None, force_generator=True)\n+    output3 = gen2.random_normal(shape=[2, 3])\n+    output4 = gen2.random_normal(shape=[2, 3])\n+\n+    gen3 = backend.RandomGenerator(seed=None, force_generator=True)\n+    output5 = gen3.random_normal(shape=[2, 3])\n+    output6 = gen3.random_normal(shape=[2, 3])\n+\n+    if tf.compat.v1.executing_eagerly():\n+      # The generator is only used in the tf2 with eager.\n+      self.assertAllEqual(output1, output3)\n+      self.assertAllEqual(output2, output4)\n+\n+      # Also make sure different generator instance are still producing\n+      # different result\n+      self.assertNotAllEqual(output3, output5)\n+      self.assertNotAllEqual(output4, output6)\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -62,6 +62,7 @@ def set_random_seed(seed):\n   random.seed(seed)\n   np.random.seed(seed)\n   tf.random.set_seed(seed)\n+  backend._SEED_GENERATOR.generator = random.Random(seed)  # pylint:disable=protected-access\n \n \n def is_tensor_or_tensor_list(v):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f20ed7b3d281a722087e50681be0e1a49bf5d7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1540 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -784,7 +784,6 @@ class KerasObjectLoader:\n \n   def _infer_inputs(self, layer_node_id, convert_to_shapes=False):\n     \"\"\"Infers input shape of layer from SavedModel functions.\"\"\"\n-    coder = tf.__internal__.saved_model.StructureCoder()\n     call_fn_id = self._search_for_child_node(\n         layer_node_id, ['call_and_return_all_conditional_losses'])\n     if call_fn_id is None:\n@@ -796,7 +795,7 @@ class KerasObjectLoader:\n       return None\n     call_fn_name = concrete_functions[0]\n     call_fn_proto = self._proto.concrete_functions[call_fn_name]\n-    structured_input_signature = coder.decode_proto(\n+    structured_input_signature = tf.__internal__.saved_model.decode_proto(\n         call_fn_proto.canonicalized_input_signature)\n     inputs = structured_input_signature[0][0]\n     if convert_to_shapes:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7e9199c623137edcfa2cf2b634e4dc52cea68436", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 104 | Lines Deleted: 59 | Files Changed: 2 | Hunks: 36 | Methods Changed: 22 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 163 | Churn Cumulative: 4198 | Contributors (this commit): 8 | Commits (past 90d): 17 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,11 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n LOWER_AND_STRIP_PUNCTUATION = \"lower_and_strip_punctuation\"\n-SPLIT_ON_WHITESPACE = \"whitespace\"\n+STRIP_PUNCTUATION = \"strip_punctuation\"\n+LOWER = \"lower\"\n+\n+WHITESPACE = \"whitespace\"\n+CHARACTER = \"character\"\n \n TF_IDF = utils.TF_IDF\n INT = utils.INT\n@@ -101,12 +105,20 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n       contains 1 OOV token, so the effective number of tokens is `(max_tokens -\n       1 - (1 if output_mode == \"int\" else 0))`.\n     standardize: Optional specification for standardization to apply to the\n-      input text. Values can be None (no standardization),\n-      `\"lower_and_strip_punctuation\"` (lowercase and remove punctuation) or a\n-      Callable. Default is `\"lower_and_strip_punctuation\"`.\n-    split: Optional specification for splitting the input text. Values can be\n-      None (no splitting), `\"whitespace\"` (split on ASCII whitespace), or a\n-      Callable. The default is `\"whitespace\"`.\n+      input text. Values can be:\n+        - `None`: No standardization.\n+        - `lower_and_strip_punctuation`: Text will be lowercased and all\n+          punctuation removed.\n+        - `lower`: Text will be lowercased.\n+        - `trip_punctuation`: All punctuation will be removed.\n+        - Callable: Inputs will passed to the callable function, which should\n+          standardized and returned.\n+    split: Optional specification for splitting the input text. Values can be:\n+        - `None`: No splitting.\n+        - `\"whitespace\"`: Split on whitespace.\n+        - `\"character\"`: Split on each unicode character.\n+        - Callable: Standardized inputs will passed to the callable function,\n+          which should split and returned.\n     ngrams: Optional specification for ngrams to create from the possibly-split\n       input text. Values can be None, an integer or tuple of integers; passing\n       an integer will create ngrams up to that integer, and passing a tuple of\n@@ -246,19 +258,21 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     elif \"dtype\" not in kwargs:\n       kwargs[\"dtype\"] = tf.string\n \n-    # 'standardize' must be one of (None, LOWER_AND_STRIP_PUNCTUATION, callable)\n+    # 'standardize' must be one of\n+    # (None, LOWER_AND_STRIP_PUNCTUATION, LOWER, STRIP_PUNCTUATION, callable)\n     layer_utils.validate_string_arg(\n         standardize,\n-        allowable_strings=(LOWER_AND_STRIP_PUNCTUATION),\n+        allowable_strings=(LOWER_AND_STRIP_PUNCTUATION, LOWER,\n+                           STRIP_PUNCTUATION),\n         layer_name=\"TextVectorization\",\n         arg_name=\"standardize\",\n         allow_none=True,\n         allow_callables=True)\n \n-    # 'split' must be one of (None, SPLIT_ON_WHITESPACE, callable)\n+    # 'split' must be one of (None, WHITESPACE, CHARACTER, callable)\n     layer_utils.validate_string_arg(\n         split,\n-        allowable_strings=(SPLIT_ON_WHITESPACE),\n+        allowable_strings=(WHITESPACE, CHARACTER),\n         layer_name=\"TextVectorization\",\n         arg_name=\"split\",\n         allow_none=True,\n@@ -438,27 +452,20 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n \n   def _preprocess(self, inputs):\n     inputs = utils.ensure_tensor(inputs, dtype=tf.string)\n-    if self._standardize == LOWER_AND_STRIP_PUNCTUATION:\n+    if self._standardize in (LOWER, LOWER_AND_STRIP_PUNCTUATION):\n       if tf_utils.is_ragged(inputs):\n-        lowercase_inputs = tf.ragged.map_flat_values(\n-            tf.strings.lower, inputs)\n+        inputs = tf.ragged.map_flat_values(tf.strings.lower, inputs)\n         # Depending on configuration, we may never touch the non-data tensor\n         # in the ragged inputs tensor. If that is the case, and this is the\n         # only layer in the keras model, running it will throw an error.\n         # To get around this, we wrap the result in an identity.\n-        lowercase_inputs = tf.identity(lowercase_inputs)\n+        inputs = tf.identity(inputs)\n       else:\n-        lowercase_inputs = tf.strings.lower(inputs)\n-      inputs = tf.strings.regex_replace(lowercase_inputs, DEFAULT_STRIP_REGEX,\n-                                        \"\")\n-    elif callable(self._standardize):\n+        inputs = tf.strings.lower(inputs)\n+    if self._standardize in (STRIP_PUNCTUATION, LOWER_AND_STRIP_PUNCTUATION):\n+      inputs = tf.strings.regex_replace(inputs, DEFAULT_STRIP_REGEX, \"\")\n+    if callable(self._standardize):\n       inputs = self._standardize(inputs)\n-    elif self._standardize is not None:\n-      raise ValueError((\"%s is not a supported standardization. \"\n-                        \"TextVectorization supports the following options \"\n-                        \"for `standardize`: None, \"\n-                        \"'lower_and_strip_punctuation', or a \"\n-                        \"Callable.\") % self._standardize)\n \n     if self._split is not None:\n       # If we are splitting, we validate that the 1st axis is of dimension 1 and\n@@ -472,10 +479,12 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n               f\"inputs.shape={inputs.shape} with rank={inputs.shape.rank}\")\n         else:\n           inputs = tf.squeeze(inputs, axis=-1)\n-      if self._split == SPLIT_ON_WHITESPACE:\n+      if self._split == WHITESPACE:\n         # This treats multiple whitespaces as one whitespace, and strips leading\n         # and trailing whitespace.\n         inputs = tf.strings.split(inputs)\n+      elif self._split == CHARACTER:\n+        inputs = tf.strings.unicode_split(inputs, \"UTF-8\")\n       elif callable(self._split):\n         inputs = self._split(inputs)\n       else:\n\n@@ -108,7 +108,7 @@ def _get_end_to_end_test_cases():\n           \"kwargs\": {\n               \"max_tokens\": None,\n               \"standardize\": None,\n-              \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n+              \"split\": text_vectorization.WHITESPACE,\n               \"output_mode\": text_vectorization.INT\n           },\n           \"expected_output\": [[2, 3, 4], [5, 5, 0], [4, 2, 0], [1, 0, 0]],\n@@ -126,7 +126,7 @@ def _get_end_to_end_test_cases():\n           \"kwargs\": {\n               \"max_tokens\": None,\n               \"standardize\": None,\n-              \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n+              \"split\": text_vectorization.WHITESPACE,\n               \"output_mode\": text_vectorization.INT\n           },\n           \"expected_output\": [[2, 3, 4], [5, 5, 0], [4, 2, 0], [1, 0, 0]],\n@@ -164,7 +164,7 @@ def _get_end_to_end_test_cases():\n               \"max_tokens\": 5,\n               \"pad_to_max_tokens\": True,\n               \"standardize\": None,\n-              \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n+              \"split\": text_vectorization.WHITESPACE,\n               \"output_mode\": text_vectorization.MULTI_HOT\n           },\n           \"expected_output\": [[0, 1, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1],\n@@ -203,7 +203,7 @@ def _get_end_to_end_test_cases():\n               \"max_tokens\": 5,\n               \"pad_to_max_tokens\": True,\n               \"standardize\": None,\n-              \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n+              \"split\": text_vectorization.WHITESPACE,\n               \"output_mode\": text_vectorization.COUNT\n           },\n           \"expected_output\": [[0, 1, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 2],\n@@ -243,7 +243,7 @@ def _get_end_to_end_test_cases():\n               \"max_tokens\": 5,\n               \"pad_to_max_tokens\": True,\n               \"standardize\": None,\n-              \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n+              \"split\": text_vectorization.WHITESPACE,\n               \"output_mode\": text_vectorization.TF_IDF\n           },\n           \"expected_output\": [[0., 0.847298, 0.847298, 0., 0.],\n@@ -446,7 +446,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     vectorization = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         pad_to_max_tokens=False)\n     vectorization.set_vocabulary(vocab)\n     output = vectorization(tf.ragged.constant(data, inner_shape=(1,)))\n@@ -482,10 +482,11 @@ class TextVectorizationPreprocessingTest(\n     # (b/145726907)\n     model.summary()\n \n-  def test_normalization(self):\n-    input_array = np.array([[\"Earth\", \"wInD\", \"aNd\", \"firE\"],\n+  @parameterized.parameters([list, np.array, tf.constant, tf.ragged.constant])\n+  def test_lower_and_strip_punctuation(self, data_fn):\n+    input_array = data_fn([[\"Earth\", \"wInD\", \"aNd\", \"firE\"],\n                            [\"fire|\", \"an<>d\", \"{earth}\", \"michigan@%$\"]])\n-    expected_output = np.array([[b\"earth\", b\"wind\", b\"and\", b\"fire\"],\n+    expected_output = data_fn([[b\"earth\", b\"wind\", b\"and\", b\"fire\"],\n                                [b\"fire\", b\"and\", b\"earth\", b\"michigan\"]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n@@ -500,16 +501,36 @@ class TextVectorizationPreprocessingTest(\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_normalization_ragged_inputs(self):\n-    input_array = tf.ragged.constant([[\"Earth\", \"wInD\", \"aNd\", \"firE\"],\n-                                               [\"fire|\", \"an<>d\", \"{earth}\"]])\n-    expected_output = [[b\"earth\", b\"wind\", b\"and\", b\"fire\"],\n-                       [b\"fire\", b\"and\", b\"earth\"]]\n+  @parameterized.parameters([list, np.array, tf.constant, tf.ragged.constant])\n+  def test_strip_punctuation(self, data_fn):\n+    input_array = data_fn([[\"Earth\", \"wInD\", \"aNd\", \"firE\"],\n+                           [\"fire|\", \"an<>d\", \"{earth}\", \"michigan@%$\"]])\n+    expected_output = data_fn([[b\"Earth\", b\"wInD\", b\"aNd\", b\"firE\"],\n+                               [b\"fire\", b\"and\", b\"earth\", b\"michigan\"]])\n \n-    input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.string)\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n-        standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,\n+        standardize=text_vectorization.STRIP_PUNCTUATION,\n+        split=None,\n+        ngrams=None,\n+        output_mode=None)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_dataset = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_dataset)\n+\n+  @parameterized.parameters([list, np.array, tf.constant, tf.ragged.constant])\n+  def test_lower(self, data_fn):\n+    input_array = data_fn([[\"Earth\", \"wInD\", \"aNd\", \"firE\"],\n+                           [\"fire|\", \"an<>d\", \"{earth}\", \"michigan@$\"]])\n+    expected_output = data_fn([[b\"earth\", b\"wind\", b\"and\", b\"fire\"],\n+                               [b\"fire|\", b\"an<>d\", b\"{earth}\", b\"michigan@$\"]])\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = text_vectorization.TextVectorization(\n+        max_tokens=None,\n+        standardize=text_vectorization.LOWER,\n         split=None,\n         ngrams=None,\n         output_mode=None)\n@@ -538,7 +559,7 @@ class TextVectorizationPreprocessingTest(\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_string_splitting(self):\n+  def test_whitespace_splitting(self):\n     input_array = np.array([[\"earth wind and fire\"],\n                             [\"\\tfire\\tand\\nearth    michigan  \"]])\n     expected_output = [[b\"earth\", b\"wind\", b\"and\", b\"fire\"],\n@@ -548,7 +569,25 @@ class TextVectorizationPreprocessingTest(\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n+        ngrams=None,\n+        output_mode=None)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_dataset = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_dataset)\n+\n+  def test_character_splitting(self):\n+    input_array = np.array([[\"earthwind\"],\n+                            [\"and fire\"]])\n+    expected_output = [[b\"e\", b\"a\", b\"r\", b\"t\", b\"h\", b\"w\", b\"i\", b\"n\", b\"d\"],\n+                       [b\"a\", b\"n\", b\"d\", b\" \", b\"f\", b\"i\", b\"r\", b\"e\"]]\n+\n+    input_data = keras.Input(shape=(1,), dtype=tf.string)\n+    layer = text_vectorization.TextVectorization(\n+        max_tokens=None,\n+        standardize=None,\n+        split=text_vectorization.CHARACTER,\n         ngrams=None,\n         output_mode=None)\n     int_data = layer(input_data)\n@@ -671,7 +710,7 @@ class TextVectorizationPreprocessingTest(\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         ngrams=2,\n         output_mode=None)\n     int_data = layer(input_data)\n@@ -685,7 +724,7 @@ class TextVectorizationPreprocessingTest(\n         vocabulary=[\"a\"],\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         output_mode=None)\n     with self.assertRaisesRegex(ValueError, \"last shape dimension must be 1\"):\n       _ = layer(input_data)\n@@ -696,18 +735,15 @@ class TextVectorizationPreprocessingTest(\n         vocabulary=[\"a\"],\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         output_mode=None)\n     with self.assertRaisesRegex(ValueError, \"last shape dimension must be 1\"):\n       _ = layer(input_data)\n \n   def test_standardization_with_invalid_standardize_arg(self):\n-    input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = text_vectorization.TextVectorization(vocabulary=[\"a\"])\n-    layer._standardize = \"unsupported\"\n-    with self.assertRaisesRegex(ValueError,\n-                                \".*is not a supported standardization.*\"):\n-      _ = layer(input_data)\n+    with self.assertRaisesRegex(ValueError, \"Unkown value for `standardize`\"):\n+      text_vectorization.TextVectorization(\n+          vocabulary=[\"a\"], standardize=\"unsupported\")\n \n   def test_splitting_with_invalid_split_arg(self):\n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n@@ -864,7 +900,7 @@ class TextVectorizationOutputTest(\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         output_mode=text_vectorization.INT)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n@@ -888,7 +924,7 @@ class TextVectorizationOutputTest(\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         output_mode=text_vectorization.INT,\n         ragged=True)\n     layer.set_vocabulary(vocab_data)\n@@ -916,7 +952,7 @@ class TextVectorizationOutputTest(\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         output_mode=text_vectorization.INT,\n         output_sequence_length=output_sequence_length)\n     layer.set_vocabulary(vocab_data)\n@@ -943,7 +979,7 @@ class TextVectorizationOutputTest(\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         output_mode=text_vectorization.INT,\n         output_sequence_length=output_sequence_length)\n     layer.set_vocabulary(vocab_data)\n@@ -970,7 +1006,7 @@ class TextVectorizationOutputTest(\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         output_mode=text_vectorization.INT,\n         output_sequence_length=output_sequence_length)\n     layer.set_vocabulary(vocab_data)\n@@ -1451,7 +1487,7 @@ class TextVectorizationModelBuildingTest(\n     layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n-        split=text_vectorization.SPLIT_ON_WHITESPACE,\n+        split=text_vectorization.WHITESPACE,\n         output_mode=text_vectorization.INT,\n         output_sequence_length=output_sequence_length)\n     layer.set_vocabulary(vocab_data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c2b3c3afe7bbc8589b2356ff7f39f9caaa7a223", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 16209 | Contributors (this commit): 116 | Commits (past 90d): 28 | Contributors (cumulative): 116 | DMM Complexity: 1.0\n\nDIFF:\n@@ -472,8 +472,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         A tensor if there is a single output, or\n         a list of tensors if there are more than one outputs.\n     \"\"\"\n-    raise NotImplementedError('When subclassing the `Model` class, you should '\n-                              'implement a `call()` method.')\n+    raise NotImplementedError('Unimplemented `tf.keras.Model.call()`: if you '\n+                              'intend to create a `Model` with the Functional '\n+                              'API, please provide `inputs` and `outputs` '\n+                              'arguments. Otherwise, subclass `Model` with an '\n+                              'overridden `call()` method.')\n \n   @traceback_utils.filter_traceback\n   def compile(self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c05994382eacce97141eb41ed1d9b91c5661cd4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 39 | Churn Cumulative: 546 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -77,7 +77,20 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     \"\"\"\n     raise NotImplementedError\n \n-  def _compute_gradients(self, loss, var_list, tape=None):\n+  def compute_gradients(self, loss, var_list, tape=None):\n+    \"\"\"Compute gradients of loss on trainable variables.\n+\n+    Args:\n+      loss: `Tensor` or callable. If a callable, `loss` should take no arguments\n+        and return the value to minimize.\n+      var_list: list or tuple of `Variable` objects to update to minimize\n+        `loss`.\n+      tape: (Optional) `tf.GradientTape`.\n+\n+    Returns:\n+      A list of (gradient, variable) pairs. Variable is always present, but\n+      gradient can be `None`.\n+    \"\"\"\n     if tape is None:\n       tape = tf.GradientTape()\n     if callable(loss):\n@@ -85,7 +98,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         tape.watch(var_list)\n         loss = loss()\n     grads = tape.gradient(loss, var_list)\n-    return grads, var_list\n+    return zip(grads, var_list)\n \n   def _clip_gradients(self, grads):\n     clipped_grads = []\n@@ -250,8 +263,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n \n     Args:\n       loss: `Tensor` or callable. If a callable, `loss` should take no arguments\n-        and return the value to minimize. If a `Tensor`, the `tape` argument\n-        must be passed.\n+        and return the value to minimize.\n       var_list: list or tuple of `Variable` objects to update to minimize\n         `loss`.\n       tape: (Optional) `tf.GradientTape`.\n@@ -259,8 +271,8 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     Returns:\n       None\n     \"\"\"\n-    grads, var_list = self._compute_gradients(loss, var_list, tape)\n-    self.apply_gradients(zip(grads, var_list))\n+    grads_and_vars = self.compute_gradients(loss, var_list, tape)\n+    self.apply_gradients(grads_and_vars)\n \n   def apply_gradients(self, grads_and_vars):\n     \"\"\"Apply gradients to variables.\n@@ -409,7 +421,18 @@ class Optimizer(_BaseOptimizer):\n       variable = variable._distributed_container()\n     return super(Optimizer, self)._var_key(variable)\n \n-  def _aggregate_gradients(self, grads_and_vars):\n+  def aggregate_gradients(self, grads_and_vars):\n+    \"\"\"Aggregate gradients on all devices.\n+\n+    By default we will perform reduce_sum of gradients across devices. Users can\n+    implement their own aggregation logic by overriding this method.\n+\n+    Args:\n+      grads_and_vars: List of (gradient, variable) pairs.\n+\n+    Returns:\n+      List of (gradient, variable) pairs.\n+    \"\"\"\n     return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n \n   def apply_gradients(self, grads_and_vars, skip_gradients_aggregation=False):\n@@ -429,7 +452,7 @@ class Optimizer(_BaseOptimizer):\n       RuntimeError: If called in a cross-replica context.\n     \"\"\"\n     if not skip_gradients_aggregation:\n-      grads_and_vars = self._aggregate_gradients(grads_and_vars)\n+      grads_and_vars = self.aggregate_gradients(grads_and_vars)\n     super().apply_gradients(grads_and_vars)\n \n   def _internal_apply_gradients(self, grads_and_vars):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38f4fbff9e3952c7e3f501e652e2dfe819f6962b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 124 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 8 | Methods Changed: 15 | Complexity Δ (Sum/Max): 12/11 | Churn Δ: 132 | Churn Cumulative: 2551 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -105,7 +105,8 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n     path = os.path.join(self.get_temp_dir(), 'model')\n     save.save_model(self.model, path, save_format='tf')\n     self.assert_saved_model(path)\n-    with self.assertRaisesRegex(ValueError, 'input shapes have not been set'):\n+    with self.assertRaisesRegex(\n+        ValueError, r'Model.*cannot be saved.*as opposed to `model.call\\(\\).*'):\n       save.save_model(self.subclassed_model, path, save_format='tf')\n     self.subclassed_model.predict(np.random.random((3, 5)))\n     save.save_model(self.subclassed_model, path, save_format='tf')\n\n@@ -16,6 +16,7 @@\n \n # pylint: disable=g-bad-import-order, g-direct-tensorflow-import\n import tensorflow.compat.v2 as tf\n+import keras\n \n import copy\n import os\n@@ -81,11 +82,23 @@ def model_call_inputs(model, keep_original_batch_size=False):\n \n \n def raise_model_input_error(model):\n+  if isinstance(model, keras.models.Sequential):\n     raise ValueError(\n-      f'Model {model} cannot be saved because the input shapes have not '\n-      f'been set. Usually, input shapes are automatically determined when '\n-      f'calling `.fit()` or `.predict()`. To manually set the shapes, call '\n-      f'`model.build(input_shape)')\n+        f'Model {model} cannot be saved because the input shape is not '\n+        'available. Please specify an input shape either by calling '\n+        '`build(input_shape)` directly, or by calling the model on actual '\n+        'data using `Model()`, `Model.fit()`, or `Model.predict()`.')\n+\n+  # If the model is not a `Sequential`, it is intended to be a subclassed model.\n+  raise ValueError(\n+      f'Model {model} cannot be saved either because the input shape is not '\n+      'available or because the forward pass of the model is not defined.'\n+      'To define a forward pass, please override `Model.call()`. To specify '\n+      'an input shape, either call `build(input_shape)` directly, or call '\n+      'the model on actual data using `Model()`, `Model.fit()`, or '\n+      '`Model.predict()`. If you have a custom training step, please make '\n+      'sure to invoke the forward pass in train step through '\n+      '`Model.__call__`, i.e. `model(inputs)`, as opposed to `model.call()`.')\n \n \n def trace_model_call(model, input_signature=None):\n\n@@ -49,7 +49,7 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n     inputs = tf.ones((8, 5))\n \n     if input_dim is None:\n-      with self.assertRaisesRegex(ValueError, 'input shapes have not been set'):\n+      with self.assertRaisesRegex(ValueError, '.*input shape is not availabl*'):\n         saving_utils.trace_model_call(model)\n       model._set_inputs(inputs)\n \n@@ -110,7 +110,7 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n         np.random.random((10, input_dim)).astype(np.float32))\n \n     if testing_utils.get_model_type() == 'subclass':\n-      with self.assertRaisesRegex(ValueError, 'input shapes have not been set'):\n+      with self.assertRaisesRegex(ValueError, '.*input shape is not availabl*'):\n         saving_utils.trace_model_call(model)\n \n     model.compile(\n@@ -167,7 +167,7 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n     model = testing_utils.get_small_sequential_mlp(10, 3, None)\n     inputs = tf.ones((8, 5))\n \n-    with self.assertRaisesRegex(ValueError, 'input shapes have not been set'):\n+    with self.assertRaisesRegex(ValueError, '.*input shape is not availabl*'):\n       saving_utils.trace_model_call(model)\n \n     fn = saving_utils.trace_model_call(\n@@ -394,5 +394,107 @@ class ExtractModelMetricsTest(keras_parameterized.TestCase):\n       self.assertEqual(set(extract_metric_names), set(extract_metrics.keys()))\n \n \n+class UnbuiltModelSavingErrorMessageTest(keras_parameterized.TestCase):\n+\n+  def setUp(self):\n+    super(UnbuiltModelSavingErrorMessageTest, self).setUp()\n+    if not tf.__internal__.tf2.enabled():\n+      self.skipTest('The test does not intend to cover TF1.')\n+\n+  def test_sequential(self):\n+    model = sequential.Sequential([keras.layers.Dense(10)])\n+    optimizer = gradient_descent.SGD()\n+    model.compile(optimizer, loss='mse', steps_per_execution=10)\n+\n+    # Forward pass not called yet. Input shape not available and thus error.\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Model.*cannot be saved.*specify an input shape either by calling.*'):\n+      model.save(os.path.join(self.get_temp_dir(), 'my_saved_model'))\n+\n+  def test_functional(self):\n+    inputs = keras.Input(shape=(32,))\n+    outputs = keras.layers.Dense(1)(inputs)\n+    model = keras.Model(inputs, outputs)\n+    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n+\n+    x = np.random.random((1000, 32))\n+    y = np.random.random((1000, 1))\n+    model.fit(x, y, epochs=3)\n+\n+    # Functional model always has an input shape, so should save just fine.\n+    model.save(os.path.join(self.get_temp_dir(), 'my_saved_model'))\n+\n+  def test_subclass_forward_pass_by_layer_underscore_call(self):\n+\n+    class CustomModel(keras.Model):\n+\n+      def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.dense1 = keras.layers.Dense(1)\n+\n+      def train_step(self, data):\n+        x, y = data\n+        with tf.GradientTape() as tape:\n+          y_pred = self.dense1(x, training=True)\n+          loss = self.compiled_loss(y, y_pred)\n+\n+        gradients = tape.gradient(loss, self.trainable_variables)\n+        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n+        return {}\n+\n+    subclassed_model = CustomModel()\n+    subclassed_model.compile(optimizer='adam', loss='mse')\n+\n+    x = np.random.random((1000, 32))\n+    y = np.random.random((1000, 1))\n+    subclassed_model.fit(x, y, epochs=1)\n+\n+    # Saving of this subclassed model is supposed to raise an error, even if\n+    # `fit` has been called. This is because the model does not have `call()`\n+    # overridden. Forward pass using `layer.__call__` works for training, but\n+    # saving requires that `call()` be used.\n+    with self.assertRaisesRegex(\n+        ValueError, r'Model.*cannot be saved.*as opposed to `model.call\\(\\).*'):\n+      subclassed_model.save(os.path.join(self.get_temp_dir(), 'my_saved_model'))\n+\n+  def test_subclass_forward_pass_by_model_call(self):\n+\n+    class CustomModel(keras.Model):\n+\n+      def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.dense1 = keras.layers.Dense(1)\n+\n+      def call(self, inputs):\n+        return self.dense1(inputs)\n+\n+      def train_step(self, data):\n+        x, y = data\n+        with tf.GradientTape() as tape:\n+          y_pred = self.call(x)\n+          loss = self.compiled_loss(y, y_pred)\n+\n+        gradients = tape.gradient(loss, self.trainable_variables)\n+        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n+        return {}\n+\n+    subclassed_model = CustomModel()\n+    subclassed_model.compile(optimizer='adam', loss='mse')\n+\n+    x = np.random.random((1000, 32))\n+    y = np.random.random((1000, 1))\n+    subclassed_model.fit(x, y, epochs=1)\n+\n+    # Saving of this subclassed model is supposed to raise an error, even if\n+    # `fit` has been called. This is because the model has `call()` overridden,\n+    # but the forward pass uses `Model.call` as opposed to `Model.__call__`, and\n+    # as a result the `Model` is not really built. The error message hints the\n+    # user to use `Model.__call__`, i.e., `Model(inputs)` instead.\n+    with self.assertRaisesRegex(\n+        ValueError, r'Model.*cannot be saved.*as opposed to `model.call\\(\\).*'):\n+      subclassed_model.save(os.path.join(self.get_temp_dir(), 'my_saved_model'))\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a49764cea6f33eee835ad9932e4d1309bce36531", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1931 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -395,6 +395,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n       net = df.DenseFeatures([price1, price2])(features)\n       with _initialized_session() as sess:\n         with self.assertRaisesRegex(tf.errors.OpError,\n+                                    'Dimension 0 in both shapes must be equal|'\n                                     'Dimensions of inputs should match'):\n           sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})\n \n\n@@ -380,6 +380,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n       net = df.DenseFeatures([price1, price2])(features)\n       with _initialized_session() as sess:\n         with self.assertRaisesRegex(tf.errors.OpError,\n+                                    'Dimension 0 in both shapes must be equal|'\n                                     'Dimensions of inputs should match'):\n           sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bda64d14739f4d232cea179212189f62ba988df0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 288 | Lines Deleted: 46 | Files Changed: 2 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): 11/6 | Churn Δ: 334 | Churn Cumulative: 12278 | Contributors (this commit): 27 | Commits (past 90d): 9 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,13 +16,11 @@\n # pylint: disable=g-doc-return-or-yield\n \"\"\"Built-in metrics.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import abc\n import types\n+from typing import List, Tuple, Union\n import warnings\n \n-import numpy as np\n from keras import activations\n from keras import backend\n from keras.engine import base_layer\n@@ -49,6 +47,9 @@ from keras.utils.generic_utils import deserialize_keras_object\n from keras.utils.generic_utils import serialize_keras_object\n from keras.utils.generic_utils import to_list\n from keras.utils.tf_utils import is_tensor_or_variable\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n@@ -2998,57 +2999,37 @@ class KLDivergence(MeanMetricWrapper):\n         kullback_leibler_divergence, name, dtype=dtype)\n \n \n-@keras_export('keras.metrics.MeanIoU')\n-class MeanIoU(Metric):\n-  \"\"\"Computes the mean Intersection-Over-Union metric.\n+class _IoUBase(Metric):\n+  \"\"\"Computes the confusion matrix for Intersection-Over-Union metrics.\n \n-  Mean Intersection-Over-Union is a common evaluation metric for semantic image\n-  segmentation, which first computes the IOU for each semantic class and then\n-  computes the average over classes. IOU is defined as follows:\n-    IOU = true_positive / (true_positive + false_positive + false_negative).\n-  The predictions are accumulated in a confusion matrix, weighted by\n-  `sample_weight` and the metric is then calculated from it.\n+  Intersection-Over-Union is a common evaluation metric for semantic image\n+  segmentation.\n+\n+  For an individual class, the IoU metric is defined as follows:\n+\n+  ```\n+  iou = true_positives / (true_positives + false_positives + false_negatives)\n+  ```\n+\n+  From IoUs of individual classes, the MeanIoU can be computed as the mean of\n+  the individual IoUs.\n+\n+  To compute IoUs, the predictions are accumulated in a confusion matrix,\n+  weighted by `sample_weight` and the metric is then calculated from it.\n \n   If `sample_weight` is `None`, weights default to 1.\n   Use `sample_weight` of 0 to mask values.\n \n   Args:\n     num_classes: The possible number of labels the prediction task can have.\n-      This value must be provided, since a confusion matrix of dimension =\n-      [num_classes, num_classes] will be allocated.\n+      This value must be provided, since a confusion matrix of size\n+      `(num_classes, num_classes)` will be allocated.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n-\n-  Standalone usage:\n-\n-  >>> # cm = [[1, 1],\n-  >>> #        [1, 1]]\n-  >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n-  >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n-  >>> # result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33\n-  >>> m = tf.keras.metrics.MeanIoU(num_classes=2)\n-  >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n-  >>> m.result().numpy()\n-  0.33333334\n-\n-  >>> m.reset_state()\n-  >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1],\n-  ...                sample_weight=[0.3, 0.3, 0.3, 0.1])\n-  >>> m.result().numpy()\n-  0.23809525\n-\n-  Usage with `compile()` API:\n-\n-  ```python\n-  model.compile(\n-    optimizer='sgd',\n-    loss='mse',\n-    metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])\n-  ```\n   \"\"\"\n \n   def __init__(self, num_classes, name=None, dtype=None):\n-    super(MeanIoU, self).__init__(name=name, dtype=dtype)\n+    super(_IoUBase, self).__init__(name=name, dtype=dtype)\n     self.num_classes = num_classes\n \n     # Variable to accumulate the predictions in the confusion matrix.\n@@ -3095,6 +3076,182 @@ class MeanIoU(Metric):\n         dtype=self._dtype)\n     return self.total_cm.assign_add(current_cm)\n \n+  def reset_state(self):\n+    backend.set_value(\n+        self.total_cm, np.zeros((self.num_classes, self.num_classes)))\n+\n+\n+@keras_export('keras.metrics.IoU')\n+class IoU(_IoUBase):\n+  \"\"\"Computes the Intersection-Over-Union metric for a specific target classes.\n+\n+  Intersection-Over-Union is a common evaluation metric for semantic image\n+  segmentation.\n+\n+  For an individual class, the IoU metric is defined as follows:\n+\n+  ```\n+  iou = true_positives / (true_positives + false_positives + false_negatives)\n+  ```\n+\n+  To compute IoUs, the predictions are accumulated in a confusion matrix,\n+  weighted by `sample_weight` and the metric is then calculated from it.\n+\n+  If `sample_weight` is `None`, weights default to 1.\n+  Use `sample_weight` of 0 to mask values.\n+\n+  Note, this class first computes IoUs for all individual classes, then returns\n+  a 1D tensor of IoUs for the classes that are specified by `target_class_ids`.\n+\n+  Args:\n+    num_classes: The possible number of labels the prediction task can have.\n+      A confusion matrix of dimension = [num_classes, num_classes] will be\n+      allocated to accumulate predictions from which the metric is calculated.\n+    target_class_ids: A tuple or list of target class ids for which the metric\n+      is returned.\n+    name: (Optional) string name of the metric instance.\n+    dtype: (Optional) data type of the metric result.\n+\n+  Standalone usage:\n+\n+  >>> # cm = [[1, 1],\n+  >>> #        [1, 1]]\n+  >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n+  >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n+  >>> # iou = [0.33, 0.33]\n+  >>> m = tf.keras.metrics.IoU(num_classes=2, target_class_id=0)\n+  >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n+  >>> m.result().numpy()\n+  0.33333334\n+\n+  >>> m.reset_state()\n+  >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1],\n+  ...                sample_weight=[0.3, 0.3, 0.3, 0.1])\n+  >>> # cm = [[0.3, 0.3],\n+  >>> #        [0.3, 0.1]]\n+  >>> # sum_row = [0.6, 0.4], sum_col = [0.6, 0.4], true_positives = [0.3, 0.1]\n+  >>> # iou = [0.33, 0.14]\n+  >>> m.result().numpy()\n+  0.33\n+\n+  Usage with `compile()` API:\n+\n+  ```python\n+  model.compile(\n+    optimizer='sgd',\n+    loss='mse',\n+    metrics=[tf.keras.metrics.IoU(num_classes=2, target_class_id=0)])\n+  ```\n+  \"\"\"\n+\n+  def __init__(\n+      self,\n+      num_classes: int,\n+      target_class_ids: Union[List[int], Tuple[int, ...]],\n+      name=None,\n+      dtype=None,\n+  ):\n+    super(IoU, self).__init__(\n+        name=name,\n+        num_classes=num_classes,\n+        dtype=dtype,\n+    )\n+    if max(target_class_ids) >= num_classes:\n+      raise ValueError(\n+          f'Target class id {max(target_class_ids)} is out of range, which is '\n+          f'[{0}, {num_classes}).')\n+    self.target_class_ids = list(target_class_ids)\n+\n+  def result(self):\n+    \"\"\"Compute the intersection-over-union via the confusion matrix.\"\"\"\n+    sum_over_row = tf.cast(\n+        tf.reduce_sum(self.total_cm, axis=0), dtype=self._dtype)\n+    sum_over_col = tf.cast(\n+        tf.reduce_sum(self.total_cm, axis=1), dtype=self._dtype)\n+    true_positives = tf.cast(\n+        tf.linalg.tensor_diag_part(self.total_cm), dtype=self._dtype)\n+\n+    # sum_over_row + sum_over_col =\n+    #     2 * true_positives + false_positives + false_negatives.\n+    denominator = sum_over_row + sum_over_col - true_positives\n+\n+    iou = tf.math.divide_no_nan(true_positives, denominator)\n+\n+    return tf.gather(iou, self.target_class_ids, name='iou')\n+\n+  def get_config(self):\n+    config = {\n+        'num_classes': self.num_classes,\n+        'target_class_ids': self.target_class_ids,\n+    }\n+    base_config = super(IoU, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export('keras.metrics.MeanIoU')\n+class MeanIoU(_IoUBase):\n+  \"\"\"Computes the mean Intersection-Over-Union metric.\n+\n+  Intersection-Over-Union is a common evaluation metric for semantic image\n+  segmentation.\n+\n+  For an individual class, the IoU metric is defined as follows:\n+\n+  ```\n+  iou = true_positives / (true_positives + false_positives + false_negatives)\n+  ```\n+\n+  To compute IoUs, the predictions are accumulated in a confusion matrix,\n+  weighted by `sample_weight` and the metric is then calculated from it.\n+\n+  If `sample_weight` is `None`, weights default to 1.\n+  Use `sample_weight` of 0 to mask values.\n+\n+  Note that this class first computes IoUs for all individual classes, then\n+  returns the mean of these values.\n+\n+  Args:\n+    num_classes: The possible number of labels the prediction task can have.\n+      This value must be provided, since a confusion matrix of dimension =\n+      [num_classes, num_classes] will be allocated.\n+    name: (Optional) string name of the metric instance.\n+    dtype: (Optional) data type of the metric result.\n+\n+  Standalone usage:\n+\n+  >>> # cm = [[1, 1],\n+  >>> #        [1, 1]]\n+  >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n+  >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n+  >>> # result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33\n+  >>> m = tf.keras.metrics.MeanIoU(num_classes=2)\n+  >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n+  >>> m.result().numpy()\n+  0.33333334\n+\n+  >>> m.reset_state()\n+  >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1],\n+  ...                sample_weight=[0.3, 0.3, 0.3, 0.1])\n+  >>> m.result().numpy()\n+  0.23809525\n+\n+  Usage with `compile()` API:\n+\n+  ```python\n+  model.compile(\n+    optimizer='sgd',\n+    loss='mse',\n+    metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])\n+  ```\n+  \"\"\"\n+\n+  def __init__(self, num_classes, name=None, dtype=None):\n+    super(MeanIoU, self).__init__(\n+        name=name,\n+        num_classes=num_classes,\n+        dtype=dtype,\n+    )\n+\n   def result(self):\n     \"\"\"Compute the mean intersection-over-union via the confusion matrix.\"\"\"\n     sum_over_row = tf.cast(\n@@ -3119,10 +3276,6 @@ class MeanIoU(Metric):\n     return tf.math.divide_no_nan(\n         tf.reduce_sum(iou, name='mean_iou'), num_valid_entries)\n \n-  def reset_state(self):\n-    backend.set_value(\n-        self.total_cm, np.zeros((self.num_classes, self.num_classes)))\n-\n   def get_config(self):\n     config = {'num_classes': self.num_classes}\n     base_config = super(MeanIoU, self).get_config()\n\n@@ -1283,6 +1283,95 @@ class MeanRelativeErrorTest(tf.test.TestCase):\n     self.assertEqual(self.evaluate(result), 0)\n \n \n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class IoUTest(tf.test.TestCase):\n+\n+  def test_config(self):\n+    obj = metrics.IoU(\n+        num_classes=2, target_class_ids=[1, 0], name='iou_class_1')\n+    self.assertEqual(obj.name, 'iou_class_1')\n+    self.assertEqual(obj.num_classes, 2)\n+    self.assertEqual(obj.target_class_ids, [1, 0])\n+\n+    obj2 = metrics.IoU.from_config(obj.get_config())\n+    self.assertEqual(obj2.name, 'iou_class_1')\n+    self.assertEqual(obj2.num_classes, 2)\n+    self.assertEqual(obj2.target_class_ids, [1, 0])\n+\n+  def test_unweighted(self):\n+    y_pred = [0, 1, 0, 1]\n+    y_true = [0, 0, 1, 1]\n+\n+    obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n+    self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+\n+    result = obj(y_true, y_pred)\n+\n+    # cm = [[1, 1],\n+    #       [1, 1]]\n+    # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n+    # iou = true_positives / (sum_row + sum_col - true_positives))\n+    expected_result = tf.constant([1 / (2 + 2 - 1), 1 / (2 + 2 - 1)])\n+    self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+  def test_weighted(self):\n+    y_pred = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n+    y_true = tf.constant([0, 0, 1, 1])\n+    sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n+\n+    obj = metrics.IoU(num_classes=2, target_class_ids=[1, 0])\n+    self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+\n+    result = obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+    # cm = [[0.2, 0.3],\n+    #       [0.4, 0.1]]\n+    # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+    # iou = true_positives / (sum_row + sum_col - true_positives))\n+    expected_result = tf.constant(\n+        [0.1 / (0.4 + 0.5 - 0.1), 0.2 / (0.6 + 0.5 - 0.2)])\n+    self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+  def test_multi_dim_input(self):\n+    y_pred = tf.constant([[0, 1], [0, 1]], dtype=tf.float32)\n+    y_true = tf.constant([[0, 0], [1, 1]])\n+    sample_weight = tf.constant([[0.2, 0.3], [0.4, 0.1]])\n+\n+    obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n+    self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+\n+    result = obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+    # cm = [[0.2, 0.3],\n+    #       [0.4, 0.1]]\n+    # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n+    # iou = true_positives / (sum_row + sum_col - true_positives))\n+    expected_result = tf.constant(\n+        [0.2 / (0.6 + 0.5 - 0.2), 0.1 / (0.4 + 0.5 - 0.1)])\n+    self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+  def test_zero_valid_entries(self):\n+    obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n+    self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+    self.assertAllClose(\n+        self.evaluate(obj.result()), tf.constant([0, 0]), atol=1e-3)\n+\n+  def test_zero_and_non_zero_entries(self):\n+    y_pred = tf.constant([1], dtype=tf.float32)\n+    y_true = tf.constant([1])\n+\n+    obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n+    self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+    result = obj(y_true, y_pred)\n+\n+    # cm = [[0, 0],\n+    #       [0, 1]]\n+    # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n+    # iou = true_positives / (sum_row + sum_col - true_positives))\n+    expected_result = tf.constant([0, 1 / (1 + 1 - 1)])\n+    self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class MeanIoUTest(tf.test.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
