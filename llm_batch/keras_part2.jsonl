{"custom_id": "keras#a1c9d0823cd284213c884be0964cd47c47d5e666", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 15 | Churn Cumulative: 20 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,21 @@ def lecun_uniform(shape):\n     scale = 1./np.sqrt(m)\n     return uniform(shape, scale)\n \n+def glorot_normal(shape):\n+    ''' Reference: Glorot & Bengio, AISTATS 2010\n+    '''\n+    fan_in = shape[0] if len(shape) == 2 else np.prod(shape[1:])\n+    fan_out = shape[1] if len(shape) == 2 else shape[0]\n+    s = np.sqrt(2. / (fan_in + fan_out)\n+    return normal(shape, s)\n+\n+def he_normal(shape):\n+    ''' Reference:  He et al., http://arxiv.org/abs/1502.01852\n+    '''\n+    fan_in = shape[1] if len(shape) == 2 else np.prod(shape[1:])\n+    s = np.sqrt(2. / fan_in)\n+    return normal(shape, s)\n+\n def orthogonal(shape, scale=1.1):\n     ''' From Lasagne\n     '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1df04c1376a20dbe1a2f15f20d2d34a9462454a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,7 @@ def glorot_normal(shape):\n     '''\n     fan_in = shape[0] if len(shape) == 2 else np.prod(shape[1:])\n     fan_out = shape[1] if len(shape) == 2 else shape[0]\n-    s = np.sqrt(2. / (fan_in + fan_out)\n+    s = np.sqrt(2. / (fan_in + fan_out))\n     return normal(shape, s)\n \n def he_normal(shape):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#333cf3dfe2b716f46040bb579afde9bbdd5938af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 51 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -152,7 +152,7 @@ class TimeDistributedDense(Layer):\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n \n-        self.input = T.matrix()\n+        self.input = T.tensor3()\n         self.W = self.init((self.input_dim, self.output_dim))\n         self.b = shared_zeros((self.output_dim))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0af6b6c7f5cbad394673bc962dd248f50fd821ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 102 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n import numpy as np\n import random\n \n-def pad_sequences(seqs, maxlen=None, dtype='int32'):\n+def pad_sequences(sequences, maxlen=None, dtype='int32'):\n     \"\"\"\n         Pad each sequence to the same lenght: \n         the lenght of the longuest sequence.\n@@ -9,16 +9,15 @@ def pad_sequences(seqs, maxlen=None, dtype='int32'):\n         If maxlen is provided, any sequence longer\n         than maxlen is truncated to maxlen.\n     \"\"\"\n-    lengths = [len(s) for s in seqs]\n+    lengths = [len(s) for s in sequences]\n \n-    nb_samples = len(seqs)\n+    nb_samples = len(sequences)\n     if maxlen is None:\n         maxlen = np.max(lengths)\n \n     x = np.zeros((nb_samples, maxlen)).astype(dtype)\n-    for idx, s in enumerate(seqs):\n+    for idx, s in enumerate(sequences):\n         x[idx, :lengths[idx]] = s[:maxlen]\n-\n     return x\n \n \n@@ -45,7 +44,8 @@ def make_sampling_table(size, sampling_factor=1e-5):\n \n \n def skipgrams(sequence, vocabulary_size, \n-    window_size=4, negative_samples=1., shuffle=True, categorical=False, seed=None, sampling_table=None):\n+    window_size=4, negative_samples=1., shuffle=True, \n+    categorical=False, sampling_table=None):\n     ''' \n         Take a sequence (list of indexes of words), \n         returns couples of [word_index, other_word index] and labels (1s or 0s),\n@@ -94,7 +94,6 @@ def skipgrams(sequence, vocabulary_size,\n             labels += [0]*nb_negative_samples\n \n     if shuffle:\n-        if not seed:\n         seed = random.randint(0,10e6)\n         random.seed(seed)\n         random.shuffle(couples)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b3e44484ba10f2a4a3020dd223447cd66a5519f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 45 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,16 +23,17 @@ def text_to_word_sequence(text, filters=base_filter(), lower=True, split=\" \"):\n     return filter(None, seq)\n \n \n-def one_hot(text, n):\n+def one_hot(text, n, filters=base_filter(), lower=True, split=\" \"):\n     seq = text_to_word_sequence(text)\n-    return [abs(hash(w))%n for w in seq]\n+    return [(abs(hash(w))%(n-1)+1) for w in seq]\n \n \n class Tokenizer(object):\n-    def __init__(self, filters=base_filter(), lower=True, nb_words=None):\n+    def __init__(self, nb_words=None, filters=base_filter(), lower=True, split=\" \"):\n         self.word_counts = {}\n         self.word_docs = {}\n         self.filters = filters\n+        self.split = split\n         self.lower = lower\n         self.nb_words = nb_words\n         self.document_count = 0\n@@ -45,7 +46,7 @@ class Tokenizer(object):\n         self.document_count = 0\n         for text in texts:\n             self.document_count += 1\n-            seq = text_to_word_sequence(text, self.filters, self.lower)\n+            seq = text_to_word_sequence(text, self.filters, self.lower, self.split)\n             for w in seq:\n                 if w in self.word_counts:\n                     self.word_counts[w] += 1\n@@ -106,7 +107,7 @@ class Tokenizer(object):\n         '''\n         nb_words = self.nb_words\n         for text in texts:\n-            seq = text_to_word_sequence(text, self.filters, self.lower)\n+            seq = text_to_word_sequence(text, self.filters, self.lower, self.split)\n             vect = []\n             for w in seq:\n                 i = self.word_index.get(w)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b96fc2855a835e86b36c5255daab44f57af8624", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 90 | Lines Deleted: 25 | Files Changed: 4 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/6 | Churn Δ: 115 | Churn Cumulative: 307 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,8 @@ from keras.preprocessing import sequence\n from keras.optimizers import SGD, RMSprop, Adagrad\n from keras.utils import np_utils\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation, Embedding\n+from keras.layers.core import Dense, Dropout, Activation\n+from keras.layers.embeddings import Embedding\n from keras.layers.recurrent import LSTM, GRU\n from keras.datasets import imdb\n \n\n@@ -45,7 +45,7 @@ print \"Building model...\"\n model = Sequential()\n model.add(Dense(max_words, 256, init='normal'))\n model.add(Activation('relu'))\n-#model.add(BatchNormalization(input_shape=(256,))) # try without batch normalization (doesn't work as well!)\n+model.add(BatchNormalization(input_shape=(256,))) # try without batch normalization (doesn't work as well!)\n model.add(Dropout(0.5))\n model.add(Dense(256, nb_classes, init='normal'))\n model.add(Activation('softmax'))\n\n@@ -172,27 +172,4 @@ class TimeDistributedDense(Layer):\n                                 outputs_info=None)\n         return output.dimshuffle(1,0,2)\n \n-class Embedding(Layer):\n-    '''\n-        Turn a list of integers >=0 into a dense vector of fixed size. \n-        eg. [4, 50, 123, 26] -> [0.25, 0.1]\n-\n-        @input_dim: size of vocabulary (highest input integer + 1)\n-        @out_dim: size of dense representation\n-    '''\n-    def __init__(self, input_dim, output_dim, init='uniform', weights=None):\n-        self.init = initializations.get(init)\n-        self.input_dim = input_dim\n-        self.output_dim = output_dim\n-\n-        self.input = T.imatrix()\n-        self.W = self.init((self.input_dim, self.output_dim))\n-        self.params = [self.W]\n-\n-        if weights is not None:\n-            self.set_weights(weights)\n-\n-    def output(self, train):\n-        X = self.get_input(train)\n-        return self.W[X]\n \n\n@@ -0,0 +1,87 @@\n+import theano\n+import theano.tensor as T\n+\n+from .. import activations, initializations\n+from ..layers.core import Layer\n+\n+\n+class Embedding(Layer):\n+    '''\n+        Turn a list of integers >=0 into a dense vector of fixed size. \n+        eg. [4, 50, 123, 26] -> [0.25, 0.1]\n+\n+        @input_dim: size of vocabulary (highest input integer + 1)\n+        @out_dim: size of dense representation\n+    '''\n+    def __init__(self, input_dim, output_dim, init='uniform', weights=None):\n+        self.init = initializations.get(init)\n+        self.input_dim = input_dim\n+        self.output_dim = output_dim\n+        self.normalize = normalize\n+\n+        self.input = T.imatrix()\n+        self.W = self.init((self.input_dim, self.output_dim))\n+        self.params = [self.W]\n+\n+        if weights is not None:\n+            self.set_weights(weights)\n+\n+    def output(self, train=False):\n+        X = self.get_input(train)\n+        out = self.W[X]\n+        return out\n+\n+\n+class WordContextProduct(Layer):\n+    '''\n+        This layer turns a pair of words (a pivot word + a context word, \n+        ie. a word from the same context, or a random, out-of-context word),\n+        indentified by their index in a vocabulary, into two dense reprensentations\n+        (word representation and context representation).\n+\n+        Then it returns activation(dot(pivot_embedding, context_embedding)),\n+        which can be trained to encode the probability \n+        of finding the context word in the context of the pivot word\n+        (or reciprocally depending on your training procedure).\n+\n+        The layer ingests integer tensors of shape:\n+        (nb_samples, 2)\n+        and outputs a float tensor of shape\n+        (nb_samples, 1)\n+\n+        The 2nd dimension encodes (pivot, context).\n+        input_dim is the size of the vocabulary.\n+\n+        For more context, see Mikolov et al.:\n+            Efficient Estimation of Word reprensentations in Vector Space\n+            http://arxiv.org/pdf/1301.3781v3.pdf\n+    '''\n+    def __init__(self, input_dim, proj_dim=128, \n+        init='uniform', activation='sigmoid', weights=None):\n+        self.input_dim = input_dim\n+        self.proj_dim = proj_dim\n+        self.normalize = normalize\n+        self.init = initializations.get(init)\n+        self.activation = activations.get(activation)\n+\n+        self.input = T.imatrix()\n+        # two different embeddings for pivot word and its context\n+        # because p(w|c) != p(c|w)\n+        self.W_w = self.init((input_dim, proj_dim))\n+        self.W_c = self.init((input_dim, proj_dim))\n+\n+        self.params = [self.W_w, self.W_c]\n+\n+        if weights is not None:\n+            self.set_weights(weights)\n+\n+\n+    def output(self, train=False):\n+        X = self.get_input(train)\n+        w = self.W_w[X[:, 0]] # nb_samples, proj_dim\n+        c = self.W_c[X[:, 1]] # nb_samples, proj_dim\n+\n+        dot = T.sum(w * c, axis=1)\n+        dot = theano.tensor.reshape(dot, (X.shape[0], 1))\n+        return self.activation(dot)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c61c4ed885ed2b0f4b56b334f2a5c3e9b4fd0b53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 103 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+# -*- coding: utf-8 -*-\n import numpy as np\n import random\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#81bdb74ef34f2cb43997e5c1489cd5cb30d82fba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 209 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 209 | Churn Cumulative: 209 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,209 @@\n+\n+'''\n+    We loop over words in a dataset, and for each word, we look at a context window around the word. \n+    We generate pairs of (pivot_word, other_word_from_same_context) with label 1,\n+    and pairs of (pivot_word, random_word) with label 0 (skip-gram method).\n+\n+    We use the layer WordContextProduct to learn embeddings for the word couples,\n+    and compute a proximity score between the embeddings (= p(context|word)),\n+    trained with our positive and negative labels.\n+\n+    We then use the weights computed by WordContextProduct to instantiate\n+    a simple Embedding, which we use to encode words and demonstrate\n+    that the geometry of the embedding space captures certain useful semantic properties.\n+\n+    Read more about skip-gram in this particularly gnomic paper by Mikolov et al.: \n+        http://arxiv.org/pdf/1301.3781v3.pdf\n+\n+    Note: you should run this on GPU, otherwise training will be quite slow. \n+    On a EC2 GPU instance, expect 3 hours per 10e6 comments (~10e8 words) per epoch with dim_proj=256.\n+    Should be much faster on a modern GPU.\n+\n+    GPU command:\n+        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python skipgram_word_embeddings.py\n+\n+    Dataset: 5,845,908 Hacker News comments. \n+    Obtain the dataset at: \n+        https://mega.co.nz/#F!YohlwD7R!wec0yNO86SeaNGIYQBOR0A \n+        (HNCommentsAll.1perline.json.bz2)\n+'''\n+\n+import numpy as np\n+import theano\n+import cPickle\n+import os, re, json\n+\n+from keras.preprocessing import sequence, text\n+from keras.optimizers import SGD, RMSprop, Adagrad\n+from keras.utils import np_utils, generic_utils\n+from keras.models import Sequential\n+from keras.layers.embeddings import WordContextProduct, Embedding\n+\n+max_features = 50000 # vocabulary size: top 50,000 most common words in data\n+skip_top = 100 # ignore top 100 most common words\n+nb_epoch = 1\n+dim_proj = 256 # embedding space dimension\n+\n+save = False\n+load = False\n+train_model = True\n+save_dir = os.path.expanduser(\"~/.keras/models\")\n+model_load_fname = \"HN_skipgram_model_full_256.pkl\"\n+model_save_fname = \"HN_skipgram_model_full_256-0.pkl\"\n+tokenizer_fname = \"HN_tokenizer.pkl\"\n+\n+data_path = os.path.expanduser(\"~/\")+\"HNCommentsAll.1perline.json\"\n+\n+# text preprocessing utils\n+html_tags = re.compile(r'<.*?>')\n+to_replace = [('&#x27;', \"'\")]\n+hex_tags = re.compile(r'&.*?;')\n+\n+def clean_comment(comment):\n+    c = str(comment.encode(\"utf-8\"))\n+    c = html_tags.sub(' ', c)\n+    for tag, char in to_replace:\n+        c = c.replace(tag, char)\n+    c = hex_tags.sub(' ', c)\n+    return c\n+\n+def text_generator(path=data_path):\n+    f = open(path)\n+    for i, l in enumerate(f):\n+        comment_data = json.loads(l)\n+        comment_text = comment_data[\"comment_text\"]\n+        comment_text = clean_comment(comment_text)\n+        if i % 10000 == 0:\n+            print i\n+        yield comment_text\n+    f.close()\n+\n+# model management\n+if load:\n+    print 'Load tokenizer...'\n+    tokenizer = cPickle.load(open(os.path.join(save_dir, tokenizer_fname)))\n+    print 'Load model...'\n+    model = cPickle.load(open(os.path.join(save_dir, model_load_fname)))\n+else:\n+    print \"Fit tokenizer...\"\n+    tokenizer = text.Tokenizer(nb_words=max_features)\n+    tokenizer.fit_on_texts(text_generator())\n+    if save:\n+        print \"Save tokenizer...\"\n+        if not os.path.exists(save_dir):\n+            os.makedirs(save_dir)\n+        cPickle.dump(tokenizer, open(os.path.join(save_dir, tokenizer_fname), \"w\"))\n+\n+# training process\n+if train_model:\n+    if not load:\n+        print 'Build model...'\n+        model = Sequential()\n+        model.add(WordContextProduct(max_features, proj_dim=dim_proj, init=\"normal\"))\n+        model.compile(loss='mse', optimizer='rmsprop')\n+\n+    sampling_table = sequence.make_sampling_table(max_features)\n+\n+    for e in range(nb_epoch):\n+        print '-'*40\n+        print 'Epoch', e\n+        print '-'*40\n+\n+        progbar = generic_utils.Progbar(tokenizer.document_count)\n+        samples_seen = 0\n+        losses = []\n+        \n+        for i, seq in enumerate(tokenizer.texts_to_sequences_generator(text_generator())):\n+            # get skipgram couples for one text in the dataset\n+            couples, labels = sequence.skipgrams(seq, max_features, window_size=4, negative_samples=1., sampling_table=sampling_table)\n+            if couples:\n+                # one gradient update per sentence (one sentence = a few 1000s of word couples)\n+                X = np.array(couples, dtype=\"int32\")\n+                loss = model.train(X, labels)\n+                losses.append(loss)\n+                if len(losses) % 100 == 0:\n+                    progbar.update(i, values=[(\"loss\", np.mean(losses))])\n+                    losses = []\n+                samples_seen += len(labels)\n+        print 'Samples seen:', samples_seen\n+    print \"Training completed!\"\n+\n+    if save:\n+        print \"Saving model...\"\n+        if not os.path.exists(save_dir):\n+            os.makedirs(save_dir)\n+        cPickle.dump(model, open(os.path.join(save_dir, model_save_fname), \"w\"))\n+\n+\n+print \"It's test time!\"\n+\n+# recover the embedding weights trained with skipgram:\n+weights = model.layers[0].get_weights()[0]\n+\n+# we no longer need this\n+del model\n+\n+weights[:skip_top] = np.zeros((skip_top, dim_proj))\n+norm_weights = np_utils.normalize(weights)\n+\n+word_index = tokenizer.word_index\n+reverse_word_index = dict([(v, k) for k, v in word_index.items()])\n+word_index = tokenizer.word_index\n+\n+def embed_word(w):\n+    i = word_index.get(w)\n+    if (not i) or (i<skip_top) or (i>=max_features):\n+        return None\n+    return norm_weights[i]\n+\n+def closest_to_point(point, nb_closest=10):\n+    proximities = np.dot(norm_weights, point)\n+    tups = zip(range(len(proximities)), proximities)\n+    tups.sort(key=lambda x: x[1], reverse=True)\n+    return [(reverse_word_index.get(t[0]), t[1]) for t in tups[:nb_closest]]  \n+\n+def closest_to_word(word, nb_closest=10):\n+    i = word_index.get(w)\n+    if (not i) or (i<skip_top) or (i>=max_features):\n+        return []\n+    return closest_to_point(norm_weights[i].T, nb_closest)\n+\n+\n+''' the resuls in comments below were for: \n+    5.8M HN comments\n+    dim_proj = 256\n+    nb_epoch = 2\n+    optimizer = rmsprop\n+    loss = mse\n+    max_features = 50000\n+    skip_top = 100\n+    negative_samples = 1.\n+    window_size = 4\n+    and frequency subsampling of factor 10e-5. \n+'''\n+\n+words = [\"article\", # post, story, hn, read, comments\n+\"3\", # 6, 4, 5, 2\n+\"two\", # three, few, several, each\n+\"great\", # love, nice, working, looking\n+\"data\", # information, memory, database\n+\"money\", # company, pay, customers, spend\n+\"years\", # ago, year, months, hours, week, days\n+\"android\", # ios, release, os, mobible, beta\n+\"javascript\", # js, css, compiler, library, jquery, ruby\n+\"look\", # looks, looking\n+\"business\", # industry, professional, customers\n+\"company\", # companies, startup, founders, startups\n+\"after\", # before, once, until\n+\"own\", # personal, our, having\n+\"us\", # united, country, american, tech, diversity, usa, china, sv\n+\"using\", # javascript, js, tools (lol)\n+\"here\", # hn, post, comments\n+]\n+\n+for w in words:\n+    res = closest_to_word(w)\n+    print '====', w\n+    for r in res:\n+        print r\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b7a5d4de702b863eaa86d609da5b2c77a8c96240", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 219 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -8,9 +8,9 @@\n     and compute a proximity score between the embeddings (= p(context|word)),\n     trained with our positive and negative labels.\n \n-    We then use the weights computed by WordContextProduct to instantiate\n-    a simple Embedding, which we use to encode words and demonstrate\n-    that the geometry of the embedding space captures certain useful semantic properties.\n+    We then use the weights computed by WordContextProduct to encode words \n+    and demonstrate that the geometry of the embedding space \n+    captures certain useful semantic properties.\n \n     Read more about skip-gram in this particularly gnomic paper by Mikolov et al.: \n         http://arxiv.org/pdf/1301.3781v3.pdf\n@@ -49,7 +49,7 @@ load = False\n train_model = True\n save_dir = os.path.expanduser(\"~/.keras/models\")\n model_load_fname = \"HN_skipgram_model_full_256.pkl\"\n-model_save_fname = \"HN_skipgram_model_full_256-0.pkl\"\n+model_save_fname = \"HN_skipgram_model_full_256.pkl\"\n tokenizer_fname = \"HN_tokenizer.pkl\"\n \n data_path = os.path.expanduser(\"~/\")+\"HNCommentsAll.1perline.json\"\n@@ -162,7 +162,7 @@ def closest_to_point(point, nb_closest=10):\n     tups.sort(key=lambda x: x[1], reverse=True)\n     return [(reverse_word_index.get(t[0]), t[1]) for t in tups[:nb_closest]]  \n \n-def closest_to_word(word, nb_closest=10):\n+def closest_to_word(w, nb_closest=10):\n     i = word_index.get(w)\n     if (not i) or (i<skip_top) or (i>=max_features):\n         return []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7dcd4b789558507490b8d529ce11386d32ca3d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 312 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -189,7 +189,7 @@ words = [\"article\", # post, story, hn, read, comments\n \"data\", # information, memory, database\n \"money\", # company, pay, customers, spend\n \"years\", # ago, year, months, hours, week, days\n-\"android\", # ios, release, os, mobible, beta\n+\"android\", # ios, release, os, mobile, beta\n \"javascript\", # js, css, compiler, library, jquery, ruby\n \"look\", # looks, looking\n \"business\", # industry, professional, customers\n\n@@ -7,8 +7,8 @@ from ..layers.core import Layer\n \n class Embedding(Layer):\n     '''\n-        Turn a list of integers >=0 into a dense vector of fixed size. \n-        eg. [4, 50, 123, 26] -> [0.25, 0.1]\n+        Turn positive integers (indexes) into denses vectors of fixed size. \n+        eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n \n         @input_dim: size of vocabulary (highest input integer + 1)\n         @out_dim: size of dense representation\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e16891e5e1044cabbf058409c0b75d26428afd81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 93 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,7 +17,6 @@ class Embedding(Layer):\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n-        self.normalize = normalize\n \n         self.input = T.imatrix()\n         self.W = self.init((self.input_dim, self.output_dim))\n@@ -60,7 +59,6 @@ class WordContextProduct(Layer):\n         init='uniform', activation='sigmoid', weights=None):\n         self.input_dim = input_dim\n         self.proj_dim = proj_dim\n-        self.normalize = normalize\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f595fe7f7856ae9ea8a1e8439e1342757f0f3a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 45 | Churn Cumulative: 47 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -100,7 +100,9 @@ class Adagrad(Optimizer):\n \n \n class Adadelta(Optimizer):\n-    \n+    '''\n+        Reference: http://arxiv.org/abs/1212.5701\n+    '''\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n         self.__dict__.update(locals())\n \n@@ -125,11 +127,52 @@ class Adadelta(Optimizer):\n             updates.append((d_a, new_d_a))\n         return updates\n \n+\n+class Adam(Optimizer):\n+    '''\n+        Reference: http://arxiv.org/abs/1412.6980\n+\n+        Default parameters follow those provided in the original paper\n+\n+        lambda is renamed kappa.\n+    '''\n+    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8, *args, **kwargs):\n+        self.__dict__.update(locals())\n+        self.iterations = shared_scalar(0)\n+\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n+        updates = [(self.iterations, self.iterations+1.)]\n+\n+        i = self.iterations\n+        beta_1_t = self.beta_1 * (self.kappa**i)\n+\n+        # the update below seems missing from the paper, but is obviously required\n+        beta_2_t = self.beta_2 * (self.kappa**i) \n+\n+        for p, g in zip(params, grads):\n+            m = theano.shared(p.get_value() * 0.) # zero init of moment\n+            v = theano.shared(p.get_value() * 0.) # zero init of velocity\n+\n+            m_t = ((1. - self.beta_1) * m) + (self.beta_1 * g)\n+            v_t = (self.beta_2 * v) + (1 - self.beta_2) * (g**2)\n+\n+            m_b_t = m_t / (1 - beta_1_t)\n+            v_b_t = v_t / (1 - beta_2_t)\n+\n+            p_t = p - self.lr * m_b_t / (T.sqrt(v_t) + self.epsilon)\n+\n+            updates.append((m, m_t))\n+            updates.append((v, v_t))\n+            updates.append((p, p_t))\n+        return updates\n+\n # aliases\n sgd = SGD\n rmsprop = RMSprop\n adagrad = Adagrad\n adadelta = Adadelta\n+adam = Adam\n \n from utils.generic_utils import get_from_module\n def get(identifier):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2662d81a9c925501831f0acb805c9e8adcde0a32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 49 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -160,7 +160,7 @@ class Adam(Optimizer):\n             m_b_t = m_t / (1 - beta_1_t)\n             v_b_t = v_t / (1 - beta_2_t)\n \n-            p_t = p - self.lr * m_b_t / (T.sqrt(v_t) + self.epsilon)\n+            p_t = p - self.lr * m_b_t / (T.sqrt(v_b_t) + self.epsilon)\n \n             updates.append((m, m_t))\n             updates.append((v, v_t))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eda1232f163a7bba882184143fca13e2be78c379", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 82 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -24,6 +24,11 @@ from keras.datasets import imdb\n     - LSTM loss decrease during training can be quite different \n     from what you see with CNNs/MLPs/etc. It's more or less a sigmoid\n     instead of an inverse exponential.\n+\n+    GPU command:\n+        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n+\n+    180s/epoch on GPU (GT 650M), vs. 400s/epoch on CPU (2.4Ghz Core i7).\n '''\n \n max_features=20000\n@@ -50,10 +55,10 @@ model.add(Dense(128, 1))\n model.add(Activation('sigmoid'))\n \n # try using different optimizers and different optimizer configs\n-model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n+model.compile(loss='binary_crossentropy', optimizer='adam')\n \n print \"Train...\"\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10, verbose=1)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=5, verbose=1)\n score = model.evaluate(X_test, y_test, batch_size=batch_size)\n print 'Test score:', score\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb150bd06564b2d981f686bd3847b88941b58550", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 70 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.25\n\nDIFF:\n@@ -8,6 +8,7 @@ import time, copy\n from utils.generic_utils import Progbar\n \n def standardize_y(y):\n+    if not hasattr(y, 'shape'):\n         y = np.asarray(y)\n     if len(y.shape) == 1:\n         y = np.reshape(y, (len(y), 1))\n@@ -85,7 +86,10 @@ class Sequential(object):\n             for batch_index in range(0, nb_batch):\n                 batch_start = batch_index*batch_size\n                 batch_end = min(len(X), (batch_index+1)*batch_size)\n+                if shuffle:\n                     batch_ids = index_array[batch_start:batch_end]\n+                else:\n+                    batch_ids = slice(batch_start, batch_end)\n \n                 X_batch = X[batch_ids]\n                 y_batch = y[batch_ids]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#616537f3085aa00c41871a2226c4fdd4d6e8c669", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 119 | Lines Deleted: 48 | Files Changed: 1 | Hunks: 25 | Methods Changed: 15 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 167 | Churn Cumulative: 237 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 0.18518518518518517\n\nDIFF:\n@@ -13,6 +13,10 @@ def standardize_y(y):\n         y = np.reshape(y, (len(y), 1))\n     return y\n \n+def make_batches(size, batch_size):\n+    nb_batch = int(np.ceil(size/float(batch_size)))\n+    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n+\n class Sequential(object):\n     def __init__(self):\n         self.layers = []\n@@ -24,7 +28,7 @@ class Sequential(object):\n             self.layers[-1].connect(self.layers[-2])\n         self.params += [p for p in layer.params]\n \n-    def compile(self, optimizer, loss):\n+    def compile(self, optimizer, loss, class_mode=\"categorical\"):\n         self.optimizer = optimizers.get(optimizer)\n         self.loss = objectives.get(loss)\n \n@@ -34,38 +38,70 @@ class Sequential(object):\n         self.y_test = self.layers[-1].output(train=False)\n \n         # output of model\n-        self.Y = T.matrix() # TODO: support for custom output shapes\n+        self.y = T.matrix() # TODO: support for custom output shapes\n+\n+        train_loss = self.loss(self.y, self.y_train)\n+        test_score = self.loss(self.y, self.y_test)\n+\n+        if class_mode == \"categorical\":\n+            train_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1), T.argmax(self.y_train, axis=-1)))\n+            test_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1), T.argmax(self.y_test, axis=-1)))\n+\n+        elif class_mode == \"binary\":\n+            train_accuracy = T.mean(T.eq(self.y, T.round(self.y_train)))\n+            test_accuracy = T.mean(T.eq(self.y, T.round(self.y_test)))\n+        else:\n+            raise Exception(\"Invalid class mode:\" + str(class_mode))\n+        self.class_mode = class_mode\n \n-        train_loss = self.loss(self.Y, self.y_train)\n-        test_score = self.loss(self.Y, self.y_test)\n         updates = self.optimizer.get_updates(self.params, train_loss)\n \n-        self._train = theano.function([self.X, self.Y], train_loss, \n+        self._train = theano.function([self.X, self.y], train_loss, \n+            updates=updates, allow_input_downcast=True)\n+        self._train_with_acc = theano.function([self.X, self.y], [train_loss, train_accuracy], \n             updates=updates, allow_input_downcast=True)\n         self._predict = theano.function([self.X], self.y_test, \n             allow_input_downcast=True)\n-        self._test = theano.function([self.X, self.Y], test_score, \n+        self._test = theano.function([self.X, self.y], test_score, \n+            allow_input_downcast=True)\n+        self._test_with_acc = theano.function([self.X, self.y], [test_score, test_accuracy], \n             allow_input_downcast=True)\n \n-    def train(self, X, y):\n+    def train(self, X, y, accuracy=False):\n         y = standardize_y(y)\n-        loss = self._train(X, y)\n-        return loss\n+        if accuracy:\n+            return self._train_with_acc(X, y)\n+        else:\n+            return self._train(X, y)\n         \n-    def test(self, X, y):\n+\n+    def test(self, X, y, accuracy=False):\n         y = standardize_y(y)\n-        score = self._test(X, y)\n-        return score\n+        if accuracy:\n+            return self._test_with_acc(X, y)\n+        else:\n+            return self._test(X, y)\n+\n \n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1,\n-            validation_split=0., shuffle=True):\n-        # If a validation split size is given (e.g. validation_split=0.2)\n-        # then split X into smaller X and X_val,\n-        # and split y into smaller y and y_val.\n+            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False):\n         y = standardize_y(y)\n \n         do_validation = False\n-        if validation_split > 0 and validation_split < 1:\n+        if validation_data:\n+            try:\n+                X_val, y_val = validation_data\n+            except:\n+                raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val).\")\n+            do_validation = True\n+            y_val = standardize_y(y_val)\n+            if verbose:\n+                print \"Train on %d samples, validate on %d samples\" % (len(y), len(y_val))\n+        else:\n+            if 0 < validation_split < 1:\n+                # If a validation split size is given (e.g. validation_split=0.2)\n+                # then split X into smaller X and X_val,\n+                # and split y into smaller y and y_val.\n                 do_validation = True\n                 split_at = int(len(X) * (1 - validation_split))\n                 (X, X_val) = (X[0:split_at], X[split_at:])\n@@ -80,57 +116,92 @@ class Sequential(object):\n             if shuffle:\n                 np.random.shuffle(index_array)\n \n-            nb_batch = int(np.ceil(len(X)/float(batch_size)))\n+            batches = make_batches(len(X), batch_size)\n             progbar = Progbar(target=len(X))\n-            for batch_index in range(0, nb_batch):\n-                batch_start = batch_index*batch_size\n-                batch_end = min(len(X), (batch_index+1)*batch_size)\n+            for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n-\n                 X_batch = X[batch_ids]\n                 y_batch = y[batch_ids]\n+\n+                if show_accuracy:\n+                    loss, acc = self._train_with_acc(X_batch, y_batch)\n+                else:\n                     loss = self._train(X_batch, y_batch)\n                 \n+                # logging\n                 if verbose:\n-                    is_last_batch = (batch_index == nb_batch - 1)\n+                    is_last_batch = (batch_index == len(batches) - 1)\n                     if not is_last_batch or not do_validation:\n+                        if show_accuracy:\n+                            progbar.update(batch_end, [('loss', loss), ('acc.', acc)])\n+                        else:\n                             progbar.update(batch_end, [('loss', loss)])\n                     else:\n-                        progbar.update(batch_end, [('loss', loss), ('val. loss', self.test(X_val, y_val))])\n+                        if show_accuracy:\n+                            val_loss, val_acc = self.test(X_val, y_val, accuracy=True)\n+                            progbar.update(batch_end, [('loss', loss), ('acc.', acc), ('val. loss', val_loss), ('val. acc.', val_acc)])\n+                        else:\n+                            val_loss = self.test(X_val, y_val, accuracy=False)\n+                            progbar.update(batch_end, [('loss', loss), ('val. loss', val_loss)])\n \n-    def predict_proba(self, X, batch_size=128):\n-        for batch_index in range(0, len(X)/batch_size+1):\n-            batch = range(batch_index*batch_size, min(len(X), (batch_index+1)*batch_size))\n-            if not batch:\n-                break\n-            batch_preds = self._predict(X[batch])\n+            \n+    def predict_proba(self, X, batch_size=128, verbose=1):\n+        batches = make_batches(len(X), batch_size)\n+        if verbose:\n+            progbar = Progbar(target=len(X))\n+        for batch_index, (batch_start, batch_end) in enumerate(batches):\n+            X_batch = X[batch_start:batch_end]\n+            batch_preds = self._predict(X_batch)\n \n             if batch_index == 0:\n                 shape = (len(X),) + batch_preds.shape[1:]\n                 preds = np.zeros(shape)\n-            preds[batch] = batch_preds\n+            preds[batch_start:batch_end] = batch_preds\n+\n+            if verbose:\n+                progbar.update(batch_end)\n         return preds\n \n-    def predict_classes(self, X, batch_size=128):\n-        proba = self.predict_proba(X, batch_size=batch_size)\n-        # The last dimension is the one containing the class probas\n-        classes_dim = proba.ndim - 1\n-        if proba.shape[classes_dim] > 1:\n-            return proba.argmax(axis=classes_dim)\n+\n+    def predict_classes(self, X, batch_size=128, verbose=1):\n+        proba = self.predict_proba(X, batch_size=batch_size, verbose=verbose)\n+        if self.class_mode == \"categorical\":\n+            return proba.argmax(axis=-1)\n         else:\n             return (proba>0.5).astype('int32')\n \n-    def evaluate(self, X, y, batch_size=128):\n+\n+    def evaluate(self, X, y, batch_size=128, show_accuracy=False, verbose=1):\n         y = standardize_y(y)\n-        av_score = 0.\n-        samples = 0\n-        for batch_index in range(0, len(X)/batch_size+1):\n-            batch = range(batch_index*batch_size, min(len(X), (batch_index+1)*batch_size))\n-            if not batch:\n-                break\n-            score = self._test(X[batch], y[batch])\n-            av_score += len(batch)*score\n-            samples += len(batch)\n-        return av_score/samples\n+\n+        if show_accuracy:\n+            tot_acc = 0.\n+        tot_score = 0.\n+\n+        batches = make_batches(len(X), batch_size)\n+        progbar = Progbar(target=len(X))\n+        for batch_index, (batch_start, batch_end) in enumerate(batches):\n+            X_batch = X[batch_start:batch_end]\n+            y_batch = y[batch_start:batch_end]\n+\n+            if show_accuracy:\n+                loss, acc = self._test_with_acc(X_batch, y_batch)\n+                tot_acc += acc\n+            else:\n+                loss = self._test(X_batch, y_batch)\n+            tot_score += loss\n+\n+            if verbose:\n+                if show_accuracy:\n+                    progbar.update(batch_end, [('loss', loss), ('acc.', acc)])\n+                else:\n+                    progbar.update(batch_end, [('loss', loss)])\n+\n+        if show_accuracy:\n+            return tot_score/len(batches), tot_acc/len(batches)\n+        else:\n+            return tot_score/len(batches)\n+                \n+\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2635c32d4bf4e3e47d8120a4cf48ceb0b67c48c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 52 | Churn Cumulative: 52 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.358974358974359\n\nDIFF:\n@@ -0,0 +1,52 @@\n+import h5py, numpy\n+from collections import defaultdict\n+\n+class HDF5Matrix:\n+    \n+    refs = defaultdict(int)\n+\n+    def __init__(self, datapath, dataset, start, end, normalizer=None):\n+        if datapath not in self.refs.keys():\n+            f = h5py.File(datapath)\n+            self.refs[datapath] = f\n+        else:\n+            f = self.refs[datapath]\n+        self.start = start\n+        self.end = end\n+        self.data = f[dataset]\n+        self.normalizer = normalizer\n+    \n+    def __len__(self):\n+        return self.end - self.start\n+\n+    def __getitem__(self, key):\n+        if isinstance(key, slice):\n+            if key.stop + self.start <= self.end:\n+                idx = slice(key.start+self.start, key.stop + self.start)\n+            else:\n+                raise IndexError\n+        elif isinstance(key, int):\n+            if key + self.start < self.end:\n+                idx = key+self.start\n+            else:\n+                raise IndexError\n+        elif isinstance(key, numpy.ndarray):\n+            if numpy.max(key) + self.start < self.end:\n+                idx = (self.start + key).tolist()\n+            else:\n+                raise IndexError\n+        elif isinstance(key, list):\n+            if max(key) + self.start < self.end:\n+                idx = map(lambda x: x + self.start, key)\n+            else:\n+                raise IndexError\n+        if self.normalizer is not None:\n+            return self.normalizer(self.data[idx])\n+        else:\n+            return self.data[idx]\n+\n+    @property\n+    def shape(self):\n+        return tuple([self.end - self.start, self.data.shape[1]])\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1629f7f35becfd4d4350ff5b366998609414f830", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 162 | Lines Deleted: 112 | Files Changed: 24 | Hunks: 115 | Methods Changed: 15 | Complexity Δ (Sum/Max): 62/52 | Churn Δ: 274 | Churn Cumulative: 1543 | Contributors (this commit): 9 | Commits (past 90d): 102 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n from keras.datasets import cifar10\n from keras.preprocessing.image import ImageDataGenerator\n from keras.models import Sequential\n@@ -5,6 +7,7 @@ from keras.layers.core import Dense, Dropout, Activation, Flatten\n from keras.layers.convolutional import Convolution2D, MaxPooling2D\n from keras.optimizers import SGD, Adadelta, Adagrad\n from keras.utils import np_utils, generic_utils\n+from six.moves import range\n \n '''\n     Train a (fairly simple) deep CNN on the CIFAR10 small images dataset.\n@@ -14,6 +17,10 @@ from keras.utils import np_utils, generic_utils\n \n     It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n     (it's still underfitting at that point, though).\n+\n+    Note: the data was pickled with Python 2, and some encoding issues might prevent you\n+    from loading it in Python 3. You might have to load it in Python 2, \n+    save it in a different format, load it in Python 3 and repickle it.\n '''\n \n batch_size = 32\n@@ -23,8 +30,8 @@ data_augmentation = True\n \n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = cifar10.load_data(test_split=0.1)\n-print X_train.shape[0], 'train samples'\n-print X_test.shape[0], 'test samples'\n+print(X_train.shape[0], 'train samples')\n+print(X_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n Y_train = np_utils.to_categorical(y_train, nb_classes)\n@@ -59,7 +66,7 @@ sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n model.compile(loss='categorical_crossentropy', optimizer=sgd)\n \n if not data_augmentation:\n-    print \"Not using data augmentation or normalization\"\n+    print(\"Not using data augmentation or normalization\")\n \n     X_train = X_train.astype(\"float32\")\n     X_test = X_test.astype(\"float32\")\n@@ -67,10 +74,10 @@ if not data_augmentation:\n     X_test /= 255\n     model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=10)\n     score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n-    print 'Test score:', score\n+    print('Test score:', score)\n \n else:\n-    print \"Using real time data augmentation\"\n+    print(\"Using real time data augmentation\")\n \n     # this will do preprocessing and realtime data augmentation\n     datagen = ImageDataGenerator(\n@@ -90,17 +97,17 @@ else:\n     datagen.fit(X_train)\n \n     for e in range(nb_epoch):\n-        print '-'*40\n-        print 'Epoch', e\n-        print '-'*40\n-        print \"Training...\"\n+        print('-'*40)\n+        print('Epoch', e)\n+        print('-'*40)\n+        print(\"Training...\")\n         # batch train with realtime data augmentation\n         progbar = generic_utils.Progbar(X_train.shape[0])\n         for X_batch, Y_batch in datagen.flow(X_train, Y_train):\n             loss = model.train(X_batch, Y_batch)\n             progbar.add(X_batch.shape[0], values=[(\"train loss\", loss)])\n \n-        print \"Testing...\"\n+        print(\"Testing...\")\n         # test time!\n         progbar = generic_utils.Progbar(X_test.shape[0])\n         for X_batch, Y_batch in datagen.flow(X_test, Y_test):\n\n@@ -1,3 +1,5 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n import numpy as np\n \n from keras.preprocessing import sequence\n@@ -28,25 +30,25 @@ from keras.datasets import imdb\n     GPU command:\n         THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n \n-    180s/epoch on GPU (GT 650M), vs. 400s/epoch on CPU (2.4Ghz Core i7).\n+    250s/epoch on GPU (GT 650M), vs. 400s/epoch on CPU (2.4Ghz Core i7).\n '''\n \n max_features=20000\n maxlen = 100 # cut texts after this number of words (among top max_features most common words)\n batch_size = 16\n \n-print \"Loading data...\"\n+print(\"Loading data...\")\n (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n-print len(X_train), 'train sequences'\n-print len(X_test), 'test sequences'\n+print(len(X_train), 'train sequences')\n+print(len(X_test), 'test sequences')\n \n-print \"Pad sequences (samples x time)\"\n+print(\"Pad sequences (samples x time)\")\n X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n-print 'X_train shape:', X_train.shape\n-print 'X_test shape:', X_test.shape\n+print('X_train shape:', X_train.shape)\n+print('X_test shape:', X_test.shape)\n \n-print 'Build model...'\n+print('Build model...')\n model = Sequential()\n model.add(Embedding(max_features, 256))\n model.add(LSTM(256, 128)) # try using a GRU instead, for fun\n@@ -55,14 +57,14 @@ model.add(Dense(128, 1))\n model.add(Activation('sigmoid'))\n \n # try using different optimizers and different optimizer configs\n-model.compile(loss='binary_crossentropy', optimizer='adam')\n+model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=\"binary\")\n \n-print \"Train...\"\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=5, verbose=1)\n+print(\"Train...\")\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=5, validation_split=0.1, show_accuracy=True)\n score = model.evaluate(X_test, y_test, batch_size=batch_size)\n-print 'Test score:', score\n+print('Test score:', score)\n \n classes = model.predict_classes(X_test, batch_size=batch_size)\n acc = np_utils.accuracy(classes, y_test)\n-print 'Test accuracy:', acc\n+print('Test accuracy:', acc)\n \n\n@@ -1,3 +1,5 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n import numpy as np\n \n from keras.datasets import reuters\n@@ -20,28 +22,28 @@ from keras.preprocessing.text import Tokenizer\n max_words = 10000\n batch_size = 16\n \n-print \"Loading data...\"\n+print(\"Loading data...\")\n (X_train, y_train), (X_test, y_test) = reuters.load_data(nb_words=max_words, test_split=0.2)\n-print len(X_train), 'train sequences'\n-print len(X_test), 'test sequences'\n+print(len(X_train), 'train sequences')\n+print(len(X_test), 'test sequences')\n \n nb_classes = np.max(y_train)+1\n-print nb_classes, 'classes'\n+print(nb_classes, 'classes')\n \n-print \"Vectorizing sequence data...\"\n+print(\"Vectorizing sequence data...\")\n tokenizer = Tokenizer(nb_words=max_words)\n X_train = tokenizer.sequences_to_matrix(X_train, mode=\"binary\")\n X_test = tokenizer.sequences_to_matrix(X_test, mode=\"binary\")\n-print 'X_train shape:', X_train.shape\n-print 'X_test shape:', X_test.shape\n+print('X_train shape:', X_train.shape)\n+print('X_test shape:', X_test.shape)\n \n-print \"Convert class vector to binary class matrix (for use with categorical_crossentropy)\"\n+print(\"Convert class vector to binary class matrix (for use with categorical_crossentropy)\")\n Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n-print 'Y_train shape:', Y_train.shape\n-print 'Y_test shape:', Y_test.shape\n+print('Y_train shape:', Y_train.shape)\n+print('Y_test shape:', Y_test.shape)\n \n-print \"Building model...\"\n+print(\"Building model...\")\n model = Sequential()\n model.add(Dense(max_words, 256, init='normal'))\n model.add(Activation('relu'))\n@@ -50,14 +52,16 @@ model.add(Dropout(0.5))\n model.add(Dense(256, nb_classes, init='normal'))\n model.add(Activation('softmax'))\n \n-model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n \n-print \"Training...\"\n+model.compile(loss='categorical_crossentropy', optimizer='adam')\n+print(model.optimizer)\n+\n+print(\"Training...\")\n model.fit(X_train, Y_train, nb_epoch=5, batch_size=batch_size)\n score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n-print 'Test score:', score\n+print('Test score:', score)\n \n classes = model.predict_classes(X_test, batch_size=batch_size)\n acc = np_utils.accuracy(classes, y_test)\n-print 'Test accuracy:', acc\n+print('Test accuracy:', acc)\n \n\n@@ -27,10 +27,12 @@\n         https://mega.co.nz/#F!YohlwD7R!wec0yNO86SeaNGIYQBOR0A \n         (HNCommentsAll.1perline.json.bz2)\n '''\n+from __future__ import absolute_import\n+from __future__ import print_function\n \n import numpy as np\n import theano\n-import cPickle\n+import six.moves.cPickle\n import os, re, json\n \n from keras.preprocessing import sequence, text\n@@ -38,6 +40,8 @@ from keras.optimizers import SGD, RMSprop, Adagrad\n from keras.utils import np_utils, generic_utils\n from keras.models import Sequential\n from keras.layers.embeddings import WordContextProduct, Embedding\n+from six.moves import range\n+from six.moves import zip\n \n max_features = 50000 # vocabulary size: top 50,000 most common words in data\n skip_top = 100 # ignore top 100 most common words\n@@ -74,30 +78,30 @@ def text_generator(path=data_path):\n         comment_text = comment_data[\"comment_text\"]\n         comment_text = clean_comment(comment_text)\n         if i % 10000 == 0:\n-            print i\n+            print(i)\n         yield comment_text\n     f.close()\n \n # model management\n if load:\n-    print 'Load tokenizer...'\n-    tokenizer = cPickle.load(open(os.path.join(save_dir, tokenizer_fname)))\n-    print 'Load model...'\n-    model = cPickle.load(open(os.path.join(save_dir, model_load_fname)))\n+    print('Load tokenizer...')\n+    tokenizer = six.moves.cPickle.load(open(os.path.join(save_dir, tokenizer_fname)))\n+    print('Load model...')\n+    model = six.moves.cPickle.load(open(os.path.join(save_dir, model_load_fname)))\n else:\n-    print \"Fit tokenizer...\"\n+    print(\"Fit tokenizer...\")\n     tokenizer = text.Tokenizer(nb_words=max_features)\n     tokenizer.fit_on_texts(text_generator())\n     if save:\n-        print \"Save tokenizer...\"\n+        print(\"Save tokenizer...\")\n         if not os.path.exists(save_dir):\n             os.makedirs(save_dir)\n-        cPickle.dump(tokenizer, open(os.path.join(save_dir, tokenizer_fname), \"w\"))\n+        six.moves.cPickle.dump(tokenizer, open(os.path.join(save_dir, tokenizer_fname), \"w\"))\n \n # training process\n if train_model:\n     if not load:\n-        print 'Build model...'\n+        print('Build model...')\n         model = Sequential()\n         model.add(WordContextProduct(max_features, proj_dim=dim_proj, init=\"normal\"))\n         model.compile(loss='mse', optimizer='rmsprop')\n@@ -105,9 +109,9 @@ if train_model:\n     sampling_table = sequence.make_sampling_table(max_features)\n \n     for e in range(nb_epoch):\n-        print '-'*40\n-        print 'Epoch', e\n-        print '-'*40\n+        print('-'*40)\n+        print('Epoch', e)\n+        print('-'*40)\n \n         progbar = generic_utils.Progbar(tokenizer.document_count)\n         samples_seen = 0\n@@ -125,17 +129,17 @@ if train_model:\n                     progbar.update(i, values=[(\"loss\", np.mean(losses))])\n                     losses = []\n                 samples_seen += len(labels)\n-        print 'Samples seen:', samples_seen\n-    print \"Training completed!\"\n+        print('Samples seen:', samples_seen)\n+    print(\"Training completed!\")\n \n     if save:\n-        print \"Saving model...\"\n+        print(\"Saving model...\")\n         if not os.path.exists(save_dir):\n             os.makedirs(save_dir)\n-        cPickle.dump(model, open(os.path.join(save_dir, model_save_fname), \"w\"))\n+        six.moves.cPickle.dump(model, open(os.path.join(save_dir, model_save_fname), \"w\"))\n \n \n-print \"It's test time!\"\n+print(\"It's test time!\")\n \n # recover the embedding weights trained with skipgram:\n weights = model.layers[0].get_weights()[0]\n@@ -147,7 +151,7 @@ weights[:skip_top] = np.zeros((skip_top, dim_proj))\n norm_weights = np_utils.normalize(weights)\n \n word_index = tokenizer.word_index\n-reverse_word_index = dict([(v, k) for k, v in word_index.items()])\n+reverse_word_index = dict([(v, k) for k, v in list(word_index.items())])\n word_index = tokenizer.word_index\n \n def embed_word(w):\n@@ -158,7 +162,7 @@ def embed_word(w):\n \n def closest_to_point(point, nb_closest=10):\n     proximities = np.dot(norm_weights, point)\n-    tups = zip(range(len(proximities)), proximities)\n+    tups = list(zip(list(range(len(proximities))), proximities))\n     tups.sort(key=lambda x: x[1], reverse=True)\n     return [(reverse_word_index.get(t[0]), t[1]) for t in tups[:nb_closest]]  \n \n@@ -203,7 +207,7 @@ words = [\"article\", # post, story, hn, read, comments\n \n for w in words:\n     res = closest_to_word(w)\n-    print '====', w\n+    print('====', w)\n     for r in res:\n-        print r\n+        print(r)\n \n\n@@ -1,3 +1,4 @@\n+from __future__ import absolute_import\n import theano\n import theano.tensor as T\n import types\n@@ -28,6 +29,6 @@ def hard_sigmoid(x):\n def linear(x):\n     return x\n \n-from utils.generic_utils import get_from_module\n+from .utils.generic_utils import get_from_module\n def get(identifier):\n     return get_from_module(identifier, globals(), 'activation function')\n\\ No newline at end of file\n\n@@ -1,8 +1,10 @@\n-from data_utils import get_file\n+# -*- coding: utf-8 -*-\n+from __future__ import absolute_import\n+from .data_utils import get_file\n import random\n-import cPickle\n import numpy as np\n-from PIL import Image\n+import six.moves.cPickle\n+from six.moves import range\n \n def load_data(test_split=0.1, seed=113):\n     dirname = \"cifar-10-batches-py\"\n@@ -15,7 +17,7 @@ def load_data(test_split=0.1, seed=113):\n     for i in range(1, 6):\n         fpath = path + '/data_batch_' + str(i)\n         f = open(fpath, 'rb')\n-        d = cPickle.load(f)\n+        d = six.moves.cPickle.load(f)\n         f.close()\n         data = d[\"data\"]\n         labels = d[\"labels\"]\n\n@@ -1,3 +1,5 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n import urllib, tarfile\n import inspect, os\n from ..utils.generic_utils import Progbar\n@@ -16,7 +18,7 @@ def get_file(fname, origin, untar=False):\n     try:\n         f = open(fpath)\n     except:\n-        print 'Downloading data from',  origin\n+        print('Downloading data from',  origin)\n \n         global progbar\n         progbar = None\n@@ -32,7 +34,7 @@ def get_file(fname, origin, untar=False):\n \n     if untar:\n         if not os.path.exists(untar_fpath):\n-            print 'Untaring file...'\n+            print('Untaring file...')\n             tfile = tarfile.open(fpath, 'r:gz')\n             tfile.extractall(path=datadir)\n             tfile.close()\n\n@@ -1,7 +1,9 @@\n-import cPickle\n+from __future__ import absolute_import\n+import six.moves.cPickle\n import gzip\n-from data_utils import get_file\n+from .data_utils import get_file\n import random\n+from six.moves import zip\n \n def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113):\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/imdb.pkl\")\n@@ -11,7 +13,7 @@ def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0, maxlen=None, test_spli\n     else:\n         f = open(path, 'rb')\n \n-    X, labels = cPickle.load(f)\n+    X, labels = six.moves.cPickle.load(f)\n     f.close()\n \n     random.seed(seed)\n\n@@ -1,8 +1,11 @@\n # -*- coding: utf-8 -*-\n-from data_utils import get_file\n+from __future__ import absolute_import\n+from __future__ import print_function\n+from .data_utils import get_file\n import string\n import random\n-import cPickle\n+import six.moves.cPickle\n+from six.moves import zip\n \n def make_reuters_dataset(path='datasets/temp/reuters21578/', min_samples_per_topic=15):\n     import os\n@@ -34,15 +37,15 @@ def make_reuters_dataset(path='datasets/temp/reuters21578/', min_samples_per_top\n                 wire_bodies.append(body)\n \n     # only keep most common topics\n-    items = topic_counts.items()\n+    items = list(topic_counts.items())\n     items.sort(key = lambda x: x[1])\n     kept_topics = set()\n     for x in items:\n-        print x[0] + ': ' + str(x[1])\n+        print(x[0] + ': ' + str(x[1]))\n         if x[1] >= min_samples_per_topic:\n             kept_topics.add(x[0])\n-    print '-'\n-    print 'Kept topics:', len(kept_topics)\n+    print('-')\n+    print('Kept topics:', len(kept_topics))\n \n     # filter wires with rare topics\n     kept_wires = []\n@@ -64,15 +67,15 @@ def make_reuters_dataset(path='datasets/temp/reuters21578/', min_samples_per_top\n     tokenizer.fit_on_texts(kept_wires)\n     X = tokenizer.texts_to_sequences(kept_wires)\n \n-    print 'Sanity check:'\n+    print('Sanity check:')\n     for w in [\"banana\", \"oil\", \"chocolate\", \"the\", \"dsft\"]:\n-        print '...index of', w, ':', tokenizer.word_index.get(w)\n+        print('...index of', w, ':', tokenizer.word_index.get(w))\n \n     dataset = (X, labels) \n-    print '-'\n-    print 'Saving...'\n-    cPickle.dump(dataset, open('datasets/data/reuters.pkl', 'w'))\n-    cPickle.dump(tokenizer.word_index, open('datasets/data/reuters_word_index.pkl', 'w'))\n+    print('-')\n+    print('Saving...')\n+    six.moves.cPickle.dump(dataset, open('datasets/data/reuters.pkl', 'w'))\n+    six.moves.cPickle.dump(tokenizer.word_index, open('datasets/data/reuters_word_index.pkl', 'w'))\n \n \n \n@@ -80,7 +83,7 @@ def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_s\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters.pkl\")\n     f = open(path, 'rb')\n \n-    X, labels = cPickle.load(f)\n+    X, labels = six.moves.cPickle.load(f)\n     f.close()\n     random.seed(seed)\n     random.shuffle(X)\n@@ -113,7 +116,7 @@ def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_s\n def get_word_index(path=\"reuters_word_index.pkl\"):\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl\")\n     f = open(path, 'rb')\n-    return cPickle.load(f)\n+    return six.moves.cPickle.load(f)\n \n \n if __name__ == \"__main__\":\n\n@@ -1,8 +1,9 @@\n+from __future__ import absolute_import\n import theano\n import theano.tensor as T\n import numpy as np\n \n-from utils.theano_utils import sharedX, shared_zeros\n+from .utils.theano_utils import sharedX, shared_zeros\n \n def uniform(shape, scale=0.05):\n     return sharedX(np.random.uniform(low=-scale, high=scale, size=shape))\n@@ -49,6 +50,6 @@ def zero(shape):\n     return shared_zeros(shape)\n \n \n-from utils.generic_utils import get_from_module\n+from .utils.generic_utils import get_from_module\n def get(identifier):\n     return get_from_module(identifier, globals(), 'initialization')\n\n@@ -1,4 +1,6 @@\n # -*- coding: utf-8 -*-\n+from __future__ import absolute_import\n+\n import theano\n import theano.tensor as T\n from theano.tensor.signal import downsample\n\n@@ -1,4 +1,6 @@\n # -*- coding: utf-8 -*-\n+from __future__ import absolute_import\n+\n import theano\n import theano.tensor as T\n \n@@ -7,6 +9,7 @@ from ..utils.theano_utils import shared_zeros, floatX\n from ..utils.generic_utils import make_tuple\n \n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n+from six.moves import zip\n srng = RandomStreams()\n \n class Layer(object):\n\n@@ -1,3 +1,4 @@\n+from __future__ import absolute_import\n import theano\n import theano.tensor as T\n \n\n@@ -1,4 +1,5 @@\n # -*- coding: utf-8 -*-\n+from __future__ import absolute_import\n import theano\n import theano.tensor as T\n import numpy as np\n@@ -6,6 +7,7 @@ import numpy as np\n from .. import activations, initializations\n from ..utils.theano_utils import shared_zeros, alloc_zeros_matrix\n from ..layers.core import Layer\n+from six.moves import range\n \n class SimpleRNN(Layer):\n     '''\n\n@@ -1,11 +1,14 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n import theano\n import theano.tensor as T\n import numpy as np\n \n-import optimizers\n-import objectives\n+from . import optimizers\n+from . import objectives\n import time, copy\n-from utils.generic_utils import Progbar\n+from .utils.generic_utils import Progbar\n+from six.moves import range\n \n def standardize_y(y):\n     if not hasattr(y, 'shape'):\n@@ -97,7 +100,7 @@ class Sequential(object):\n             do_validation = True\n             y_val = standardize_y(y_val)\n             if verbose:\n-                print \"Train on %d samples, validate on %d samples\" % (len(y), len(y_val))\n+                print(\"Train on %d samples, validate on %d samples\" % (len(y), len(y_val)))\n         else:\n             if 0 < validation_split < 1:\n                 # If a validation split size is given (e.g. validation_split=0.2)\n@@ -108,12 +111,12 @@ class Sequential(object):\n                 (X, X_val) = (X[0:split_at], X[split_at:])\n                 (y, y_val) = (y[0:split_at], y[split_at:])\n                 if verbose:\n-                    print \"Train on %d samples, validate on %d samples\" % (len(y), len(y_val))\n+                    print(\"Train on %d samples, validate on %d samples\" % (len(y), len(y_val)))\n         \n         index_array = np.arange(len(X))\n         for epoch in range(nb_epoch):\n             if verbose:\n-                print 'Epoch', epoch\n+                print('Epoch', epoch)\n             if shuffle:\n                 np.random.shuffle(index_array)\n \n\n@@ -1,6 +1,8 @@\n+from __future__ import absolute_import\n import theano\n import theano.tensor as T\n import numpy as np\n+from six.moves import range\n \n epsilon = 1.0e-15\n \n@@ -32,7 +34,7 @@ def binary_crossentropy(y_true, y_pred):\n mse = MSE = mean_squared_error\n mae = MAE = mean_absolute_error\n \n-from utils.generic_utils import get_from_module\n+from .utils.generic_utils import get_from_module\n def get(identifier):\n     return get_from_module(identifier, globals(), 'objective')\n \n\n@@ -1,8 +1,10 @@\n+from __future__ import absolute_import\n import theano\n import theano.tensor as T\n import numpy as np\n \n-from utils.theano_utils import shared_zeros, shared_scalar\n+from .utils.theano_utils import shared_zeros, shared_scalar\n+from six.moves import zip\n \n def clip_norm(g, c, n):\n     if c > 0:\n@@ -174,6 +176,6 @@ adagrad = Adagrad\n adadelta = Adadelta\n adam = Adam\n \n-from utils.generic_utils import get_from_module\n+from .utils.generic_utils import get_from_module\n def get(identifier):\n     return get_from_module(identifier, globals(), 'optimizer', instantiate=True)\n\n@@ -1,4 +1,5 @@\n-from PIL import Image\n+from __future__ import absolute_import\n+\n import numpy as np\n from scipy import ndimage\n from scipy import linalg\n@@ -6,6 +7,7 @@ from scipy import linalg\n from os import listdir\n from os.path import isfile, join\n import random, math\n+from six.moves import range\n \n '''\n     Fairly basic set of tools for realtime data augmentation on image data.\n@@ -74,6 +76,7 @@ def random_zoom(x, rg, fill_mode=\"nearest\", cval=0.):\n \n \n def array_to_img(x, scale=True):\n+    from PIL import Image\n     x = x.transpose(1, 2, 0) \n     if scale:\n         x += max(-np.min(x), 0)\n@@ -93,6 +96,7 @@ def img_to_array(img):\n \n \n def load_img(path, grayscale=False):\n+    from PIL import Image\n     img = Image.open(open(path))\n     if grayscale:\n         img = img.convert('L')\n\n@@ -1,6 +1,8 @@\n+from __future__ import absolute_import\n # -*- coding: utf-8 -*-\n import numpy as np\n import random\n+from six.moves import range\n \n def pad_sequences(sequences, maxlen=None, dtype='int32'):\n     \"\"\"\n@@ -37,7 +39,7 @@ def make_sampling_table(size, sampling_factor=1e-5):\n         where gamma is the Euler–Mascheroni constant.\n     '''\n     gamma = 0.577\n-    rank = np.array(range(size))\n+    rank = np.array(list(range(size)))\n     rank[0] = 1\n     inv_fq = rank * (np.log(rank) + gamma) + 0.5 - 1./(12.*rank)\n     f = sampling_factor * inv_fq\n\n@@ -3,9 +3,12 @@\n     These preprocessing utils would greatly benefit\n     from a fast Cython rewrite.\n '''\n+from __future__ import absolute_import\n \n import string\n import numpy as np\n+from six.moves import range\n+from six.moves import zip\n \n def base_filter():\n     f = string.punctuation\n@@ -20,7 +23,7 @@ def text_to_word_sequence(text, filters=base_filter(), lower=True, split=\" \"):\n         text = text.lower()\n     text = text.translate(string.maketrans(filters, split*len(filters)))\n     seq = text.split(split)\n-    return filter(None, seq)\n+    return [_f for _f in seq if _f]\n \n \n def one_hot(text, n, filters=base_filter(), lower=True, split=\" \"):\n@@ -58,13 +61,13 @@ class Tokenizer(object):\n                 else:\n                     self.word_docs[w] = 1\n \n-        wcounts = self.word_counts.items()\n+        wcounts = list(self.word_counts.items())\n         wcounts.sort(key = lambda x: x[1], reverse=True)\n         sorted_voc = [wc[0] for wc in wcounts]\n-        self.word_index = dict(zip(sorted_voc, range(1, len(sorted_voc)+1)))\n+        self.word_index = dict(list(zip(sorted_voc, list(range(1, len(sorted_voc)+1)))))\n \n         self.index_docs = {}\n-        for w, c in self.word_docs.items():\n+        for w, c in list(self.word_docs.items()):\n             self.index_docs[self.word_index[w]] = c\n \n \n@@ -153,7 +156,7 @@ class Tokenizer(object):\n                     counts[j] = 1.\n                 else:\n                     counts[j] += 1\n-            for j, c in counts.items():\n+            for j, c in list(counts.items()):\n                 if mode == \"count\":\n                     X[i][j] = c\n                 elif mode == \"freq\":\n\n@@ -1,3 +1,4 @@\n+from __future__ import absolute_import\n import numpy as np\n import time\n import sys\n\n@@ -1,5 +1,8 @@\n+from __future__ import absolute_import\n import numpy as np\n import scipy as sp\n+from six.moves import range\n+from six.moves import zip\n \n def to_categorical(y, nb_classes=None):\n     '''Convert class vector (integers from 0 to nb_classes)\n\n@@ -1,3 +1,4 @@\n+from __future__ import absolute_import\n import numpy as np\n import theano\n import theano.tensor as T\n\n@@ -1,6 +1,5 @@\n-#!/usr/bin/env python\n-\n-from distutils.core import setup\n+from setuptools import setup\n+from setuptools import find_packages\n \n setup(name='Keras',\n       version='0.0.1',\n@@ -9,12 +8,6 @@ setup(name='Keras',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n       license='MIT',\n-      packages=[\n-        'keras', \n-        'keras.layers', \n-        'keras.preprocessing', \n-        'keras.datasets', \n-        'keras.utils',\n-      ],\n+      packages=find_packages(),\n       # TODO: dependencies\n )\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
