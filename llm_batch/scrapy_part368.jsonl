{"custom_id": "scrapy#c7c7a488b950806888691f58dda0b06478b98c7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 760 | Contributors (this commit): 24 | Commits (past 90d): 10 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -95,10 +95,10 @@ def xmliter_lxml(\n     namespace: Optional[str] = None,\n     prefix: str = \"x\",\n ) -> Generator[Selector, Any, None]:\n-    reader: \"SupportsReadClose[bytes]\" = _StreamReader(obj)\n+    reader = _StreamReader(obj)\n     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n     iterable = etree.iterparse(\n-        reader,\n+        cast(\"SupportsReadClose[bytes]\", reader),\n         encoding=reader.encoding,\n         events=(\"end\", \"start-ns\"),\n         huge_tree=True,\n@@ -109,6 +109,7 @@ def xmliter_lxml(\n         prefix, nodename = nodename.split(\":\", maxsplit=1)\n     for event, data in iterable:\n         if event == \"start-ns\":\n+            assert isinstance(data, tuple)\n             if needs_namespace_resolution:\n                 _prefix, _namespace = data\n                 if _prefix != prefix:\n@@ -118,6 +119,7 @@ def xmliter_lxml(\n                 selxpath = f\"//{prefix}:{nodename}\"\n                 tag = f\"{{{namespace}}}{nodename}\"\n             continue\n+        assert isinstance(data, etree._Element)\n         node = data\n         if node.tag != tag:\n             continue\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27781a85e738052e0441c81d773b3ec124194594", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 4 | Churn Cumulative: 1361 | Contributors (this commit): 22 | Commits (past 90d): 4 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -149,10 +149,10 @@ class XMLFeedSpiderTest(SpiderTest):\n         body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n         <urlset xmlns:x=\"http://www.google.com/schemas/sitemap/0.84\"\n                 xmlns:y=\"http://www.example.com/schemas/extras/1.0\">\n-        <url><x:loc>http://www.example.com/Special-Offers.html</loc><y:updated>2009-08-16</updated>\n+        <url><x:loc>http://www.example.com/Special-Offers.html</x:loc><y:updated>2009-08-16</y:updated>\n             <other value=\"bar\" y:custom=\"fuu\"/>\n         </url>\n-        <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</updated><other value=\"foo\"/></url>\n+        <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</y:updated><other value=\"foo\"/></url>\n         </urlset>\"\"\"\n         response = XmlResponse(url=\"http://example.com/sitemap.xml\", body=body)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c67f73069570dd6dbe8427f55d6f9e65af07132a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 197 | Lines Deleted: 97 | Files Changed: 18 | Hunks: 58 | Methods Changed: 32 | Complexity Δ (Sum/Max): 16/10 | Churn Δ: 294 | Churn Cumulative: 19044 | Contributors (this commit): 113 | Commits (past 90d): 28 | Contributors (cumulative): 346 | DMM Complexity: 0.9375\n\nDIFF:\n@@ -4,7 +4,7 @@ from typing import TYPE_CHECKING, Any, List\n from scrapy.exceptions import NotConfigured\n from scrapy.settings import Settings\n from scrapy.utils.conf import build_component_list\n-from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n@@ -32,9 +32,7 @@ class AddonManager:\n         for clspath in build_component_list(settings[\"ADDONS\"]):\n             try:\n                 addoncls = load_object(clspath)\n-                addon = create_instance(\n-                    addoncls, settings=settings, crawler=self.crawler\n-                )\n+                addon = build_from_crawler(addoncls, self.crawler)\n                 addon.update_settings(settings)\n                 self.addons.append(addon)\n             except NotConfigured as e:\n\n@@ -20,7 +20,7 @@ from scrapy.core.downloader.tls import (\n     openssl_methods,\n )\n from scrapy.settings import BaseSettings\n-from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     from twisted.internet._sslverify import ClientTLSOptions\n@@ -165,18 +165,16 @@ def load_context_factory_from_settings(settings, crawler):\n     context_factory_cls = load_object(settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"])\n     # try method-aware context factory\n     try:\n-        context_factory = create_instance(\n-            objcls=context_factory_cls,\n-            settings=settings,\n-            crawler=crawler,\n+        context_factory = build_from_crawler(\n+            context_factory_cls,\n+            crawler,\n             method=ssl_method,\n         )\n     except TypeError:\n         # use context factory defaults\n-        context_factory = create_instance(\n-            objcls=context_factory_cls,\n-            settings=settings,\n-            crawler=crawler,\n+        context_factory = build_from_crawler(\n+            context_factory_cls,\n+            crawler,\n         )\n         msg = (\n             f\"{settings['DOWNLOADER_CLIENTCONTEXTFACTORY']} does not accept \"\n\n@@ -9,7 +9,7 @@ from twisted.internet.defer import Deferred\n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured, NotSupported\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import without_none_values\n \n if TYPE_CHECKING:\n@@ -55,10 +55,9 @@ class DownloadHandlers:\n             dhcls = load_object(path)\n             if skip_lazy and getattr(dhcls, \"lazy\", True):\n                 return None\n-            dh = create_instance(\n-                objcls=dhcls,\n-                settings=self._crawler.settings,\n-                crawler=self._crawler,\n+            dh = build_from_crawler(\n+                dhcls,\n+                self._crawler,\n             )\n         except NotConfigured as ex:\n             self._notconfigured[scheme] = str(ex)\n\n@@ -1,6 +1,6 @@\n \"\"\"Download handlers for http and https schemes\n \"\"\"\n-from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import to_unicode\n \n \n@@ -30,10 +30,9 @@ class HTTP10DownloadHandler:\n \n         host, port = to_unicode(factory.host), factory.port\n         if factory.scheme == b\"https\":\n-            client_context_factory = create_instance(\n-                objcls=self.ClientContextFactory,\n-                settings=self._settings,\n-                crawler=self._crawler,\n+            client_context_factory = build_from_crawler(\n+                self.ClientContextFactory,\n+                self._crawler,\n             )\n             return reactor.connectSSL(host, port, factory, client_context_factory)\n         return reactor.connectTCP(host, port, factory)\n\n@@ -2,7 +2,7 @@ from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.misc import create_instance\n+from scrapy.utils.misc import build_from_crawler\n \n \n class S3DownloadHandler:\n@@ -50,10 +50,9 @@ class S3DownloadHandler:\n                 )\n             )\n \n-        _http_handler = create_instance(\n-            objcls=httpdownloadhandler,\n-            settings=settings,\n-            crawler=crawler,\n+        _http_handler = build_from_crawler(\n+            httpdownloadhandler,\n+            crawler,\n         )\n         self._download_http = _http_handler.download_request\n \n\n@@ -34,7 +34,7 @@ from scrapy.settings import BaseSettings, Settings\n from scrapy.signalmanager import SignalManager\n from scrapy.spiders import Spider\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n-from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.reactor import CallLaterOnce\n \n if TYPE_CHECKING:\n@@ -358,9 +358,7 @@ class ExecutionEngine:\n             raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n         logger.info(\"Spider opened\", extra={\"spider\": spider})\n         nextcall = CallLaterOnce(self._next_request)\n-        scheduler = create_instance(\n-            self.scheduler_cls, settings=None, crawler=self.crawler\n-        )\n+        scheduler = build_from_crawler(self.scheduler_cls, self.crawler)\n         start_requests = yield self.scraper.spidermw.process_start_requests(\n             start_requests, spider\n         )\n\n@@ -14,7 +14,7 @@ from scrapy.http.request import Request\n from scrapy.spiders import Spider\n from scrapy.statscollectors import StatsCollector\n from scrapy.utils.job import job_dir\n-from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n@@ -202,7 +202,7 @@ class Scheduler(BaseScheduler):\n         \"\"\"\n         dupefilter_cls = load_object(crawler.settings[\"DUPEFILTER_CLASS\"])\n         return cls(\n-            dupefilter=create_instance(dupefilter_cls, crawler.settings, crawler),\n+            dupefilter=build_from_crawler(dupefilter_cls, crawler),\n             jobdir=job_dir(crawler.settings),\n             dqclass=load_object(crawler.settings[\"SCHEDULER_DISK_QUEUE\"]),\n             mqclass=load_object(crawler.settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n@@ -322,10 +322,9 @@ class Scheduler(BaseScheduler):\n \n     def _mq(self):\n         \"\"\"Create a new priority queue instance, with in-memory storage\"\"\"\n-        return create_instance(\n+        return build_from_crawler(\n             self.pqclass,\n-            settings=None,\n-            crawler=self.crawler,\n+            self.crawler,\n             downstream_queue_cls=self.mqclass,\n             key=\"\",\n         )\n@@ -334,10 +333,9 @@ class Scheduler(BaseScheduler):\n         \"\"\"Create a new priority queue instance, with disk storage\"\"\"\n         assert self.dqdir\n         state = self._read_dqs_state(self.dqdir)\n-        q = create_instance(\n+        q = build_from_crawler(\n             self.pqclass,\n-            settings=None,\n-            crawler=self.crawler,\n+            self.crawler,\n             downstream_queue_cls=self.dqclass,\n             key=self.dqdir,\n             startprios=state,\n\n@@ -39,7 +39,7 @@ from scrapy.utils.log import (\n     log_reactor_info,\n     log_scrapy_info,\n )\n-from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.reactor import (\n     install_reactor,\n@@ -109,10 +109,9 @@ class Crawler:\n         lf_cls: Type[LogFormatter] = load_object(self.settings[\"LOG_FORMATTER\"])\n         self.logformatter = lf_cls.from_crawler(self)\n \n-        self.request_fingerprinter = create_instance(\n+        self.request_fingerprinter = build_from_crawler(\n             load_object(self.settings[\"REQUEST_FINGERPRINTER_CLASS\"]),\n-            settings=self.settings,\n-            crawler=self,\n+            self,\n         )\n \n         reactor_class: str = self.settings[\"TWISTED_REACTOR\"]\n@@ -404,7 +403,7 @@ class CrawlerProcess(CrawlerRunner):\n             d.addBoth(self._stop_reactor)\n \n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n-        resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n+        resolver = build_from_crawler(resolver_class, self, reactor=reactor)\n         resolver.install_on_reactor()\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n\n@@ -28,7 +28,7 @@ from scrapy.utils.defer import maybe_deferred_to_future\n from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import without_none_values\n \n logger = logging.getLogger(__name__)\n@@ -371,7 +371,7 @@ class FeedSlot:\n             self._exporting = True\n \n     def _get_instance(self, objcls, *args, **kwargs):\n-        return create_instance(objcls, self.settings, self.crawler, *args, **kwargs)\n+        return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n \n     def _get_exporter(self, file, format, *args, **kwargs):\n         return self._get_instance(self.exporters[format], file, *args, **kwargs)\n\n@@ -23,7 +23,7 @@ from scrapy import Spider\n from scrapy.exceptions import NotConfigured\n from scrapy.settings import Settings\n from scrapy.utils.defer import process_chain, process_parallel\n-from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n@@ -64,7 +64,10 @@ class MiddlewareManager:\n         for clspath in mwlist:\n             try:\n                 mwcls = load_object(clspath)\n-                mw = create_instance(mwcls, settings, crawler)\n+                if crawler is not None:\n+                    mw = build_from_crawler(mwcls, crawler)\n+                else:\n+                    mw = build_from_settings(mwcls, settings)\n                 middlewares.append(mw)\n                 enabled.append(clspath)\n             except NotConfigured as e:\n\n@@ -1,7 +1,7 @@\n import hashlib\n import logging\n \n-from scrapy.utils.misc import create_instance\n+from scrapy.utils.misc import build_from_crawler\n \n logger = logging.getLogger(__name__)\n \n@@ -72,9 +72,8 @@ class ScrapyPriorityQueue:\n         self.curprio = min(startprios)\n \n     def qfactory(self, key):\n-        return create_instance(\n+        return build_from_crawler(\n             self.downstream_queue_cls,\n-            None,\n             self.crawler,\n             self.key + \"/\" + str(key),\n         )\n\n@@ -25,6 +25,7 @@ from typing import (\n     cast,\n )\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import Item\n from scrapy.utils.datatypes import LocalWeakReferencedCache\n \n@@ -142,6 +143,13 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n        Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n        extension has not been implemented correctly).\n     \"\"\"\n+    warnings.warn(\n+        \"The create_instance() function is deprecated. \"\n+        \"Please use build_from_crawler() or build_from_settings() instead.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n+\n     if settings is None:\n         if crawler is None:\n             raise ValueError(\"Specify at least one of settings and crawler.\")\n@@ -160,6 +168,45 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n     return instance\n \n \n+def build_from_crawler(objcls, crawler, /, *args, **kwargs):\n+    \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n+\n+    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n+\n+    Raises ``TypeError`` if the resulting instance is ``None``.\n+    \"\"\"\n+    if hasattr(objcls, \"from_crawler\"):\n+        instance = objcls.from_crawler(crawler, *args, **kwargs)\n+        method_name = \"from_crawler\"\n+    elif hasattr(objcls, \"from_settings\"):\n+        instance = objcls.from_settings(crawler.settings, *args, **kwargs)\n+        method_name = \"from_settings\"\n+    else:\n+        instance = objcls(*args, **kwargs)\n+        method_name = \"__new__\"\n+    if instance is None:\n+        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n+    return instance\n+\n+\n+def build_from_settings(objcls, settings, /, *args, **kwargs):\n+    \"\"\"Construct a class instance using its ``from_settings`` constructor.\n+\n+    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n+\n+    Raises ``TypeError`` if the resulting instance is ``None``.\n+    \"\"\"\n+    if hasattr(objcls, \"from_settings\"):\n+        instance = objcls.from_settings(settings, *args, **kwargs)\n+        method_name = \"from_settings\"\n+    else:\n+        instance = objcls(*args, **kwargs)\n+        method_name = \"__new__\"\n+    if instance is None:\n+        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n+    return instance\n+\n+\n @contextmanager\n def set_environ(**kwargs: str) -> Generator[None, Any, None]:\n     \"\"\"Temporarily set environment variables inside the context manager and\n\n@@ -89,7 +89,7 @@ class AddonManagerTest(unittest.TestCase):\n             self.assertEqual([a.number for a in manager.addons], expected_order)\n             self.assertEqual(crawler.settings.getint(\"KEY1\"), expected_order[-1])\n \n-    def test_create_instance(self):\n+    def test_build_from_crawler(self):\n         settings_dict = {\n             \"ADDONS\": {\"tests.test_addons.CreateInstanceAddon\": 0},\n             \"MYADDON\": {\"MYADDON_KEY\": \"val\"},\n@@ -167,12 +167,12 @@ class AddonManagerTest(unittest.TestCase):\n                 pass\n \n         with patch(\"scrapy.addons.logger\") as logger_mock:\n-            with patch(\"scrapy.addons.create_instance\") as create_instance_mock:\n+            with patch(\"scrapy.addons.build_from_crawler\") as build_from_crawler_mock:\n                 settings_dict = {\n                     \"ADDONS\": {LoggedAddon: 1},\n                 }\n                 addon = LoggedAddon()\n-                create_instance_mock.return_value = addon\n+                build_from_crawler_mock.return_value = addon\n                 crawler = get_crawler(settings_dict=settings_dict)\n                 logger_mock.info.assert_called_once_with(\n                     \"Enabled addons:\\n%(addons)s\",\n\n@@ -29,7 +29,7 @@ from scrapy.http import Headers, HtmlResponse, Request\n from scrapy.http.response.text import TextResponse\n from scrapy.responsetypes import responsetypes\n from scrapy.spiders import Spider\n-from scrapy.utils.misc import create_instance\n+from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler, skip_if_no_boto\n from tests import NON_EXISTING_RESOLVABLE\n@@ -109,7 +109,7 @@ class FileTestCase(unittest.TestCase):\n         # add a special char to check that they are handled correctly\n         self.tmpname = Path(self.mktemp() + \"^\")\n         Path(self.tmpname).write_text(\"0123456789\", encoding=\"utf-8\")\n-        handler = create_instance(FileDownloadHandler, None, get_crawler())\n+        handler = build_from_crawler(FileDownloadHandler, get_crawler())\n         self.download_request = handler.download_request\n \n     def tearDown(self):\n@@ -257,8 +257,8 @@ class HttpTestCase(unittest.TestCase):\n         else:\n             self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n         self.portno = self.port.getHost().port\n-        self.download_handler = create_instance(\n-            self.download_handler_cls, None, get_crawler()\n+        self.download_handler = build_from_crawler(\n+            self.download_handler_cls, get_crawler()\n         )\n         self.download_request = self.download_handler.download_request\n \n@@ -557,7 +557,7 @@ class Http11TestCase(HttpTestCase):\n \n     def test_download_broken_content_allow_data_loss_via_setting(self, url=\"broken\"):\n         crawler = get_crawler(settings_dict={\"DOWNLOAD_FAIL_ON_DATALOSS\": False})\n-        download_handler = create_instance(self.download_handler_cls, None, crawler)\n+        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n         request = Request(self.getURL(url))\n         d = download_handler.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.flags)\n@@ -590,7 +590,7 @@ class Https11TestCase(Http11TestCase):\n         crawler = get_crawler(\n             settings_dict={\"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\": True}\n         )\n-        download_handler = create_instance(self.download_handler_cls, None, crawler)\n+        download_handler = build_from_crawler(self.download_handler_cls, crawler)\n         try:\n             with LogCapture() as log_capture:\n                 request = Request(self.getURL(\"file\"))\n@@ -669,9 +669,7 @@ class Https11CustomCiphers(unittest.TestCase):\n         crawler = get_crawler(\n             settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"CAMELLIA256-SHA\"}\n         )\n-        self.download_handler = create_instance(\n-            self.download_handler_cls, None, crawler\n-        )\n+        self.download_handler = build_from_crawler(self.download_handler_cls, crawler)\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -751,8 +749,8 @@ class HttpProxyTestCase(unittest.TestCase):\n         wrapper = WrappingFactory(site)\n         self.port = reactor.listenTCP(0, wrapper, interface=\"127.0.0.1\")\n         self.portno = self.port.getHost().port\n-        self.download_handler = create_instance(\n-            self.download_handler_cls, None, get_crawler()\n+        self.download_handler = build_from_crawler(\n+            self.download_handler_cls, get_crawler()\n         )\n         self.download_request = self.download_handler.download_request\n \n@@ -830,10 +828,9 @@ class S3AnonTestCase(unittest.TestCase):\n     def setUp(self):\n         skip_if_no_boto()\n         crawler = get_crawler()\n-        self.s3reqh = create_instance(\n-            objcls=S3DownloadHandler,\n-            settings=None,\n-            crawler=crawler,\n+        self.s3reqh = build_from_crawler(\n+            S3DownloadHandler,\n+            crawler,\n             httpdownloadhandler=HttpDownloadHandlerMock,\n             # anon=True, # implicit\n         )\n@@ -861,10 +858,9 @@ class S3TestCase(unittest.TestCase):\n     def setUp(self):\n         skip_if_no_boto()\n         crawler = get_crawler()\n-        s3reqh = create_instance(\n-            objcls=S3DownloadHandler,\n-            settings=None,\n-            crawler=crawler,\n+        s3reqh = build_from_crawler(\n+            S3DownloadHandler,\n+            crawler,\n             aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n             aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n             httpdownloadhandler=HttpDownloadHandlerMock,\n@@ -889,10 +885,9 @@ class S3TestCase(unittest.TestCase):\n     def test_extra_kw(self):\n         try:\n             crawler = get_crawler()\n-            create_instance(\n-                objcls=S3DownloadHandler,\n-                settings=None,\n-                crawler=crawler,\n+            build_from_crawler(\n+                S3DownloadHandler,\n+                crawler,\n                 extra_kw=True,\n             )\n         except Exception as e:\n@@ -1039,9 +1034,7 @@ class BaseFTPTestCase(unittest.TestCase):\n         self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n         self.portNum = self.port.getHost().port\n         crawler = get_crawler()\n-        self.download_handler = create_instance(\n-            FTPDownloadHandler, crawler.settings, crawler\n-        )\n+        self.download_handler = build_from_crawler(FTPDownloadHandler, crawler)\n         self.addCleanup(self.port.stopListening)\n \n     def tearDown(self):\n@@ -1185,9 +1178,7 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n         self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n         self.portNum = self.port.getHost().port\n         crawler = get_crawler()\n-        self.download_handler = create_instance(\n-            FTPDownloadHandler, crawler.settings, crawler\n-        )\n+        self.download_handler = build_from_crawler(FTPDownloadHandler, crawler)\n         self.addCleanup(self.port.stopListening)\n \n     def tearDown(self):\n@@ -1197,9 +1188,7 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n class DataURITestCase(unittest.TestCase):\n     def setUp(self):\n         crawler = get_crawler()\n-        self.download_handler = create_instance(\n-            DataURIDownloadHandler, crawler.settings, crawler\n-        )\n+        self.download_handler = build_from_crawler(DataURIDownloadHandler, crawler)\n         self.download_request = self.download_handler.download_request\n         self.spider = Spider(\"foo\")\n \n\n@@ -11,7 +11,7 @@ from twisted.web.http import H2_ENABLED\n \n from scrapy.http import Request\n from scrapy.spiders import Spider\n-from scrapy.utils.misc import create_instance\n+from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.test import get_crawler\n from tests.mockserver import ssl_context_factory\n from tests.test_downloader_handlers import (\n@@ -240,8 +240,8 @@ class Https2ProxyTestCase(Http11ProxyTestCase):\n             interface=self.host,\n         )\n         self.portno = self.port.getHost().port\n-        self.download_handler = create_instance(\n-            self.download_handler_cls, None, get_crawler()\n+        self.download_handler = build_from_crawler(\n+            self.download_handler_cls, get_crawler()\n         )\n         self.download_request = self.download_handler.download_request\n \n\n@@ -440,7 +440,7 @@ class SettingsTest(unittest.TestCase):\n \n     def test_passing_objects_as_values(self):\n         from scrapy.core.downloader.handlers.file import FileDownloadHandler\n-        from scrapy.utils.misc import create_instance\n+        from scrapy.utils.misc import build_from_crawler\n         from scrapy.utils.test import get_crawler\n \n         class TestPipeline:\n@@ -468,7 +468,7 @@ class SettingsTest(unittest.TestCase):\n \n         myhandler = settings.getdict(\"DOWNLOAD_HANDLERS\").pop(\"ftp\")\n         self.assertEqual(myhandler, FileDownloadHandler)\n-        myhandler_instance = create_instance(myhandler, None, get_crawler())\n+        myhandler_instance = build_from_crawler(myhandler, get_crawler())\n         self.assertIsInstance(myhandler_instance, FileDownloadHandler)\n         self.assertTrue(hasattr(myhandler_instance, \"download_request\"))\n \n\n@@ -7,6 +7,8 @@ from unittest import mock\n from scrapy.item import Field, Item\n from scrapy.utils.misc import (\n     arg_to_iter,\n+    build_from_crawler,\n+    build_from_settings,\n     create_instance,\n     load_object,\n     rel_has_nofollow,\n@@ -153,6 +155,78 @@ class UtilsMiscTestCase(unittest.TestCase):\n         with self.assertRaises(TypeError):\n             create_instance(m, settings, None)\n \n+    def test_build_from_crawler(self):\n+        settings = mock.MagicMock()\n+        crawler = mock.MagicMock(spec_set=[\"settings\"])\n+        args = (True, 100.0)\n+        kwargs = {\"key\": \"val\"}\n+\n+        def _test_with_crawler(mock, settings, crawler):\n+            build_from_crawler(mock, crawler, *args, **kwargs)\n+            if hasattr(mock, \"from_crawler\"):\n+                mock.from_crawler.assert_called_once_with(crawler, *args, **kwargs)\n+                if hasattr(mock, \"from_settings\"):\n+                    self.assertEqual(mock.from_settings.call_count, 0)\n+                self.assertEqual(mock.call_count, 0)\n+            elif hasattr(mock, \"from_settings\"):\n+                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n+                self.assertEqual(mock.call_count, 0)\n+            else:\n+                mock.assert_called_once_with(*args, **kwargs)\n+\n+        # Check usage of correct constructor using three mocks:\n+        #   1. with no alternative constructors\n+        #   2. with from_crawler() constructor\n+        #   3. with from_settings() and from_crawler() constructor\n+        spec_sets = (\n+            [\"__qualname__\"],\n+            [\"__qualname__\", \"from_crawler\"],\n+            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n+        )\n+        for specs in spec_sets:\n+            m = mock.MagicMock(spec_set=specs)\n+            _test_with_crawler(m, settings, crawler)\n+            m.reset_mock()\n+\n+        # Check adoption of crawler\n+        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_crawler\"])\n+        m.from_crawler.return_value = None\n+        with self.assertRaises(TypeError):\n+            build_from_crawler(m, crawler, *args, **kwargs)\n+\n+    def test_build_from_settings(self):\n+        settings = mock.MagicMock()\n+        args = (True, 100.0)\n+        kwargs = {\"key\": \"val\"}\n+\n+        def _test_with_settings(mock, settings):\n+            build_from_settings(mock, settings, *args, **kwargs)\n+            if hasattr(mock, \"from_settings\"):\n+                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n+                self.assertEqual(mock.call_count, 0)\n+            else:\n+                mock.assert_called_once_with(*args, **kwargs)\n+\n+        # Check usage of correct constructor using three mocks:\n+        #   1. with no alternative constructors\n+        #   2. with from_settings() constructor\n+        #   3. with from_settings() and from_crawler() constructor\n+        spec_sets = (\n+            [\"__qualname__\"],\n+            [\"__qualname__\", \"from_settings\"],\n+            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n+        )\n+        for specs in spec_sets:\n+            m = mock.MagicMock(spec_set=specs)\n+            _test_with_settings(m, settings)\n+            m.reset_mock()\n+\n+        # Check adoption of crawler settings\n+        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n+        m.from_settings.return_value = None\n+        with self.assertRaises(TypeError):\n+            build_from_settings(m, settings, *args, **kwargs)\n+\n     def test_set_environ(self):\n         assert os.environ.get(\"some_test_environ\") is None\n         with set_environ(some_test_environ=\"test_value\"):\n\n@@ -24,7 +24,7 @@ from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n from scrapy.http import Headers, Request\n from scrapy.settings import Settings\n-from scrapy.utils.misc import create_instance\n+from scrapy.utils.misc import build_from_settings\n from scrapy.utils.python import to_bytes, to_unicode\n from tests.mockserver import (\n     BrokenDownloadResource,\n@@ -470,8 +470,8 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n     def testPayload(self):\n         s = \"0123456789\" * 10\n         settings = Settings({\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers})\n-        client_context_factory = create_instance(\n-            ScrapyClientContextFactory, settings=settings, crawler=None\n+        client_context_factory = build_from_settings(\n+            ScrapyClientContextFactory, settings\n         )\n         return getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n@@ -482,8 +482,8 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n         settings = Settings(\n             {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"}\n         )\n-        client_context_factory = create_instance(\n-            ScrapyClientContextFactory, settings=settings, crawler=None\n+        client_context_factory = build_from_settings(\n+            ScrapyClientContextFactory, settings\n         )\n         d = getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d25cfe5315c9c0346776529a6e14ffb405913d2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 436 | Contributors (this commit): 23 | Commits (past 90d): 4 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,5 +12,6 @@ from scrapy.http.request.json_request import JsonRequest\n from scrapy.http.request.rpc import XmlRpcRequest\n from scrapy.http.response import Response\n from scrapy.http.response.html import HtmlResponse\n+from scrapy.http.response.json import JsonResponse\n from scrapy.http.response.text import TextResponse\n from scrapy.http.response.xml import XmlResponse\n\n@@ -0,0 +1,12 @@\n+\"\"\"\n+This module implements the JsonResponse class that is used when the response\n+has a JSON MIME type in its Content-Type header.\n+\n+See documentation in docs/topics/request-response.rst\n+\"\"\"\n+\n+from scrapy.http.response.text import TextResponse\n+\n+\n+class JsonResponse(TextResponse):\n+    pass\n\n@@ -21,9 +21,9 @@ class ResponseTypes:\n         \"application/xhtml+xml\": \"scrapy.http.HtmlResponse\",\n         \"application/vnd.wap.xhtml+xml\": \"scrapy.http.HtmlResponse\",\n         \"application/xml\": \"scrapy.http.XmlResponse\",\n-        \"application/json\": \"scrapy.http.TextResponse\",\n-        \"application/x-json\": \"scrapy.http.TextResponse\",\n-        \"application/json-amazonui-streaming\": \"scrapy.http.TextResponse\",\n+        \"application/json\": \"scrapy.http.JsonResponse\",\n+        \"application/x-json\": \"scrapy.http.JsonResponse\",\n+        \"application/json-amazonui-streaming\": \"scrapy.http.JsonResponse\",\n         \"application/javascript\": \"scrapy.http.TextResponse\",\n         \"application/x-javascript\": \"scrapy.http.TextResponse\",\n         \"text/xml\": \"scrapy.http.XmlResponse\",\n\n@@ -1,6 +1,13 @@\n import unittest\n \n-from scrapy.http import Headers, HtmlResponse, Response, TextResponse, XmlResponse\n+from scrapy.http import (\n+    Headers,\n+    HtmlResponse,\n+    JsonResponse,\n+    Response,\n+    TextResponse,\n+    XmlResponse,\n+)\n from scrapy.responsetypes import responsetypes\n \n \n@@ -40,8 +47,9 @@ class ResponseTypesTest(unittest.TestCase):\n             (\"application/vnd.wap.xhtml+xml; charset=utf-8\", HtmlResponse),\n             (\"application/xml; charset=UTF-8\", XmlResponse),\n             (\"application/octet-stream\", Response),\n-            (\"application/x-json; encoding=UTF8;charset=UTF-8\", TextResponse),\n-            (\"application/json-amazonui-streaming;charset=UTF-8\", TextResponse),\n+            (\"application/json; encoding=UTF8;charset=UTF-8\", JsonResponse),\n+            (\"application/x-json; encoding=UTF8;charset=UTF-8\", JsonResponse),\n+            (\"application/json-amazonui-streaming;charset=UTF-8\", JsonResponse),\n             (b\"application/x-download; filename=\\x80dummy.txt\", Response),\n         ]\n         for source, cls in mappings:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e78acb65798a1eb4e55a472232e77d55e6c7cd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 34 | Churn Cumulative: 926 | Contributors (this commit): 23 | Commits (past 90d): 6 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -112,14 +112,14 @@ class MediaPipeline:\n         info.downloading.add(fp)\n         dfd = mustbe_deferred(self.media_to_download, request, info, item=item)\n         dfd.addCallback(self._check_media_to_download, request, info, item=item)\n+        dfd.addErrback(self._log_exception)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n-        dfd.addErrback(\n-            lambda f: logger.error(\n-                f.value, exc_info=failure_to_exc_info(f), extra={\"spider\": info.spider}\n-            )\n-        )\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n+    def _log_exception(self, result):\n+        logger.exception(result)\n+        return result\n+\n     def _modify_media_request(self, request):\n         if self.handle_httpstatus_list:\n             request.meta[\"handle_httpstatus_list\"] = self.handle_httpstatus_list\n\n@@ -9,6 +9,7 @@ from w3lib.url import add_or_replace_parameter\n \n from scrapy import signals\n from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.misc import load_object\n from tests.mockserver import MockServer\n from tests.spiders import SimpleSpider\n \n@@ -193,6 +194,29 @@ class FileDownloadCrawlTestCase(TestCase):\n             crawler.stats.get_value(\"downloader/response_status_count/302\"), 3\n         )\n \n+    @defer.inlineCallbacks\n+    def test_download_media_file_path_error(self):\n+        cls = load_object(self.pipeline_class)\n+\n+        class ExceptionRaisingMediaPipeline(cls):\n+            def file_path(self, request, response=None, info=None, *, item=None):\n+                return 1 / 0\n+\n+        settings = {\n+            **self.settings,\n+            \"ITEM_PIPELINES\": {ExceptionRaisingMediaPipeline: 1},\n+        }\n+        runner = CrawlerRunner(settings)\n+        crawler = self._create_crawler(MediaDownloadSpider, runner=runner)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/files/images/\"),\n+                media_key=self.media_key,\n+                media_urls_key=self.media_urls_key,\n+                mockserver=self.mockserver,\n+            )\n+        self.assertIn(\"ZeroDivisionError\", str(log))\n+\n \n skip_pillow: Optional[str]\n try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40e623b2768598e36c4f367bd166b36fffceb3f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 1945 | Contributors (this commit): 43 | Commits (past 90d): 3 | Contributors (cumulative): 59 | DMM Complexity: 0.0\n\nDIFF:\n@@ -340,7 +340,9 @@ class FilesPipeline(MediaPipeline):\n     DEFAULT_FILES_URLS_FIELD = \"file_urls\"\n     DEFAULT_FILES_RESULT_FIELD = \"files\"\n \n-    def __init__(self, store_uri, download_func=None, settings=None):\n+    def __init__(\n+        self, store_uri: Union[str, PathLike], download_func=None, settings=None\n+    ):\n         store_uri = _to_string(store_uri)\n         if not store_uri:\n             raise NotConfigured\n\n@@ -8,7 +8,8 @@ import hashlib\n import warnings\n from contextlib import suppress\n from io import BytesIO\n-from typing import Dict, Tuple\n+from os import PathLike\n+from typing import Dict, Tuple, Union\n \n from itemadapter import ItemAdapter\n \n@@ -53,7 +54,9 @@ class ImagesPipeline(FilesPipeline):\n     DEFAULT_IMAGES_URLS_FIELD = \"image_urls\"\n     DEFAULT_IMAGES_RESULT_FIELD = \"images\"\n \n-    def __init__(self, store_uri, download_func=None, settings=None):\n+    def __init__(\n+        self, store_uri: Union[str, PathLike], download_func=None, settings=None\n+    ):\n         try:\n             from PIL import Image\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7b2b097b18c0b30d06cea4b803d5d42aca98715", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 471 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -67,7 +67,7 @@ def extension(settings=None):\n \n class TestPeriodicLog(unittest.TestCase):\n     def test_extension_enabled(self):\n-        # Expected that settings for this extension loaded succesfully\n+        # Expected that settings for this extension loaded successfully\n         # And on certain conditions - extension raising NotConfigured\n \n         # \"PERIODIC_LOG_STATS\": True -> set to {\"enabled\": True}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d445a3224ecb0abfdf56a93e181692d3b5e4a6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 30 | Files Changed: 11 | Hunks: 15 | Methods Changed: 19 | Complexity Δ (Sum/Max): -31/0 | Churn Δ: 45 | Churn Cumulative: 15532 | Contributors (this commit): 78 | Commits (past 90d): 22 | Contributors (cumulative): 207 | DMM Complexity: 0.0\n\nDIFF:\n@@ -103,8 +103,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n     ) -> Union[Generator, AsyncGenerator]:\n         def process_sync(iterable: Iterable) -> Generator:\n             try:\n-                for r in iterable:\n-                    yield r\n+                yield from iterable\n             except Exception as ex:\n                 exception_result = self._process_spider_exception(\n                     response, spider, Failure(ex), exception_processor_index\n\n@@ -131,8 +131,7 @@ class CrawlSpider(Spider):\n     def _handle_failure(self, failure, errback):\n         if errback:\n             results = errback(failure) or ()\n-            for request_or_item in iterate_spider_output(results):\n-                yield request_or_item\n+            yield from iterate_spider_output(results)\n \n     def _compile_rules(self):\n         self._rules = []\n\n@@ -58,8 +58,7 @@ class XMLFeedSpider(Spider):\n \n         for selector in nodes:\n             ret = iterate_spider_output(self.parse_node(response, selector))\n-            for result_item in self.process_results(response, ret):\n-                yield result_item\n+            yield from self.process_results(response, ret)\n \n     def _parse(self, response, **kwargs):\n         if not hasattr(self, \"parse_node\"):\n@@ -133,8 +132,7 @@ class CSVFeedSpider(Spider):\n             response, self.delimiter, self.headers, quotechar=self.quotechar\n         ):\n             ret = iterate_spider_output(self.parse_row(response, row))\n-            for result_item in self.process_results(response, ret):\n-                yield result_item\n+            yield from self.process_results(response, ret)\n \n     def _parse(self, response, **kwargs):\n         if not hasattr(self, \"parse_row\"):\n\n@@ -33,8 +33,7 @@ class SitemapSpider(Spider):\n         attributes, for example, you can filter locs with lastmod greater\n         than a given date (see docs).\n         \"\"\"\n-        for entry in entries:\n-            yield entry\n+        yield from entries\n \n     def _parse_sitemap(self, response):\n         if response.url.endswith(\"/robots.txt\"):\n\n@@ -57,8 +57,7 @@ def iflatten(x: Iterable) -> Iterable:\n     Similar to ``.flatten()``, but returns iterator instead\"\"\"\n     for el in x:\n         if is_listlike(el):\n-            for el_ in iflatten(el):\n-                yield el_\n+            yield from iflatten(el)\n         else:\n             yield el\n \n\n@@ -44,8 +44,7 @@ def _serialize_headers(\n     for header in headers:\n         if header in request.headers:\n             yield header\n-            for value in request.headers.getlist(header):\n-                yield value\n+            yield from request.headers.getlist(header)\n \n \n def request_fingerprint(\n\n@@ -301,8 +301,7 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n \n     def parse(self, response):\n         self.seedsseen.append(response.meta.get(\"seed\"))\n-        for req in super().parse(response):\n-            yield req\n+        yield from super().parse(response)\n \n \n class SingleRequestSpider(MetaSpider):\n\n@@ -673,8 +673,7 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n             name = \"testspider\"\n \n             def parse(self, response):\n-                for item in items:\n-                    yield item\n+                yield from items\n \n         data = yield self.run_and_export(TestSpider, settings)\n         return data\n@@ -2696,8 +2695,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             name = \"testspider\"\n \n             def parse(self, response):\n-                for item in items:\n-                    yield item\n+                yield from items\n \n         with MockServer() as server:\n             TestSpider.start_urls = [server.url(\"/\")]\n\n@@ -317,8 +317,7 @@ class CrawlSpiderTest(SpiderTest):\n             rules = (Rule(LinkExtractor(), process_links=\"dummy_process_links\"),)\n \n             def dummy_process_links(self, links):\n-                for link in links:\n-                    yield link\n+                yield from links\n \n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n\n@@ -170,8 +170,7 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n \n class ProcessSpiderOutputSimpleMiddleware:\n     def process_spider_output(self, response, result, spider):\n-        for r in result:\n-            yield r\n+        yield from result\n \n \n class ProcessSpiderOutputAsyncGenMiddleware:\n@@ -182,8 +181,7 @@ class ProcessSpiderOutputAsyncGenMiddleware:\n \n class ProcessSpiderOutputUniversalMiddleware:\n     def process_spider_output(self, response, result, spider):\n-        for r in result:\n-            yield r\n+        yield from result\n \n     async def process_spider_output_async(self, response, result, spider):\n         async for r in result:\n@@ -324,8 +322,7 @@ class ProcessSpiderOutputInvalidResult(BaseAsyncSpiderMiddlewareTestCase):\n \n class ProcessStartRequestsSimpleMiddleware:\n     def process_start_requests(self, start_requests, spider):\n-        for r in start_requests:\n-            yield r\n+        yield from start_requests\n \n \n class ProcessStartRequestsSimple(BaseAsyncSpiderMiddlewareTestCase):\n\n@@ -107,8 +107,7 @@ class DeferUtilsTest(unittest.TestCase):\n class IterErrbackTest(unittest.TestCase):\n     def test_iter_errback_good(self):\n         def itergood():\n-            for x in range(10):\n-                yield x\n+            yield from range(10)\n \n         errors = []\n         out = list(iter_errback(itergood(), errors.append))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42c481cb4a12a81e88923930e21a661eee967a5f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1183 | Contributors (this commit): 34 | Commits (past 90d): 2 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,6 @@ import functools\n import hashlib\n import logging\n import mimetypes\n-import os\n import time\n from collections import defaultdict\n from contextlib import suppress\n@@ -66,7 +65,7 @@ class FSFilesStore:\n         absolute_path = self._get_filesystem_path(path)\n         try:\n             last_modified = absolute_path.stat().st_mtime\n-        except os.error:\n+        except OSError:\n             return {}\n \n         with absolute_path.open(\"rb\") as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68fccb1d58f291f70e864fd8ecd167887bda4112", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 888 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,7 @@ sys.exit(mitmdump())\n         self.proc = Popen(\n             [\n                 sys.executable,\n+                \"-u\",\n                 \"-c\",\n                 script,\n                 \"--listen-host\",\n@@ -46,7 +47,7 @@ sys.exit(mitmdump())\n             stdout=PIPE,\n         )\n         line = self.proc.stdout.readline().decode(\"utf-8\")\n-        host_port = re.search(r\"listening at http://([^:]+:\\d+)\", line).group(1)\n+        host_port = re.search(r\"listening at (?:http://)?([^:]+:\\d+)\", line).group(1)\n         address = f\"http://{self.auth_user}:{self.auth_pass}@{host_port}\"\n         return address\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
