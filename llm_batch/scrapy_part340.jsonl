{"custom_id": "scrapy#f85c3f3d68b03b12c0af3b3aa09ab5faad19bc37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2217 | Contributors (this commit): 29 | Commits (past 90d): 3 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -109,6 +109,7 @@ class FileTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tmpname = self.mktemp()\n+        # add a special char to check that they are handled correctly\n         with open(self.tmpname + '^', 'w') as f:\n             f.write('0123456789')\n         handler = create_instance(FileDownloadHandler, None, get_crawler())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f9a29f03d9a0eb9173a91f225177b7bee7d382c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 130 | Lines Deleted: 175 | Files Changed: 86 | Hunks: 189 | Methods Changed: 87 | Complexity Δ (Sum/Max): 23/17 | Churn Δ: 305 | Churn Cumulative: 41638 | Contributors (this commit): 217 | Commits (past 90d): 165 | Contributors (cumulative): 1101 | DMM Complexity: 0.2926829268292683\n\nDIFF:\n@@ -21,7 +21,7 @@ collect_ignore = [\n     *_py_files(\"tests/CrawlerRunner\"),\n ]\n \n-with Path('tests/ignores.txt').open() as reader:\n+with Path('tests/ignores.txt').open(encoding=\"utf-8\") as reader:\n     for line in reader:\n         file_path = line.strip()\n         if file_path and file_path[0] != '#':\n\n@@ -1,8 +1,8 @@\n+from operator import itemgetter\n from docutils.parsers.rst.roles import set_classes\n from docutils import nodes\n from docutils.parsers.rst import Directive\n from sphinx.util.nodes import make_refnode\n-from operator import itemgetter\n \n \n class settingslist_node(nodes.General, nodes.Element):\n\n@@ -1,15 +1,17 @@\n from doctest import ELLIPSIS, NORMALIZE_WHITESPACE\n from pathlib import Path\n \n-from scrapy.http.response.html import HtmlResponse\n from sybil import Sybil\n+from sybil.parsers.doctest import DocTestParser\n+from sybil.parsers.skip import skip\n+\n try:\n     # >2.0.1\n     from sybil.parsers.codeblock import PythonCodeBlockParser\n except ImportError:\n     from sybil.parsers.codeblock import CodeBlockParser as PythonCodeBlockParser\n-from sybil.parsers.doctest import DocTestParser\n-from sybil.parsers.skip import skip\n+\n+from scrapy.http.response.html import HtmlResponse\n \n \n def load_response(url: str, filename: str) -> HtmlResponse:\n\n@@ -13,6 +13,7 @@ Author: dufferzafar\n \"\"\"\n \n import re\n+import sys\n from pathlib import Path\n \n \n@@ -28,11 +29,11 @@ def main():\n \n     # Read lines from the linkcheck output file\n     try:\n-        with Path(\"build/linkcheck/output.txt\").open() as out:\n+        with Path(\"build/linkcheck/output.txt\").open(encoding=\"utf-8\") as out:\n             output_lines = out.readlines()\n     except IOError:\n         print(\"linkcheck output not found; please run linkcheck first.\")\n-        exit(1)\n+        sys.exit(1)\n \n     # For every line, fix the respective file\n     for line in output_lines:\n@@ -52,12 +53,12 @@ def main():\n \n                     # Update the previous file\n                     if _filename:\n-                        Path(_filename).write_text(_contents)\n+                        Path(_filename).write_text(_contents, encoding=\"utf-8\")\n \n                     _filename = newfilename\n \n                     # Read the new file to memory\n-                    _contents = Path(_filename).read_text()\n+                    _contents = Path(_filename).read_text(encoding=\"utf-8\")\n \n                 _contents = _contents.replace(match.group(3), match.group(4))\n         else:\n\n@@ -95,7 +95,7 @@ class ScrapyCommand:\n             self.settings.set('LOG_ENABLED', False, priority='cmdline')\n \n         if opts.pidfile:\n-            Path(opts.pidfile).write_text(str(os.getpid()) + os.linesep)\n+            Path(opts.pidfile).write_text(str(os.getpid()) + os.linesep, encoding=\"utf-8\")\n \n         if opts.pdb:\n             failure.startDebugMode()\n\n@@ -63,7 +63,7 @@ class Command(ScrapyCommand):\n         if opts.dump:\n             template_file = self._find_template(opts.dump)\n             if template_file:\n-                print(template_file.read_text())\n+                print(template_file.read_text(encoding=\"utf-8\"))\n             return\n         if len(args) != 2:\n             raise UsageError()\n\n@@ -6,14 +6,12 @@ from itemadapter import is_item, ItemAdapter\n from w3lib.url import is_url\n \n from twisted.internet.defer import maybeDeferred\n-\n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.http import Request\n from scrapy.utils import display\n from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n from scrapy.exceptions import UsageError\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -33,5 +33,4 @@ class HTTP10DownloadHandler:\n                 crawler=self._crawler,\n             )\n             return reactor.connectSSL(host, port, factory, client_context_factory)\n-        else:\n         return reactor.connectTCP(host, port, factory)\n\n@@ -26,7 +26,6 @@ from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.python import to_bytes, to_unicode\n \n-\n logger = logging.getLogger(__name__)\n \n \n@@ -289,7 +288,6 @@ class ScrapyAgent:\n                     bindAddress=bindaddress,\n                     pool=self._pool,\n                 )\n-            else:\n             proxyScheme = proxyScheme or b'http'\n             proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, '', '', ''))\n             return self._ProxyAgent(\n@@ -567,7 +565,7 @@ class _ResponseReader(protocol.Protocol):\n                 self._finish_response(flags=[\"dataloss\"])\n                 return\n \n-            elif not self._fail_on_dataloss_warned:\n+            if not self._fail_on_dataloss_warned:\n                 logger.warning(\"Got data loss in %s. If you want to process broken \"\n                                \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n                                \" -- This message won't be shown in further requests\",\n\n@@ -7,7 +7,6 @@ from twisted.internet.ssl import AcceptableCiphers\n \n from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -1,8 +1,8 @@\n import re\n from time import time\n from urllib.parse import urlparse, urlunparse, urldefrag\n-\n from twisted.web.http import HTTPClient\n+\n from twisted.internet import defer\n from twisted.internet.protocol import ClientFactory\n \n\n@@ -140,7 +140,7 @@ class ScrapyProxyH2Agent(H2Agent):\n         connect_timeout: Optional[float] = None,\n         bind_address: Optional[bytes] = None,\n     ) -> None:\n-        super(ScrapyProxyH2Agent, self).__init__(\n+        super().__init__(\n             reactor=reactor,\n             pool=pool,\n             context_factory=context_factory,\n\n@@ -42,7 +42,7 @@ class InvalidNegotiatedProtocol(H2Error):\n         self.negotiated_protocol = negotiated_protocol\n \n     def __str__(self) -> str:\n-        return (f\"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}\")\n+        return f\"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}\"\n \n \n class RemoteTerminatedConnection(H2Error):\n\n@@ -333,9 +333,9 @@ class Scheduler(BaseScheduler):\n         path = Path(dqdir, 'active.json')\n         if not path.exists():\n             return []\n-        with path.open() as f:\n+        with path.open(encoding=\"utf-8\") as f:\n             return json.load(f)\n \n     def _write_dqs_state(self, dqdir: str, state: list) -> None:\n-        with Path(dqdir, 'active.json').open('w') as f:\n+        with Path(dqdir, 'active.json').open('w', encoding=\"utf-8\") as f:\n             json.dump(state, f)\n\n@@ -152,7 +152,7 @@ class Scraper:\n         \"\"\"\n         if isinstance(result, Response):\n             return self.spidermw.scrape_response(self.call_spider, result, request, spider)\n-        else:  # result is a Failure\n+        # else result is a Failure\n         dfd = self.call_spider(result, request, spider)\n         return dfd.addErrback(self._log_download_errors, result, request, spider)\n \n@@ -276,14 +276,12 @@ class Scraper:\n                 return self.signals.send_catch_log_deferred(\n                     signal=signals.item_dropped, item=item, response=response,\n                     spider=spider, exception=output.value)\n-            else:\n             logkws = self.logformatter.item_error(item, ex, response, spider)\n             logger.log(*logformatter_adapter(logkws), extra={'spider': spider},\n                        exc_info=failure_to_exc_info(output))\n             return self.signals.send_catch_log_deferred(\n                 signal=signals.item_error, item=item, response=response,\n                 spider=spider, failure=output)\n-        else:\n         logkws = self.logformatter.scraped(output, response, spider)\n         if logkws is not None:\n             logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n\n@@ -121,7 +121,6 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 if dfd.called:\n                     # the result is available immediately if _process_spider_output didn't do downgrading\n                     return dfd.result\n-                else:\n                 # we forbid waiting here because otherwise we would need to return a deferred from\n                 # _process_spider_exception too, which complicates the architecture\n                 msg = f\"Async iterable returned from {method.__qualname__} cannot be downgraded\"\n@@ -213,7 +212,6 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n         if last_result_is_async:\n             return MutableAsyncChain(result, recovered)\n-        else:\n         return MutableChain(result, recovered)  # type: ignore[arg-type]\n \n     async def _process_callback_output(self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]\n@@ -227,7 +225,6 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         result = await maybe_deferred_to_future(self._process_spider_output(response, spider, result))\n         if isinstance(result, AsyncIterable):\n             return MutableAsyncChain(result, recovered)\n-        else:\n         if isinstance(recovered, AsyncIterable):\n             recovered_collected = await collect_asyncgen(recovered)\n             recovered = MutableChain(recovered_collected)\n\n@@ -9,7 +9,6 @@ from scrapy.http.cookies import CookieJar\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n-\n logger = logging.getLogger(__name__)\n \n \n@@ -129,7 +128,7 @@ class CookiesMiddleware:\n         \"\"\"\n         if not request.cookies:\n             return []\n-        elif isinstance(request.cookies, dict):\n+        if isinstance(request.cookies, dict):\n             cookies = ({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n         else:\n             cookies = request.cookies\n\n@@ -8,7 +8,6 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n-\n ACCEPTED_ENCODINGS = [b'gzip', b'deflate']\n \n try:\n\n@@ -8,7 +8,6 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.response import get_meta_refresh\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n-\n logger = logging.getLogger(__name__)\n \n \n@@ -56,7 +55,6 @@ class BaseRedirectMiddleware:\n                          {'reason': reason, 'redirected': redirected, 'request': request},\n                          extra={'spider': spider})\n             return redirected\n-        else:\n         logger.debug(\"Discarding %(request)s: max redirections reached\",\n                      {'request': request}, extra={'spider': spider})\n         raise IgnoreRequest(\"max redirections reached\")\n\n@@ -113,7 +113,6 @@ def get_retry_request(\n         stats.inc_value(f'{stats_base_key}/count')\n         stats.inc_value(f'{stats_base_key}/reason_count/{reason}')\n         return new_request\n-    else:\n     stats.inc_value(f'{stats_base_key}/max_reached')\n     logger.error(\n         \"Gave up retrying %(request)s (failed %(retry_times)d times): \"\n\n@@ -81,7 +81,6 @@ class RobotsTxtMiddleware:\n                 return result\n             self._parsers[netloc].addCallback(cb)\n             return d\n-        else:\n         return self._parsers[netloc]\n \n     def _logerror(self, failure, request, spider):\n\n@@ -1,9 +1,9 @@\n+from twisted.web import http\n+\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.python import global_object_name, to_bytes\n from scrapy.utils.request import request_httprepr\n \n-from twisted.web import http\n-\n \n def get_header_size(headers):\n     size = 0\n\n@@ -55,7 +55,7 @@ class RFPDupeFilter(BaseDupeFilter):\n         self.debug = debug\n         self.logger = logging.getLogger(__name__)\n         if path:\n-            self.file = Path(path, 'requests.seen').open('a+')\n+            self.file = Path(path, 'requests.seen').open('a+', encoding=\"utf-8\")\n             self.file.seek(0)\n             self.fingerprints.update(x.rstrip() for x in self.file)\n \n\n@@ -334,9 +334,9 @@ class PythonItemExporter(BaseItemExporter):\n     def _serialize_value(self, value):\n         if isinstance(value, Item):\n             return self.export_item(value)\n-        elif is_item(value):\n+        if is_item(value):\n             return dict(self._serialize_item(value))\n-        elif is_listlike(value):\n+        if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n         encode_func = to_bytes if self.binary else to_unicode\n         if isinstance(value, (str, bytes)):\n\n@@ -17,7 +17,6 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.project import data_path\n from scrapy.utils.python import to_bytes, to_unicode\n \n-\n logger = logging.getLogger(__name__)\n \n \n@@ -81,24 +80,23 @@ class RFC2616Policy:\n         if b'no-store' in cc:\n             return False\n         # Never cache 304 (Not Modified) responses\n-        elif response.status == 304:\n+        if response.status == 304:\n             return False\n         # Cache unconditionally if configured to do so\n-        elif self.always_store:\n+        if self.always_store:\n             return True\n         # Any hint on response expiration is good\n-        elif b'max-age' in cc or b'Expires' in response.headers:\n+        if b'max-age' in cc or b'Expires' in response.headers:\n             return True\n         # Firefox fallbacks this statuses to one year expiration if none is set\n-        elif response.status in (300, 301, 308):\n+        if response.status in (300, 301, 308):\n             return True\n         # Other statuses without expiration requires at least one validator\n-        elif response.status in (200, 203, 401):\n+        if response.status in (200, 203, 401):\n             return b'Last-Modified' in response.headers or b'ETag' in response.headers\n         # Any other is probably not eligible for caching\n         # Makes no sense to cache responses that does not contain expiration\n         # info and can not be revalidated\n-        else:\n         return False\n \n     def is_cached_response_fresh(self, cachedresponse, request):\n\n@@ -11,6 +11,7 @@ import binascii\n import os\n \n from twisted.internet import protocol\n+\n try:\n     from twisted.conch import manhole, telnet\n     from twisted.conch.insults import insults\n@@ -26,7 +27,6 @@ from scrapy.utils.engine import print_engine_status\n from scrapy.utils.reactor import listen_tcp\n from scrapy.utils.decorators import defers\n \n-\n logger = logging.getLogger(__name__)\n \n # signal to update telnet variables\n\n@@ -56,7 +56,7 @@ class AutoThrottle:\n                 {\n                     'slot': key, 'concurrency': conc,\n                     'delay': slot.delay * 1000, 'delaydiff': diff * 1000,\n-                    'latency': latency * 1000, 'size': size\n+                    'latency': latency * 1000, 'size': size,\n                 },\n                 extra={'spider': spider}\n             )\n\n@@ -37,11 +37,10 @@ class Headers(CaselessDict):\n     def _tobytes(self, x):\n         if isinstance(x, bytes):\n             return x\n-        elif isinstance(x, str):\n+        if isinstance(x, str):\n             return x.encode(self.encoding)\n-        elif isinstance(x, int):\n+        if isinstance(x, int):\n             return str(x).encode(self.encoding)\n-        else:\n         raise TypeError(f'Unsupported value type: {type(x)}')\n \n     def __getitem__(self, key):\n\n@@ -185,7 +185,7 @@ class Request(object_ref):\n         }\n         for attr in self.attributes:\n             d.setdefault(attr, getattr(self, attr))\n-        if type(self) is not Request:\n+        if type(self) is not Request:  # pylint: disable=unidiomatic-typecheck\n             d[\"_class\"] = self.__module__ + '.' + self.__class__.__name__\n         return d\n \n\n@@ -190,7 +190,7 @@ def _select_value(ele: SelectElement, n: str, v: str):\n         # And for select tags without options\n         o = ele.value_options\n         return (n, o[0]) if o else (None, None)\n-    elif v is not None and multiple:\n+    if v is not None and multiple:\n         # This is a workround to bug in lxml fixed 2.3.1\n         # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n         selected_options = ele.xpath('.//option[@selected]')\n@@ -236,7 +236,7 @@ def _get_clickable(clickdata: Optional[dict], form: FormElement) -> Optional[Tup\n     el = form.xpath(xpath)\n     if len(el) == 1:\n         return (el[0].get('name'), el[0].get('value') or '')\n-    elif len(el) > 1:\n+    if len(el) > 1:\n         raise ValueError(f\"Multiple elements found ({el!r}) matching the \"\n                          f\"criteria in clickdata: {clickdata!r}\")\n     else:\n\n@@ -5,7 +5,7 @@ import operator\n from functools import partial\n from urllib.parse import urljoin, urlparse\n \n-import lxml.etree as etree\n+from lxml import etree\n from parsel.csstranslator import HTMLTranslator\n from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url, safe_url_string\n\n@@ -18,7 +18,7 @@ class ItemPipelineManager(MiddlewareManager):\n         return build_component_list(settings.getwithbase('ITEM_PIPELINES'))\n \n     def _add_middleware(self, pipe):\n-        super(ItemPipelineManager, self)._add_middleware(pipe)\n+        super()._add_middleware(pipe)\n         if hasattr(pipe, 'process_item'):\n             self.methods['process_item'].append(deferred_f_from_coro_f(pipe.process_item))\n \n\n@@ -221,7 +221,6 @@ class GCSFilesStore:\n                 checksum = blob.md5_hash\n                 last_modified = time.mktime(blob.updated.timetuple())\n                 return {'checksum': checksum, 'last_modified': last_modified}\n-            else:\n             return {}\n         blob_path = self._get_blob_path(path)\n         return threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(_onsuccess)\n@@ -229,7 +228,6 @@ class GCSFilesStore:\n     def _get_content_type(self, headers):\n         if headers and 'Content-Type' in headers:\n             return headers['Content-Type']\n-        else:\n         return 'application/octet-stream'\n \n     def _get_blob_path(self, path):\n\n@@ -42,9 +42,8 @@ class ResponseTypes:\n         \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n         if mimetype is None:\n             return Response\n-        elif mimetype in self.classes:\n+        if mimetype in self.classes:\n             return self.classes[mimetype]\n-        else:\n         basetype = f\"{mimetype.split('/')[0]}/*\"\n         return self.classes.get(basetype, Response)\n \n@@ -83,7 +82,6 @@ class ResponseTypes:\n         mimetype, encoding = self.mimetypes.guess_type(filename)\n         if mimetype and not encoding:\n             return self.from_mimetype(mimetype)\n-        else:\n         return Response\n \n     def from_body(self, body):\n\n@@ -4,7 +4,6 @@ from abc import ABCMeta, abstractmethod\n \n from scrapy.utils.python import to_unicode\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -24,7 +24,6 @@ def get_settings_priority(priority):\n     \"\"\"\n     if isinstance(priority, str):\n         return SETTINGS_PRIORITIES[priority]\n-    else:\n     return priority\n \n \n@@ -260,7 +259,6 @@ class BaseSettings(MutableMapping):\n         \"\"\"\n         if len(self) > 0:\n             return max(self.getpriority(name) for name in self)\n-        else:\n         return get_settings_priority('default')\n \n     def __setitem__(self, name, value):\n\n@@ -65,7 +65,7 @@ class OffsiteMiddleware:\n         for domain in allowed_domains:\n             if domain is None:\n                 continue\n-            elif url_pattern.match(domain):\n+            if url_pattern.match(domain):\n                 message = (\"allowed_domains accepts only domains, not URLs. \"\n                            f\"Ignoring URL entry {domain} in allowed_domains.\")\n                 warnings.warn(message, URLWarning)\n\n@@ -189,7 +189,6 @@ class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n         origin = self.origin(response_url)\n         if origin == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n-        else:\n         return origin\n \n \n@@ -216,7 +215,7 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n         origin = self.origin(response_url)\n         if origin == self.origin(request_url):\n             return self.stripped_referrer(response_url)\n-        elif (\n+        if (\n             self.tls_protected(response_url) and self.potentially_trustworthy(request_url)\n             or not self.tls_protected(response_url)\n         ):\n\n@@ -26,7 +26,7 @@ def _identity_process_request(request, response):\n def _get_method(method, spider):\n     if callable(method):\n         return method\n-    elif isinstance(method, str):\n+    if isinstance(method, str):\n         return getattr(spider, method, None)\n \n \n\n@@ -69,7 +69,7 @@ class SitemapSpider(Spider):\n         \"\"\"\n         if isinstance(response, XmlResponse):\n             return response.body\n-        elif gzip_magic_number(response):\n+        if gzip_magic_number(response):\n             return gunzip(response.body)\n         # actual gzipped sitemap files are decompressed above ;\n         # if we are here (response body is not gzipped)\n@@ -80,7 +80,7 @@ class SitemapSpider(Spider):\n         # without actually being a .xml.gz file in the first place,\n         # merely XML gzip-compressed on the fly,\n         # in other word, here, we have plain XML\n-        elif response.url.endswith('.xml') or response.url.endswith('.xml.gz'):\n+        if response.url.endswith('.xml') or response.url.endswith('.xml.gz'):\n             return response.body\n \n \n\n@@ -35,7 +35,6 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n                 else:\n                     compbs.set(convert(k), v, priority=prio)\n             return compbs\n-        else:\n         _check_components(compdict)\n         return {convert(k): v for k, v in compdict.items()}\n \n@@ -181,7 +180,6 @@ def feed_process_params_from_cli(settings, output: List[str], output_format=None\n             )\n             warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n             return {output[0]: {'format': output_format}}\n-        else:\n         raise UsageError(\n             'The -t command-line option cannot be used if multiple output '\n             'URIs are specified'\n\n@@ -58,9 +58,8 @@ def defer_succeed(result) -> Deferred:\n def defer_result(result) -> Deferred:\n     if isinstance(result, Deferred):\n         return result\n-    elif isinstance(result, failure.Failure):\n+    if isinstance(result, failure.Failure):\n         return defer_fail(result)\n-    else:\n     return defer_succeed(result)\n \n \n@@ -267,7 +266,6 @@ def deferred_from_coro(o) -> Any:\n             # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n             return ensureDeferred(o)\n-        else:\n         # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n         event_loop = get_asyncio_event_loop_policy().get_event_loop()\n         return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n@@ -295,11 +293,10 @@ def maybeDeferred_coro(f: Callable, *args, **kw) -> Deferred:\n \n     if isinstance(result, Deferred):\n         return result\n-    elif asyncio.isfuture(result) or inspect.isawaitable(result):\n+    if asyncio.isfuture(result) or inspect.isawaitable(result):\n         return deferred_from_coro(result)\n-    elif isinstance(result, failure.Failure):\n+    if isinstance(result, failure.Failure):\n         return defer.fail(result)\n-    else:\n     return defer.succeed(result)\n \n \n@@ -352,5 +349,4 @@ def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n     \"\"\"\n     if not is_asyncio_reactor_installed():\n         return d\n-    else:\n     return deferred_to_future(d)\n\n@@ -5,8 +5,8 @@ pprint and pformat wrappers with colorization support\n import ctypes\n import platform\n import sys\n-from packaging.version import Version as parse_version\n from pprint import pformat as pformat_\n+from packaging.version import Version as parse_version\n \n \n def _enable_windows_terminal_processing():\n\n@@ -1,5 +1,4 @@\n import posixpath\n-\n from ftplib import error_perm, FTP\n from posixpath import dirname\n \n\n@@ -138,7 +138,6 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n                            {'csvlnum': csv_r.line_num, 'csvrow': len(row),\n                             'csvheader': len(headers)})\n             continue\n-        else:\n         yield dict(zip(headers, row))\n \n \n@@ -152,11 +151,9 @@ def _body_or_str(obj, unicode=True):\n     if isinstance(obj, Response):\n         if not unicode:\n             return obj.body\n-        elif isinstance(obj, TextResponse):\n+        if isinstance(obj, TextResponse):\n             return obj.text\n-        else:\n         return obj.body.decode('utf-8')\n-    elif isinstance(obj, str):\n+    if isinstance(obj, str):\n         return obj if unicode else obj.encode('utf-8')\n-    else:\n     return obj.decode('utf-8') if unicode else obj\n\n@@ -30,9 +30,8 @@ def arg_to_iter(arg):\n     \"\"\"\n     if arg is None:\n         return []\n-    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n+    if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n         return arg\n-    else:\n     return [arg]\n \n \n@@ -49,7 +48,6 @@ def load_object(path):\n     if not isinstance(path, str):\n         if callable(path):\n             return path\n-        else:\n         raise TypeError(\"Unexpected argument type, expected string \"\n                         f\"or object, got: {type(path)}\")\n \n@@ -115,7 +113,6 @@ def extract_regex(regex, text, encoding='utf-8'):\n \n     if isinstance(text, str):\n         return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n-    else:\n     return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])\n             for s in strings]\n \n\n@@ -187,9 +187,8 @@ def get_func_args(func, stripself=False):\n     elif hasattr(func, '__call__'):\n         if inspect.isroutine(func):\n             return []\n-        elif getattr(func, '__name__', None) == '__call__':\n+        if getattr(func, '__name__', None) == '__call__':\n             return []\n-        else:\n         return get_func_args(func.__call__, True)\n     else:\n         raise TypeError(f'{type(func)} is not callable')\n\n@@ -9,14 +9,14 @@ import webbrowser\n from typing import Any, Callable, Iterable, Optional, Tuple, Union\n from weakref import WeakKeyDictionary\n \n+from twisted.web import http\n+from w3lib import html\n import scrapy\n from scrapy.http.response import Response\n \n-from twisted.web import http\n+\n from scrapy.utils.python import to_bytes, to_unicode\n from scrapy.utils.decorators import deprecated\n-from w3lib import html\n-\n \n _baseurl_cache: \"WeakKeyDictionary[Response, str]\" = WeakKeyDictionary()\n \n\n@@ -16,23 +16,22 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n     def default(self, o):\n         if isinstance(o, set):\n             return list(o)\n-        elif isinstance(o, datetime.datetime):\n+        if isinstance(o, datetime.datetime):\n             return o.strftime(f\"{self.DATE_FORMAT} {self.TIME_FORMAT}\")\n-        elif isinstance(o, datetime.date):\n+        if isinstance(o, datetime.date):\n             return o.strftime(self.DATE_FORMAT)\n-        elif isinstance(o, datetime.time):\n+        if isinstance(o, datetime.time):\n             return o.strftime(self.TIME_FORMAT)\n-        elif isinstance(o, decimal.Decimal):\n+        if isinstance(o, decimal.Decimal):\n             return str(o)\n-        elif isinstance(o, defer.Deferred):\n+        if isinstance(o, defer.Deferred):\n             return str(o)\n-        elif is_item(o):\n+        if is_item(o):\n             return ItemAdapter(o).asdict()\n-        elif isinstance(o, Request):\n+        if isinstance(o, Request):\n             return f\"<{type(o).__name__} {o.method} {o.url}>\"\n-        elif isinstance(o, Response):\n+        if isinstance(o, Response):\n             return f\"<{type(o).__name__} {o.status} {o.url}>\"\n-        else:\n         return super().default(o)\n \n \n\n@@ -12,11 +12,10 @@ logger = logging.getLogger(__name__)\n def iterate_spider_output(result):\n     if inspect.isasyncgen(result):\n         return result\n-    elif inspect.iscoroutine(result):\n+    if inspect.iscoroutine(result):\n         d = deferred_from_coro(result)\n         d.addCallback(iterate_spider_output)\n         return d\n-    else:\n     return arg_to_iter(deferred_from_coro(result))\n \n \n\n@@ -58,7 +58,7 @@ setup(\n         'Tracker': 'https://github.com/scrapy/scrapy/issues',\n     },\n     description='A high-level Web Crawling and Web Scraping framework',\n-    long_description=open('README.rst').read(),\n+    long_description=open('README.rst', encoding=\"utf-8\").read(),\n     author='Scrapy developers',\n     maintainer='Pablo Hoffman',\n     maintainer_email='pablo@pablohoffman.com',\n\n@@ -1,6 +1,7 @@\n+from twisted.internet import reactor  # noqa: F401\n+\n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from twisted.internet import reactor  # noqa: F401\n \n \n class NoRequestsSpider(scrapy.Spider):\n\n@@ -1,6 +1,7 @@\n+from twisted.internet import reactor  # noqa: F401\n+\n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from twisted.internet import reactor  # noqa: F401\n \n \n class NoRequestsSpider(scrapy.Spider):\n\n@@ -1,6 +1,8 @@\n+from twisted.internet import selectreactor\n+\n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from twisted.internet import selectreactor\n+\n selectreactor.install()\n \n \n\n@@ -1,7 +1,7 @@\n-import scrapy\n-from scrapy.crawler import CrawlerProcess\n from twisted.internet.main import installReactor\n from twisted.internet.selectreactor import SelectReactor\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n \n \n class SelectReactorSubclass(SelectReactor):\n\n@@ -1,6 +1,8 @@\n+from twisted.internet import selectreactor\n+\n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from twisted.internet import selectreactor\n+\n selectreactor.install()\n \n \n\n@@ -8,7 +8,6 @@ from twisted.python.runtime import platform\n from scrapy import Spider, Request\n from scrapy.crawler import CrawlerRunner\n from scrapy.utils.log import configure_logging\n-\n from tests.mockserver import MockServer, MockDNSServer\n \n \n@@ -30,9 +29,10 @@ class LocalhostSpider(Spider):\n \n     def parse(self, response):\n         netloc = urlparse(response.url).netloc\n-        self.logger.info(\"Host: %s\" % netloc.split(\":\")[0])\n-        self.logger.info(\"Type: %s\" % type(response.ip_address))\n-        self.logger.info(\"IP address: %s\" % response.ip_address)\n+        host = netloc.split(\":\")[0]\n+        self.logger.info(f\"Host: {host}\")\n+        self.logger.info(f\"Type: {type(response.ip_address)}\")\n+        self.logger.info(f\"IP address: {response.ip_address}\")\n \n \n if __name__ == \"__main__\":\n\n@@ -29,7 +29,6 @@ def getarg(request, name, default=None, type=None):\n         if type is not None:\n             value = type(value)\n         return value\n-    else:\n     return default\n \n \n\n@@ -6,7 +6,7 @@ class CheckCommandTest(CommandTest):\n     command = 'check'\n \n     def setUp(self):\n-        super(CheckCommandTest, self).setUp()\n+        super().setUp()\n         self.spider_name = 'check_spider'\n         self.spider = (self.proj_mod_path / 'spiders' / 'checkspider.py').resolve()\n \n@@ -24,7 +24,7 @@ class CheckSpider(scrapy.Spider):\n         {contracts}\n         \\\"\\\"\\\"\n         {parse_def}\n-        \"\"\")\n+        \"\"\", encoding=\"utf-8\")\n \n     def _test_contract(self, contracts='', parse_def='pass'):\n         self._write_contract(contracts, parse_def)\n\n@@ -92,7 +92,7 @@ class MyBadCrawlSpider(CrawlSpider):\n \n     def parse(self, response):\n         return [scrapy.Item(), dict(foo='bar')]\n-\"\"\")\n+\"\"\", encoding=\"utf-8\")\n \n         (self.proj_mod_path / 'pipelines.py').write_text(\"\"\"\n import logging\n@@ -103,9 +103,9 @@ class MyPipeline:\n     def process_item(self, item, spider):\n         logging.info('It Works!')\n         return item\n-\"\"\")\n+\"\"\", encoding=\"utf-8\")\n \n-        with (self.proj_mod_path / 'settings.py').open(\"a\") as f:\n+        with (self.proj_mod_path / 'settings.py').open(\"a\", encoding=\"utf-8\") as f:\n             f.write(f\"\"\"\n ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n \"\"\")\n@@ -256,7 +256,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         self.assertTrue(file_path.is_file())\n \n         content = '[\\n{},\\n{\"foo\": \"bar\"}\\n]'\n-        self.assertEqual(file_path.read_text(), content)\n+        self.assertEqual(file_path.read_text(encoding=\"utf-8\"), content)\n \n     def test_parse_add_options(self):\n         command = parse.Command()\n\n@@ -107,7 +107,7 @@ class ProjectTest(unittest.TestCase):\n     def find_in_file(self, filename: Union[str, os.PathLike], regex) -> Optional[re.Match]:\n         \"\"\"Find first pattern occurrence in file\"\"\"\n         pattern = re.compile(regex)\n-        with Path(filename).open(\"r\") as f:\n+        with Path(filename).open(\"r\", encoding=\"utf-8\") as f:\n             for line in f:\n                 match = pattern.search(line)\n                 if match is not None:\n@@ -475,7 +475,7 @@ class GenspiderCommandTest(CommandTest):\n         assert file_path.exists()\n \n         # change name of spider but not its file name\n-        with file_path.open('r+') as spider_file:\n+        with file_path.open('r+', encoding=\"utf-8\") as spider_file:\n             file_data = spider_file.read()\n             file_data = file_data.replace(\"name = \\'example\\'\", \"name = \\'renamed\\'\")\n             spider_file.seek(0)\n@@ -489,14 +489,14 @@ class GenspiderCommandTest(CommandTest):\n             self.assertIn(f\"Created spider {file_name!r} using template \\'basic\\' in module\", out)\n             modify_time_after = file_path.stat().st_mtime\n             self.assertNotEqual(modify_time_after, modify_time_before)\n-            file_contents_after = file_path.read_text()\n+            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n             p, out, err = self.proc('genspider', file_name, 'example.com')\n             self.assertIn(f\"{file_path.resolve()} already exists\", out)\n             modify_time_after = file_path.stat().st_mtime\n             self.assertEqual(modify_time_after, modify_time_before)\n-            file_contents_after = file_path.read_text()\n+            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n             self.assertEqual(file_contents_after, file_contents_before)\n \n     def test_same_filename_as_existing_spider_force(self):\n@@ -536,7 +536,7 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n         self.assertIn(f\"Created spider {file_name!r} using template \\'basic\\' \", out)\n         assert file_path.exists()\n         modify_time_before = file_path.stat().st_mtime\n-        file_contents_before = file_path.read_text()\n+        file_contents_before = file_path.read_text(encoding=\"utf-8\")\n \n         if force:\n             # use different template to ensure contents were changed\n@@ -544,14 +544,14 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n             self.assertIn(f\"Created spider {file_name!r} using template \\'crawl\\' \", out)\n             modify_time_after = file_path.stat().st_mtime\n             self.assertNotEqual(modify_time_after, modify_time_before)\n-            file_contents_after = file_path.read_text()\n+            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n             p, out, err = self.proc('genspider', file_name, 'example.com')\n             self.assertIn(f\"{Path(self.temp_path, file_name + '.py').resolve()} already exists\", out)\n             modify_time_after = file_path.stat().st_mtime\n             self.assertEqual(modify_time_after, modify_time_before)\n-            file_contents_after = file_path.read_text()\n+            file_contents_after = file_path.read_text(encoding=\"utf-8\")\n             self.assertEqual(file_contents_after, file_contents_before)\n \n     def test_same_name_as_existing_file_force(self):\n@@ -596,7 +596,7 @@ class BadSpider(scrapy.Spider):\n             fname = (tmpdir / name).resolve()\n         else:\n             fname = (tmpdir / self.spider_filename).resolve()\n-        fname.write_text(content)\n+        fname.write_text(content, encoding=\"utf-8\")\n         try:\n             yield str(fname)\n         finally:\n@@ -754,11 +754,11 @@ class MySpider(scrapy.Spider):\n         )\n         return []\n \"\"\"\n-        Path(self.cwd, \"example.json\").write_text(\"not empty\")\n+        Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n         args = ['-O', 'example.json']\n         log = self.get_log(spider_code, args=args)\n         self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n-        with Path(self.cwd, \"example.json\").open() as f2:\n+        with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n             first_line = f2.readline()\n         self.assertNotEqual(first_line, \"not empty\")\n \n@@ -798,7 +798,7 @@ class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n     spider_filename = 'myspider.pyw'\n \n     def setUp(self):\n-        super(WindowsRunSpiderCommandTest, self).setUp()\n+        super().setUp()\n \n     def test_start_requests_errors(self):\n         log = self.get_log(self.badspider, name='badspider.pyw')\n@@ -860,7 +860,7 @@ class ViewCommandTest(CommandTest):\n class CrawlCommandTest(CommandTest):\n \n     def crawl(self, code, args=()):\n-        Path(self.proj_mod_path, 'spiders', 'myspider.py').write_text(code)\n+        Path(self.proj_mod_path, 'spiders', 'myspider.py').write_text(code, encoding=\"utf-8\")\n         return self.proc('crawl', 'myspider', *args)\n \n     def get_log(self, code, args=()):\n@@ -912,11 +912,11 @@ class MySpider(scrapy.Spider):\n         )\n         return []\n \"\"\"\n-        Path(self.cwd, \"example.json\").write_text(\"not empty\")\n+        Path(self.cwd, \"example.json\").write_text(\"not empty\", encoding=\"utf-8\")\n         args = ['-O', 'example.json']\n         log = self.get_log(spider_code, args=args)\n         self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n-        with Path(self.cwd, \"example.json\").open() as f2:\n+        with Path(self.cwd, \"example.json\").open(encoding=\"utf-8\") as f2:\n             first_line = f2.readline()\n         self.assertNotEqual(first_line, \"not empty\")\n \n\n@@ -11,6 +11,9 @@ from twisted.internet import defer\n from twisted.python.versions import Version\n from twisted.trial import unittest\n \n+from pkg_resources import parse_version\n+from w3lib import __version__ as w3lib_version\n+\n import scrapy\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -23,8 +26,6 @@ from scrapy.utils.test import get_crawler\n from scrapy.extensions.throttle import AutoThrottle\n from scrapy.extensions import telnet\n from scrapy.utils.test import get_testenv\n-from pkg_resources import parse_version\n-from w3lib import __version__ as w3lib_version\n \n from tests.mockserver import MockServer\n \n\n@@ -110,7 +110,7 @@ class FileTestCase(unittest.TestCase):\n     def setUp(self):\n         # add a special char to check that they are handled correctly\n         self.tmpname = Path(self.mktemp() + '^')\n-        Path(self.tmpname).write_text('0123456789')\n+        Path(self.tmpname).write_text(\"0123456789\", encoding=\"utf-8\")\n         handler = create_instance(FileDownloadHandler, None, get_crawler())\n         self.download_request = handler.download_request\n \n@@ -722,7 +722,6 @@ class UriResource(resource.Resource):\n         # ToDo: implement proper HTTPS proxy tests, not faking them.\n         if request.method != b'CONNECT':\n             return request.uri\n-        else:\n         return b''\n \n \n\n@@ -171,7 +171,7 @@ class Https2InvalidDNSId(Https2TestCase):\n     \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n \n     def setUp(self):\n-        super(Https2InvalidDNSId, self).setUp()\n+        super().setUp()\n         self.host = '127.0.0.1'\n \n \n@@ -190,7 +190,7 @@ class Https2InvalidDNSPattern(Https2TestCase):\n             'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=127.0.0.1\", '\n             'subject \"/C=IE/O=Scrapy/CN=127.0.0.1\"'\n         )\n-        super(Https2InvalidDNSPattern, self).setUp()\n+        super().setUp()\n \n \n @skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n@@ -245,4 +245,4 @@ class Https2ProxyTestCase(Http11ProxyTestCase):\n     @defer.inlineCallbacks\n     def test_download_with_proxy_https_timeout(self):\n         with self.assertRaises(NotImplementedError):\n-            yield super(Https2ProxyTestCase, self).test_download_with_proxy_https_timeout()\n+            yield super().test_download_with_proxy_https_timeout()\n\n@@ -1,6 +1,6 @@\n import logging\n-from testfixtures import LogCapture\n from unittest import TestCase\n+from testfixtures import LogCapture\n \n import pytest\n \n\n@@ -2,8 +2,8 @@ from unittest import TestCase, main\n from scrapy.http import Response, XmlResponse\n from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware\n from scrapy.spiders import Spider\n-from tests import get_testdata\n from scrapy.utils.test import assert_samelines\n+from tests import get_testdata\n \n \n def _test_data(formats):\n\n@@ -272,7 +272,6 @@ class RFC2616PolicyTest(DefaultStorageTest):\n             if result:\n                 assert isinstance(result, (Request, Response))\n                 return result\n-            else:\n             result = mw.process_response(request, response, self.spider)\n             assert isinstance(result, Response)\n             return result\n\n@@ -4,6 +4,7 @@ from pathlib import Path\n from unittest import TestCase, SkipTest\n from warnings import catch_warnings\n \n+from w3lib.encoding import resolve_encoding\n from scrapy.spiders import Spider\n from scrapy.http import Response, Request, HtmlResponse\n from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, ACCEPTED_ENCODINGS\n@@ -12,8 +13,6 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.gz import gunzip\n from scrapy.utils.test import get_crawler\n from tests import tests_datadir\n-from w3lib.encoding import resolve_encoding\n-\n \n SAMPLEDIR = Path(tests_datadir, 'compressed')\n \n\n@@ -400,7 +400,6 @@ class XmlItemExporterTest(BaseItemExporterTest):\n             children = list(elem.iterchildren())\n             if children:\n                 return [(child.tag, sorted(xmltuple(child))) for child in children]\n-            else:\n             return [(elem.tag, [(elem.text, ())])]\n \n         def xmlsplit(xmlcontent):\n@@ -621,7 +620,6 @@ class CustomExporterItemTest(unittest.TestCase):\n             def serialize_field(self, field, name, value):\n                 if name == 'age':\n                     return str(int(value) + 1)\n-                else:\n                 return super().serialize_field(field, name, value)\n \n         i = self.item_class(name='John', age='22')\n\n@@ -164,7 +164,7 @@ class RequestHeaders(LeafResource):\n \n \n def get_client_certificate(key_file: Path, certificate_file: Path) -> PrivateCertificate:\n-    pem = key_file.read_text() + certificate_file.read_text()\n+    pem = key_file.read_text(encoding=\"utf-8\") + certificate_file.read_text(encoding=\"utf-8\")\n \n     return PrivateCertificate.loadPEM(pem)\n \n\n@@ -223,7 +223,7 @@ class RequestTest(unittest.TestCase):\n         r1 = CustomRequest('http://www.example.com')\n         r2 = r1.copy()\n \n-        assert type(r2) is CustomRequest\n+        assert isinstance(r2, CustomRequest)\n \n     def test_replace(self):\n         \"\"\"Test Request.replace() method\"\"\"\n\n@@ -102,7 +102,7 @@ class BaseResponseTest(unittest.TestCase):\n         r1 = CustomResponse('http://www.example.com')\n         r2 = r1.copy()\n \n-        assert type(r2) is CustomResponse\n+        assert isinstance(r2, CustomResponse)\n \n     def test_replace(self):\n         \"\"\"Test Response.replace() method\"\"\"\n\n@@ -391,9 +391,8 @@ class BasicItemLoaderTest(unittest.TestCase):\n         def join(values, sep=None, loader_context=None, ignored=None):\n             if sep is not None:\n                 return sep.join(values)\n-            elif loader_context and 'sep' in loader_context:\n+            if loader_context and 'sep' in loader_context:\n                 return loader_context['sep'].join(values)\n-            else:\n             return ''.join(values)\n \n         class TestItemLoader(NameItemLoader):\n\n@@ -388,11 +388,11 @@ class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n \n     def thumb_path(self, request, thumb_id, response=None, info=None):\n         self._mockcalled.append('thumb_path')\n-        return super(MockedMediaPipelineDeprecatedMethods, self).thumb_path(request, thumb_id, response, info)\n+        return super().thumb_path(request, thumb_id, response, info)\n \n     def get_images(self, response, request, info):\n         self._mockcalled.append('get_images')\n-        return super(MockedMediaPipelineDeprecatedMethods, self).get_images(response, request, info)\n+        return super().get_images(response, request, info)\n \n     def image_downloaded(self, response, request, info):\n         self._mockcalled.append('image_downloaded')\n\n@@ -1,12 +1,12 @@\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n+from testfixtures import LogCapture\n+\n from scrapy import Request, signals\n from scrapy.http.response import Response\n from scrapy.utils.test import get_crawler\n \n-from testfixtures import LogCapture\n-\n from tests.mockserver import MockServer\n from tests.spiders import SingleRequestSpider\n \n\n@@ -91,7 +91,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n             self.checks.append(kwargs['callback'] == 'some_callback')\n             self.crawler.stats.inc_value('boolean_checks', 3)\n         elif response.url.endswith('/general_without'):\n-            self.checks.append(kwargs == {})\n+            self.checks.append(kwargs == {})  # pylint: disable=use-implicit-booleaness-not-comparison\n             self.crawler.stats.inc_value('boolean_checks')\n \n     def parse_no_kwargs(self, response):\n\n@@ -11,10 +11,8 @@ from scrapy.http import Request\n from scrapy.spiders import Spider\n from scrapy.utils.request import fingerprint\n from scrapy.utils.test import get_crawler\n-\n from tests.mockserver import MockServer\n \n-\n PATHS = [\"/a\", \"/b\", \"/c\"]\n URLS = [urljoin(\"https://example.org\", p) for p in PATHS]\n \n\n@@ -7,6 +7,7 @@ from unittest import mock\n from testfixtures import LogCapture\n from twisted.trial import unittest\n \n+from w3lib.url import safe_url_string\n from scrapy import signals\n from scrapy.settings import Settings\n from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse\n@@ -22,7 +23,6 @@ from scrapy.spiders import (\n from scrapy.linkextractors import LinkExtractor\n from scrapy.utils.test import get_crawler\n from tests import get_testdata\n-from w3lib.url import safe_url_string\n \n \n class SpiderTest(unittest.TestCase):\n\n@@ -3,6 +3,7 @@ import shutil\n import warnings\n from pathlib import Path\n \n+import tempfile\n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n \n@@ -10,7 +11,6 @@ from twisted.trial import unittest\n # ugly hack to avoid cyclic imports of scrapy.spiders when running this test\n # alone\n import scrapy\n-import tempfile\n from scrapy.interfaces import ISpiderLoader\n from scrapy.spiderloader import SpiderLoader\n from scrapy.settings import Settings\n\n@@ -6,11 +6,11 @@ from twisted.trial.unittest import TestCase as TrialTestCase\n from twisted.internet import defer\n \n from scrapy.utils.test import get_crawler\n-from tests.mockserver import MockServer\n from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n from scrapy.spidermiddlewares.httperror import HttpErrorMiddleware, HttpError\n from scrapy.settings import Settings\n+from tests.mockserver import MockServer\n from tests.spiders import MockServerSpider\n \n \n\n@@ -4,7 +4,6 @@ from twisted.trial.unittest import TestCase\n \n from scrapy import Request, Spider\n from scrapy.utils.test import get_crawler\n-\n from tests.mockserver import MockServer\n \n \n\n@@ -1,8 +1,8 @@\n from urllib.parse import urlparse\n from unittest import TestCase\n import warnings\n-\n from scrapy.http import Response, Request\n+\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n@@ -380,7 +380,7 @@ class CustomPythonOrgPolicy(ReferrerPolicy):\n         scheme = urlparse(request).scheme\n         if scheme == 'https':\n             return b'https://python.org/'\n-        elif scheme == 'http':\n+        if scheme == 'http':\n             return b'http://python.org/'\n \n \n\n@@ -16,7 +16,6 @@ from scrapy.http import Request\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n-\n \"\"\"\n Queues that handle requests\n \"\"\"\n\n@@ -193,7 +193,6 @@ class AsyncCooperatorTest(unittest.TestCase):\n             delay = random.random() / 8\n             reactor.callLater(delay, dfd.callback, None)\n             return dfd\n-        else:\n         # simulate trivial sync processing\n         results.append(o)\n \n\n@@ -2,6 +2,7 @@ import inspect\n import unittest\n from unittest import mock\n import warnings\n+\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.deprecate import create_deprecated_class, update_classpath\n \n\n@@ -47,7 +47,7 @@ def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n \n     from twisted.web.client import _makeGetterFactory\n     return _makeGetterFactory(\n-        to_bytes(url), _clientfactory, contextFactory=contextFactory, *args, **kwargs\n+        to_bytes(url), _clientfactory, contextFactory=contextFactory, *args, **kwargs,\n     ).deferred\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
