{"custom_id": "keras#7253945282b596e56f6844af305975cd2753e70f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 72 | Churn Cumulative: 842 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: 0.03571428571428571\n\nDIFF:\n@@ -1,4 +1,6 @@\n import jax\n+import numpy as np\n+import tensorflow as tf  # for nest\n \n from keras_core import backend\n from keras_core import callbacks as callbacks_module\n@@ -290,7 +292,7 @@ class Trainer(base_trainer.Trainer):\n         if use_cached_eval_dataset:\n             epoch_iterator = self._eval_epoch_iterator\n         else:\n-            # Create an iterator that yields batches for one epoch.\n+            # Create an iterator that yields batches of input/target data.\n             epoch_iterator = EpochIterator(\n                 x=x,\n                 y=y,\n@@ -421,4 +423,70 @@ class Trainer(base_trainer.Trainer):\n     def predict(\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n-        raise NotImplementedError\n+        # Create an iterator that yields batches of input data.\n+        epoch_iterator = EpochIterator(\n+            x=x,\n+            batch_size=batch_size,\n+            steps_per_epoch=steps,\n+            shuffle=False,\n+        )\n+\n+        if not self.built:\n+            # Build the model on one batch of data.\n+            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+                # Build model\n+                y_pred = self(data)\n+                break\n+\n+        # Container that configures and calls callbacks.\n+        if not isinstance(callbacks, callbacks_module.CallbackList):\n+            callbacks = callbacks_module.CallbackList(\n+                callbacks,\n+                add_history=True,\n+                add_progbar=verbose != 0,\n+                verbose=verbose,\n+                epochs=1,\n+                steps=epoch_iterator.num_batches,\n+                model=self,\n+            )\n+\n+        if not self.run_eagerly and self.jit_compile:\n+\n+            @jax.jit\n+            def predict_step(\n+                trainable_variables, non_trainable_variables, data\n+            ):\n+                return self.stateless_call(\n+                    trainable_variables, non_trainable_variables, data\n+                )\n+\n+        else:\n+            predict_step = self.stateless_call\n+\n+        callbacks.on_predict_begin()\n+\n+        trainable_variables = self.trainable_variables\n+        non_trainable_variables = self.non_trainable_variables\n+        outputs = None\n+        for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            callbacks.on_predict_batch_begin(step)\n+            batch_outputs, non_trainable_variables = predict_step(\n+                trainable_variables, non_trainable_variables, x\n+            )\n+            if outputs is None:\n+                outputs = tf.nest.map_structure(\n+                    lambda batch_output: [batch_output],\n+                    batch_outputs,\n+                )\n+            else:\n+                tf.__internal__.nest.map_structure_up_to(\n+                    batch_outputs,\n+                    lambda output, batch_output: output.append(batch_output),\n+                    outputs,\n+                    batch_outputs,\n+                )\n+            callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n+        callbacks.on_predict_end()\n+        return tf.__internal__.nest.map_structure_up_to(\n+            batch_outputs, np.concatenate, outputs\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff097e5b02ed4a1e1a24d9df7a7b67d2f0b41fd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 35 | Churn Cumulative: 129 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,14 +4,14 @@ from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n from keras_core import optimizers\n-from keras_core.models import Functional\n+from keras_core.models import Model\n \n inputs = layers.Input((128,), batch_size=32)\n x = layers.Dense(256)(inputs)\n x = layers.Dense(256)(x)\n x = layers.Dense(256)(x)\n outputs = layers.Dense(16)(x)\n-model = Functional(inputs, outputs)\n+model = Model(inputs, outputs)\n model.summary()\n \n x = np.random.random((50000, 128))\n\n@@ -28,15 +28,14 @@ class Model(Trainer, Layer):\n     - no masking support\n     \"\"\"\n \n-    # def __new__(cls, *args, **kwargs):\n-    #     # Signature detection\n-    #     if functional_init_arguments(args, kwargs) and cls == Model:\n-    #         # Functional model\n-    #         from keras_core.models import functional\n+    def __new__(cls, *args, **kwargs):\n+        # Signature detection\n+        if functional_init_arguments(args, kwargs) and cls == Model:\n+            # Functional model\n+            from keras_core.models import functional\n \n-    #         return functional.Functional(*args, **kwargs, skip_init=True)\n-    #     else:\n-    #         return super(Model, cls).__new__(cls, *args, **kwargs)\n+            return functional.Functional(*args, **kwargs, skip_init=True)\n+        return Layer.__new__(cls)\n \n     def __init__(self, trainable=True, name=None, dtype=None):\n         Trainer.__init__(self)\n\n@@ -0,0 +1,16 @@\n+from keras_core import layers\n+from keras_core import testing\n+from keras_core.layers.core.input_layer import Input\n+from keras_core.models.functional import Functional\n+from keras_core.models.model import Model\n+\n+\n+class ModelTest(testing.TestCase):\n+    def test_functional_rerouting(self):\n+        input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n+        input_b = Input(shape=(3,), batch_size=2, name=\"input_b\")\n+        x = input_a + input_b\n+        x = layers.Dense(5)(x)\n+        outputs = layers.Dense(4)(x)\n+        model = Model([input_a, input_b], outputs)\n+        self.assertTrue(isinstance(model, Functional))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6603a1ed466a165afd0d920beeea20953fa6b14d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 1878 | Contributors (this commit): 2 | Commits (past 90d): 46 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.epoch_iterator import EpochIterator\n \n \n-class Trainer(base_trainer.Trainer):\n+class JAXTrainer(base_trainer.Trainer):\n     def compute_loss_and_updates(\n         self, trainable_variables, non_trainable_variables, x, y, sample_weight\n     ):\n\n@@ -12,7 +12,7 @@ from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.epoch_iterator import EpochIterator\n \n \n-class Trainer(base_trainer.Trainer):\n+class TensorFlowTrainer(base_trainer.Trainer):\n     def __init__(self):\n         super().__init__()\n         self.train_function = None\n@@ -290,7 +290,7 @@ class Trainer(base_trainer.Trainer):\n         if use_cached_eval_dataset:\n             epoch_iterator = self._eval_epoch_iterator\n         else:\n-            # Create an iterator that yields batches for one epoch.\n+            # Create an iterator that yields batches of input/target data.\n             epoch_iterator = TFEpochIterator(\n                 x=x,\n                 y=y,\n\n@@ -4,9 +4,9 @@ from keras_core.layers.layer import Layer\n from keras_core.utils import summary_utils\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow.trainer import Trainer\n+    from keras_core.backend.tensorflow.trainer import TensorFlowTrainer as Trainer\n elif backend.backend() == \"jax\":\n-    from keras_core.backend.jax.trainer import Trainer\n+    from keras_core.backend.jax.trainer import JAXTrainer as Trainer\n else:\n     Trainer = None\n \n\n@@ -9,9 +9,9 @@ from keras_core import optimizers\n from keras_core import testing\n \n if backend.backend() == \"jax\":\n-    from keras_core.backend.jax.trainer import Trainer\n+    from keras_core.backend.jax.trainer import JAXTrainer as Trainer\n else:\n-    from keras_core.backend.tensorflow.trainer import Trainer\n+    from keras_core.backend.tensorflow.trainer import TensorFlowTrainer as Trainer\n \n \n # A model is just a layer mixed in with a Trainer.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6307e4edc8848838b37a6bcbfe62682a3721cbe1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 11/6 | Churn Δ: 79 | Churn Cumulative: 281 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,11 +17,12 @@\n import abc\n import threading\n \n+from absl import logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export(\"keras.utils.TimedThread\", v1=[])\n-class TimedThread(threading.Thread):\n+class TimedThread:\n     \"\"\"Time-based interval Threads.\n \n     Runs a timed thread every x seconds. It can be used to run a threaded\n@@ -30,15 +31,17 @@ class TimedThread(threading.Thread):\n     Args:\n         interval: The interval, in seconds, to wait between calls to the\n             `on_interval` function.\n-        **kwargs: additional args that are passed to `threading.Thread`.\n+        **kwargs: additional args that are passed to `threading.Thread`. By\n+            default, `Thread` is started as a `daemon` thread unless\n+            overridden by the user in `kwargs`.\n \n     Examples:\n \n     ```python\n     class TimedLogIterations(keras.utils.TimedThread):\n-        def __init__(self, model, interval, *args, **kwargs):\n+        def __init__(self, model, interval):\n             self.model = model\n-            super().__init__(interval, *args, **kwargs)\n+            super().__init__(interval)\n \n         def on_interval(self):\n             # Logs Optimizer iterations every x seconds\n@@ -92,9 +95,10 @@ class TimedThread(threading.Thread):\n \n     def __init__(self, interval, **kwargs):\n         self.interval = interval\n-        daemon = kwargs.pop(\"daemon\", True)\n-        self.thread_stop_event = threading.Event()\n-        super().__init__(target=self._call_on_interval, daemon=daemon, **kwargs)\n+        self.daemon = kwargs.pop(\"daemon\", True)\n+        self.thread_kwargs = kwargs\n+        self.thread = None\n+        self.thread_stop_event = None\n \n     def _call_on_interval(self):\n         # Runs indefinitely once thread is started\n@@ -102,10 +106,30 @@ class TimedThread(threading.Thread):\n             self.on_interval()\n             self.thread_stop_event.wait(self.interval)\n \n+    def start(self):\n+        \"\"\"Creates and starts the thread run.\"\"\"\n+        if self.thread and self.thread.is_alive():\n+            logging.warning(\"Thread is already running.\")\n+            return\n+        self.thread = threading.Thread(\n+            target=self._call_on_interval,\n+            daemon=self.daemon,\n+            **self.thread_kwargs\n+        )\n+        self.thread_stop_event = threading.Event()\n+        self.thread.start()\n+\n     def stop(self):\n         \"\"\"Stops the thread run.\"\"\"\n+        if self.thread_stop_event:\n             self.thread_stop_event.set()\n \n+    def is_alive(self):\n+        \"\"\"Returns True if thread is running. Otherwise returns False.\"\"\"\n+        if self.thread:\n+            return self.thread.is_alive()\n+        return False\n+\n     def __enter__(self):\n         # Starts the thread in context manager\n         self.start()\n\n@@ -42,6 +42,47 @@ class TimedThreadTest(test_combinations.TestCase):\n         time.sleep(0.1)\n         self.assertFalse(log_thread.is_alive())\n \n+    def test_timed_thread_restart(self):\n+        # Verfiy that thread can be started and stopped multiple times.\n+        class LogThread(timed_threads.TimedThread):\n+            def on_interval(self):\n+                logging.info(\"Thread Run\")\n+\n+        log_thread = LogThread(interval=0.1)\n+        for _ in range(2):\n+            self.assertFalse(log_thread.is_alive())\n+            with self.assertLogs(level=\"INFO\") as logs:\n+                log_thread.start()\n+                time.sleep(1)\n+                self.assertTrue(log_thread.is_alive())\n+                log_thread.stop()\n+            self.assertIn(\"INFO:absl:Thread Run\", logs.output)\n+            time.sleep(0.1)\n+            self.assertFalse(log_thread.is_alive())\n+\n+    def test_timed_thread_running_warning(self):\n+        # Verfiy thread start warning if its already running\n+        class LogThread(timed_threads.TimedThread):\n+            def on_interval(self):\n+                logging.info(\"Thread Run\")\n+\n+        log_thread = LogThread(interval=0.1)\n+        self.assertFalse(log_thread.is_alive())\n+        with self.assertLogs(level=\"INFO\") as logs:\n+            log_thread.start()\n+            time.sleep(1)\n+            self.assertTrue(log_thread.is_alive())\n+            self.assertIn(\"INFO:absl:Thread Run\", logs.output)\n+        with self.assertLogs(level=\"WARNING\") as logs:\n+            log_thread.start()\n+            self.assertIn(\n+                \"WARNING:absl:Thread is already running.\", logs.output\n+            )\n+            self.assertTrue(log_thread.is_alive())\n+        log_thread.stop()\n+        time.sleep(0.1)\n+        self.assertFalse(log_thread.is_alive())\n+\n     def test_timed_thread_callback_model_fit(self):\n         class LogThreadCallback(\n             timed_threads.TimedThread, keras.callbacks.Callback\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d1ca982a64b86c1ac3a113f66c4f17986fba243", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 14 | Churn Cumulative: 15889 | Contributors (this commit): 40 | Commits (past 90d): 18 | Contributors (cumulative): 43 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2599,9 +2599,21 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         ):\n             # Check input assumptions set after layer building, e.g. input\n             # shape.\n+            try:\n                 outputs = self._keras_tensor_symbolic_call(\n                     inputs, input_masks, args, kwargs\n                 )\n+            except TypeError as e:\n+                if \"DictWrapper\" in str(e):\n+                    raise TypeError(\n+                        f\"{self} could not be deserialized properly. Please\"\n+                        \" ensure that components that are Python object\"\n+                        \" instances (layers, models, etc.) returned by\"\n+                        \" `get_config()` are explicitly deserialized in the\"\n+                        \" model's `from_config()` method.\"\n+                    ) from e\n+                else:\n+                    raise e\n \n             if outputs is None:\n                 raise ValueError(\n\n@@ -859,7 +859,7 @@ class SavingV3BattleTest(tf.test.TestCase, parameterized.TestCase):\n \n         model.save(temp_filepath)\n \n-        with self.assertRaisesRegex(TypeError, \"object is not callable\"):\n+        with self.assertRaisesRegex(TypeError, \"are explicitly deserialized\"):\n             _ = keras.models.load_model(temp_filepath)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#251355fbe20501f910ed942c9bdcf6528a3d8bb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 2368 | Contributors (this commit): 35 | Commits (past 90d): 7 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -314,9 +314,9 @@ def relu(x, alpha=0.0, max_value=None, threshold=0.0):\n             function below which values will be damped or set to zero.\n \n     Returns:\n-        A `Tensor` representing the input tensor,\n-        transformed by the relu activation function.\n-        Tensor will be of the same shape and dtype of input `x`.\n+        A `Tensor` representing the input tensor, transformed by the relu\n+        activation function. Tensor will be of the same shape and dtype of\n+        input `x`.\n     \"\"\"\n     return backend.relu(\n         x, alpha=alpha, max_value=max_value, threshold=threshold\n@@ -535,6 +535,8 @@ def serialize(activation, use_legacy_format=False):\n \n     Args:\n         activation : Function object.\n+        use_legacy_format: Boolean, whether to use the legacy format for\n+            serialization.\n \n     Returns:\n         String denoting the name attribute of the input function\n@@ -610,6 +612,8 @@ def deserialize(name, custom_objects=None, use_legacy_format=False):\n         name: The name of the activation function.\n         custom_objects: Optional `{function_name: function_obj}`\n             dictionary listing user-provided activation functions.\n+        use_legacy_format: Boolean, whether to use the legacy format for\n+            deserialization.\n \n     Returns:\n         Corresponding activation function.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#49af17f4d629cc496203671402fbc16b93e3b97e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2371 | Contributors (this commit): 35 | Commits (past 90d): 8 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -358,7 +358,8 @@ def gelu(x, approximate=False):\n         if `approximate` is `False`.\n \n     Reference:\n-        - [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)\n+        - [Gaussian Error Linear Units (GELUs)]\n+        (https://arxiv.org/abs/1606.08415)\n     \"\"\"\n     return tf.nn.gelu(x, approximate)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fedc0a1d83bf0fd47c7855c8c6d7c116767fccc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 1254 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -508,12 +508,20 @@ def deserialize_keras_object(\n     gco = object_registration._GLOBAL_CUSTOM_OBJECTS\n     custom_objects = {**custom_objects, **tlco, **gco}\n \n+    # Optional deprecated argument for legacy deserialization call\n+    printable_module_name = kwargs.pop(\"printable_module_name\", \"object\")\n+    if kwargs:\n+        raise ValueError(\n+            \"The following argument(s) are not supported: \"\n+            f\"{list(kwargs.keys())}\"\n+        )\n+\n     # Fall back to legacy deserialization for all TF1 users or if\n     # wrapped by in_tf_saved_model_scope() to explicitly use legacy\n     # saved_model logic.\n     if not tf.__internal__.tf2.enabled() or in_tf_saved_model_scope():\n         return legacy_serialization.deserialize_keras_object(\n-            config, module_objects, custom_objects\n+            config, module_objects, custom_objects, printable_module_name\n         )\n \n     if config is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#abec420c6722c4a3d9a893acb9cf45363a0e64e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2375 | Contributors (this commit): 35 | Commits (past 90d): 9 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -358,8 +358,8 @@ def gelu(x, approximate=False):\n         if `approximate` is `False`.\n \n     Reference:\n-        - [Gaussian Error Linear Units (GELUs)]\n-        (https://arxiv.org/abs/1606.08415)\n+        - [Gaussian Error Linear Units (GELUs)](\n+            https://arxiv.org/abs/1606.08415)\n     \"\"\"\n     return tf.nn.gelu(x, approximate)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c910dbb7194861f9220f32f3b1a6a0cf0d642d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 92 | Files Changed: 9 | Hunks: 31 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 121 | Churn Cumulative: 2766 | Contributors (this commit): 4 | Commits (past 90d): 80 | Contributors (cumulative): 19 | DMM Complexity: 0.10526315789473684\n\nDIFF:\n@@ -4,27 +4,28 @@ from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n from keras_core import optimizers\n-from keras_core.models import Model\n+from keras_core import Model\n \n inputs = layers.Input((128,), batch_size=32)\n-x = layers.Dense(256)(inputs)\n-x = layers.Dense(256)(x)\n-x = layers.Dense(256)(x)\n+x = layers.Dense(256, activation=\"relu\")(inputs)\n+x = layers.Dense(256, activation=\"relu\")(x)\n+x = layers.Dense(256, activation=\"relu\")(x)\n outputs = layers.Dense(16)(x)\n model = Model(inputs, outputs)\n+\n model.summary()\n \n x = np.random.random((50000, 128))\n y = np.random.random((50000, 16))\n batch_size = 32\n-epochs = 10\n+epochs = 6\n \n model.compile(\n     optimizer=optimizers.SGD(learning_rate=0.001),\n     loss=losses.MeanSquaredError(),\n     metrics=[metrics.MeanSquaredError()],\n )\n-history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+history = model.fit(x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n \n print(\"History:\")\n print(history.history)\n\n@@ -10,8 +10,8 @@ from keras_core import optimizers\n class MyModel(Model):\n     def __init__(self, hidden_dim, output_dim):\n         super().__init__()\n-        self.dense1 = layers.Dense(hidden_dim)\n-        self.dense2 = layers.Dense(hidden_dim)\n+        self.dense1 = layers.Dense(hidden_dim, activation=\"relu\")\n+        self.dense2 = layers.Dense(hidden_dim, activation=\"relu\")\n         self.dense3 = layers.Dense(output_dim)\n \n     def call(self, x):\n@@ -21,19 +21,20 @@ class MyModel(Model):\n \n \n model = MyModel(hidden_dim=256, output_dim=16)\n-model.summary()\n \n x = np.random.random((50000, 128))\n y = np.random.random((50000, 16))\n batch_size = 32\n-epochs = 10\n+epochs = 6\n \n model.compile(\n     optimizer=optimizers.SGD(learning_rate=0.001),\n     loss=losses.MeanSquaredError(),\n     metrics=[metrics.MeanSquaredError()],\n )\n-history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+history = model.fit(x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n \n print(\"History:\")\n print(history.history)\n+\n+model.summary()\n\n@@ -52,7 +52,7 @@ class MiniBatchNorm(Layer):\n             initializers.Zeros()(shape), trainable=False, name=\"mean\"\n         )\n         self.variance = backend.Variable(\n-            initializers.GlorotUniform()(shape),\n+            initializers.Ones()(shape),\n             trainable=False,\n             name=\"variance\",\n         )\n@@ -61,8 +61,8 @@ class MiniBatchNorm(Layer):\n \n     def call(self, inputs, training=False):\n         if training:\n-            mean = jnp.mean(inputs, axis=(0,))  # TODO: extend to rank 3+\n-            variance = jnp.var(inputs, axis=(0,))\n+            mean = ops.mean(inputs, axis=(0,))  # TODO: extend to rank 3+\n+            variance = ops.var(inputs, axis=(0,))\n             outputs = (inputs - mean) / (variance + self.epsilon)\n             self.variance.assign(\n                 self.variance * self.momentum + variance * (1.0 - self.momentum)\n\n@@ -1,3 +1,8 @@\n+from keras_core import backend\n+\n+def relu(x):\n+    return backend.nn.relu(x)\n+\n def identity(x):\n     return x\n \n@@ -5,4 +10,6 @@ def identity(x):\n def get(identifier):\n     if identifier is None:\n         return identity\n+    if identifier == \"relu\":\n+        return relu\n     return identifier\n\n@@ -238,7 +238,6 @@ class JAXTrainer(base_trainer.Trainer):\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n-                        epochs=1,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n\n@@ -231,12 +231,11 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             if validation_data and self._should_eval(epoch, validation_freq):\n                 # Create EpochIterator for evaluation and cache it.\n                 if getattr(self, \"_eval_epoch_iterator\", None) is None:\n-                    self._eval_epoch_iterator = EpochIterator(\n+                    self._eval_epoch_iterator = TFEpochIterator(\n                         x=val_x,\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n-                        epochs=1,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n\n@@ -1,6 +1,5 @@\n from keras_core import backend\n from keras_core import operations as ops\n-from keras_core.api_export import keras_core_export\n from keras_core.losses.loss import Loss\n from keras_core.losses.loss import squeeze_to_same_rank\n \n@@ -25,7 +24,6 @@ class LossFunctionWrapper(Loss):\n         raise NotImplementedError\n \n \n-@keras_core_export(\"keras_core.losses.MeanSquaredError\")\n class MeanSquaredError(LossFunctionWrapper):\n     \"\"\"Computes the mean of squares of errors between labels and predictions.\n \n@@ -48,7 +46,6 @@ class MeanSquaredError(LossFunctionWrapper):\n         super().__init__(mean_squared_error, reduction=reduction, name=name)\n \n \n-@keras_core_export(\"keras_core.losses.MeanAbsoluteError\")\n class MeanAbsoluteError(LossFunctionWrapper):\n     \"\"\"Computes the mean of absolute difference between labels and predictions.\n \n@@ -71,7 +68,6 @@ class MeanAbsoluteError(LossFunctionWrapper):\n         super().__init__(mean_absolute_error, reduction=reduction, name=name)\n \n \n-@keras_core_export(\"keras_core.losses.MeanAbsolutePercentageError\")\n class MeanAbsolutePercentageError(LossFunctionWrapper):\n     \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n \n@@ -98,7 +94,6 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n         )\n \n \n-@keras_core_export(\"keras_core.losses.MeanSquaredLogarithmicError\")\n class MeanSquaredLogarithmicError(LossFunctionWrapper):\n     \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n \n@@ -125,10 +120,6 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n         )\n \n \n-@keras_core_export(\n-    \"keras_core.metrics.mean_squared_error\",\n-    \"keras_core.losses.mean_squared_error\",\n-)\n def mean_squared_error(y_true, y_pred):\n     \"\"\"Computes the mean squared error between labels and predictions.\n \n@@ -157,10 +148,6 @@ def mean_squared_error(y_true, y_pred):\n     return ops.mean(ops.square(y_true - y_pred), axis=-1)\n \n \n-@keras_core_export(\n-    \"keras_core.metrics.mean_absolute_error\",\n-    \"keras_core.losses.mean_absolute_error\",\n-)\n def mean_absolute_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute error between labels and predictions.\n \n@@ -187,10 +174,6 @@ def mean_absolute_error(y_true, y_pred):\n     return ops.mean(ops.abs(y_true - y_pred), axis=-1)\n \n \n-@keras_core_export(\n-    \"keras_core.metrics.mean_absolute_percentage_error\",\n-    \"keras_core.losses.mean_absolute_percentage_error\",\n-)\n def mean_absolute_percentage_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n \n@@ -222,10 +205,6 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     return 100.0 * ops.mean(diff, axis=-1)\n \n \n-@keras_core_export(\n-    \"keras_core.metrics.mean_squared_logarithmic_error\",\n-    \"keras_core.losses.mean_squared_logarithmic_error\",\n-)\n def mean_squared_logarithmic_error(y_true, y_pred):\n     \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n \n\n@@ -1,26 +1,13 @@\n from keras_core import operations as ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.losses import losses\n-from keras_core.losses.losses import mean_squared_error\n from keras_core.metrics import reduction_metrics\n-from keras_core.metrics.metric import Metric\n \n \n-@keras_core_export(\"keras_core.metrics.MeanSquaredError\")\n+def mean_squared_error(y_true, y_pred):\n+    ndim = len(y_pred.shape)\n+    return ops.mean((y_true - y_pred) ** 2, axis=list(range(1, ndim)))\n+\n+\n class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n-    \"\"\"Computes the mean squared error between `y_true` and `y_pred`.\n-\n-    Args:\n-        name: (Optional) string name of the metric instance.\n-        dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = keras_core.metrics.MeanSquaredError()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n-    >>> m.result()\n-    0.25\n-    \"\"\"\n     def __init__(self, name=\"mean_squared_error\", dtype=None):\n         super().__init__(fn=mean_squared_error, name=name, dtype=dtype)\n \n\n@@ -1,36 +0,0 @@\n-import numpy as np\n-\n-from keras_core import operations as ops\n-from keras_core import testing\n-from keras_core.metrics import regression_metrics\n-\n-\n-class MeanSquaredErrorTest(testing.TestCase):\n-    def test_config(self):\n-        # TODO\n-        pass\n-\n-    def test_unweighted(self):\n-        mse_obj = regression_metrics.MeanSquaredError()\n-        y_true = np.array(\n-            [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n-        )\n-        y_pred = np.array(\n-            [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n-        )\n-\n-        update_op = mse_obj.update_state(y_true, y_pred)\n-        result = mse_obj.result()\n-        self.assertAllClose(0.5, result, atol=1e-5)\n-\n-    def test_weighted(self):\n-        mse_obj = regression_metrics.MeanSquaredError()\n-        y_true = np.array(\n-            [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n-        )\n-        y_pred = np.array(\n-            [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n-        )\n-        sample_weight = np.array([1.0, 1.5, 2.0, 2.5])\n-        result = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.54285, result, atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6fa099ac0346c8684907cd64dba9eacf018e0ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 3 | Files Changed: 5 | Hunks: 15 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 747 | Contributors (this commit): 4 | Commits (past 90d): 40 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,10 @@\n import numpy as np\n \n+from keras_core import Model\n from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n from keras_core import optimizers\n-from keras_core import Model\n \n inputs = layers.Input((128,), batch_size=32)\n x = layers.Dense(256, activation=\"relu\")(inputs)\n@@ -25,7 +25,9 @@ model.compile(\n     loss=losses.MeanSquaredError(),\n     metrics=[metrics.MeanSquaredError()],\n )\n-history = model.fit(x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n+history = model.fit(\n+    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n+)\n \n print(\"History:\")\n print(history.history)\n\n@@ -32,7 +32,9 @@ model.compile(\n     loss=losses.MeanSquaredError(),\n     metrics=[metrics.MeanSquaredError()],\n )\n-history = model.fit(x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n+history = model.fit(\n+    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n+)\n \n print(\"History:\")\n print(history.history)\n\n@@ -1,8 +1,10 @@\n from keras_core import backend\n \n+\n def relu(x):\n     return backend.nn.relu(x)\n \n+\n def identity(x):\n     return x\n \n\n@@ -126,8 +126,10 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n \n \n @keras_core_export(\n+    [\n         \"keras_core.metrics.mean_squared_error\",\n         \"keras_core.losses.mean_squared_error\",\n+    ]\n )\n def mean_squared_error(y_true, y_pred):\n     \"\"\"Computes the mean squared error between labels and predictions.\n@@ -158,8 +160,10 @@ def mean_squared_error(y_true, y_pred):\n \n \n @keras_core_export(\n+    [\n         \"keras_core.metrics.mean_absolute_error\",\n         \"keras_core.losses.mean_absolute_error\",\n+    ]\n )\n def mean_absolute_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute error between labels and predictions.\n@@ -188,8 +192,10 @@ def mean_absolute_error(y_true, y_pred):\n \n \n @keras_core_export(\n+    [\n         \"keras_core.metrics.mean_absolute_percentage_error\",\n         \"keras_core.losses.mean_absolute_percentage_error\",\n+    ]\n )\n def mean_absolute_percentage_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n@@ -223,8 +229,10 @@ def mean_absolute_percentage_error(y_true, y_pred):\n \n \n @keras_core_export(\n+    [\n         \"keras_core.metrics.mean_squared_logarithmic_error\",\n         \"keras_core.losses.mean_squared_logarithmic_error\",\n+    ]\n )\n def mean_squared_logarithmic_error(y_true, y_pred):\n     \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n\n@@ -21,6 +21,7 @@ class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n     >>> m.result()\n     0.25\n     \"\"\"\n+\n     def __init__(self, name=\"mean_squared_error\", dtype=None):\n         super().__init__(fn=mean_squared_error, name=name, dtype=dtype)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce72fe1f4236c4da328160ed539e5d210af403de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 39 | Files Changed: 9 | Hunks: 28 | Methods Changed: 11 | Complexity Δ (Sum/Max): 7/9 | Churn Δ: 200 | Churn Cumulative: 2959 | Contributors (this commit): 5 | Commits (past 90d): 93 | Contributors (cumulative): 26 | DMM Complexity: 0.9761904761904762\n\nDIFF:\n@@ -1,33 +1,30 @@\n import numpy as np\n \n-from keras_core import Model\n from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n from keras_core import optimizers\n+from keras_core.models import Model\n \n inputs = layers.Input((128,), batch_size=32)\n-x = layers.Dense(256, activation=\"relu\")(inputs)\n-x = layers.Dense(256, activation=\"relu\")(x)\n-x = layers.Dense(256, activation=\"relu\")(x)\n+x = layers.Dense(256)(inputs)\n+x = layers.Dense(256)(x)\n+x = layers.Dense(256)(x)\n outputs = layers.Dense(16)(x)\n model = Model(inputs, outputs)\n-\n model.summary()\n \n x = np.random.random((50000, 128))\n y = np.random.random((50000, 16))\n batch_size = 32\n-epochs = 6\n+epochs = 10\n \n model.compile(\n     optimizer=optimizers.SGD(learning_rate=0.001),\n     loss=losses.MeanSquaredError(),\n     metrics=[metrics.MeanSquaredError()],\n )\n-history = model.fit(\n-    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n-)\n+history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n \n print(\"History:\")\n print(history.history)\n\n@@ -10,8 +10,8 @@ from keras_core import optimizers\n class MyModel(Model):\n     def __init__(self, hidden_dim, output_dim):\n         super().__init__()\n-        self.dense1 = layers.Dense(hidden_dim, activation=\"relu\")\n-        self.dense2 = layers.Dense(hidden_dim, activation=\"relu\")\n+        self.dense1 = layers.Dense(hidden_dim)\n+        self.dense2 = layers.Dense(hidden_dim)\n         self.dense3 = layers.Dense(output_dim)\n \n     def call(self, x):\n@@ -21,22 +21,19 @@ class MyModel(Model):\n \n \n model = MyModel(hidden_dim=256, output_dim=16)\n+model.summary()\n \n x = np.random.random((50000, 128))\n y = np.random.random((50000, 16))\n batch_size = 32\n-epochs = 6\n+epochs = 10\n \n model.compile(\n     optimizer=optimizers.SGD(learning_rate=0.001),\n     loss=losses.MeanSquaredError(),\n     metrics=[metrics.MeanSquaredError()],\n )\n-history = model.fit(\n-    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n-)\n+history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n \n print(\"History:\")\n print(history.history)\n-\n-model.summary()\n\n@@ -52,7 +52,7 @@ class MiniBatchNorm(Layer):\n             initializers.Zeros()(shape), trainable=False, name=\"mean\"\n         )\n         self.variance = backend.Variable(\n-            initializers.Ones()(shape),\n+            initializers.GlorotUniform()(shape),\n             trainable=False,\n             name=\"variance\",\n         )\n@@ -61,8 +61,8 @@ class MiniBatchNorm(Layer):\n \n     def call(self, inputs, training=False):\n         if training:\n-            mean = ops.mean(inputs, axis=(0,))  # TODO: extend to rank 3+\n-            variance = ops.var(inputs, axis=(0,))\n+            mean = jnp.mean(inputs, axis=(0,))  # TODO: extend to rank 3+\n+            variance = jnp.var(inputs, axis=(0,))\n             outputs = (inputs - mean) / (variance + self.epsilon)\n             self.variance.assign(\n                 self.variance * self.momentum + variance * (1.0 - self.momentum)\n\n@@ -1,10 +1,3 @@\n-from keras_core import backend\n-\n-\n-def relu(x):\n-    return backend.nn.relu(x)\n-\n-\n def identity(x):\n     return x\n \n@@ -12,6 +5,4 @@ def identity(x):\n def get(identifier):\n     if identifier is None:\n         return identity\n-    if identifier == \"relu\":\n-        return relu\n     return identifier\n\n@@ -238,6 +238,7 @@ class JAXTrainer(base_trainer.Trainer):\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n+                        epochs=1,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n\n@@ -231,11 +231,12 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             if validation_data and self._should_eval(epoch, validation_freq):\n                 # Create EpochIterator for evaluation and cache it.\n                 if getattr(self, \"_eval_epoch_iterator\", None) is None:\n-                    self._eval_epoch_iterator = TFEpochIterator(\n+                    self._eval_epoch_iterator = EpochIterator(\n                         x=val_x,\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n+                        epochs=1,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n\n@@ -126,10 +126,8 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n \n \n @keras_core_export(\n-    [\n     \"keras_core.metrics.mean_squared_error\",\n     \"keras_core.losses.mean_squared_error\",\n-    ]\n )\n def mean_squared_error(y_true, y_pred):\n     \"\"\"Computes the mean squared error between labels and predictions.\n@@ -160,10 +158,8 @@ def mean_squared_error(y_true, y_pred):\n \n \n @keras_core_export(\n-    [\n     \"keras_core.metrics.mean_absolute_error\",\n     \"keras_core.losses.mean_absolute_error\",\n-    ]\n )\n def mean_absolute_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute error between labels and predictions.\n@@ -192,10 +188,8 @@ def mean_absolute_error(y_true, y_pred):\n \n \n @keras_core_export(\n-    [\n     \"keras_core.metrics.mean_absolute_percentage_error\",\n     \"keras_core.losses.mean_absolute_percentage_error\",\n-    ]\n )\n def mean_absolute_percentage_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n@@ -229,10 +223,8 @@ def mean_absolute_percentage_error(y_true, y_pred):\n \n \n @keras_core_export(\n-    [\n     \"keras_core.metrics.mean_squared_logarithmic_error\",\n     \"keras_core.losses.mean_squared_logarithmic_error\",\n-    ]\n )\n def mean_squared_logarithmic_error(y_true, y_pred):\n     \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n\n@@ -21,7 +21,6 @@ class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n     >>> m.result()\n     0.25\n     \"\"\"\n-\n     def __init__(self, name=\"mean_squared_error\", dtype=None):\n         super().__init__(fn=mean_squared_error, name=name, dtype=dtype)\n \n\n@@ -0,0 +1,144 @@\n+from keras_core import operations as ops\n+from keras_core.optimizers import optimizer\n+\n+\n+class Adam(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Adam algorithm.\n+\n+    Adam optimization is a stochastic gradient descent method that is based on\n+    adaptive estimation of first-order and second-order moments.\n+\n+    According to\n+    [Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\n+    the method is \"*computationally\n+    efficient, has little memory requirement, invariant to diagonal rescaling of\n+    gradients, and is well suited for problems that are large in terms of\n+    data/parameters*\".\n+\n+    Args:\n+      learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n+        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        learning rate. Defaults to `0.001`.\n+      beta_1: A float value or a constant float tensor, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n+      beta_2: A float value or a constant float tensor, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        exponential decay rate for the 2nd moment estimates. Defaults to\n+        `0.999`.\n+      epsilon: A small constant for numerical stability. This epsilon is\n+        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n+        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n+        `1e-7`.\n+      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n+        the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n+      {{base_optimizer_keyword_args}}\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_1=0.9,\n+        beta_2=0.999,\n+        epsilon=1e-7,\n+        amsgrad=False,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"Adam\",\n+        **kwargs\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs\n+        )\n+        self.beta_1 = beta_1\n+        self.beta_2 = beta_2\n+        self.epsilon = epsilon\n+        self.amsgrad = amsgrad\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Adam optimizer has 3 types of variables: momentums, velocities and\n+        velocity_hat (only set when amsgrad is applied),\n+\n+        Args:\n+          var_list: list of model variables to build Adam variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._momentums = []\n+        self._velocities = []\n+        for var in var_list:\n+            self._momentums.append(\n+                self.add_variable_from_reference(\n+                    model_variable=var, variable_name=\"m\"\n+                )\n+            )\n+            self._velocities.append(\n+                self.add_variable_from_reference(\n+                    model_variable=var, variable_name=\"v\"\n+                )\n+            )\n+        if self.amsgrad:\n+            self._velocity_hats = []\n+            for var in var_list:\n+                self._velocity_hats.append(\n+                    self.add_variable_from_reference(\n+                        model_variable=var, variable_name=\"vhat\"\n+                    )\n+                )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        beta_1_power = None\n+        beta_2_power = None\n+        lr = ops.cast(self.learning_rate, variable.dtype)\n+        local_step = ops.cast(self.iterations + 1, variable.dtype)\n+        beta_1_power = ops.power(ops.cast(self.beta_1, variable.dtype), local_step)\n+        beta_2_power = ops.power(ops.cast(self.beta_2, variable.dtype), local_step)\n+\n+        m = self._momentums[self._get_variable_index(variable)]\n+        v = self._velocities[self._get_variable_index(variable)]\n+\n+        alpha = lr * ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n+\n+        m.assign(m + (gradient - m) * (1 - self.beta_1))\n+        v.assign(v + (ops.square(gradient) - v) * (1 - self.beta_2))\n+        if self.amsgrad:\n+            v_hat = self._velocity_hats[self._get_variable_index(variable)]\n+            v_hat.assign(ops.maximum(v_hat, v))\n+            v = v_hat\n+        variable.assign(m - (m * alpha) / (ops.sqrt(v) + self.epsilon))\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"beta_1\": self.beta_1,\n+                \"beta_2\": self.beta_2,\n+                \"epsilon\": self.epsilon,\n+                \"amsgrad\": self.amsgrad,\n+            }\n+        )\n+        return config\n+\n+\n+Adam.__doc__ = Adam.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f06a9ce03746badbc0ac92295f83a91066ee738a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 17 | Files Changed: 8 | Hunks: 27 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 56 | Churn Cumulative: 2871 | Contributors (this commit): 5 | Commits (past 90d): 100 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,30 +1,33 @@\n import numpy as np\n \n+from keras_core import Model\n from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n from keras_core import optimizers\n-from keras_core.models import Model\n \n inputs = layers.Input((128,), batch_size=32)\n-x = layers.Dense(256)(inputs)\n-x = layers.Dense(256)(x)\n-x = layers.Dense(256)(x)\n+x = layers.Dense(256, activation=\"relu\")(inputs)\n+x = layers.Dense(256, activation=\"relu\")(x)\n+x = layers.Dense(256, activation=\"relu\")(x)\n outputs = layers.Dense(16)(x)\n model = Model(inputs, outputs)\n+\n model.summary()\n \n x = np.random.random((50000, 128))\n y = np.random.random((50000, 16))\n batch_size = 32\n-epochs = 10\n+epochs = 6\n \n model.compile(\n     optimizer=optimizers.SGD(learning_rate=0.001),\n     loss=losses.MeanSquaredError(),\n     metrics=[metrics.MeanSquaredError()],\n )\n-history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+history = model.fit(\n+    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n+)\n \n print(\"History:\")\n print(history.history)\n\n@@ -10,8 +10,8 @@ from keras_core import optimizers\n class MyModel(Model):\n     def __init__(self, hidden_dim, output_dim):\n         super().__init__()\n-        self.dense1 = layers.Dense(hidden_dim)\n-        self.dense2 = layers.Dense(hidden_dim)\n+        self.dense1 = layers.Dense(hidden_dim, activation=\"relu\")\n+        self.dense2 = layers.Dense(hidden_dim, activation=\"relu\")\n         self.dense3 = layers.Dense(output_dim)\n \n     def call(self, x):\n@@ -21,19 +21,22 @@ class MyModel(Model):\n \n \n model = MyModel(hidden_dim=256, output_dim=16)\n-model.summary()\n \n x = np.random.random((50000, 128))\n y = np.random.random((50000, 16))\n batch_size = 32\n-epochs = 10\n+epochs = 6\n \n model.compile(\n     optimizer=optimizers.SGD(learning_rate=0.001),\n     loss=losses.MeanSquaredError(),\n     metrics=[metrics.MeanSquaredError()],\n )\n-history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+history = model.fit(\n+    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n+)\n \n print(\"History:\")\n print(history.history)\n+\n+model.summary()\n\n@@ -52,7 +52,7 @@ class MiniBatchNorm(Layer):\n             initializers.Zeros()(shape), trainable=False, name=\"mean\"\n         )\n         self.variance = backend.Variable(\n-            initializers.GlorotUniform()(shape),\n+            initializers.Ones()(shape),\n             trainable=False,\n             name=\"variance\",\n         )\n@@ -61,8 +61,8 @@ class MiniBatchNorm(Layer):\n \n     def call(self, inputs, training=False):\n         if training:\n-            mean = jnp.mean(inputs, axis=(0,))  # TODO: extend to rank 3+\n-            variance = jnp.var(inputs, axis=(0,))\n+            mean = ops.mean(inputs, axis=(0,))  # TODO: extend to rank 3+\n+            variance = ops.var(inputs, axis=(0,))\n             outputs = (inputs - mean) / (variance + self.epsilon)\n             self.variance.assign(\n                 self.variance * self.momentum + variance * (1.0 - self.momentum)\n\n@@ -1,3 +1,10 @@\n+from keras_core import backend\n+\n+\n+def relu(x):\n+    return backend.nn.relu(x)\n+\n+\n def identity(x):\n     return x\n \n@@ -5,4 +12,6 @@ def identity(x):\n def get(identifier):\n     if identifier is None:\n         return identity\n+    if identifier == \"relu\":\n+        return relu\n     return identifier\n\n@@ -238,7 +238,6 @@ class JAXTrainer(base_trainer.Trainer):\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n-                        epochs=1,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n\n@@ -231,12 +231,11 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             if validation_data and self._should_eval(epoch, validation_freq):\n                 # Create EpochIterator for evaluation and cache it.\n                 if getattr(self, \"_eval_epoch_iterator\", None) is None:\n-                    self._eval_epoch_iterator = EpochIterator(\n+                    self._eval_epoch_iterator = TFEpochIterator(\n                         x=val_x,\n                         y=val_y,\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n-                        epochs=1,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n\n@@ -126,8 +126,10 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n \n \n @keras_core_export(\n+    [\n         \"keras_core.metrics.mean_squared_error\",\n         \"keras_core.losses.mean_squared_error\",\n+    ]\n )\n def mean_squared_error(y_true, y_pred):\n     \"\"\"Computes the mean squared error between labels and predictions.\n@@ -158,8 +160,10 @@ def mean_squared_error(y_true, y_pred):\n \n \n @keras_core_export(\n+    [\n         \"keras_core.metrics.mean_absolute_error\",\n         \"keras_core.losses.mean_absolute_error\",\n+    ]\n )\n def mean_absolute_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute error between labels and predictions.\n@@ -188,8 +192,10 @@ def mean_absolute_error(y_true, y_pred):\n \n \n @keras_core_export(\n+    [\n         \"keras_core.metrics.mean_absolute_percentage_error\",\n         \"keras_core.losses.mean_absolute_percentage_error\",\n+    ]\n )\n def mean_absolute_percentage_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n@@ -223,8 +229,10 @@ def mean_absolute_percentage_error(y_true, y_pred):\n \n \n @keras_core_export(\n+    [\n         \"keras_core.metrics.mean_squared_logarithmic_error\",\n         \"keras_core.losses.mean_squared_logarithmic_error\",\n+    ]\n )\n def mean_squared_logarithmic_error(y_true, y_pred):\n     \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n\n@@ -21,6 +21,7 @@ class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n     >>> m.result()\n     0.25\n     \"\"\"\n+\n     def __init__(self, name=\"mean_squared_error\", dtype=None):\n         super().__init__(fn=mean_squared_error, name=name, dtype=dtype)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
