{"custom_id": "scrapy#1a30a7774b7d530394fdd761f2a59403b778fe10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 57 | Files Changed: 1 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 96 | Churn Cumulative: 353 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,7 @@\n from os.path import join\n \n+import pytest\n+\n from twisted.trial import unittest\n from twisted.internet import defer\n \n@@ -10,63 +12,6 @@ from scrapy.utils.testproc import ProcessTest\n from tests import tests_datadir\n \n \n-class ShellURLTest(unittest.TestCase):\n-    pass\n-\n-def create_guess_scheme_t(args):\n-    def do_expected(self):\n-        url = guess_scheme(args[0])\n-        assert url.startswith(args[1]), \\\n-            'Wrong scheme guessed: for `%s` got `%s`, expected `%s...`' % (\n-                args[0], url, args[1])\n-    return do_expected\n-\n-def create_skipped_scheme_t(args):\n-    def do_expected(self):\n-        raise unittest.SkipTest(args[2])\n-        url = guess_scheme(args[0])\n-        assert url.startswith(args[1])\n-    return do_expected\n-\n-for k, args in enumerate ([\n-            ('/index',                              'file://'),\n-            ('/index.html',                         'file://'),\n-            ('./index.html',                        'file://'),\n-            ('../index.html',                       'file://'),\n-            ('../../index.html',                    'file://'),\n-            ('./data/index.html',                   'file://'),\n-            ('.hidden/data/index.html',             'file://'),\n-            ('/home/user/www/index.html',           'file://'),\n-            ('//home/user/www/index.html',          'file://'),\n-            ('file:///home/user/www/index.html',    'file://'),\n-\n-            ('index.html',                          'http://'),\n-            ('example.com',                         'http://'),\n-            ('www.example.com',                     'http://'),\n-            ('www.example.com/index.html',          'http://'),\n-            ('http://example.com',                  'http://'),\n-            ('http://example.com/index.html',       'http://'),\n-            ('localhost',                           'http://'),\n-            ('localhost/index.html',                'http://'),\n-\n-            # some corner cases (default to http://)\n-            ('/',                                   'http://'),\n-            ('.../test',                            'http://'),\n-\n-        ], start=1):\n-    t_method = create_guess_scheme_t(args)\n-    t_method.__name__ = 'test_uri_%03d' % k\n-    setattr (ShellURLTest, t_method.__name__, t_method)\n-\n-# TODO: the following tests do not pass with current implementation\n-for k, args in enumerate ([\n-            ('C:\\absolute\\path\\to\\a\\file.html',     'file://',\n-             'Windows filepath are not supported for scrapy shell'),\n-        ], start=1):\n-    t_method = create_skipped_scheme_t(args)\n-    t_method.__name__ = 'test_uri_skipped_%03d' % k\n-    setattr (ShellURLTest, t_method.__name__, t_method)\n-\n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n \n@@ -136,3 +81,40 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n                                        check_code=False)\n         self.assertEqual(errcode, 1, out or err)\n         self.assertIn(b'DNS lookup failed', err)\n+\n+\n+@pytest.mark.parametrize(\"url, scheme\", [\n+    ('/index',                              'file://'),\n+    ('/index.html',                         'file://'),\n+    ('./index.html',                        'file://'),\n+    ('../index.html',                       'file://'),\n+    ('../../index.html',                    'file://'),\n+    ('./data/index.html',                   'file://'),\n+    ('.hidden/data/index.html',             'file://'),\n+    ('/home/user/www/index.html',           'file://'),\n+    ('//home/user/www/index.html',          'file://'),\n+    ('file:///home/user/www/index.html',    'file://'),\n+\n+    ('index.html',                          'http://'),\n+    ('example.com',                         'http://'),\n+    ('www.example.com',                     'http://'),\n+    ('www.example.com/index.html',          'http://'),\n+    ('http://example.com',                  'http://'),\n+    ('http://example.com/index.html',       'http://'),\n+    ('localhost',                           'http://'),\n+    ('localhost/index.html',                'http://'),\n+\n+    # some corner cases (default to http://)\n+    ('/',                                   'http://'),\n+    ('.../test',                            'http://'),\n+\n+    pytest.mark.xfail(\n+        (r'C:\\absolute\\path\\to\\a\\file.html', 'file://'),\n+         reason = 'Windows filepath are not supported for scrapy shell'\n+    ),\n+])\n+def test_guess_scheme(url, scheme):\n+    guessed_url = guess_scheme(url)\n+    assert guessed_url.startswith(scheme), \\\n+        'Wrong scheme guessed: for `%s` got `%s`, expected `%s...`' % (\n+            url, guessed_url, scheme)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c75f1fe46a8a2a3c471eeef2c023754ee5e6c2f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 50 | Churn Cumulative: 257 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -261,13 +261,13 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         self.assertXmlEquivalent(fp.getvalue(), expected_value)\n \n     def _check_output(self):\n-        expected_value = u'<?xml version=\"1.0\" ?>\\n<items><item><age>22</age><name>John\\xa3</name></item></items>'\n+        expected_value = b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><age>22</age><name>John\\xc2\\xa3</name></item></items>'\n         self.assertXmlEquivalent(self.output.getvalue(), expected_value)\n \n     def test_multivalued_fields(self):\n         self.assertExportResult(\n             TestItem(name=[u'John\\xa3', u'Doe']),\n-            u'<?xml version=\"1.0\" ?>\\n<items><item><name><value>John\\xa3</value><value>Doe</value></name></item></items>'\n+            b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><name><value>John\\xc2\\xa3</value><value>Doe</value></name></item></items>'\n         )\n \n     def test_nested_item(self):\n@@ -276,19 +276,19 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         i3 = TestItem(name=u'buz', age=i2)\n \n         self.assertExportResult(i3,\n-            u'<?xml version=\"1.0\" ?>\\n'\n-            u'<items>'\n-                u'<item>'\n-                    u'<age>'\n-                        u'<age>'\n-                            u'<age>22</age>'\n-                            u'<name>foo\\xa3hoo</name>'\n-                        u'</age>'\n-                        u'<name>bar</name>'\n-                    u'</age>'\n-                    u'<name>buz</name>'\n-                u'</item>'\n-            u'</items>'\n+            b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n+            b'<items>'\n+                b'<item>'\n+                    b'<age>'\n+                        b'<age>'\n+                            b'<age>22</age>'\n+                            b'<name>foo\\xc2\\xa3hoo</name>'\n+                        b'</age>'\n+                        b'<name>bar</name>'\n+                    b'</age>'\n+                    b'<name>buz</name>'\n+                b'</item>'\n+            b'</items>'\n         )\n \n     def test_nested_list_item(self):\n@@ -297,16 +297,16 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         i3 = TestItem(name=u'buz', age=[i1, i2])\n \n         self.assertExportResult(i3,\n-            u'<?xml version=\"1.0\" ?>\\n'\n-            u'<items>'\n-                u'<item>'\n-                    u'<age>'\n-                        u'<value><name>foo</name></value>'\n-                        u'<value><name>bar</name><v2><egg><value>spam</value></egg></v2></value>'\n-                    u'</age>'\n-                    u'<name>buz</name>'\n-                u'</item>'\n-            u'</items>'\n+            b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n+            b'<items>'\n+                b'<item>'\n+                    b'<age>'\n+                        b'<value><name>foo</name></value>'\n+                        b'<value><name>bar</name><v2><egg><value>spam</value></egg></v2></value>'\n+                    b'</age>'\n+                    b'<name>buz</name>'\n+                b'</item>'\n+            b'</items>'\n         )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#935b1da8c2d1d801f4d7d9ce8f498c1d7a527644", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 86 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -14,6 +14,7 @@ from xml.sax.saxutils import XMLGenerator\n from scrapy.utils.serialize import ScrapyJSONEncoder\n from scrapy.utils.python import to_bytes, to_unicode, to_native_str, is_listlike\n from scrapy.item import BaseItem\n+from scrapy.exceptions import ScrapyDeprecationWarning\n import warnings\n \n \n@@ -252,7 +253,7 @@ class PythonItemExporter(BaseItemExporter):\n         if self.binary:\n             warnings.warn(\n                 \"PythonItemExporter will drop support for binary export in the future\",\n-                PendingDeprecationWarning)\n+                ScrapyDeprecationWarning)\n \n     def serialize_field(self, field, name, value):\n         serializer = field.get('serializer', self._serialize_value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9fbe6f3e814578f90f206031ae99a344b842e400", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 13 | Churn Cumulative: 90 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.1\n\nDIFF:\n@@ -5,7 +5,6 @@ import json\n from io import BytesIO\n import tempfile\n import shutil\n-import six\n from six.moves.urllib.parse import urlparse\n \n from zope.interface.verify import verifyObject\n@@ -187,10 +186,22 @@ class FeedExportTest(unittest.TestCase):\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         self.assertEqual(rows, parsed)\n \n+    @defer.inlineCallbacks\n+    def assertExportedXml(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({'FEED_FORMAT': 'xml'})\n+        data = yield self.exported_data(items, settings)\n+        rows = [{k: v for k, v in row.items() if v} for row in rows]\n+        import lxml.etree\n+        root = lxml.etree.fromstring(data)\n+        got_rows = [{e.tag: e.text for e in it} for it in root.findall('item')]\n+        self.assertEqual(rows, got_rows)\n+\n     @defer.inlineCallbacks\n     def assertExported(self, items, header, rows, settings=None, ordered=True):\n         yield self.assertExportedCsv(items, header, rows, settings, ordered)\n         yield self.assertExportedJsonLines(items, rows, settings)\n+        yield self.assertExportedXml(items, rows, settings)\n \n     @defer.inlineCallbacks\n     def test_export_items(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9704226ee4c933e1214e029d66005f1bee2fb766", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -53,8 +53,8 @@ class MailSenderTest(unittest.TestCase):\n         self.assertEqual(len(payload), 2)\n \n         text, attach = payload\n-        self.assertEqual(text.get_payload(decode=True), 'body')\n-        self.assertEqual(attach.get_payload(decode=True), 'content')\n+        self.assertEqual(text.get_payload(decode=True), b'body')\n+        self.assertEqual(attach.get_payload(decode=True), b'content')\n \n     def _catch_mail_sent(self, **kwargs):\n         self.catched_msg = dict(**kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1be90323c27bfda7588d4437a1996c5e92eb452d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 354 | Contributors (this commit): 7 | Commits (past 90d): 17 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -437,6 +437,8 @@ class S3AnonTestCase(unittest.TestCase):\n         import boto\n     except ImportError:\n         skip = 'missing boto library'\n+    if six.PY3:\n+        skip = 'S3 not supported on Py3'\n \n     def setUp(self):\n         self.s3reqh = S3DownloadHandler(Settings(),\n@@ -459,6 +461,8 @@ class S3TestCase(unittest.TestCase):\n         import boto\n     except ImportError:\n         skip = 'missing boto library'\n+    if six.PY3:\n+        skip = 'S3 not supported on Py3'\n \n     # test use same example keys than amazon developer guide\n     # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c44fac2b54e72202787fda9d221f62a234151d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 31 | Churn Cumulative: 121 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -197,11 +197,42 @@ class FeedExportTest(unittest.TestCase):\n         got_rows = [{e.tag: e.text for e in it} for it in root.findall('item')]\n         self.assertEqual(rows, got_rows)\n \n+    def _load_until_eof(self, data, load_func):\n+        bytes_output = BytesIO(data)\n+        result = []\n+        while True:\n+            try:\n+                result.append(load_func(bytes_output))\n+            except EOFError:\n+                break\n+        return result\n+\n+    @defer.inlineCallbacks\n+    def assertExportedPickle(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({'FEED_FORMAT': 'pickle'})\n+        data = yield self.exported_data(items, settings)\n+        expected = [{k: v for k, v in row.items() if v} for row in rows]\n+        import pickle\n+        result = self._load_until_eof(data, load_func=pickle.load)\n+        self.assertEqual(expected, result)\n+\n+    @defer.inlineCallbacks\n+    def assertExportedMarshal(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({'FEED_FORMAT': 'marshal'})\n+        data = yield self.exported_data(items, settings)\n+        expected = [{k: v for k, v in row.items() if v} for row in rows]\n+        import marshal\n+        result = self._load_until_eof(data, load_func=marshal.load)\n+        self.assertEqual(expected, result)\n+\n     @defer.inlineCallbacks\n     def assertExported(self, items, header, rows, settings=None, ordered=True):\n         yield self.assertExportedCsv(items, header, rows, settings, ordered)\n         yield self.assertExportedJsonLines(items, rows, settings)\n         yield self.assertExportedXml(items, rows, settings)\n+        yield self.assertExportedPickle(items, rows, settings)\n \n     @defer.inlineCallbacks\n     def test_export_items(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb8ab2427bff636375b64fdc459562432c8ac420", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 530 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -116,8 +116,8 @@ def escape_ajax(url):\n def add_http_if_no_scheme(url):\n     \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n     match = re.match(r\"^\\w+://\", url, flags=re.I)\n-    parts = urlparse(url)\n     if not match:\n+        parts = urlparse(url)\n         scheme = \"http:\" if parts.netloc else \"http://\"\n         url = scheme + url\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23b3336c1feb3acb603a935f11e442557d6434f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 261 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -90,6 +90,10 @@ class PythonItemExporterTest(BaseItemExporterTest):\n     def _get_exporter(self, **kwargs):\n         return PythonItemExporter(binary=False, **kwargs)\n \n+    def test_invalid_option(self):\n+        with self.assertRaisesRegexp(TypeError, \"Unexpected options: invalid_option\"):\n+            PythonItemExporter(invalid_option='something')\n+\n     def test_nested_item(self):\n         i1 = TestItem(name=u'Joseph', age='22')\n         i2 = dict(name=u'Maria', age=i1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2dfdde3c79a5be468302a1e825cc5ad77444a8ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 18 | Churn Cumulative: 365 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -192,9 +192,16 @@ class CsvItemExporter(BaseItemExporter):\n \n         fields = self._get_serialized_fields(item, default_value='',\n                                              include_empty=True)\n-        values = [to_native_str(x) for _, x in fields]\n+        values = list(self._build_row(x for _, x in fields))\n         self.csv_writer.writerow(values)\n \n+    def _build_row(self, values):\n+        for s in values:\n+            try:\n+                yield to_native_str(s)\n+            except TypeError:\n+                yield to_native_str(repr(s))\n+\n     def _write_headers_and_set_fields_to_export(self, item):\n         if self.include_headers_line:\n             if not self.fields_to_export:\n@@ -204,7 +211,7 @@ class CsvItemExporter(BaseItemExporter):\n                 else:\n                     # use fields declared in Item\n                     self.fields_to_export = list(item.fields.keys())\n-            row = [to_native_str(s) for s in self.fields_to_export]\n+            row = list(self._build_row(self.fields_to_export))\n             self.csv_writer.writerow(row)\n \n \n\n@@ -237,6 +237,13 @@ class CsvItemExporterTest(BaseItemExporterTest):\n                 expected='\"Mary,Paul\",John\\r\\n',\n             )\n \n+    def test_join_multivalue_not_strings(self):\n+        self.assertExportResult(\n+            item=dict(name='John', friends=[4, 8]),\n+            include_headers_line=False,\n+            expected='\"[4, 8]\",John\\r\\n',\n+        )\n+\n \n class XmlItemExporterTest(BaseItemExporterTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d0eacfe0f90263035f98beb8fe6b5a8f182d5a1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 383 | Contributors (this commit): 4 | Commits (past 90d): 15 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -158,7 +158,7 @@ class XmlItemExporter(BaseItemExporter):\n             if not isinstance(serialized_value, six.text_type):\n                 serialized_value = serialized_value.decode(self.encoding)\n             return self.xg.characters(serialized_value)\n-    else:\n+    else:  # pragma: no cover\n         def _xg_characters(self, serialized_value):\n             return self.xg.characters(serialized_value)\n \n\n@@ -1,6 +1,8 @@\n from __future__ import absolute_import\n import re\n import json\n+import marshal\n+import tempfile\n import unittest\n from io import BytesIO\n from six.moves import cPickle as pickle\n@@ -12,7 +14,8 @@ from scrapy.item import Item, Field\n from scrapy.utils.python import to_unicode\n from scrapy.exporters import (\n     BaseItemExporter, PprintItemExporter, PickleItemExporter, CsvItemExporter,\n-    XmlItemExporter, JsonLinesItemExporter, JsonItemExporter, PythonItemExporter\n+    XmlItemExporter, JsonLinesItemExporter, JsonItemExporter,\n+    PythonItemExporter, MarshalItemExporter\n )\n \n \n@@ -163,6 +166,17 @@ class PickleItemExporterTest(BaseItemExporterTest):\n         self.assertEqual(pickle.load(f), i2)\n \n \n+class MarshalItemExporterTest(BaseItemExporterTest):\n+\n+    def _get_exporter(self, **kwargs):\n+        self.output = tempfile.TemporaryFile()\n+        return MarshalItemExporter(self.output, **kwargs)\n+\n+    def _check_output(self):\n+        self.output.seek(0)\n+        self._assert_expected_item(marshal.load(self.output))\n+\n+\n class CsvItemExporterTest(BaseItemExporterTest):\n     def _get_exporter(self, **kwargs):\n         return CsvItemExporter(self.output, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7070dae48da02fa29aa1650af4df28561e343436", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 213 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -137,10 +137,18 @@ class MultiValueDict(dict):\n         for key, value in six.iteritems(kwargs):\n             self.setlistdefault(key, []).append(value)\n \n+\n class SiteNode(object):\n     \"\"\"Class to represent a site node (page, image or any other file)\"\"\"\n \n     def __init__(self, url):\n+        warnings.warn(\n+            \"scrapy.utils.datatypes.SiteNode is deprecated \"\n+            \"and will be removed in future releases.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2\n+        )\n+\n         self.url = url\n         self.itemnames = []\n         self.children = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c2aa50ea20d2333eb131c6aae7b9842d646e32e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 230 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,11 +7,22 @@ This module must not depend on any module outside the Standard Library.\n \n import copy\n import six\n+import warnings\n from collections import OrderedDict\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n \n class MultiValueDictKeyError(KeyError):\n-    pass\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"scrapy.utils.datatypes.MultiValueDictKeyError is deprecated \"\n+            \"and will be removed in future releases.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2\n+        )\n+        super(MultiValueDictKeyError, self).__init__(*args, **kwargs)\n+\n \n class MultiValueDict(dict):\n     \"\"\"\n@@ -31,6 +42,10 @@ class MultiValueDict(dict):\n     single name-value pairs.\n     \"\"\"\n     def __init__(self, key_to_list_mapping=()):\n+        warnings.warn(\"scrapy.utils.datatypes.MultiValueDict is deprecated \"\n+                      \"and will be removed in future releases.\",\n+                      category=ScrapyDeprecationWarning,\n+                      stacklevel=2)\n         dict.__init__(self, key_to_list_mapping)\n \n     def __repr__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1cffa99e0d524ad6a3f989893de1693042e92f92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 31 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,7 @@ class TestOffsiteMiddleware(TestCase):\n         self.mw.spider_opened(self.spider)\n \n     def _get_spiderargs(self):\n-        return dict(name='foo', allowed_domains=['scrapytest.org', 'scrapy.org'])\n+        return dict(name='foo', allowed_domains=['scrapytest.org', 'scrapy.org', 'scrapy.test.org'])\n \n     def test_process_spider_output(self):\n         res = Response('http://scrapytest.org')\n@@ -24,13 +24,16 @@ class TestOffsiteMiddleware(TestCase):\n         onsite_reqs = [Request('http://scrapytest.org/1'),\n                        Request('http://scrapy.org/1'),\n                        Request('http://sub.scrapy.org/1'),\n-                       Request('http://offsite.tld/letmepass', dont_filter=True)]\n+                       Request('http://offsite.tld/letmepass', dont_filter=True),\n+                       Request('http://scrapy.test.org/')]\n         offsite_reqs = [Request('http://scrapy2.org'),\n                        Request('http://offsite.tld/'),\n                        Request('http://offsite.tld/scrapytest.org'),\n                        Request('http://offsite.tld/rogue.scrapytest.org'),\n                        Request('http://rogue.scrapytest.org.haha.com'),\n-                       Request('http://roguescrapytest.org')]\n+                       Request('http://roguescrapytest.org'),\n+                       Request('http://test.org/'),\n+                       Request('http://notscrapy.test.org/')]\n         reqs = onsite_reqs + offsite_reqs\n \n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7608da8868baba981fa1d84f7691caaf1e0bd5d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 128 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -44,9 +44,11 @@ class MiddlewareManager(object):\n                     logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n                                    {'clsname': clsname, 'eargs': e.args[0]},\n                                    extra={'crawler': crawler})\n+\n+        enabled = [x.__class__.__name__ for x in middlewares]\n         logger.info(\"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n                     {'componentname': cls.component_name,\n-                     'enabledlist': pprint.pformat(mwlist)},\n+                     'enabledlist': pprint.pformat(enabled)},\n                     extra={'crawler': crawler})\n         return cls(*middlewares)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ee8d8650a5d9041d6eeddddabcd16c0e0e8d9c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from collections import defaultdict\n from twisted.internet import reactor\n \n from scrapy import signals\n+from scrapy.exceptions import NotConfigured\n \n \n class CloseSpider(object):\n@@ -23,6 +24,9 @@ class CloseSpider(object):\n             'errorcount': crawler.settings.getint('CLOSESPIDER_ERRORCOUNT'),\n             }\n \n+        if not any(self.close_on.values()):\n+            raise NotConfigured\n+\n         self.counter = defaultdict(int)\n \n         if self.close_on.get('errorcount'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0349bbf9d3691ad89a5e265db4220fb3e64324ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 7 | Churn Cumulative: 7 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import os\n from six.moves import cPickle as pickle\n \n from scrapy import signals\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.job import job_dir\n \n class SpiderState(object):\n@@ -12,7 +13,11 @@ class SpiderState(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        obj = cls(job_dir(crawler.settings))\n+        jobdir = job_dir(crawler.settings)\n+        if not jobdir:\n+            raise NotConfigured\n+\n+        obj = cls(jobdir)\n         crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n         crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n         return obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#29695375d16ae20e3a97dc78bc12662996a9319b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,8 @@ from twisted.trial import unittest\n \n from scrapy.extensions.spiderstate import SpiderState\n from scrapy.spiders import Spider\n+from scrapy.exceptions import NotConfigured\n+from scrapy.utils.test import get_crawler\n \n \n class SpiderStateTest(unittest.TestCase):\n@@ -34,3 +36,7 @@ class SpiderStateTest(unittest.TestCase):\n         ss.spider_opened(spider)\n         self.assertEqual(spider.state, {})\n         ss.spider_closed(spider)\n+\n+    def test_not_configured(self):\n+        crawler = get_crawler(Spider)\n+        self.assertRaises(NotConfigured, SpiderState.from_crawler, crawler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c22a4e3bb84448f778613717c3749f226df7880f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 131 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,6 +28,7 @@ class MiddlewareManager(object):\n     def from_settings(cls, settings, crawler=None):\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n+        enabled = []\n         for clspath in mwlist:\n             try:\n                 mwcls = load_object(clspath)\n@@ -38,6 +39,7 @@ class MiddlewareManager(object):\n                 else:\n                     mw = mwcls()\n                 middlewares.append(mw)\n+                enabled.append(clspath)\n             except NotConfigured as e:\n                 if e.args:\n                     clsname = clspath.split('.')[-1]\n@@ -45,7 +47,6 @@ class MiddlewareManager(object):\n                                    {'clsname': clsname, 'eargs': e.args[0]},\n                                    extra={'crawler': crawler})\n \n-        enabled = [x.__class__.__name__ for x in middlewares]\n         logger.info(\"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n                     {'componentname': cls.component_name,\n                      'enabledlist': pprint.pformat(enabled)},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4bcbb77bcc7d7668340baa064db15f29617cf0cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 24 | Files Changed: 9 | Hunks: 22 | Methods Changed: 14 | Complexity Δ (Sum/Max): 14/13 | Churn Δ: 58 | Churn Cumulative: 2678 | Contributors (this commit): 32 | Commits (past 90d): 22 | Contributors (cumulative): 67 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,7 +63,7 @@ class AjaxCrawlMiddleware(object):\n         Return True if a page without hash fragment could be \"AJAX crawlable\"\n         according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n         \"\"\"\n-        body = response.body_as_unicode()[:self.lookup_bytes]\n+        body = response.text[:self.lookup_bytes]\n         return _has_ajaxcrawlable_meta(body)\n \n \n\n@@ -83,8 +83,8 @@ class RobotsTxtMiddleware(object):\n     def _parse_robots(self, response, netloc):\n         rp = robotparser.RobotFileParser(response.url)\n         body = ''\n-        if hasattr(response, 'body_as_unicode'):\n-            body = response.body_as_unicode()\n+        if hasattr(response, 'text'):\n+            body = response.text\n         else: # last effort try\n             try:\n                 body = response.body.decode('utf-8')\n\n@@ -64,8 +64,8 @@ def _urlencode(seq, enc):\n \n def _get_form(response, formname, formid, formnumber, formxpath):\n     \"\"\"Find the form element \"\"\"\n-    text = response.body_as_unicode()\n-    root = create_root_node(text, lxml.html.HTMLParser, base_url=get_base_url(response))\n+    root = create_root_node(response.text, lxml.html.HTMLParser,\n+                            base_url=get_base_url(response))\n     forms = root.xpath('//form')\n     if not forms:\n         raise ValueError(\"No <form> element found in %s\" % response)\n\n@@ -67,6 +67,11 @@ class TextResponse(Response):\n             self._cached_ubody = html_to_unicode(charset, self.body)[1]\n         return self._cached_ubody\n \n+    @property\n+    def text(self):\n+        \"\"\" Body as unicode \"\"\"\n+        return self.body_as_unicode()\n+\n     def urljoin(self, url):\n         \"\"\"Join this Response's url with a possible relative url to form an\n         absolute interpretation of the latter.\"\"\"\n\n@@ -60,7 +60,7 @@ class Selector(_ParselSelector, object_ref):\n             response = _response_from_text(text, st)\n \n         if response is not None:\n-            text = response.body_as_unicode()\n+            text = response.text\n             kwargs.setdefault('base_url', response.url)\n \n         self.response = response\n\n@@ -137,7 +137,7 @@ def _body_or_str(obj, unicode=True):\n         if not unicode:\n             return obj.body\n         elif isinstance(obj, TextResponse):\n-            return obj.body_as_unicode()\n+            return obj.text\n         else:\n             return obj.body.decode('utf-8')\n     elif isinstance(obj, six.text_type):\n\n@@ -25,7 +25,7 @@ _baseurl_cache = weakref.WeakKeyDictionary()\n def get_base_url(response):\n     \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n     if response not in _baseurl_cache:\n-        text = response.body_as_unicode()[0:4096]\n+        text = response.text[0:4096]\n         _baseurl_cache[response] = html.get_base_url(text, response.url,\n             response.encoding)\n     return _baseurl_cache[response]\n@@ -37,7 +37,7 @@ _metaref_cache = weakref.WeakKeyDictionary()\n def get_meta_refresh(response):\n     \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n-        text = response.body_as_unicode()[0:4096]\n+        text = response.text[0:4096]\n         text = _noscript_re.sub(u'', text)\n         text = _script_re.sub(u'', text)\n         _metaref_cache[response] = html.get_meta_refresh(text, response.url,\n\n@@ -55,12 +55,11 @@ class TestSpider(Spider):\n \n     def parse_item(self, response):\n         item = self.item_cls()\n-        body = response.body_as_unicode()\n-        m = self.name_re.search(body)\n+        m = self.name_re.search(response.text)\n         if m:\n             item['name'] = m.group(1)\n         item['url'] = response.url\n-        m = self.price_re.search(body)\n+        m = self.price_re.search(response.text)\n         if m:\n             item['price'] = m.group(1)\n         return item\n\n@@ -107,9 +107,11 @@ class BaseResponseTest(unittest.TestCase):\n             body_bytes = body\n \n         assert isinstance(response.body, bytes)\n+        assert isinstance(response.text, six.text_type)\n         self._assert_response_encoding(response, encoding)\n         self.assertEqual(response.body, body_bytes)\n         self.assertEqual(response.body_as_unicode(), body_unicode)\n+        self.assertEqual(response.text, body_unicode)\n \n     def _assert_response_encoding(self, response, encoding):\n         self.assertEqual(response.encoding, resolve_encoding(encoding))\n@@ -171,6 +173,10 @@ class TextResponseTest(BaseResponseTest):\n         self.assertTrue(isinstance(r1.body_as_unicode(), six.text_type))\n         self.assertEqual(r1.body_as_unicode(), unicode_string)\n \n+        # check response.text\n+        self.assertTrue(isinstance(r1.text, six.text_type))\n+        self.assertEqual(r1.text, unicode_string)\n+\n     def test_encoding(self):\n         r1 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=utf-8\"]}, body=b\"\\xc2\\xa3\")\n         r2 = self.response_class(\"http://www.example.com\", encoding='utf-8', body=u\"\\xa3\")\n@@ -219,12 +225,12 @@ class TextResponseTest(BaseResponseTest):\n                                  headers={\"Content-type\": [\"text/html; charset=utf-8\"]},\n                                  body=b\"\\xef\\xbb\\xbfWORD\\xe3\\xab\")\n         self.assertEqual(r6.encoding, 'utf-8')\n-        self.assertEqual(r6.body_as_unicode(), u'WORD\\ufffd\\ufffd')\n+        self.assertEqual(r6.text, u'WORD\\ufffd\\ufffd')\n \n     def test_bom_is_removed_from_body(self):\n         # Inferring encoding from body also cache decoded body as sideeffect,\n         # this test tries to ensure that calling response.encoding and\n-        # response.body_as_unicode() in indistint order doesn't affect final\n+        # response.text in indistint order doesn't affect final\n         # values for encoding and decoded body.\n         url = 'http://example.com'\n         body = b\"\\xef\\xbb\\xbfWORD\"\n@@ -233,9 +239,9 @@ class TextResponseTest(BaseResponseTest):\n         # Test response without content-type and BOM encoding\n         response = self.response_class(url, body=body)\n         self.assertEqual(response.encoding, 'utf-8')\n-        self.assertEqual(response.body_as_unicode(), u'WORD')\n+        self.assertEqual(response.text, u'WORD')\n         response = self.response_class(url, body=body)\n-        self.assertEqual(response.body_as_unicode(), u'WORD')\n+        self.assertEqual(response.text, u'WORD')\n         self.assertEqual(response.encoding, 'utf-8')\n \n         # Body caching sideeffect isn't triggered when encoding is declared in\n@@ -243,9 +249,9 @@ class TextResponseTest(BaseResponseTest):\n         # body\n         response = self.response_class(url, headers=headers, body=body)\n         self.assertEqual(response.encoding, 'utf-8')\n-        self.assertEqual(response.body_as_unicode(), u'WORD')\n+        self.assertEqual(response.text, u'WORD')\n         response = self.response_class(url, headers=headers, body=body)\n-        self.assertEqual(response.body_as_unicode(), u'WORD')\n+        self.assertEqual(response.text, u'WORD')\n         self.assertEqual(response.encoding, 'utf-8')\n \n     def test_replace_wrong_encoding(self):\n@@ -253,18 +259,18 @@ class TextResponseTest(BaseResponseTest):\n         r = self.response_class(\"http://www.example.com\", encoding='utf-8', body=b'PREFIX\\xe3\\xabSUFFIX')\n         # XXX: Policy for replacing invalid chars may suffer minor variations\n         # but it should always contain the unicode replacement char (u'\\ufffd')\n-        assert u'\\ufffd' in r.body_as_unicode(), repr(r.body_as_unicode())\n-        assert u'PREFIX' in r.body_as_unicode(), repr(r.body_as_unicode())\n-        assert u'SUFFIX' in r.body_as_unicode(), repr(r.body_as_unicode())\n+        assert u'\\ufffd' in r.text, repr(r.text)\n+        assert u'PREFIX' in r.text, repr(r.text)\n+        assert u'SUFFIX' in r.text, repr(r.text)\n \n         # Do not destroy html tags due to encoding bugs\n         r = self.response_class(\"http://example.com\", encoding='utf-8', \\\n                 body=b'\\xf0<span>value</span>')\n-        assert u'<span>value</span>' in r.body_as_unicode(), repr(r.body_as_unicode())\n+        assert u'<span>value</span>' in r.text, repr(r.text)\n \n         # FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse\n-        #r = self.response_class(\"http://www.example.com\", body='PREFIX\\xe3\\xabSUFFIX')\n-        #assert u'\\ufffd' in r.body_as_unicode(), repr(r.body_as_unicode())\n+        #r = self.response_class(\"http://www.example.com\", body=b'PREFIX\\xe3\\xabSUFFIX')\n+        #assert u'\\ufffd' in r.text, repr(r.text)\n \n     def test_selector(self):\n         body = b\"<html><head><title>Some page</title><body></body></html>\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dc8701ea429d4ded2f66d6b7c8fbce0bbcd0041a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 316 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -123,6 +123,18 @@ class RobotsTxtMiddlewareTest(unittest.TestCase):\n         deferred.addCallback(lambda _: self.assertTrue(middleware._logerror.called))\n         return deferred\n \n+    def test_robotstxt_immediate_error(self):\n+        self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n+        err = error.DNSLookupError('Robotstxt address not found')\n+        def immediate_failure(request, spider):\n+            deferred = Deferred()\n+            deferred.errback(failure.Failure(err))\n+            return deferred\n+        self.crawler.engine.download.side_effect = immediate_failure\n+\n+        middleware = RobotsTxtMiddleware(self.crawler)\n+        return self.assertNotIgnored(Request('http://site.local'), middleware)\n+\n     def test_ignore_robotstxt_request(self):\n         self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n         def ignore_request(request, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b2beb3e85d2e82977d259eea71402809d00d197e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 69 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,9 @@ import logging\n \n from six.moves.urllib import robotparser\n \n+from twisted.internet import reactor\n from twisted.internet.defer import Deferred, maybeDeferred\n+from twisted.internet.task import deferLater\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n@@ -57,7 +59,13 @@ class RobotsTxtMiddleware(object):\n                 priority=self.DOWNLOAD_PRIORITY,\n                 meta={'dont_obey_robotstxt': True}\n             )\n-            dfd = self.crawler.engine.download(robotsreq, spider)\n+            # engine.download() can return an already-called deferred, e.g. if a\n+            # middleware returns a response in process_request(). Using\n+            # deferLater() ensures that the error callback isn't called\n+            # immediately upon being added, so that it doesn't remove the key\n+            # before we check for it.\n+            dfd = deferLater(reactor, 0, self.crawler.engine.download,\n+                             robotsreq, spider)\n             dfd.addCallback(self._parse_robots, netloc)\n             dfd.addErrback(self._logerror, robotsreq, spider)\n             dfd.addErrback(self._robots_error, netloc)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f1d971a5c0cdfe0f4fe5619146cd6818324fc98e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 21 | Churn Cumulative: 404 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -273,10 +273,10 @@ class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_dict(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n-        if self.binary:\n-            return to_bytes(value, encoding=self.encoding)\n-        else:\n-            return to_unicode(value, encoding=self.encoding)\n+        encode_func = to_bytes if self.binary else to_unicode\n+        if isinstance(value, (six.text_type, bytes)):\n+            return encode_func(value, encoding=self.encoding)\n+        return value\n \n     def _serialize_dict(self, value):\n         for key, val in six.iteritems(value):\n\n@@ -134,6 +134,19 @@ class PythonItemExporterTest(BaseItemExporterTest):\n         expected = {b'name': b'John\\xc2\\xa3', b'age': b'22'}\n         self.assertEqual(expected, exporter.export_item(value))\n \n+    def test_other_python_types_item(self):\n+        from datetime import datetime\n+        now = datetime.now()\n+        item = {\n+            'boolean': False,\n+            'number': 22,\n+            'time': now,\n+            'float': 3.14,\n+        }\n+        ie = self._get_exporter()\n+        exported = ie.export_item(item)\n+        self.assertEqual(exported, item)\n+\n \n class PprintItemExporterTest(BaseItemExporterTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
