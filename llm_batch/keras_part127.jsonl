{"custom_id": "keras#773d4ce8cb31f220b9fd07812dd98d6581198d71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 326 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -168,7 +168,7 @@ def recall(y_true, y_pred):\n     return recall\n \n \n-def fbeta_score(y_true, y_pred, beta):\n+def fbeta_score(y_true, y_pred, beta=1):\n     '''Calculates the F score, the weighted harmonic mean of precision and recall.\n \n     This is useful for multi-label classification, where input samples can be\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#88f3b3f75e2988a044dc603858754515e09a6d7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 454 | Contributors (this commit): 9 | Commits (past 90d): 8 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n '''\n import numpy as np\n import matplotlib.pyplot as plt\n+from scipy.stats import norm\n \n from keras.layers import Input, Dense, Lambda\n from keras.models import Model\n@@ -82,9 +83,10 @@ generator = Model(decoder_input, _x_decoded_mean)\n n = 15  # figure with 15x15 digits\n digit_size = 28\n figure = np.zeros((digit_size * n, digit_size * n))\n-# we will sample n points within [-15, 15] standard deviations\n-grid_x = np.linspace(-15, 15, n)\n-grid_y = np.linspace(-15, 15, n)\n+# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n+# to produce values of the latent variables z, since the prior of the latent space is Gaussian\n+grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n+grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n \n for i, yi in enumerate(grid_x):\n     for j, xi in enumerate(grid_y):\n@@ -95,5 +97,5 @@ for i, yi in enumerate(grid_x):\n                j * digit_size: (j + 1) * digit_size] = digit\n \n plt.figure(figsize=(10, 10))\n-plt.imshow(figure)\n+plt.imshow(figure, cmap='Greys_r')\n plt.show()\n\n@@ -5,6 +5,7 @@ Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n '''\n import numpy as np\n import matplotlib.pyplot as plt\n+from scipy.stats import norm\n \n from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n from keras.layers import Convolution2D, Deconvolution2D\n@@ -153,9 +154,10 @@ generator = Model(decoder_input, _x_decoded_mean_squash)\n n = 15  # figure with 15x15 digits\n digit_size = 28\n figure = np.zeros((digit_size * n, digit_size * n))\n-# we will sample n points within [-15, 15] standard deviations\n-grid_x = np.linspace(-15, 15, n)\n-grid_y = np.linspace(-15, 15, n)\n+# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n+# to produce values of the latent variables z, since the prior of the latent space is Gaussian\n+grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n+grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n \n for i, yi in enumerate(grid_x):\n     for j, xi in enumerate(grid_y):\n@@ -167,5 +169,5 @@ for i, yi in enumerate(grid_x):\n                j * digit_size: (j + 1) * digit_size] = digit\n \n plt.figure(figsize=(10, 10))\n-plt.imshow(figure)\n+plt.imshow(figure, cmap='Greys_r')\n plt.show()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dfb606bb196c3be6f13a3c67b21251f48a198b9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 647 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -64,7 +64,6 @@ class MaxPooling1D(_Pooling1D):\n             2 will halve the input.\n             If None, it will default to `pool_length`.\n         border_mode: 'valid' or 'same'.\n-            Note: 'same' will only work with TensorFlow for the time being.\n     '''\n \n     def __init__(self, pool_length=2, stride=None,\n@@ -87,7 +86,6 @@ class AveragePooling1D(_Pooling1D):\n         stride: integer, or None. Stride value.\n             If None, it will default to `pool_length`.\n         border_mode: 'valid' or 'same'.\n-            Note: 'same' will only work with TensorFlow for the time being.\n \n     # Input shape\n         3D tensor with shape: `(samples, steps, features)`.\n@@ -179,7 +177,6 @@ class MaxPooling2D(_Pooling2D):\n         strides: tuple of 2 integers, or None. Strides values.\n             If None, it will default to `pool_size`.\n         border_mode: 'valid' or 'same'.\n-            Note: 'same' will only work with TensorFlow for the time being.\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n@@ -221,7 +218,6 @@ class AveragePooling2D(_Pooling2D):\n         strides: tuple of 2 integers, or None. Strides values.\n             If None, it will default to `pool_size`.\n         border_mode: 'valid' or 'same'.\n-            Note: 'same' will only work with TensorFlow for the time being.\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#628bc6e03e1024f5d9aecd75db9465cb51cc9209", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 9 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 332 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -70,10 +70,10 @@ def build_generator(latent_size):\n     latent = Input(shape=(latent_size, ))\n \n     # this will be our label\n-    image_class = Input(shape=(1, 1), dtype='int32')\n+    image_class = Input(shape=(1,), dtype='int32')\n \n     # 10 classes in MNIST\n-    cls = Flatten()(Embedding(10, latent_size, input_length=1,\n+    cls = Flatten()(Embedding(10, latent_size,\n                               init='glorot_normal')(image_class))\n \n     # hadamard product between z-space and a class conditional embedding\n@@ -145,7 +145,7 @@ if __name__ == '__main__':\n                       loss='binary_crossentropy')\n \n     latent = Input(shape=(latent_size, ))\n-    image_class = Input(shape=(1, 1), dtype='int32')\n+    image_class = Input(shape=(1,), dtype='int32')\n \n     # get a fake image\n     fake = generator([latent, image_class])\n@@ -199,10 +199,10 @@ if __name__ == '__main__':\n \n             # generate a batch of fake images, using the generated labels as a\n             # conditioner. We reshape the sampled labels to be\n-            # (batch_size, 1, 1) so that we can feed them into the embedding\n+            # (batch_size, 1) so that we can feed them into the embedding\n             # layer as a length one sequence\n             generated_images = generator.predict(\n-                [noise, sampled_labels.reshape((-1, 1, 1))], verbose=0)\n+                [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n \n             X = np.concatenate((image_batch, generated_images))\n             y = np.array([1] * batch_size + [0] * batch_size)\n@@ -226,7 +226,7 @@ if __name__ == '__main__':\n             trick = np.ones(2 * batch_size)\n \n             epoch_gen_loss.append(combined.train_on_batch(\n-                [noise, sampled_labels.reshape((-1, 1, 1))], [trick, sampled_labels]))\n+                [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels]))\n \n             discriminator.trainable = True\n \n@@ -240,7 +240,7 @@ if __name__ == '__main__':\n         # sample some labels from p_c and generate images from them\n         sampled_labels = np.random.randint(0, 10, nb_test)\n         generated_images = generator.predict(\n-            [noise, sampled_labels.reshape((-1, 1, 1))], verbose=False)\n+            [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n \n         X = np.concatenate((X_test, generated_images))\n         y = np.array([1] * nb_test + [0] * nb_test)\n@@ -259,7 +259,7 @@ if __name__ == '__main__':\n         trick = np.ones(2 * nb_test)\n \n         generator_test_loss = combined.evaluate(\n-            [noise, sampled_labels.reshape((-1, 1, 1))],\n+            [noise, sampled_labels.reshape((-1, 1))],\n             [trick, sampled_labels], verbose=False)\n \n         generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n@@ -296,7 +296,7 @@ if __name__ == '__main__':\n \n         sampled_labels = np.array([\n             [i] * 10 for i in range(10)\n-        ]).reshape(-1, 1, 1)\n+        ]).reshape(-1, 1)\n \n         # get a batch to display\n         generated_images = generator.predict(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce814302acee2c474a88b2f4bfcdc92ff866f94f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 787 | Files Changed: 7 | Hunks: 8 | Methods Changed: 27 | Complexity Δ (Sum/Max): -109/0 | Churn Δ: 791 | Churn Cumulative: 17037 | Contributors (this commit): 101 | Commits (past 90d): 54 | Contributors (cumulative): 132 | DMM Complexity: 0.4531914893617021\n\nDIFF:\n@@ -903,7 +903,7 @@ class Layer(object):\n         composing the weights of the layer.\n         '''\n         if not self.built:\n-            if self.__class__.__name__ in {'Sequential', 'Graph'}:\n+            if self.__class__.__name__ in {'Sequential'}:\n                 self.build()\n             else:\n                 raise Exception('You tried to call `count_params` on ' +\n\n@@ -252,9 +252,6 @@ def collect_trainable_weights(layer):\n     elif layer.__class__.__name__ == 'Model':\n         for sublayer in layer.layers:\n             weights += collect_trainable_weights(sublayer)\n-    elif layer.__class__.__name__ == 'Graph':\n-        for sublayer in layer._graph_nodes.values():\n-            weights += collect_trainable_weights(sublayer)\n     else:\n         weights += layer.trainable_weights\n     # dedupe weights\n\n\n@@ -1,777 +0,0 @@\n-from collections import OrderedDict\n-import warnings\n-import copy\n-\n-from .. import backend as K\n-from ..layers import InputLayer, Layer, Merge\n-from ..engine.training import Model\n-\n-\n-class Graph(Model):\n-    '''Arbitrary connection graph.\n-\n-    THIS IS A LEGACY MODEL AND SHOULD NOT BE USED\n-    except for backwards compatibility support.\n-\n-    For multi-inputs/multi-outputs models, or\n-    models using shared layers, use the functional API instead.\n-    '''\n-\n-    def __init__(self, name=None):\n-        # model attributes\n-        self.inbound_nodes = []\n-        self.outbound_nodes = []\n-        self.built = False\n-        self.supports_masking = False\n-\n-        # legacy attributes (we prefix them with _graph_)\n-        self._graph_namespace = set()  # strings\n-        self._graph_nodes = OrderedDict()  # layer-like\n-        self._graph_inputs = OrderedDict()  # layer-like\n-        self._graph_outputs = OrderedDict()  # layer-like\n-        self._graph_input_config = []  # dicts\n-        self._graph_output_config = []  # dicts\n-        self._graph_node_config = []  # dicts\n-        self._graph_shared_nodes_names = []\n-\n-        if not name:\n-            prefix = 'graph_'\n-            name = prefix + str(K.get_uid(prefix))\n-        self.name = name\n-\n-    def __call__(self, x, mask=None):\n-        self.build()\n-        return super(Graph, self).__call__(x, mask)\n-\n-    def build(self, input_shape=None):\n-        # this will crash if the input/output layers have multiple nodes\n-        # no plans to support that case since Graph is deprecated\n-        input_tensors = [layer.output for layer in self._graph_inputs.values()]\n-        output_tensors = [layer.output for layer in self._graph_outputs.values()]\n-        # actually create the model\n-        super(Graph, self).__init__(input_tensors,\n-                                    output_tensors,\n-                                    name=self.name)\n-        self.built = True\n-\n-    def compile(self, optimizer, loss,\n-                metrics=[],\n-                sample_weight_modes=None,\n-                loss_weights=None,\n-                **kwargs):\n-        '''Configures the learning process.\n-\n-        # Arguments\n-            optimizer: str (name of optimizer) or optimizer object.\n-                See [optimizers](optimizers.md).\n-            loss: dictionary mapping the name(s) of the output(s) to\n-                a loss function (string name of objective function or\n-                objective function. See [objectives](objectives.md)).\n-            metrics: list of str (name of metrics) or\n-                list of metrics functions. See [metrics](metrics.md).\n-            sample_weight_modes: optional dictionary mapping certain\n-                output names to a sample weight mode (\"temporal\" and None\n-                are the only supported modes). If you need to do\n-                timestep-wise loss weighting on one of your graph outputs,\n-                you will need to set the sample weight mode for this output\n-                to \"temporal\".\n-            loss_weights: dictionary you can pass to specify a weight\n-                coefficient for each loss function (in a multi-output model).\n-                If no loss weight is specified for an output,\n-                the weight for this output's loss will be considered to be 1.\n-            kwargs: for Theano backend, these are passed into K.function.\n-                Ignored for Tensorflow backend.\n-        '''\n-        # create the underlying Model\n-        if not self.built:\n-            self.build()\n-        super(Graph, self).compile(optimizer, loss,\n-                                   metrics=metrics,\n-                                   sample_weight_mode=sample_weight_modes,\n-                                   loss_weights=loss_weights,\n-                                   **kwargs)\n-\n-    def add_input(self, name, input_shape=None,\n-                  batch_input_shape=None, dtype='float'):\n-        '''Adds an input to the graph.\n-\n-        # Arguments:\n-            name: string. The name of the new input.\n-                Must be unique in the graph.\n-            input_shape: a tuple of integers,\n-                the expected shape of the input samples.\n-                Does not include the batch size.\n-            batch_input_shape: a tuple of integers,\n-                the expected shape of the whole input batch,\n-                including the batch size.\n-            dtype: 'float', or 'int'.\n-        '''\n-        if name in self._graph_namespace:\n-            raise Exception('Duplicate node identifier: ' + name)\n-        self._graph_namespace.add(name)\n-        self.built = False\n-\n-        if dtype[:3] == 'int':\n-            dtype = 'int32'\n-        elif dtype[:5] == 'float':\n-            dtype = K.floatx()\n-        else:\n-            raise Exception('Uknown dtype (should be \"int\" or \"float\"): ' +\n-                            str(dtype))\n-\n-        # create input layer\n-        input_layer = InputLayer(input_shape=input_shape,\n-                                 batch_input_shape=batch_input_shape,\n-                                 name=name, input_dtype=dtype)\n-        self._graph_inputs[name] = input_layer\n-\n-        # append input config to self._graph_input_config\n-        config = {'name': name, 'dtype': dtype}\n-        if batch_input_shape:\n-            config['batch_input_shape'] = batch_input_shape\n-        else:\n-            config['input_shape'] = input_shape\n-        self._graph_input_config.append(config)\n-\n-    def add_node(self, layer, name, input=None, inputs=[],\n-                 merge_mode='concat', concat_axis=-1, dot_axes=-1,\n-                 create_output=False):\n-        '''Adds a node in the graph. It can be connected to multiple\n-        inputs, which will first be merged into one tensor\n-        according to the mode specified.\n-\n-        # Arguments\n-            layer: the layer at the node.\n-            name: name for the node.\n-            input: when connecting the layer to a single input,\n-                this is the name of the incoming node.\n-            inputs: when connecting the layer to multiple inputs,\n-                this is a list of names of incoming nodes.\n-            merge_mode: one of {concat, sum, dot, ave, mul}\n-            concat_axis: when `merge_mode=='concat'`, this is the\n-                input concatenation axis.\n-            dot_axes: when `merge_mode='dot'`,\n-                this is the contraction axes specification;\n-                see the `Merge` layer for details.\n-            create_output: boolean. Set this to `True` if you want the output\n-                of your node to be an output of the graph.\n-        '''\n-        if name in self._graph_namespace:\n-            raise Exception('Duplicate node identifier: ' + name)\n-        self._graph_namespace.add(name)\n-        layer.name = name\n-        self.built = False\n-\n-        if input:\n-            if input not in self._graph_namespace:\n-                raise Exception('Unknown node/input identifier: ' + input)\n-            if input in self._graph_nodes:\n-                layer.add_inbound_node(self._graph_nodes[input])\n-            elif input in self._graph_inputs:\n-                layer.add_inbound_node(self._graph_inputs[input])\n-        if inputs:\n-            to_merge = []\n-            for n in inputs:\n-                if n in self._graph_nodes:\n-                    to_merge.append(self._graph_nodes[n])\n-                elif n in self._graph_inputs:\n-                    to_merge.append(self._graph_inputs[n])\n-                else:\n-                    raise Exception('Unknown identifier: ' + n)\n-            merge = Merge(to_merge, mode=merge_mode,\n-                          concat_axis=concat_axis, dot_axes=dot_axes,\n-                          name='merge_inputs_for_' + name)\n-            layer.add_inbound_node(merge)\n-        self._graph_nodes[name] = layer\n-        self._graph_node_config.append({'name': name,\n-                                        'input': input,\n-                                        'inputs': inputs,\n-                                        'merge_mode': merge_mode,\n-                                        'concat_axis': concat_axis,\n-                                        'dot_axes': dot_axes,\n-                                        'create_output': create_output})\n-        if create_output:\n-            self.add_output(name, input=name)\n-\n-    def add_shared_node(self, layer, name, inputs=[], merge_mode=None,\n-                        concat_axis=-1, dot_axes=-1, outputs=[],\n-                        create_output=False):\n-        '''Used to share a same layer across multiple nodes.\n-\n-        Supposed, for instance, that you want to apply one same `Dense` layer\n-        after two different nodes ('node_a' and 'node_b').\n-        You can then add the dense layer as a shared node by calling:\n-\n-        ```python\n-        model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'], ...)\n-        ```\n-\n-        If you want access to the output of dense(node_a) and dense(node_b) separately,\n-        you can add these outputs to the Graph by passing an `outputs` argument:\n-\n-        ```python\n-        model.add_shared_node(my_dense, name='shared_dense', inputs=['node_a', 'node_b'],\n-                              outputs=['dense_output_a', 'dense_outputs_b'])\n-        ```\n-\n-        Otherwise you can merge these different outputs via `merge_mode`.\n-        In that case you can access the merged output\n-        under the identifier `name`.\n-\n-        # Arguments\n-            layer: The layer to be shared across multiple inputs\n-            name: Name of the shared node\n-            inputs: List of names of input nodes\n-            merge_mode: Same meaning as `merge_mode` argument of `add_node()`\n-            concat_axis: Same meaning as `concat_axis` argument of `add_node()`\n-            dot_axes: Same meaning as `dot_axes` argument of `add_node()`\n-            outputs: Used when `merge_mode=None`. Names for the output nodes.\n-            create_output: Same meaning as `create_output` argument of `add_node()`.\n-        '''\n-        if name in self._graph_namespace:\n-            raise Exception('Duplicate node identifier: ' + name)\n-        self._graph_namespace.add(name)\n-        self.built = False\n-\n-        for o in outputs:\n-            if o in self._graph_namespace:\n-                raise Exception('Duplicate node identifier: ' + o)\n-        if merge_mode:\n-            if merge_mode not in {'sum', 'ave', 'mul', 'dot', 'cos', 'concat'}:\n-                raise Exception('Invalid merge mode:', merge_mode)\n-        input_layers = []\n-        for i in range(len(inputs)):\n-            input = inputs[i]\n-            if input in self._graph_nodes:\n-                n = self._graph_nodes[input]\n-                input_layers.append(n)\n-            elif input in self._graph_inputs:\n-                n = self._graph_inputs[input]\n-                input_layers.append(n)\n-            else:\n-                raise Exception('Unknown identifier: ' + input)\n-\n-        created_node_indices = []\n-        for input_layer in input_layers:\n-            created_node_indices.append(len(layer.inbound_nodes))\n-            layer.add_inbound_node(input_layer)\n-\n-        if merge_mode:\n-            layer.name = 'input_for_' + name\n-            # collect all output nodes of layer and merge them into a single output\n-            merge = Merge([layer for _ in range(len(inputs))],\n-                          mode=merge_mode,\n-                          concat_axis=concat_axis, dot_axes=dot_axes,\n-                          node_indices=created_node_indices,\n-                          name=name)\n-            self._graph_nodes[name] = merge\n-            if create_output:\n-                self.add_output(name, input=name)\n-        else:\n-            layer.name = name\n-            # create one new layer per output node of layer,\n-            # and add them to the Graph with their own identifiers\n-            if len(outputs) != len(inputs):\n-                raise Exception('When using merge_mode=None, '\n-                                'you should provide a list of '\n-                                'output names (`output` argument) '\n-                                'the same size as `input`.')\n-            for i in range(len(outputs)):\n-                output_layer_name = outputs[i]\n-                output_layer = Layer(name=output_layer_name)\n-                output_layer.add_inbound_node(layer, created_node_indices[i])\n-                self._graph_namespace.add(output_layer_name)\n-                self._graph_nodes[output_layer_name] = output_layer\n-                if create_output:\n-                    self.add_output(output_layer_name, input=output_layer_name)\n-\n-        self._graph_node_config.append({'name': name,\n-                                        'layer': {\n-                                            'config': layer.get_config(),\n-                                            'class_name': layer.__class__.__name__,\n-                                        },\n-                                        'inputs': inputs,\n-                                        'merge_mode': merge_mode,\n-                                        'concat_axis': concat_axis,\n-                                        'dot_axes': dot_axes,\n-                                        'outputs': outputs,\n-                                        'create_output': create_output if merge_mode else False})\n-        self._graph_shared_nodes_names.append(name)\n-\n-    def add_output(self, name, input=None, inputs=[],\n-                   merge_mode='concat', concat_axis=-1, dot_axes=-1):\n-        '''Adds an output to the graph.\n-\n-        This output can merge several node outputs into a single output.\n-\n-        # Arguments\n-            name: name of the output.\n-            input: when connecting the layer to a single input,\n-                this is the name of the incoming node.\n-            inputs: when connecting the layer to multiple inputs,\n-                this is a list of names of incoming nodes.\n-            merge_mode: one of {concat, sum, dot, ave, mul}\n-            concat_axis: when `merge_mode=='concat'`, this is the\n-                input concatenation axis.\n-            dot_axes: when `merge_mode='dot'`,\n-                this is the contraction axes specification;\n-                see the `Merge layer for details.\n-        '''\n-        if name not in self._graph_namespace:\n-            self._graph_namespace.add(name)\n-        if name in self._graph_outputs:\n-            raise Exception('Duplicate output identifier:', name)\n-        self.built = False\n-\n-        if input:\n-            if input in self._graph_nodes:\n-                layer = self._graph_nodes[input]\n-            elif input in self._graph_inputs:\n-                layer = self._graph_inputs[input]\n-            else:\n-                raise Exception('Unknown node/input identifier: ' + input)\n-            if layer.name == name:\n-                self._graph_outputs[name] = layer\n-            else:\n-                layer.name = name\n-                self._graph_outputs[name] = layer\n-        if inputs:\n-            to_merge = []\n-            for n in inputs:\n-                if n not in self._graph_nodes:\n-                    raise Exception('Unknown identifier: ' + n)\n-                to_merge.append(self._graph_nodes[n])\n-            merge = Merge(to_merge, mode=merge_mode,\n-                          concat_axis=concat_axis, dot_axes=dot_axes,\n-                          name=name)\n-            self._graph_outputs[name] = merge\n-\n-        self._graph_output_config.append({'name': name,\n-                                          'input': input,\n-                                          'inputs': inputs,\n-                                          'merge_mode': merge_mode,\n-                                          'concat_axis': concat_axis,\n-                                          'dot_axes': dot_axes})\n-\n-    def _get_x(self, data):\n-        x = []\n-        for key in self._graph_inputs.keys():\n-            if key not in data:\n-                raise Exception('Expected to be provided an array '\n-                                '(in dict argument `data`) for input \"' +\n-                                key + '\".')\n-            x.append(data[key])\n-        return x\n-\n-    def _get_y(self, data):\n-        y = []\n-        for key in self._graph_outputs.keys():\n-            if key not in data:\n-                raise Exception('Expected to be provided an array '\n-                                '(in dict argument `data`) for output \"' +\n-                                key + '\".')\n-            y.append(data[key])\n-        return y\n-\n-    def fit(self, data, batch_size=32, nb_epoch=10, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True,\n-            class_weight=None, sample_weight=None, **kwargs):\n-        '''Trains the model for a fixed number of epochs.\n-\n-        Returns a history object. Its `history` attribute is a record of\n-        training loss values at successive epochs,\n-        as well as validation loss values (if applicable).\n-\n-        # Arguments\n-            data: dictionary mapping input names and outputs names to\n-                appropriate Numpy arrays. All arrays should contain\n-                the same number of samples.\n-            batch_size: int. Number of samples per gradient update.\n-            nb_epoch: int.\n-            verbose: 0 for no logging to stdout,\n-                1 for progress bar logging, 2 for one log line per epoch.\n-            callbacks: `keras.callbacks.Callback` list. List of callbacks\n-                to apply during training. See [callbacks](callbacks.md).\n-            validation_split: float (0. < x < 1). Fraction of the data to\n-                use as held-out validation data.\n-            validation_data: dictionary mapping input names and outputs names\n-                to appropriate Numpy arrays to be used as\n-                held-out validation data.\n-                All arrays should contain the same number of samples.\n-                Will override validation_split.\n-            shuffle: boolean. Whether to shuffle the samples at each epoch.\n-            class_weight: dictionary mapping output names to\n-                class weight dictionaries.\n-            sample_weight: dictionary mapping output names to\n-                numpy arrays of sample weights.\n-        '''\n-        if 'show_accuracy' in kwargs:\n-            kwargs.pop('show_accuracy')\n-            warnings.warn('The \"show_accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if kwargs:\n-            raise Exception('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n-        x = self._get_x(data)\n-        y = self._get_y(data)\n-\n-        if type(validation_data) is tuple:\n-            raise Exception('Cannot used sample_weight with '\n-                            'validation data with legacy Graph model. '\n-                            'validation_data should be a dictionary.')\n-        if validation_data:\n-            val_x = self._get_x(validation_data)\n-            val_y = self._get_y(validation_data)\n-            validation_data = (val_x, val_y)\n-        return super(Graph, self).fit(x, y,\n-                                      batch_size=batch_size,\n-                                      nb_epoch=nb_epoch,\n-                                      verbose=verbose,\n-                                      callbacks=callbacks,\n-                                      validation_split=validation_split,\n-                                      validation_data=validation_data,\n-                                      shuffle=shuffle,\n-                                      class_weight=class_weight,\n-                                      sample_weight=sample_weight)\n-\n-    def evaluate(self, data, batch_size=128,\n-                 verbose=0, sample_weight={}, **kwargs):\n-        '''Computes the loss on some input data, batch by batch.\n-\n-        Returns the scalar test loss over the data,\n-        or a list of metrics values (starting with the test loss)\n-        if applicable.\n-\n-        Arguments: see `fit` method.\n-        '''\n-        if 'show_accuracy' in kwargs:\n-            kwargs.pop('show_accuracy')\n-            warnings.warn('The \"show_accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if kwargs:\n-            raise Exception('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n-        x = self._get_x(data)\n-        y = self._get_y(data)\n-        return super(Graph, self).evaluate(x, y,\n-                                           batch_size=batch_size,\n-                                           verbose=verbose,\n-                                           sample_weight=sample_weight)\n-\n-    def predict(self, data, batch_size=128, verbose=0):\n-        '''Generates output predictions for the input samples\n-        batch by batch.\n-\n-        Arguments: see `fit` method.\n-        '''\n-        x = self._get_x(data)\n-        output_list = super(Graph, self).predict(x, batch_size=batch_size,\n-                                                 verbose=verbose)\n-        if not isinstance(output_list, list):\n-            output_list = [output_list]\n-        return dict(zip(self._graph_outputs, output_list))\n-\n-    def train_on_batch(self, data,\n-                       class_weight={},\n-                       sample_weight={}, **kwargs):\n-        '''Single gradient update on a batch of samples.\n-\n-        Returns the scalar train loss over the data,\n-        or a list of metrics values (starting with the test loss)\n-        if applicable.\n-\n-        Arguments: see `fit` method.\n-        '''\n-        if 'accuracy' in kwargs:\n-            kwargs.pop('accuracy')\n-            warnings.warn('The \"accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if kwargs:\n-            raise Exception('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n-        x = self._get_x(data)\n-        y = self._get_y(data)\n-        return super(Graph, self).train_on_batch(x, y,\n-                                                 sample_weight=sample_weight,\n-                                                 class_weight=class_weight)\n-\n-    def test_on_batch(self, data, sample_weight={}, **kwargs):\n-        '''Test the network on a single batch of samples.\n-\n-        Returns the scalar test loss over the data,\n-        or a list of metrics values (starting with the test loss)\n-        if applicable.\n-\n-        Arguments: see `fit` method.\n-        '''\n-        if 'accuracy' in kwargs:\n-            kwargs.pop('accuracy')\n-            warnings.warn('The \"accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if kwargs:\n-            raise Exception('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n-        x = self._get_x(data)\n-        y = self._get_y(data)\n-        return super(Graph, self).test_on_batch(x, y,\n-                                                sample_weight=sample_weight)\n-\n-    def predict_on_batch(self, data):\n-        output_list = super(Graph, self).predict_on_batch(data)\n-        if not isinstance(output_list, list):\n-            output_list = [output_list]\n-        return dict(zip(self._graph_outputs, output_list))\n-\n-    def fit_generator(self, generator, samples_per_epoch, nb_epoch,\n-                      verbose=1, callbacks=[],\n-                      validation_data=None, nb_val_samples=None,\n-                      class_weight={},\n-                      max_q_size=10, nb_worker=1,\n-                      pickle_safe=False, **kwargs):\n-        '''Fits a model on data generated batch-by-batch by a Python generator.\n-        The generator is run in parallel to the model, for efficiency.\n-        For instance, this allows you to do real-time data augmentation\n-        on images on CPU in parallel to training your model on GPU.\n-\n-        # Arguments\n-            generator: a generator.\n-                The output of the generator must be either a tuple\n-                of dictionaries `(input_data, sample_weight)`\n-                or a dictionary `input_data`\n-                (mapping names of inputs and outputs to Numpy arrays).\n-                All arrays should contain the same number of samples.\n-                The generator is expected to loop over its data\n-                indefinitely. An epoch finishes when `samples_per_epoch`\n-                samples have been seen by the model.\n-            samples_per_epoch: integer, number of samples to process before\n-                going to the next epoch.\n-            nb_epoch: integer, total number of iterations on the data.\n-            verbose: verbosity mode, 0, 1, or 2.\n-            callbacks: list of callbacks to be called during training.\n-            validation_data: dictionary mapping input names and outputs names\n-                to appropriate Numpy arrays to be used as\n-                held-out validation data, or a generator yielding such\n-                dictionaries. All arrays should contain the same number\n-                of samples. If a generator, will be called until more than\n-                `nb_val_samples` examples have been generated at the\n-                end of every epoch. These examples will then be used\n-                as the validation data.\n-            nb_val_samples: number of samples to use from validation\n-                generator at the end of every epoch.\n-            class_weight: dictionary mapping class indices to a weight\n-                for the class.\n-\n-        # Returns\n-            A `History` object.\n-\n-        # Examples\n-\n-        ```python\n-            def generate_arrays_from_file(path):\n-                while 1:\n-                    f = open(path)\n-                    for line in f:\n-                        # create Numpy arrays of input data\n-                        # and labels, from each line in the file\n-                        x1, x2, y = process_line(line)\n-                        yield ({'input_1': x1, 'input_2': x2, 'output': y})\n-                    f.close()\n-\n-            graph.fit_generator(generate_arrays_from_file('/my_file.txt'),\n-                                samples_per_epoch=10000, nb_epoch=10)\n-        ```\n-        '''\n-        if 'show_accuracy' in kwargs:\n-            kwargs.pop('show_accuracy')\n-            warnings.warn('The \"show_accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if 'nb_val_worker' in kwargs:\n-            kwargs.pop('nb_val_worker')\n-            warnings.warn('The \"nb_val_worker\" argument is deprecated, '\n-                          'please remove it from your code.')\n-        if kwargs:\n-            raise Exception('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n-\n-        self._train_on_batch = self.train_on_batch\n-        self.train_on_batch = super(Graph, self).train_on_batch\n-        self._evaluate = self.evaluate\n-        self.evaluate = super(Graph, self).evaluate\n-\n-        if validation_data and type(validation_data) is tuple:\n-            raise Exception('Cannot use sample_weight with '\n-                            'validation_data in legacy Graph model.')\n-        if validation_data and type(validation_data) is dict:\n-            validation_data = (self._get_x(validation_data),\n-                               self._get_y(validation_data))\n-\n-        original_generator = generator\n-\n-        def fixed_generator():\n-            while 1:\n-                data = next(original_generator)\n-                if type(data) is tuple:\n-                    data, sample_weight = data\n-                    x = self._get_x(data)\n-                    y = self._get_y(data)\n-                    yield x, y, sample_weight\n-                else:\n-                    x = self._get_x(data)\n-                    y = self._get_y(data)\n-                    yield x, y\n-\n-        generator = fixed_generator()\n-        history = super(Graph, self).fit_generator(generator,\n-                                                   samples_per_epoch,\n-                                                   nb_epoch,\n-                                                   verbose=verbose,\n-                                                   callbacks=callbacks,\n-                                                   validation_data=validation_data,\n-                                                   nb_val_samples=nb_val_samples,\n-                                                   class_weight=class_weight,\n-                                                   max_q_size=max_q_size,\n-                                                   nb_worker=nb_worker,\n-                                                   pickle_safe=pickle_safe)\n-        self.train_on_batch = self._train_on_batch\n-        self.evaluate = self._evaluate\n-        return history\n-\n-    def evaluate_generator(self, generator, val_samples,\n-                           verbose=1, max_q_size=10, nb_worker=1,\n-                           pickle_safe=False, **kwargs):\n-        '''Evaluates the model on a generator. The generator should\n-        return the same kind of data with every yield as accepted\n-        by `evaluate`.\n-\n-        If `show_accuracy`, it returns a tuple `(loss, accuracy)`,\n-        otherwise it returns the loss value.\n-\n-        Arguments:\n-            generator:\n-                generator yielding dictionaries of the kind accepted\n-                by `evaluate`, or tuples of such dictionaries and\n-                associated dictionaries of sample weights.\n-            val_samples:\n-                total number of samples to generate from `generator`\n-                to use in validation.\n-\n-            Other arguments are the same as for `fit`.\n-        '''\n-        if 'show_accuracy' in kwargs:\n-            kwargs.pop('show_accuracy')\n-            warnings.warn('The \"show_accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if 'verbose' in kwargs:\n-            kwargs.pop('verbose')\n-            warnings.warn('The \"verbose\" argument is deprecated.')\n-        if kwargs:\n-            raise Exception('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n-\n-        self._test_on_batch = self.test_on_batch\n-        self.test_on_batch = super(Graph, self).test_on_batch\n-\n-        original_generator = generator\n-\n-        def fixed_generator():\n-            while 1:\n-                data = next(original_generator)\n-                if type(data) is tuple:\n-                    data, sample_weight = data\n-                    x = self._get_x(data)\n-                    y = self._get_y(data)\n-                    yield x, y, sample_weight\n-                else:\n-                    x = self._get_x(data)\n-                    y = self._get_y(data)\n-                    yield x, y\n-\n-        generator = fixed_generator()\n-        history = super(Graph, self).evaluate_generator(generator,\n-                                                        val_samples,\n-                                                        max_q_size=max_q_size,\n-                                                        nb_worker=nb_worker,\n-                                                        pickle_safe=pickle_safe)\n-        self.test_on_batch = self._test_on_batch\n-        return history\n-\n-    # get_weights, set_weights: inherited\n-    def get_config(self):\n-        config = {'input_config': self._graph_input_config,\n-                  'node_config': self._graph_node_config,\n-                  'output_config': self._graph_output_config}\n-        nodes = {}\n-        for name, node in self._graph_nodes.items():\n-            nodes[name] = {'class_name': node.__class__.__name__,\n-                           'config': node.get_config()}\n-            if name in self._graph_shared_nodes_names:\n-                nodes[name]['shared'] = True\n-        config['nodes'] = nodes\n-        return copy.deepcopy(config)\n-\n-    @classmethod\n-    def from_config(cls, config):\n-        # TODO: test legacy support\n-        from keras.utils.layer_utils import layer_from_config\n-\n-        def normalize_legacy_config(conf):\n-            if 'class_name' not in conf:\n-                class_name = conf['name']\n-                name = conf.get('custom_name')\n-                conf['name'] = name\n-                new_config = {\n-                    'class_name': class_name,\n-                    'config': conf,\n-                }\n-                return new_config\n-            return conf\n-\n-        graph = cls()\n-        inputs = config.get('input_config')\n-        for input in inputs:\n-            graph.add_input(**input)\n-\n-        nodes = config.get('node_config')\n-        for node in nodes:\n-            layer_config = config['nodes'][node['name']]\n-            layer_config = normalize_legacy_config(layer_config)\n-            if 'layer' in node:\n-                # for add_shared_node\n-                node['layer'] = layer_from_config(node['layer'])\n-            else:\n-                layer = layer_from_config(layer_config)\n-                node['layer'] = layer\n-\n-            node['create_output'] = False  # outputs will be added below\n-            if layer_config.get('shared'):\n-                graph.add_shared_node(**node)\n-            else:\n-                graph.add_node(**node)\n-\n-        outputs = config.get('output_config')\n-        for output in outputs:\n-            graph.add_output(**output)\n-        return graph\n-\n-    def load_weights(self, fname):\n-        if not self.built:\n-            self.build()\n-        super(Graph, self).load_weights(fname)\n\n@@ -11,7 +11,6 @@ from .utils.io_utils import ask_to_proceed_with_overwrite\n from .engine.training import Model\n from .engine.topology import get_source_inputs, Node, Layer, Merge\n from .optimizers import optimizer_from_config\n-from .legacy.models import Graph\n \n \n def save_model(model, filepath, overwrite=True):\n\n@@ -3,7 +3,7 @@ from __future__ import print_function\n from .generic_utils import get_from_module\n from .np_utils import convert_kernel\n from ..layers import *\n-from ..models import Model, Sequential, Graph\n+from ..models import Model, Sequential\n from .. import backend as K\n \n \n@@ -15,7 +15,7 @@ def layer_from_config(config, custom_objects={}):\n             of custom (non-Keras) objects to class/functions\n \n     # Returns\n-        Layer instance (may be Model, Sequential, Graph, Layer...)\n+        Layer instance (may be Model, Sequential, Layer...)\n     '''\n     # Insert custom layers into globals so they can\n     # be accessed by `get_from_module`.\n@@ -26,8 +26,6 @@ def layer_from_config(config, custom_objects={}):\n \n     if class_name == 'Sequential':\n         layer_class = Sequential\n-    elif class_name == 'Graph':\n-        layer_class = Graph\n     elif class_name in ['Model', 'Container']:\n         layer_class = Model\n     else:\n\n@@ -9,7 +9,7 @@ from keras import optimizers\n np.random.seed(1337)\n \n from keras import callbacks\n-from keras.models import Graph, Sequential\n+from keras.models import Sequential\n from keras.layers.core import Dense\n from keras.utils.test_utils import get_test_data\n from keras import backend as K\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#de256cb5d5f5a939f916b2827d2a974bc9eec60b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 71 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -46,5 +46,6 @@ def decode_predictions(preds, top=5):\n     for pred in preds:\n         top_indices = pred.argsort()[-top:][::-1]\n         result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]\n+        result.sort(key=lambda x: x[2], reverse=True)\n         results.append(result)\n     return results\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
