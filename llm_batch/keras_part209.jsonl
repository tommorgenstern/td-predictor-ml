{"custom_id": "keras#a67a36c7341252e24bec94c784045478766a36e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 43 | Files Changed: 2 | Hunks: 17 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 75 | Churn Cumulative: 1752 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -71,34 +71,19 @@ if sys.version[0] == '2':\n     reload(sys)\n     sys.setdefaultencoding('utf8')\n \n-import keras\n from keras import utils\n from keras import layers\n-from keras import initializers\n-from keras.layers import pooling\n-from keras.layers import local\n-from keras.layers import recurrent\n-from keras.layers import core\n-from keras.layers import noise\n-from keras.layers import normalization\n from keras.layers import advanced_activations\n-from keras.layers import embeddings\n+from keras.layers import noise\n from keras.layers import wrappers\n+from keras import initializers\n from keras import optimizers\n from keras import callbacks\n from keras import models\n-from keras.engine import topology\n from keras import losses\n from keras import metrics\n from keras import backend\n-from keras import constraints\n from keras import activations\n-from keras import regularizers\n-from keras.utils import data_utils\n-from keras.utils import io_utils\n-from keras.utils import layer_utils\n-from keras.utils import np_utils\n-from keras.utils import generic_utils\n \n \n EXCLUDE = {\n@@ -182,49 +167,51 @@ PAGES = [\n     {\n         'page': 'layers/pooling.md',\n         'classes': [\n-            pooling.MaxPooling1D,\n-            pooling.MaxPooling2D,\n-            pooling.MaxPooling3D,\n-            pooling.AveragePooling1D,\n-            pooling.AveragePooling2D,\n-            pooling.AveragePooling3D,\n-            pooling.GlobalMaxPooling1D,\n-            pooling.GlobalAveragePooling1D,\n-            pooling.GlobalMaxPooling2D,\n-            pooling.GlobalAveragePooling2D,\n+            layers.MaxPooling1D,\n+            layers.MaxPooling2D,\n+            layers.MaxPooling3D,\n+            layers.AveragePooling1D,\n+            layers.AveragePooling2D,\n+            layers.AveragePooling3D,\n+            layers.GlobalMaxPooling1D,\n+            layers.GlobalAveragePooling1D,\n+            layers.GlobalMaxPooling2D,\n+            layers.GlobalAveragePooling2D,\n         ],\n     },\n     {\n         'page': 'layers/local.md',\n         'classes': [\n-            local.LocallyConnected1D,\n-            local.LocallyConnected2D,\n+            layers.LocallyConnected1D,\n+            layers.LocallyConnected2D,\n         ],\n     },\n     {\n         'page': 'layers/recurrent.md',\n         'classes': [\n-            recurrent.RNN,\n-            recurrent.SimpleRNN,\n-            recurrent.GRU,\n-            recurrent.LSTM,\n+            layers.RNN,\n+            layers.SimpleRNN,\n+            layers.GRU,\n+            layers.LSTM,\n             layers.ConvLSTM2D,\n-            recurrent.SimpleRNNCell,\n-            recurrent.GRUCell,\n-            recurrent.LSTMCell,\n-            recurrent.StackedRNNCells,\n+            layers.SimpleRNNCell,\n+            layers.GRUCell,\n+            layers.LSTMCell,\n+            layers.StackedRNNCells,\n+            layers.CuDNNGRU,\n+            layers.CuDNNLSTM,\n         ],\n     },\n     {\n         'page': 'layers/embeddings.md',\n         'classes': [\n-            embeddings.Embedding,\n+            layers.Embedding,\n         ],\n     },\n     {\n         'page': 'layers/normalization.md',\n         'classes': [\n-            normalization.BatchNormalization,\n+            layers.BatchNormalization,\n         ],\n     },\n     {\n@@ -381,7 +368,8 @@ def class_to_source_link(cls):\n     path = module_name.replace('.', '/')\n     path += '.py'\n     line = inspect.getsourcelines(cls)[-1]\n-    link = 'https://github.com/fchollet/keras/blob/master/' + path + '#L' + str(line)\n+    link = ('https://github.com/fchollet/'\n+            'keras/blob/master/' + path + '#L' + str(line))\n     return '[[source]](' + link + ')'\n \n \n@@ -468,7 +456,8 @@ for page_data in PAGES:\n     for cls in classes:\n         subblocks = []\n         signature = get_class_signature(cls)\n-        subblocks.append('<span style=\"float:right;\">' + class_to_source_link(cls) + '</span>')\n+        subblocks.append('<span style=\"float:right;\">' +\n+                         class_to_source_link(cls) + '</span>')\n         subblocks.append('### ' + cls.__name__ + '\\n')\n         subblocks.append(code_snippet(signature))\n         docstring = cls.__doc__\n\n@@ -120,7 +120,7 @@ class _CuDNNRNN(RNN):\n class CuDNNGRU(_CuDNNRNN):\n     \"\"\"Fast GRU implementation backed by CuDNN.\n \n-    Can only be run on GPU.\n+    Can only be run on GPU, with the TensorFlow backend.\n \n     # Arguments\n         units: Positive integer, dimensionality of the output space.\n@@ -309,7 +309,7 @@ class CuDNNGRU(_CuDNNRNN):\n class CuDNNLSTM(_CuDNNRNN):\n     \"\"\"Fast LSTM implementation backed by CuDNN.\n \n-    Can only be run on GPU.\n+    Can only be run on GPU, with the TensorFlow backend.\n \n     # Arguments\n         units: Positive integer, dimensionality of the output space.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6a36339b63372b51e0aad81fcc798d3183e45e26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 86 | Files Changed: 2 | Hunks: 30 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 174 | Churn Cumulative: 896 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -16,8 +16,6 @@ from keras.layers import Dense, Dropout, Activation, Flatten\n from keras.layers import Conv2D, MaxPooling2D\n \n import os\n-import pickle\n-import numpy as np\n \n batch_size = 32\n num_classes = 10\n@@ -38,7 +36,6 @@ y_train = keras.utils.to_categorical(y_train, num_classes)\n y_test = keras.utils.to_categorical(y_test, num_classes)\n \n model = Sequential()\n-\n model.add(Conv2D(32, (3, 3), padding='same',\n                  input_shape=x_train.shape[1:]))\n model.add(Activation('relu'))\n@@ -115,37 +112,7 @@ model_path = os.path.join(save_dir, model_name)\n model.save(model_path)\n print('Saved trained model at %s ' % model_path)\n \n-# Load label names to use in prediction results\n-label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n-\n-\n-keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n-datadir_base = os.path.expanduser(keras_dir)\n-if not os.access(datadir_base, os.W_OK):\n-    datadir_base = os.path.join('/tmp', '.keras')\n-label_list_path = os.path.join(datadir_base, label_list_path)\n-\n-with open(label_list_path, mode='rb') as f:\n-    labels = pickle.load(f)\n-\n-# Evaluate model with test data set and share sample prediction results\n-evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n-                                                   batch_size=batch_size,\n-                                                   shuffle=False),\n-                                      steps=x_test.shape[0] // batch_size,\n-                                      workers=4)\n-print('Model Accuracy = %.2f' % (evaluation[1]))\n-\n-predict_gen = model.predict_generator(datagen.flow(x_test, y_test,\n-                                                   batch_size=batch_size,\n-                                                   shuffle=False),\n-                                      steps=x_test.shape[0] // batch_size,\n-                                      workers=4)\n-\n-for predict_index, predicted_y in enumerate(predict_gen):\n-    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n-    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n-    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n-                                                          predicted_label))\n-    if predict_index == num_predictions:\n-        break\n+# Score trained model.\n+scores = model.evaluate(x_test, y_test, verbose=1)\n+print('Test loss:', scores[0])\n+print('Test accuracy:', scores[1])\n\n@@ -1,10 +1,11 @@\n-''' Trains a ResNet on the CIFAR10 dataset.\n+\"\"\"Trains a ResNet on the CIFAR10 dataset.\n+\n Greater than 91% test accuracy (0.52 val_loss) after 50 epochs\n 48sec per epoch on GTX 1080Ti\n \n Deep Residual Learning for Image Recognition\n https://arxiv.org/pdf/1512.03385.pdf\n-'''\n+\"\"\"\n \n from __future__ import print_function\n import keras\n@@ -20,25 +21,43 @@ from keras.datasets import cifar10\n import numpy as np\n import os\n \n-\n+# Training params.\n batch_size = 32\n-num_classes = 10\n epochs = 100\n+data_augmentation = True\n \n+# Network architecture params.\n+num_classes = 10\n+num_filters = 64\n+num_blocks = 4\n+num_sub_blocks = 2\n+use_max_pool = False\n+\n+# Load the CIFAR10 data.\n (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n \n-# input image dimensions\n-img_rows, img_cols, channels = x_train.shape[1], x_train.shape[2], x_train.shape[3]\n+# Input image dimensions.\n+# We assume data format \"channels_last\".\n+img_rows = x_train.shape[1]\n+img_cols = x_train.shape[2]\n+channels = x_train.shape[3]\n \n if K.image_data_format() == 'channels_first':\n+    img_rows = x_train.shape[2]\n+    img_cols = x_train.shape[3]\n+    channels = x_train.shape[1]\n     x_train = x_train.reshape(x_train.shape[0], channels, img_rows, img_cols)\n     x_test = x_test.reshape(x_test.shape[0], channels, img_rows, img_cols)\n     input_shape = (channels, img_rows, img_cols)\n else:\n+    img_rows = x_train.shape[1]\n+    img_cols = x_train.shape[2]\n+    channels = x_train.shape[3]\n     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\n     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n     input_shape = (img_rows, img_cols, channels)\n \n+# Normalize data.\n x_train = x_train.astype('float32') / 255\n x_test = x_test.astype('float32') / 255\n print('x_train shape:', x_train.shape)\n@@ -46,68 +65,93 @@ print(x_train.shape[0], 'train samples')\n print(x_test.shape[0], 'test samples')\n print('y_train shape:', y_train.shape)\n \n-# convert class vectors to binary class matrices\n+# Convert class vectors to binary class matrices.\n y_train = keras.utils.to_categorical(y_train, num_classes)\n y_test = keras.utils.to_categorical(y_test, num_classes)\n \n-xin = Input(shape=input_shape)\n-\n-filters = 64\n-blocks = 4\n-sub_blocks = 2\n-x = Conv2D(filters=filters, kernel_size=7, padding='same', strides=2,\n-           kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(xin)\n+# Start model definition.\n+inputs = Input(shape=input_shape)\n+x = Conv2D(num_filters,\n+           kernel_size=7,\n+           padding='same',\n+           strides=2,\n+           kernel_initializer='he_normal',\n+           kernel_regularizer=l2(1e-4))(inputs)\n x = BatchNormalization()(x)\n x = Activation('relu')(x)\n \n-# Orig paper uses max pool after 1st conv. Reaches up 87% acc if use_max_pool = True.\n+# Orig paper uses max pool after 1st conv.\n+# Reaches up 87% acc if use_max_pool = True.\n # Cifar10 images are already too small at 32x32 to be maxpooled. So, we skip.\n-use_max_pool = False\n if use_max_pool:\n-    x = MaxPooling2D(pool_size=3, strides=2, padding=\"same\")(x)\n-    blocks = 3\n+    x = MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n+    num_blocks = 3\n \n-for i in range(blocks):\n-    for j in range(sub_blocks):\n+# Instantiate convolutional base (stack of blocks).\n+for i in range(num_blocks):\n+    for j in range(num_sub_blocks):\n         strides = 1\n         is_first_layer_but_not_first_block = j == 0 and i > 0\n         if is_first_layer_but_not_first_block:\n             strides = 2\n-        y = Conv2D(filters=filters, kernel_size=3, padding='same', strides=strides,\n-                   kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(x)\n+        y = Conv2D(num_filters,\n+                   kernel_size=3,\n+                   padding='same',\n+                   strides=strides,\n+                   kernel_initializer='he_normal',\n+                   kernel_regularizer=l2(1e-4))(x)\n         y = BatchNormalization()(y)\n         y = Activation('relu')(y)\n-        y = Conv2D(filters=filters, kernel_size=3, padding='same',\n-                   kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(y)\n+        y = Conv2D(num_filters,\n+                   kernel_size=3,\n+                   padding='same',\n+                   kernel_initializer='he_normal',\n+                   kernel_regularizer=l2(1e-4))(y)\n         y = BatchNormalization()(y)\n         if is_first_layer_but_not_first_block:\n-            x = Conv2D(filters=filters, kernel_size=1, padding='same', strides=2,\n-                       kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(x)\n+            x = Conv2D(num_filters,\n+                       kernel_size=1,\n+                       padding='same',\n+                       strides=2,\n+                       kernel_initializer='he_normal',\n+                       kernel_regularizer=l2(1e-4))(x)\n         x = keras.layers.add([x, y])\n         x = Activation('relu')(x)\n \n-    filters = 2 * filters\n+    num_filters = 2 * num_filters\n \n+# Add classifier on top.\n x = AveragePooling2D()(x)\n y = Flatten()(x)\n-yout = Dense(num_classes, activation='softmax', kernel_initializer=\"he_normal\")(y)\n-model = Model(inputs=[xin], outputs=[yout])\n-model.compile(loss='categorical_crossentropy', optimizer=Adam(),\n+outputs = Dense(num_classes,\n+                activation='softmax',\n+                kernel_initializer='he_normal')(y)\n+\n+# Instantiate and compile model.\n+model = Model(inputs=inputs, outputs=outputs)\n+model.compile(loss='categorical_crossentropy',\n+              optimizer=Adam(),\n               metrics=['accuracy'])\n model.summary()\n \n-# Save model and weights\n+# Prepare model model saving directory.\n save_dir = os.path.join(os.getcwd(), 'saved_models')\n-model_name = \"cifar10_resnet_model.hdf5\"\n+model_name = 'cifar10_resnet_model.h5'\n if not os.path.isdir(save_dir):\n     os.makedirs(save_dir)\n filepath = os.path.join(save_dir, model_name)\n-checkpoint = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n-lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n+\n+# Prepare callbacks for model saving and for learning rate decaying.\n+checkpoint = ModelCheckpoint(filepath=filepath,\n+                             verbose=1,\n+                             save_best_only=True)\n+lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n+                               cooldown=0,\n+                               patience=5,\n+                               min_lr=0.5e-6)\n callbacks = [checkpoint, lr_reducer]\n \n-data_augmentation = True\n-\n+# Run training, with or without data augmentation.\n if not data_augmentation:\n     print('Not using data augmentation.')\n     model.fit(x_train, y_train,\n@@ -142,16 +186,7 @@ else:\n                         epochs=epochs, verbose=1, workers=4,\n                         callbacks=callbacks)\n \n-\n-score = model.evaluate(x_test, y_test, verbose=1)\n-print(\"\")\n-print('Test loss:', score[0])\n-print('Test accuracy:', score[1])\n-\n-score = model.evaluate_generator(datagen.flow(x_test, y_test,\n-                                 batch_size=batch_size,\n-                                 shuffle=False),\n-                                 steps=x_test.shape[0] // batch_size,\n-                                 workers=4)\n-print('Data gen test loss:', score[0])\n-print('Data gen test accuracy:', score[1])\n+# Score trained model.\n+scores = model.evaluate(x_test, y_test, verbose=1)\n+print('Test loss:', scores[0])\n+print('Test accuracy:', scores[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b53767da71ab94169c60f46f0b8e9d820fd59d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 27 | Churn Cumulative: 4105 | Contributors (this commit): 62 | Commits (past 90d): 17 | Contributors (cumulative): 78 | DMM Complexity: 1.0\n\nDIFF:\n@@ -497,10 +497,10 @@ class EarlyStopping(Callback):\n             self.best = current\n             self.wait = 0\n         else:\n+            self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n-            self.wait += 1\n \n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:\n\n@@ -256,6 +256,31 @@ def test_EarlyStopping_reuse():\n     assert len(hist.epoch) >= patience\n \n \n+@keras_test\n+def test_EarlyStopping_patience():\n+    class DummyModel(object):\n+        def __init__(self):\n+            self.stop_training = False\n+\n+    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)\n+    early_stop.model = DummyModel()\n+\n+    losses = [0.0860, 0.1096, 0.1040, 0.1019]\n+\n+    # Should stop after epoch 3, as the loss has not improved after patience=2 epochs.\n+    epochs_trained = 0\n+    early_stop.on_train_begin()\n+\n+    for epoch in range(len(losses)):\n+        epochs_trained += 1\n+        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n+\n+        if early_stop.model.stop_training:\n+            break\n+\n+    assert epochs_trained == 3\n+\n+\n @keras_test\n def test_LearningRateScheduler():\n     np.random.seed(1337)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1a3d9de50d3b10276a019a91c67cd24c69b2c888", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 11 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 828 | Contributors (this commit): 21 | Commits (past 90d): 5 | Contributors (cumulative): 21 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -258,8 +258,6 @@ class Progbar(object):\n \n     def __init__(self, target, width=30, verbose=1, interval=0.05):\n         self.width = width\n-        if target is None:\n-            target = -1\n         self.target = target\n         self.sum_values = {}\n         self.unique_values = []\n@@ -291,6 +289,7 @@ class Progbar(object):\n         self.seen_so_far = current\n \n         now = time.time()\n+        info = ' - %ds' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                     current < self.target):\n@@ -300,7 +299,7 @@ class Progbar(object):\n             sys.stdout.write('\\b' * prev_total_width)\n             sys.stdout.write('\\r')\n \n-            if self.target is not -1:\n+            if self.target is not None:\n                 numdigits = int(np.floor(np.log10(self.target))) + 1\n                 barstr = '%%%dd/%d [' % (numdigits, self.target)\n                 bar = barstr % current\n@@ -317,19 +316,16 @@ class Progbar(object):\n             else:\n                 bar = '%7d/Unknown' % current\n \n-            sys.stdout.write(bar)\n             self.total_width = len(bar)\n+            sys.stdout.write(bar)\n \n             if current:\n                 time_per_unit = (now - self.start) / current\n             else:\n                 time_per_unit = 0\n+            if self.target is not None and current <= self.target:\n                 eta = time_per_unit * (self.target - current)\n-            info = ''\n-            if current <= self.target and self.target is not -1:\n-                info += ' - ETA: %ds' % eta\n-            else:\n-                info += ' - %ds' % (now - self.start)\n+                info = ' - ETA: %ds' % eta\n             for k in self.unique_values:\n                 info += ' - %s:' % k\n                 if isinstance(self.sum_values[k], list):\n@@ -343,17 +339,16 @@ class Progbar(object):\n \n             self.total_width += len(info)\n             if prev_total_width > self.total_width:\n-                info += ((prev_total_width - self.total_width) * ' ')\n+                info += (' ' * (prev_total_width - self.total_width))\n+\n+            if self.target is not None and current >= self.target:\n+                info += '\\n'\n \n             sys.stdout.write(info)\n             sys.stdout.flush()\n \n-            if current >= self.target and self.target is not -1:\n-                sys.stdout.write('\\n')\n-\n         elif self.verbose == 2:\n-            if current >= self.target:\n-                info = '%ds' % (now - self.start)\n+            if self.target is None or current >= self.target:\n                 for k in self.unique_values:\n                     info += ' - %s:' % k\n                     avg = np.mean(self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n@@ -361,7 +356,10 @@ class Progbar(object):\n                         info += ' %.4f' % avg\n                     else:\n                         info += ' %.4e' % avg\n-                sys.stdout.write(info + \"\\n\")\n+                info += '\\n'\n+\n+                sys.stdout.write(info)\n+                sys.stdout.flush()\n \n         self.last_update = now\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3db58c8bf1ef9d078b3cf5d828d22346df6469b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 22 | Churn Cumulative: 1774 | Contributors (this commit): 17 | Commits (past 90d): 7 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,10 +66,6 @@ import re\n import inspect\n import os\n import shutil\n-import sys\n-if sys.version[0] == '2':\n-    reload(sys)\n-    sys.setdefaultencoding('utf8')\n \n from keras import utils\n from keras import layers\n@@ -85,6 +81,11 @@ from keras import metrics\n from keras import backend\n from keras import activations\n \n+import sys\n+if sys.version[0] == '2':\n+    reload(sys)\n+    sys.setdefaultencoding('utf8')\n+\n \n EXCLUDE = {\n     'Optimizer',\n@@ -347,10 +348,17 @@ def get_class_signature(cls):\n     try:\n         class_signature = get_function_signature(cls.__init__)\n         class_signature = class_signature.replace('__init__', cls.__name__)\n-    except:\n+    except TypeError:\n         # in case the class inherits from object and does not\n         # define __init__\n         class_signature = cls.__module__ + '.' + cls.__name__ + '()'\n+\n+    parts = class_signature.split('.')\n+    if len(parts) >= 4:\n+        if parts[1] == 'layers':\n+            class_signature = 'keras.layers.' + '.'.join(parts[3:])\n+        if parts[1] == 'utils':\n+            class_signature = 'keras.utils.' + '.'.join(parts[3:])\n     return class_signature\n \n \n\n@@ -118,7 +118,7 @@ class _CuDNNRNN(RNN):\n \n \n class CuDNNGRU(_CuDNNRNN):\n-    \"\"\"Fast GRU implementation backed by CuDNN.\n+    \"\"\"Fast GRU implementation backed by [CuDNN](https://developer.nvidia.com/cudnn).\n \n     Can only be run on GPU, with the TensorFlow backend.\n \n@@ -307,7 +307,7 @@ class CuDNNGRU(_CuDNNRNN):\n \n \n class CuDNNLSTM(_CuDNNRNN):\n-    \"\"\"Fast LSTM implementation backed by CuDNN.\n+    \"\"\"Fast LSTM implementation backed by [CuDNN](https://developer.nvidia.com/cudnn).\n \n     Can only be run on GPU, with the TensorFlow backend.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3dd3e8331677e68e7dec6ed4a1cbf16b7ef19f7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 365 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 8 | Complexity Δ (Sum/Max): 23/16 | Churn Δ: 365 | Churn Cumulative: 402 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.7870370370370371\n\nDIFF:\n@@ -21,3 +21,4 @@ from .layer_utils import convert_all_kernels_in_model\n from .vis_utils import plot_model\n from .np_utils import to_categorical\n from .np_utils import normalize\n+from .training_utils import multi_gpu_model\n\n@@ -0,0 +1,149 @@\n+from ..layers.merge import concatenate\n+from .. import backend as K\n+from ..layers.core import Lambda\n+from ..engine.training import Model\n+\n+\n+def _get_available_devices():\n+    from tensorflow.python.client import device_lib\n+    local_device_protos = device_lib.list_local_devices()\n+    return [x.name for x in local_device_protos]\n+\n+\n+def multi_gpu_model(model, gpus):\n+    \"\"\"Replicates a model on different GPUs.\n+\n+    Specifically, this function implements single-machine\n+    multi-GPU data parallelism. It works in the following way:\n+\n+    - Divide the model's input(s) into multiple sub-batches.\n+    - Apply a model copy on each sub-batch. Every model copy\n+        is executed on a dedicated GPU.\n+    - Concatenate the results (on CPU) into one big batch.\n+\n+    E.g. if your `batch_size` is 64 and you use `gpus=2`,\n+    then we will divide the input into 2 sub-batches of 32 samples,\n+    process each sub-batch on one GPU, then return the full\n+    batch of 64 processed samples.\n+\n+    This induces quasi-linear speedup on up to 8 GPUs.\n+\n+    This function is only available with the TensorFlow backend\n+    for the time being.\n+\n+    # Arguments\n+        model: A Keras model instance. To avoid OOM errors,\n+            this model could have been built on CPU, for instance\n+            (see usage example below).\n+        gpus: Integer >= 2, number of on GPUs on which to create\n+            model replicas.\n+\n+    # Returns\n+        A Keras `Model` instance which can be used just like the initial\n+        `model` argument, but which distributes its workload on multiple GPUs.\n+\n+    # Example\n+\n+    ```python\n+        import tensorflow as tf\n+        from keras.applications import Xception\n+\n+        num_samples = 1000\n+        height = 224\n+        width = 224\n+        num_classes = 1000\n+\n+        # Instantiate the base model\n+        # (here, we do it on CPU, which is optional).\n+        with tf.device('/cpu:0'):\n+            model = Xception(weights=None,\n+                             input_shape=(height, width, 3),\n+                             classes=num_classes)\n+\n+        # Replicates the model on 8 GPUs.\n+        # This assumes that your machine has 8 available GPUs.\n+        parallel_model = multi_gpu_model(model, gpus=8)\n+        parallel_model.compile(loss='categorical_crossentropy',\n+                               optimizer='rmsprop')\n+\n+        # Generate dummy data.\n+        x = np.random.random((num_samples, height, width, 3))\n+        y = np.random.random((num_samples, num_classes))\n+\n+        # This `fit` call will be distributed on 8 GPUs.\n+        # Since the batch size is 256, each GPU will process 32 samples.\n+        parallel_model.fit(x, y, epochs=20, batch_size=256)\n+    ```\n+    \"\"\"\n+    if K.backend() != 'tensorflow':\n+        raise ValueError('`multi_gpu_model` is only available '\n+                         'with the TensorFlow backend.')\n+    if gpus <= 1:\n+        raise ValueError('For multi-gpu usage to be effective, '\n+                         'call `multi_gpu_model` with `gpus >= 2`. '\n+                         'Received: `gpus=%d`' % gpus)\n+\n+    import tensorflow as tf\n+\n+    target_devices = ['/cpu:0'] + ['/gpu:%d' % i for i in range(gpus)]\n+    available_devices = _get_available_devices()\n+    for device in target_devices:\n+        if device not in available_devices:\n+            raise ValueError(\n+                'To call `multi_gpu_model` with `gpus=%d`, '\n+                'we expect the following devices to be available: %s. '\n+                'However this machine only has: %s. '\n+                'Try reducing `gpus`.' % (gpus,\n+                                          target_devices,\n+                                          available_devices))\n+\n+    def get_slice(data, i, parts):\n+        shape = tf.shape(data)\n+        batch_size = shape[:1]\n+        input_shape = shape[1:]\n+        step = batch_size // parts\n+        if i == gpus - 1:\n+            size = batch_size - step * i\n+        else:\n+            size = step\n+        size = tf.concat([size, input_shape], axis=0)\n+        stride = tf.concat([step, input_shape * 0], axis=0)\n+        start = stride * i\n+        return tf.slice(data, start, size)\n+\n+    all_outputs = []\n+    for i in range(len(model.outputs)):\n+        all_outputs.append([])\n+\n+    # Place a copy of the model on each GPU,\n+    # each getting a slice of the inputs.\n+    for i in range(gpus):\n+        with tf.device('/gpu:%d' % i):\n+            with tf.name_scope('replica_%d' % i):\n+                inputs = []\n+                # Retrieve a slice of the input.\n+                for x in model.inputs:\n+                    input_shape = tuple(x.get_shape().as_list())[1:]\n+                    slice_i = Lambda(get_slice,\n+                                     output_shape=input_shape,\n+                                     arguments={'i': i,\n+                                                'parts': gpus})(x)\n+                    inputs.append(slice_i)\n+\n+                # Apply model on slice\n+                # (creating a model replica on the target device).\n+                outputs = model(inputs)\n+                if not isinstance(outputs, list):\n+                    outputs = [outputs]\n+\n+                # Save the outputs for merging back together later.\n+                for o in range(len(outputs)):\n+                    all_outputs[o].append(outputs[o])\n+\n+    # Merge outputs on CPU.\n+    with tf.device('/cpu:0'):\n+        merged = []\n+        for outputs in all_outputs:\n+            merged.append(concatenate(outputs,\n+                                      axis=0))\n+        return Model(model.inputs, merged)\n\n@@ -0,0 +1,215 @@\n+\"\"\"These tests are not meant to be run on CI.\n+\"\"\"\n+from __future__ import print_function\n+\n+import keras\n+from keras import backend as K\n+from keras.utils import multi_gpu_model\n+\n+import numpy as np\n+import pytest\n+import time\n+import tensorflow as tf\n+from keras.preprocessing.image import ImageDataGenerator\n+\n+\n+def multi_gpu_test_simple_model():\n+    print('####### test simple model')\n+    num_samples = 1000\n+    input_dim = 10\n+    output_dim = 1\n+    hidden_dim = 10\n+    gpus = 8\n+    epochs = 2\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.Dense(hidden_dim,\n+                                 input_shape=(input_dim,)))\n+    model.add(keras.layers.Dense(output_dim))\n+\n+    x = np.random.random((num_samples, input_dim))\n+    y = np.random.random((num_samples, output_dim))\n+    parallel_model = multi_gpu_model(model, gpus=gpus)\n+\n+    parallel_model.compile(loss='mse', optimizer='rmsprop')\n+    parallel_model.fit(x, y, epochs=epochs)\n+\n+\n+def multi_gpu_test_multi_io_model():\n+    print('####### test multi-io model')\n+    num_samples = 1000\n+    input_dim_a = 10\n+    input_dim_b = 5\n+    output_dim_a = 1\n+    output_dim_b = 2\n+    hidden_dim = 10\n+    gpus = 8\n+    epochs = 2\n+\n+    input_a = keras.Input((input_dim_a,))\n+    input_b = keras.Input((input_dim_b,))\n+    a = keras.layers.Dense(hidden_dim)(input_a)\n+    b = keras.layers.Dense(hidden_dim)(input_b)\n+    c = keras.layers.concatenate([a, b])\n+    output_a = keras.layers.Dense(output_dim_a)(c)\n+    output_b = keras.layers.Dense(output_dim_b)(c)\n+    model = keras.models.Model([input_a, input_b], [output_a, output_b])\n+\n+    a_x = np.random.random((num_samples, input_dim_a))\n+    b_x = np.random.random((num_samples, input_dim_b))\n+    a_y = np.random.random((num_samples, output_dim_a))\n+    b_y = np.random.random((num_samples, output_dim_b))\n+\n+    parallel_model = multi_gpu_model(model, gpus=gpus)\n+    parallel_model.compile(loss='mse', optimizer='rmsprop')\n+    parallel_model.fit([a_x, b_x], [a_y, b_y], epochs=epochs)\n+\n+\n+def multi_gpu_test_invalid_devices():\n+    input_shape = (1000, 10)\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.Dense(10,\n+                                 activation='relu',\n+                                 input_shape=input_shape[1:]))\n+    model.add(keras.layers.Dense(1, activation='sigmoid'))\n+    model.compile(loss='mse', optimizer='rmsprop')\n+\n+    x = np.random.random(input_shape)\n+    y = np.random.random((input_shape[0], 1))\n+    with pytest.raises(ValueError):\n+        parallel_model = multi_gpu_model(model, gpus=10)\n+        parallel_model.fit(x, y, epochs=2)\n+\n+\n+def multi_gpu_application_np_array_benchmark():\n+    print('####### Xception benchmark - np i/o')\n+    model_cls = keras.applications.Xception\n+\n+    num_samples = 1000\n+    height = 224\n+    width = 224\n+    num_classes = 1000\n+    epochs = 4\n+    batch_size = 40\n+    x = np.random.random((num_samples, height, width, 3))\n+    y = np.random.random((num_samples, num_classes))\n+\n+    # Baseline\n+    model = model_cls(weights=None,\n+                      input_shape=(height, width, 3),\n+                      classes=num_classes)\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='rmsprop')\n+\n+    # Training\n+    start_time = time.time()\n+    model.fit(x, y, epochs=epochs)\n+    total_time = time.time() - start_time\n+    print('baseline training:', total_time)\n+\n+    # Inference\n+    start_time = time.time()\n+    model.predict(x)\n+    total_time = time.time() - start_time\n+    print('baseline inference:', total_time)\n+\n+    for i in range(8, 9):\n+        K.clear_session()\n+        with tf.device('/cpu:0'):\n+            model = model_cls(weights=None,\n+                              input_shape=(height, width, 3),\n+                              classes=num_classes)\n+        parallel_model = multi_gpu_model(model, gpus=i)\n+        parallel_model.compile(loss='categorical_crossentropy',\n+                               optimizer='rmsprop')\n+\n+        start_time = time.time()\n+        parallel_model.fit(x, y, epochs=epochs, batch_size=batch_size)\n+        total_time = time.time() - start_time\n+        print('%d gpus training:' % i, total_time)\n+\n+        # Inference\n+        start_time = time.time()\n+        parallel_model.predict(x, batch_size=batch_size)\n+        total_time = time.time() - start_time\n+        print('%d gpus inference:' % i, total_time)\n+\n+\n+def multi_gpu_application_folder_generator_benchmark():\n+    \"\"\"Before running this test:\n+\n+    wget https://s3.amazonaws.com/img-datasets/cats_and_dogs_small.zip\n+    unzip cats_and_dogs_small.zip\n+    \"\"\"\n+    print('####### Xception benchmark - folder generator i/o')\n+    model_cls = keras.applications.Xception\n+\n+    height = 150\n+    width = 150\n+    num_classes = 2\n+    epochs = 3\n+    steps_per_epoch = 100\n+    batch_size = 64\n+\n+    # Baseline\n+    model = model_cls(weights=None,\n+                      input_shape=(height, width, 3),\n+                      classes=num_classes)\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='rmsprop')\n+\n+    datagen = ImageDataGenerator(\n+        rotation_range=40,\n+        width_shift_range=0.2,\n+        height_shift_range=0.2,\n+        shear_range=0.2,\n+        zoom_range=0.2,\n+        horizontal_flip=True,\n+        fill_mode='nearest')\n+    train_dir = '/home/ubuntu/cats_and_dogs_small/train'  # Change this\n+    train_gen = datagen.flow_from_directory(\n+        train_dir,\n+        target_size=(height, width),\n+        batch_size=batch_size,\n+        class_mode='categorical')\n+\n+    # Training\n+    start_time = time.time()\n+    model.fit_generator(train_gen,\n+                        steps_per_epoch=steps_per_epoch,\n+                        epochs=epochs,\n+                        workers=4)\n+    total_time = time.time() - start_time\n+    print('baseline training:', total_time)\n+\n+    for i in range(2, 9):\n+        K.clear_session()\n+        with tf.device('/cpu:0'):\n+            model = model_cls(weights=None,\n+                              input_shape=(height, width, 3),\n+                              classes=num_classes)\n+        parallel_model = multi_gpu_model(model, gpus=i)\n+        parallel_model.compile(loss='categorical_crossentropy',\n+                               optimizer='rmsprop')\n+\n+        train_gen = datagen.flow_from_directory(\n+            train_dir,\n+            target_size=(height, width),\n+            batch_size=batch_size,\n+            class_mode='categorical')\n+\n+        start_time = time.time()\n+        parallel_model.fit_generator(\n+            train_gen,\n+            steps_per_epoch=steps_per_epoch,\n+            epochs=epochs,\n+            workers=4 * i)\n+        total_time = time.time() - start_time\n+        print('%d gpus training:' % i, total_time)\n+\n+\n+if __name__ == '__main__':\n+    multi_gpu_test_simple_model()\n+    multi_gpu_test_multi_io_model()\n+    multi_gpu_test_invalid_devices()\n+    multi_gpu_application_np_array_benchmark()\n+    multi_gpu_application_folder_generator_benchmark()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c3b512fa98f0c560ec59a73858057bc4decab75e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 16 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,7 @@ import numpy as np\n \n import random\n from keras.datasets import mnist\n-from keras.models import Sequential, Model\n+from keras.models import Model\n from keras.layers import Dense, Dropout, Input, Lambda\n from keras.optimizers import RMSprop\n from keras import backend as K\n@@ -65,13 +65,13 @@ def create_pairs(x, digit_indices):\n def create_base_network(input_dim):\n     '''Base network to be shared (eq. to feature extraction).\n     '''\n-    seq = Sequential()\n-    seq.add(Dense(128, input_shape=(input_dim,), activation='relu'))\n-    seq.add(Dropout(0.1))\n-    seq.add(Dense(128, activation='relu'))\n-    seq.add(Dropout(0.1))\n-    seq.add(Dense(128, activation='relu'))\n-    return seq\n+    input = Input(shape=(input_dim,))\n+    x = Dense(128, activation='relu')(input)\n+    x = Dropout(0.1)(x)\n+    x = Dense(128, activation='relu')(x)\n+    x = Dropout(0.1)(x)\n+    x = Dense(128, activation='relu')(x)\n+    return Model(input, x)\n \n \n def compute_accuracy(y_true, y_pred):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#429332ba902659175ce896e98a5e531bd4071e59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1248 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -282,7 +282,8 @@ PAGES = [\n         'all_module_functions': [utils],\n         'classes': [utils.CustomObjectScope,\n                     utils.HDF5Matrix,\n-                    utils.Sequence]\n+                    utils.Sequence,\n+                    utils.multi_gpu_model]\n     },\n ]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c400d0e798275ac79f84fd2f974a8c01cd7deb15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 1262 | Contributors (this commit): 17 | Commits (past 90d): 6 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -282,8 +282,7 @@ PAGES = [\n         'all_module_functions': [utils],\n         'classes': [utils.CustomObjectScope,\n                     utils.HDF5Matrix,\n-                    utils.Sequence,\n-                    utils.multi_gpu_model]\n+                    utils.Sequence],\n     },\n ]\n \n@@ -333,6 +332,7 @@ def get_function_signature(function, method=True):\n     else:\n         kwargs = []\n     st = '%s.%s(' % (function.__module__, function.__name__)\n+\n     for a in args:\n         st += str(a) + ', '\n     for a, v in kwargs:\n@@ -340,9 +340,10 @@ def get_function_signature(function, method=True):\n             v = '\\'' + v + '\\''\n         st += str(a) + '=' + str(v) + ', '\n     if kwargs or args:\n-        return st[:-2] + ')'\n+        signature = st[:-2] + ')'\n     else:\n-        return st + ')'\n+        signature = st + ')'\n+    return signature\n \n \n def get_class_signature(cls):\n@@ -393,7 +394,6 @@ def process_class_docstring(docstring):\n     docstring = re.sub(r'\\n    # (.*)\\n',\n                        r'\\n    __\\1__\\n\\n',\n                        docstring)\n-\n     docstring = re.sub(r'    ([^\\s\\\\\\(]+):(.*)\\n',\n                        r'    - __\\1__:\\2\\n',\n                        docstring)\n@@ -408,10 +408,6 @@ def process_function_docstring(docstring):\n     docstring = re.sub(r'\\n    # (.*)\\n',\n                        r'\\n    __\\1__\\n\\n',\n                        docstring)\n-    docstring = re.sub(r'\\n        # (.*)\\n',\n-                       r'\\n        __\\1__\\n\\n',\n-                       docstring)\n-\n     docstring = re.sub(r'    ([^\\s\\\\\\(]+):(.*)\\n',\n                        r'    - __\\1__:\\2\\n',\n                        docstring)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8167253fa22a5a3fc2604193f2e2e1395c6010bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 894 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ the 100 % MobileNet on various input sizes:\n \n The weights for all 16 models are obtained and translated\n from TensorFlow checkpoints found at\n-https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md\n+https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\n \n # Reference\n - [MobileNets: Efficient Convolutional Neural Networks for\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5fa4857530bcf5c0af7140109117449b54c000dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 500 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,6 @@ Results\n '''\n \n from __future__ import print_function\n-from six.moves import xrange\n import numpy as np\n import keras\n from keras.models import Sequential\n@@ -198,7 +197,7 @@ def deeper2net_conv2d(teacher_w):\n     '''\n     filters, num_channel, kh, kw = teacher_w.shape\n     student_w = np.zeros((filters, filters, kh, kw))\n-    for i in xrange(filters):\n+    for i in range(filters):\n         student_w[i, i, (kh - 1) / 2, (kw - 1) / 2] = 1.\n     student_b = np.zeros(filters)\n     return student_w, student_b\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef4f0adffcc1f3ed9307e237411d53beae0d567f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 6258 | Contributors (this commit): 54 | Commits (past 90d): 34 | Contributors (cumulative): 66 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,7 @@ if keras.backend.image_data_format() == 'channels_first':\n     input_shape = (1, 28, 28)  # image shape\n else:\n     input_shape = (28, 28, 1)  # image shape\n-num_class = 10  # number of class\n+num_classes = 10  # number of classes\n \n \n # load and pre-process data\n@@ -224,7 +224,7 @@ def make_teacher_model(train_data, validation_data, epochs=3):\n     model.add(MaxPooling2D(2, name='pool2'))\n     model.add(Flatten(name='flatten'))\n     model.add(Dense(64, activation='relu', name='fc1'))\n-    model.add(Dense(num_class, activation='softmax', name='fc2'))\n+    model.add(Dense(num_classes, activation='softmax', name='fc2'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer=SGD(lr=0.01, momentum=0.9),\n                   metrics=['accuracy'])\n@@ -254,7 +254,7 @@ def make_wider_student_model(teacher_model, train_data,\n     model.add(Flatten(name='flatten'))\n     # a wider fc1 compared to teacher model\n     model.add(Dense(new_fc1_width, activation='relu', name='fc1'))\n-    model.add(Dense(num_class, activation='softmax', name='fc2'))\n+    model.add(Dense(num_classes, activation='softmax', name='fc2'))\n \n     # The weights for other layers need to be copied from teacher_model\n     # to student_model, except for widened layers\n@@ -318,7 +318,7 @@ def make_deeper_student_model(teacher_model, train_data,\n         model.add(Dense(64, activation='relu', name='fc1-deeper'))\n     else:\n         raise ValueError('Unsupported weight initializer: %s' % init)\n-    model.add(Dense(num_class, activation='softmax', name='fc2'))\n+    model.add(Dense(num_classes, activation='softmax', name='fc2'))\n \n     # copy weights for other layers\n     copy_weights(teacher_model, model, layer_names=[\n\n@@ -566,8 +566,8 @@ def gather(reference, indices):\n     if _get_cntk_version() >= 2.2:\n         return C.ops.gather(reference, indices)\n     else:\n-        num_class = reference.shape[0]\n-        one_hot_matrix = C.ops.one_hot(indices, num_class)\n+        num_classes = reference.shape[0]\n+        one_hot_matrix = C.ops.one_hot(indices, num_classes)\n         return C.times(one_hot_matrix, reference, output_rank=len(reference.shape) - 1)\n \n \n\n@@ -1041,7 +1041,7 @@ class DirectoryIterator(Iterator):\n             for subdir in sorted(os.listdir(directory)):\n                 if os.path.isdir(os.path.join(directory, subdir)):\n                     classes.append(subdir)\n-        self.num_class = len(classes)\n+        self.num_classes = len(classes)\n         self.class_indices = dict(zip(classes, range(len(classes))))\n \n         pool = multiprocessing.pool.ThreadPool()\n@@ -1052,7 +1052,7 @@ class DirectoryIterator(Iterator):\n                                     (os.path.join(directory, subdir)\n                                      for subdir in classes)))\n \n-        print('Found %d images belonging to %d classes.' % (self.samples, self.num_class))\n+        print('Found %d images belonging to %d classes.' % (self.samples, self.num_classes))\n \n         # second, build an index of the images in the different class subfolders\n         results = []\n@@ -1103,7 +1103,7 @@ class DirectoryIterator(Iterator):\n         elif self.class_mode == 'binary':\n             batch_y = self.classes[index_array].astype(K.floatx())\n         elif self.class_mode == 'categorical':\n-            batch_y = np.zeros((len(batch_x), self.num_class), dtype=K.floatx())\n+            batch_y = np.zeros((len(batch_x), self.num_classes), dtype=K.floatx())\n             for i, label in enumerate(self.classes[index_array]):\n                 batch_y[i, label] = 1.\n         else:\n\n@@ -17,7 +17,7 @@ def get_test_data(num_train=1000, num_test=500, input_shape=(10,),\n \n     classification=True overrides output_shape\n     (i.e. output_shape is set to (1,)) and the output\n-    consists in integers in [0, num_class-1].\n+    consists in integers in [0, num_classes-1].\n \n     Otherwise: float output with shape output_shape.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
