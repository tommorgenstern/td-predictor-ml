{"custom_id": "keras#0e4bf48c5b4d2cde23e12db0cc73d950374f5241", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 875 | Lines Deleted: 611 | Files Changed: 6 | Hunks: 13 | Methods Changed: 85 | Complexity Δ (Sum/Max): 35/117 | Churn Δ: 1486 | Churn Cumulative: 3188 | Contributors (this commit): 10 | Commits (past 90d): 73 | Contributors (cumulative): 21 | DMM Complexity: 0.9367816091954023\n\nDIFF:\n@@ -39,8 +39,9 @@ def test_model_fit():\n     batch_size = 32\n     epochs = 5\n \n+    with strategy.scope():\n         model.compile(\n-        optimizer=optimizers.SGD(learning_rate=0.001),\n+            optimizer=optimizers.SGD(learning_rate=0.001, momentum=0.01),\n             loss=losses.MeanSquaredError(),\n             metrics=[metrics.MeanSquaredError()],\n             # TODO(scottzhu): Find out where is the variable that is not created eagerly\n\n@@ -31,7 +31,8 @@ class Variable(KerasVariable, tf.__internal__.types.Tensor):\n \n     def _initialize(self, value):\n         self._value = tf.Variable(\n-            value, dtype=self._dtype, trainable=self.trainable\n+            value, dtype=self._dtype, trainable=self.trainable,\n+            name=self.name\n         )\n \n     def assign(self, value):\n\n@@ -0,0 +1,120 @@\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core.optimizers import base_optimizer\n+\n+\n+class TFOptimizer(base_optimizer.Optimizer):\n+    \"\"\"A class for Tensorflow specific optimizer logic.\n+\n+    The major behavior change for this class is for tf.distribute.\n+\n+    It will override methods from base Keras core Optimizer,\n+    which provide distribute specific functionality, e.g. variable\n+    creation, loss reduction, etc.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self._distribution_strategy = tf.distribute.get_strategy()\n+\n+    def add_variable_from_reference(self, reference_variable, name=None):\n+        if isinstance(reference_variable, backend.Variable):\n+            colocate_var = reference_variable.value\n+        else:\n+            colocate_var = reference_variable\n+\n+        with self._distribution_strategy.extended.colocate_vars_with(\n+                colocate_var):\n+            return super().add_variable_from_reference(\n+                reference_variable, name=name)\n+\n+    def _var_key(self, variable):\n+        if isinstance(variable, backend.Variable):\n+            variable = variable.value   # Convert to tf.Variable\n+        if hasattr(variable, \"_distributed_container\"):\n+            variable = variable._distributed_container()\n+        elif (\n+            isinstance(variable, tf.__internal__.CompositeTensor)\n+            and hasattr(variable, \"handle\")\n+            and hasattr(variable.handle, \"_distributed_container\")\n+        ):\n+            # For ResourceVariables, the _distributed_container attribute\n+            # is added to their handle tensors.\n+            variable = variable.handle._distributed_container()\n+        return variable._unique_id\n+\n+    def _apply_weight_decay(self, variables):\n+        if self.weight_decay is None:\n+            return\n+\n+        def distributed_apply_weight_decay(distribution, variables, **kwargs):\n+            def weight_decay_fn(variable):\n+                if self._use_weight_decay(variable):\n+                    lr = tf.cast(self.learning_rate, variable.dtype)\n+                    wd = tf.cast(self.weight_decay, variable.dtype)\n+                    variable.assign(variable - variable * wd * lr)\n+\n+            for variable in variables:\n+                distribution.extended.update(\n+                    variable, weight_decay_fn, group=False\n+                )\n+\n+        tf.__internal__.distribute.interim.maybe_merge_call(\n+            distributed_apply_weight_decay,\n+            self._distribution_strategy,\n+            variables,\n+        )\n+\n+    def _internal_apply_gradients(self, grads_and_vars):\n+        tf.__internal__.distribute.interim.maybe_merge_call(\n+            self._distributed_apply_gradients_fn,\n+            self._distribution_strategy,\n+            grads_and_vars,\n+        )\n+\n+    def _distributed_apply_gradients_fn(\n+            self, distribution, grads_and_vars, **kwargs):\n+        \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n+\n+        def apply_grad_to_update_var(var, grad):\n+            learning_rate = self._get_current_learning_rate()\n+            return self.update_step(grad, var, learning_rate)\n+\n+        for grad, var in grads_and_vars:\n+            distribution.extended.update(\n+                var, apply_grad_to_update_var, args=(grad,), group=False\n+            )\n+\n+        if self.use_ema:\n+            _, var_list = zip(*grads_and_vars)\n+            self._update_model_variables_moving_average(var_list)\n+            if self.ema_overwrite_frequency:\n+                # Only when self.ema_overwrite_frequency is not None, we\n+                # overwrite the model variables.\n+                should_overwrite_model_vars = (\n+                    self.iterations + 1\n+                ) % self.ema_overwrite_frequency == 0\n+                tf.cond(\n+                    tf.cast(should_overwrite_model_vars, tf.bool),\n+                    true_fn=lambda: self._overwrite_model_variables_with_average_value(  # noqa: E501\n+                        var_list\n+                    ),\n+                    false_fn=lambda: None,\n+                )\n+        self.iterations.assign(self.iterations + 1)\n+\n+    def _overwrite_model_variables_with_average_value(self, var_list):\n+        \"\"\"Overwrite model variables with their moving average values.\n+\n+        This function overwrites variables on each device.\n+        Args:\n+          var_list: list of model variables.\n+        \"\"\"\n+        strategy = self._distribution_strategy\n+        # Override model variable by the stored average value on all devices.\n+        for var, average_var in zip(\n+            var_list, self._model_variables_moving_average\n+        ):\n+            strategy.extended.update(\n+                var, lambda a, b: a.assign(b), args=(average_var,))\n\n@@ -0,0 +1,118 @@\n+# flake8: noqa\n+\n+import numpy as np\n+import pytest\n+import tensorflow as tf\n+from tensorflow.python.eager import context\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.sgd import SGD\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() != 'tensorflow',\n+    reason='The distribute test can only run with TF backend.',\n+)\n+class OptimizerDistributeTest(testing.TestCase):\n+\n+    def setUp(self):\n+        super().setUp()\n+        # Need at least 2 devices for distribution related tests.\n+        cpus = tf.config.list_physical_devices(\"CPU\")\n+        context._reset_context()\n+        tf.config.set_logical_device_configuration(\n+            cpus[0],\n+            [\n+                tf.config.LogicalDeviceConfiguration(),\n+                tf.config.LogicalDeviceConfiguration(),\n+            ],\n+        )\n+        self.strategy = tf.distribute.MirroredStrategy(['CPU:0', 'CPU:1'])\n+\n+    def test_config(self):\n+        with self.strategy.scope():\n+            optimizer = SGD(\n+                learning_rate=0.5,\n+                momentum=0.06,\n+                nesterov=True,\n+                weight_decay=0.004,\n+            )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        with self.strategy.scope():\n+            optimizer = SGD(learning_rate=0.5, momentum=0.06,)\n+            grads = tf.constant([1.0, 6.0, 7.0, 2.0])\n+            vars = backend.Variable([1.0, 2.0, 3.0, 4.0]).value\n+                \n+            self.strategy.run(lambda: optimizer.apply_gradients(zip([grads], [vars])))\n+            self.assertAllClose(vars, [0.5, -1.0, -0.5, 3.0], rtol=1e-4, atol=1e-4)\n+\n+    def test_weight_decay(self):\n+        with self.strategy.scope():\n+            grads, var1, var2, var3 = (\n+                tf.zeros(()),\n+                backend.Variable(2.0).value,\n+                backend.Variable(3.0, name=\"exclude\").value,\n+                backend.Variable(4.0).value,\n+            )\n+            optimizer_1 = SGD(learning_rate=1.0, weight_decay=0.004)\n+            self.strategy.run(lambda: optimizer_1.apply_gradients(zip([grads], [var1])))\n+\n+            optimizer_2 = SGD(learning_rate=1.0, weight_decay=0.004)\n+\n+            def opt2_run():\n+                optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+                optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+            self.strategy.run(opt2_run)\n+\n+            optimizer_3 = SGD(learning_rate=1.0, weight_decay=0.004)\n+\n+            def opt3_run():\n+                optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+                optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+            self.strategy.run(opt3_run)\n+\n+        self.assertAlmostEqual(var1.numpy(), 1.9760959)\n+        self.assertAlmostEqual(var2.numpy(), 3.0)\n+        self.assertAlmostEqual(var3.numpy(), 4.0)\n+\n+    def test_correctness_with_golden(self):\n+        with self.strategy.scope():\n+            optimizer = SGD(nesterov=True)\n+\n+            x = backend.Variable(np.ones([10])).value\n+            grads = np.arange(0.1, 1.1, 0.1)\n+            first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n+            0.9999, 0.9999], [0.9989, 0.9979, 0.9969, 0.9959, 0.9949, 0.9939,\n+            0.9929, 0.9919, 0.9909, 0.9899], [0.9979, 0.9959, 0.9939, 0.9919,\n+            0.9899, 0.9879, 0.9859, 0.9839, 0.9819, 0.9799], [0.9969, 0.9939,\n+            0.9909, 0.9879, 0.9849, 0.9819, 0.9789, 0.9759, 0.9729, 0.9699],\n+            [0.9959, 0.9919, 0.9879, 0.9839, 0.9799, 0.9759, 0.9719, 0.9679,\n+            0.9639, 0.9599]]\n+        )\n+        # fmt: on\n+\n+        self.strategy.run(lambda: optimizer.apply_gradients(zip([first_grads], [x])))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            self.strategy.run(lambda: optimizer.apply_gradients(zip([grads], [x])))\n+\n+    def test_clip_norm(self):\n+        with self.strategy.scope():\n+            optimizer = SGD(clipnorm=1)\n+            grad = [np.array([100.0, 100.0])]\n+            clipped_grad = optimizer._clip_gradients(grad)\n+            self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        with self.strategy.scope():\n+            optimizer = SGD(clipvalue=1)\n+            grad = [np.array([100.0, 100.0])]\n+            clipped_grad = optimizer._clip_gradients(grad)\n+            self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -0,0 +1,625 @@\n+import re\n+import warnings\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core.optimizers.schedules import learning_rate_schedule\n+from keras_core.saving import serialization_lib\n+from keras_core.utils.naming import auto_name\n+from keras_core.utils.tracking import Tracker\n+\n+\n+class Optimizer:\n+    def __init__(\n+        self,\n+        learning_rate,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=None,\n+    ):\n+        self.name = name\n+        self.weight_decay = weight_decay\n+        self.clipnorm = clipnorm\n+        self.global_clipnorm = global_clipnorm\n+        self.clipvalue = clipvalue\n+        self.use_ema = use_ema\n+\n+        if use_ema:\n+            # Verify the arguments related to EMA.\n+            if ema_momentum > 1 or ema_momentum < 0:\n+                raise ValueError(\n+                    \"`ema_momentum` must be in the range [0, 1]. \"\n+                    f\"Received: ema_momentum={ema_momentum}\"\n+                )\n+            if ema_overwrite_frequency and (\n+                not isinstance(ema_overwrite_frequency, int)\n+                or ema_overwrite_frequency < 1\n+            ):\n+                raise ValueError(\n+                    \"`ema_overwrite_frequency` must be an integer >= 1 or \"\n+                    \"None. Received: ema_overwrite_frequency=\"\n+                    f\"{ema_overwrite_frequency}\"\n+                )\n+        self.ema_momentum = ema_momentum\n+        self.ema_overwrite_frequency = ema_overwrite_frequency\n+\n+        if self.clipnorm is not None and self.global_clipnorm is not None:\n+            raise ValueError(\n+                \"Only one of `clipnorm` and `global_clipnorm` can \"\n+                f\"be set. Received: clipnorm={self.clipnorm}, \"\n+                f\"global_clipnorm={self.global_clipnorm}\"\n+            )\n+\n+        self.built = False\n+        # Note: dtype=\"int\" will resolve to int32 in JAX\n+        # (since int64 is disallowed in JAX) and to int64 in TF.\n+        self.iterations = backend.Variable(\n+            0, name=\"iteration\", dtype=\"int\", trainable=False\n+        )\n+        if isinstance(\n+            learning_rate, learning_rate_schedule.LearningRateSchedule\n+        ):\n+            self._learning_rate = learning_rate\n+        elif callable(learning_rate):\n+            self._learning_rate = learning_rate\n+        else:\n+            if not isinstance(learning_rate, float):\n+                raise ValueError(\n+                    \"Argument `learning_rate` should be float, or an instance \"\n+                    \"of LearningRateSchedule, or a callable \"\n+                    \"(that takes in the current iteration value \"\n+                    \"and returns the corresponding learning rate value). \"\n+                    f\"Received instead: learning_rate={learning_rate}\"\n+                )\n+            self._learning_rate = backend.Variable(\n+                learning_rate,\n+                name=\"learning_rate\",\n+                dtype=backend.floatx(),\n+                trainable=False,\n+            )\n+        self._variables = []\n+        self._trainable_variables = []\n+        self._tracker = Tracker(\n+            {\n+                \"variables\": (\n+                    lambda x: isinstance(x, backend.Variable),\n+                    self._variables,\n+                ),\n+            }\n+        )\n+        self._trainable_variables_indices = {}\n+\n+    def build(self, variables):\n+        for i, variable in enumerate(variables):\n+            self._trainable_variables_indices[self._var_key(variable)] = i\n+        self._trainable_variables = variables[:]\n+        self.built = True\n+\n+    def _var_key(self, variable):\n+        # Helper function to get a stable ID and the variable instance mapping.\n+        return id(variable)\n+\n+    @property\n+    def variables(self):\n+        return self._variables[:]\n+\n+    def _get_variable_index(self, variable):\n+        return self._trainable_variables_indices[self._var_key(variable)]\n+\n+    def add_variable(\n+        self,\n+        shape,\n+        initializer,\n+        dtype=None,\n+        name=None,\n+    ):\n+        self._check_super_called()\n+        initializer = initializers.get(initializer)\n+        variable = backend.Variable(\n+            initializer=initializer,\n+            shape=shape,\n+            dtype=dtype,\n+            trainable=False,\n+            name=name,\n+        )\n+        self._variables.append(variable)\n+        # Prevent double-tracking\n+        self._tracker.stored_ids[\"variables\"].add(self._var_key(variable))\n+        return variable\n+\n+    def add_variable_from_reference(self, reference_variable, name=None):\n+        \"\"\"Add an all-zeros variable with the shape and dtype of a reference\n+        variable.\n+        \"\"\"\n+        initializer = initializers.Zeros()\n+        name = name or auto_name(self.__class__.__name__)\n+        return self.add_variable(\n+            shape=reference_variable.shape,\n+            initializer=initializer,\n+            dtype=reference_variable.dtype,\n+            name=name,\n+        )\n+\n+    def _check_variables_are_known(self, variables):\n+        for v in variables:\n+            if self._var_key(v) not in self._trainable_variables_indices:\n+                raise ValueError(\n+                    f\"Unknown variable: {v}. This optimizer can only \"\n+                    \"be called for the variables it was originally built with. \"\n+                    \"When working with a new set of variables, you should \"\n+                    \"recreate a new optimizer instance.\"\n+                )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        raise NotImplementedError\n+\n+    def apply_gradients(self, grads_and_vars):\n+        grads, trainable_variables = zip(*grads_and_vars)\n+        return self.apply(grads, trainable_variables)\n+\n+    def apply(self, grads, variables=None):\n+        \"\"\"\n+        `grads` should be a list of gradient tensors\n+        with 1:1 mapping to the list of variables the optimizer was built with.\n+\n+        `variables` can be provided on the first call to build the optimizer.\n+        \"\"\"\n+        grads = list(grads)\n+        if len(grads) == 0:\n+            # It is possible that the grad is empty. In this case,\n+            # `apply_gradients` is a no-op.\n+            return\n+\n+        if variables is None:\n+            if not self.built:\n+                raise ValueError(\n+                    \"When passing `grads` without `variables`, the optimizer \"\n+                    \"must already be built on a list of variables. \"\n+                    \"Call `optimizer.build(trainable_variables)` first. \"\n+                )\n+            if len(grads) != len(self._trainable_variables_indices):\n+                raise ValueError(\n+                    \"When passing `grads` as a list of gradient tensors, the \"\n+                    f\"gradients must match `optimizer.variables` one-to-on. \"\n+                    f\"Received a list of {len(grads)} gradients, but the \"\n+                    f\"optimizer is tracking {len(self._trainable_variables)} \"\n+                    \"trainable variables.\"\n+                )\n+            trainable_variables = self._trainable_variables\n+        else:\n+            trainable_variables = list(variables)\n+            # Optionally build optimizer.\n+            if not self.built:\n+                with ops.name_scope(self.name):\n+                    self.build(trainable_variables)\n+                self.built = True\n+            self._check_variables_are_known(trainable_variables)\n+\n+        grads_and_vars = list(zip(grads, self._trainable_variables))\n+\n+        with ops.name_scope(self.name):\n+            # Filter empty gradients.\n+            grads_and_vars = self._filter_empty_gradients(grads_and_vars)\n+            if len(list(grads_and_vars)) == 0:\n+                return\n+\n+            # Apply clipping and weight decay.\n+            grads, trainable_variables = zip(*grads_and_vars)\n+            grads = self._clip_gradients(grads)\n+            self._apply_weight_decay(trainable_variables)\n+\n+            # Apply gradient updates.\n+            self._internal_apply_gradients(\n+                list(zip(grads, trainable_variables)))\n+\n+            # Apply variable constraints after applying gradients.\n+            for variable in trainable_variables:\n+                if getattr(variable, \"constraint\", None) is not None:\n+                    variable.assign(variable.constraint(variable))\n+            return self.iterations\n+\n+    def _internal_apply_gradients(self, grads_and_vars):\n+        learning_rate = self._get_current_learning_rate()\n+        for grad, var in grads_and_vars:\n+            self.update_step(grad, var, learning_rate)\n+        self.iterations.assign(self.iterations + 1)\n+\n+    def stateless_apply(self, grads, trainable_variables, optimizer_variables):\n+        self._check_super_called()\n+\n+        if not self.built:\n+            raise ValueError(\n+                \"To call stateless_apply_gradients, {self.__class__.__name__} \"\n+                \"must be built (i.e. its variables must have been created). \"\n+                \"You can build it via `optimizer.build(trainable_variables)`.\"\n+            )\n+        if len(optimizer_variables) != len(self.variables):\n+            raise ValueError(\n+                \"Argument `optimizer_variables` must be a list of tensors \"\n+                f\"corresponding 1:1 to {self.__class__.__name__}().variables. \"\n+                f\"Received list with length {len(optimizer_variables)}, but \"\n+                f\"expected {len(self.variables)} variables.\"\n+            )\n+        if len(trainable_variables) != len(self._trainable_variables):\n+            raise ValueError(\n+                \"Argument `optimizer_variables` must be a list of tensors \"\n+                \"corresponding 1:1 to the trainable variables list that \"\n+                \"the optimizer was built with. Received \"\n+                f\"len(trainable_variables) == {len(trainable_variables)} \"\n+                \"whereas the optimizer was built with \"\n+                f\"{len(self._trainable_variables)} variables.\"\n+            )\n+\n+        # Gather variable mapping\n+        mapping = list(\n+            zip(self._trainable_variables, trainable_variables)\n+        ) + list(zip(self.variables, optimizer_variables))\n+\n+        # Call in stateless scope\n+        with backend.StatelessScope(state_mapping=mapping) as scope:\n+            self.apply(grads)\n+\n+        # Gather updated variables\n+        trainable_variables = []\n+        for v in self._trainable_variables:\n+            new_v = scope.get_current_value(v)\n+            if new_v is not None:\n+                trainable_variables.append(new_v)\n+            else:\n+                trainable_variables.append(v)\n+        optimizer_variables = []\n+        for v in self.variables:\n+            new_v = scope.get_current_value(v)\n+            if new_v is not None:\n+                optimizer_variables.append(new_v)\n+            else:\n+                optimizer_variables.append(v)\n+        return trainable_variables, optimizer_variables\n+\n+    @property\n+    def learning_rate(self):\n+        return self._get_current_learning_rate()\n+\n+    @learning_rate.setter\n+    def learning_rate(self, learning_rate):\n+        if isinstance(\n+            learning_rate, learning_rate_schedule.LearningRateSchedule\n+        ):\n+            self._learning_rate = learning_rate\n+        elif callable(learning_rate):\n+            self._learning_rate = learning_rate\n+        else:\n+            if isinstance(\n+                self._learning_rate, learning_rate_schedule.LearningRateSchedule\n+            ):\n+                raise TypeError(\n+                    \"This optimizer was created with a `LearningRateSchedule`\"\n+                    \" object as its `learning_rate` constructor argument, \"\n+                    \"hence its learning rate is not settable. If you need the\"\n+                    \" learning rate to be settable, you should instantiate \"\n+                    \"the optimizer with a float `learning_rate` argument.\"\n+                )\n+            self._learning_rate.assign(learning_rate)\n+\n+    def save_own_variables(self, store):\n+        \"\"\"Get the state of this optimizer object.\"\"\"\n+        for i, variable in enumerate(self.variables):\n+            store[str(i)] = np.array(variable)\n+\n+    def load_own_variables(self, store):\n+        \"\"\"Set the state of this optimizer object.\"\"\"\n+        if len(store.keys()) != len(self.variables):\n+            msg = (\n+                f\"Skipping variable loading for optimizer '{self.name}', \"\n+                f\"because it has {len(self.variables)} variables whereas \"\n+                f\"the saved optimizer has {len(store.keys())} variables. \"\n+            )\n+            if len(self.variables) == 0:\n+                msg += (\n+                    \"This is likely because the optimizer has not been \"\n+                    \"called/built yet.\"\n+                )\n+            warnings.warn(msg, stacklevel=2)\n+            return\n+        for i, variable in enumerate(self.variables):\n+            variable.assign(store[str(i)])\n+\n+    def _get_current_learning_rate(self):\n+        if isinstance(\n+            self._learning_rate, learning_rate_schedule.LearningRateSchedule\n+        ):\n+            return self._learning_rate(self.iterations)\n+        elif callable(self._learning_rate):\n+            return self._learning_rate(self.iterations)\n+        return self._learning_rate\n+\n+    def _filter_empty_gradients(self, grads_and_vars):\n+        filtered = [(g, v) for g, v in grads_and_vars if g is not None]\n+        if not filtered:\n+            raise ValueError(\"No gradients provided for any variable.\")\n+        if len(filtered) < len(grads_and_vars):\n+            missing_grad_vars = [v for g, v in grads_and_vars if g is None]\n+            warnings.warn(\n+                \"Gradients do not exist for variables \"\n+                f\"{[v.name for v in missing_grad_vars]} when minimizing the \"\n+                \"loss. If you're using `model.compile()`, did you forget to \"\n+                \"provide a `loss` argument?\"\n+            )\n+        return filtered\n+\n+    def _clip_gradients(self, grads):\n+        if self.clipnorm and self.clipnorm > 0:\n+            clipped_grads = []\n+            for g in grads:\n+                if g is None:\n+                    clipped_grads.append(g)\n+                else:\n+                    clipped_grads.append(clip_by_norm(g, self.clipnorm))\n+            return clipped_grads\n+\n+        if self.global_clipnorm and self.global_clipnorm > 0:\n+            return clip_by_global_norm(grads, self.global_clipnorm)\n+\n+        if self.clipvalue and self.clipvalue > 0:\n+            clipped_grads = []\n+            for g in grads:\n+                if g is None:\n+                    clipped_grads.append(g)\n+                else:\n+                    clipped_grads.append(\n+                        ops.clip(g, -self.clipvalue, self.clipvalue)\n+                    )\n+            return clipped_grads\n+        return grads\n+\n+    def exclude_from_weight_decay(self, var_list=None, var_names=None):\n+        \"\"\"Exclude variables from weight decay.\n+\n+        This method must be called before the optimizer's `build` method is\n+        called. You can set specific variables to exclude out, or set a list of\n+        strings as the anchor words, if any of which appear in a variable's\n+        name, then the variable is excluded.\n+\n+        Args:\n+            var_list: A list of `tf.Variable`s to exclude from weight decay.\n+            var_names: A list of strings. If any string in `var_names` appear\n+                in the model variable's name, then this model variable is\n+                excluded from weight decay. For example, `var_names=['bias']`\n+                excludes all bias variables from weight decay.\n+        \"\"\"\n+        if hasattr(self, \"_built\") and self._built:\n+            raise ValueError(\n+                \"`exclude_from_weight_decay()` can only be configued before \"\n+                \"the optimizer is built.\"\n+            )\n+\n+        if var_list:\n+            self._exclude_from_weight_decay = [\n+                self._var_key(variable) for variable in var_list\n+            ]\n+        else:\n+            self._exclude_from_weight_decay = []\n+        self._exclude_from_weight_decay_names = var_names or []\n+\n+    def _use_weight_decay(self, variable):\n+        exclude_from_weight_decay = getattr(\n+            self, \"_exclude_from_weight_decay\", []\n+        )\n+        exclude_from_weight_decay_names = getattr(\n+            self, \"_exclude_from_weight_decay_names\", []\n+        )\n+        variable_id = self._var_key(variable)\n+        for exclude_id in exclude_from_weight_decay:\n+            if variable_id == exclude_id:\n+                return False\n+        for name in exclude_from_weight_decay_names:\n+            if re.search(name, variable.name) is not None:\n+                return False\n+        return True\n+\n+    def _apply_weight_decay(self, variables):\n+        if self.weight_decay is None:\n+            return\n+        for variable in variables:\n+            if self._use_weight_decay(variable):\n+                lr = ops.cast(self._get_current_learning_rate(), variable.dtype)\n+                wd = ops.cast(self.weight_decay, variable.dtype)\n+                variable.assign(variable - variable * wd * lr)\n+\n+    def _check_super_called(self):\n+        if not hasattr(self, \"_tracker\"):\n+            raise RuntimeError(\n+                f\"In optimizer '{self.__class__.__name__}', you forgot to call \"\n+                \"`super().__init__()` in the `__init__()` method. \"\n+                \"Go add it!\"\n+            )\n+\n+    def _update_model_variables_moving_average(self, var_list):\n+        \"\"\"Update the stored moving average using the latest value.\"\"\"\n+        if self.use_ema:\n+            for var, average in zip(\n+                var_list, self._model_variables_moving_average\n+            ):\n+                average.assign(\n+                    self.ema_momentum * average + (1 - self.ema_momentum) * var\n+                )\n+\n+    def _overwrite_model_variables_with_average_value(self, var_list):\n+        \"\"\"Overwrite model variables with its moving average.\"\"\"\n+        if len(var_list) != len(self._model_variables_moving_average):\n+            raise ValueError(\n+                f\"The length of model variables ({len(var_list)}) to \"\n+                \"override does not match the length of model variables \"\n+                \"stored in the optimizer \"\n+                f\"({len(self._model_variables_moving_average)}). Please \"\n+                \"check if the optimizer was called on your model.\"\n+            )\n+        self._overwrite_model_variables_with_average_value_helper(var_list)\n+\n+    def _overwrite_model_variables_with_average_value_helper(self, var_list):\n+        \"\"\"Helper function that overwrites model variables.\"\"\"\n+        for var, average_var in zip(\n+            var_list, self._model_variables_moving_average\n+        ):\n+            var.assign(average_var)\n+\n+    def finalize_variable_values(self, var_list):\n+        \"\"\"Set the final value of model's trainable variables.\n+\n+        Sometimes there are some extra steps before ending the variable updates,\n+        such as overriding the model variables with its average value.\n+\n+        Args:\n+          var_list: list of model variables.\n+        \"\"\"\n+        if self.use_ema:\n+            # If the optimizer uses EMA, then when finalizing, we replace the\n+            # model variable value with its moving average stored inside\n+            # optimizer.\n+            self._overwrite_model_variables_with_average_value(var_list)\n+\n+    def get_config(self):\n+        \"\"\"Returns the config of the optimizer.\n+\n+        An optimizer config is a Python dictionary (serializable)\n+        containing the configuration of an optimizer.\n+        The same optimizer can be reinstantiated later\n+        (without any saved state) from this configuration.\n+\n+        Subclass optimizer should override this method to include other\n+        hyperparameters.\n+\n+        Returns:\n+            Python dictionary.\n+        \"\"\"\n+\n+        if isinstance(\n+            self._learning_rate, learning_rate_schedule.LearningRateSchedule\n+        ):\n+            learning_rate = learning_rate_schedule.serialize(\n+                self._learning_rate\n+            )\n+        elif isinstance(self._learning_rate, backend.Variable):\n+            learning_rate = float(self._learning_rate.numpy())\n+        elif ops.is_tensor(self._learning_rate):\n+            learning_rate = float(self._learning_rate)\n+        elif callable(self._learning_rate):\n+            learning_rate = serialization_lib.serialize_keras_object(\n+                self._learning_rate\n+            )\n+\n+        config = {\n+            \"name\": self.name,\n+            \"learning_rate\": learning_rate,\n+            \"weight_decay\": self.weight_decay,\n+            \"clipnorm\": self.clipnorm,\n+            \"global_clipnorm\": self.global_clipnorm,\n+            \"clipvalue\": self.clipvalue,\n+            \"use_ema\": self.use_ema,\n+            \"ema_momentum\": self.ema_momentum,\n+            \"ema_overwrite_frequency\": self.ema_overwrite_frequency,\n+        }\n+        return config\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        \"\"\"Creates an optimizer from its config.\n+\n+        This method is the reverse of `get_config`, capable of instantiating the\n+        same optimizer from the config dictionary.\n+\n+        Args:\n+            config: A Python dictionary, typically the output of get_config.\n+            custom_objects: A Python dictionary mapping names to additional\n+              user-defined Python objects needed to recreate this optimizer.\n+\n+        Returns:\n+            An optimizer instance.\n+        \"\"\"\n+        if \"learning_rate\" in config:\n+            if isinstance(config[\"learning_rate\"], dict):\n+                config[\n+                    \"learning_rate\"\n+                ] = serialization_lib.deserialize_keras_object(\n+                    config[\"learning_rate\"], custom_objects=custom_objects\n+                )\n+        return cls(**config)\n+\n+\n+base_optimizer_keyword_args = \"\"\"name: String. The name to use\n+          for momentum accumulator weights created by\n+          the optimizer.\n+      weight_decay: Float, defaults to None. If set, weight decay is applied.\n+      clipnorm: Float. If set, the gradient of each weight is individually\n+          clipped so that its norm is no higher than this value.\n+      clipvalue: Float. If set, the gradient of each weight is clipped to be no\n+          higher than this value.\n+      global_clipnorm: Float. If set, the gradient of all weights is clipped so\n+          that their global norm is no higher than this value.\n+      use_ema: Boolean, defaults to False. If True, exponential moving average\n+          (EMA) is applied. EMA consists of computing an exponential moving\n+          average of the weights of the model (as the weight values change after\n+          each training batch), and periodically overwriting the weights with\n+          their moving average.\n+      ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n+          This is the momentum to use when computing\n+          the EMA of the model's weights:\n+          `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n+          current_variable_value`.\n+      ema_overwrite_frequency: Int or None, defaults to None. Only used if\n+          `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n+          we overwrite the model variable by its moving average.\n+          If None, the optimizer\n+          does not overwrite model variables in the middle of training, and you\n+          need to explicitly overwrite the variables at the end of training\n+          by calling `optimizer.finalize_variable_values()`\n+          (which updates the model\n+          variables in-place). When using the built-in `fit()` training loop,\n+          this happens automatically after the last epoch,\n+          and you don't need to do anything.\"\"\"\n+\n+\n+def clip_by_norm(values, clip_norm, axes=None):\n+    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n+    l2sum = ops.sum(values * values, axes, keepdims=True)\n+    pred = l2sum > 0\n+    # Two-tap tf.where trick to bypass NaN gradients\n+    l2sum_safe = ops.where(pred, l2sum, ops.ones_like(l2sum))\n+    l2norm = ops.where(pred, ops.sqrt(l2sum_safe), l2sum)\n+    intermediate = values * clip_norm\n+    values_clip = intermediate / ops.maximum(l2norm, clip_norm)\n+    return values_clip\n+\n+\n+def global_norm(value_list):\n+    \"\"\"Computes the global norm of multiple tensors.\"\"\"\n+    squared_norms = []\n+    for v in value_list:\n+        if v is not None:\n+            squared_norms.append(ops.sum(ops.square(v)))\n+    squared_norm = ops.sum(ops.stack(squared_norms))\n+    return ops.sqrt(squared_norm)\n+\n+\n+def clip_by_global_norm(value_list, clip_norm):\n+    use_norm = global_norm(value_list)\n+    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n+    scale_for_finite = clip_norm * ops.minimum(1.0 / use_norm, 1.0 / clip_norm)\n+    # If use_norm is any finite number, this is a no-op. For inf/-inf/NaN,\n+    # this will make scale NaN.\n+    scale = scale_for_finite + (use_norm - use_norm)\n+    values_clipped = []\n+    for v in value_list:\n+        if v is None:\n+            values_clipped.append(None)\n+        else:\n+            values_clipped.append(v * scale)\n+    return values_clipped\n\n@@ -1,618 +1,17 @@\n-import re\n-import warnings\n-\n-import numpy as np\n-\n from keras_core import backend\n-from keras_core import initializers\n-from keras_core import operations as ops\n+from keras_core.optimizers import base_optimizer\n from keras_core.api_export import keras_core_export\n-from keras_core.optimizers.schedules import learning_rate_schedule\n-from keras_core.saving import serialization_lib\n-from keras_core.utils.naming import auto_name\n-from keras_core.utils.tracking import Tracker\n \n \n-@keras_core_export([\"keras_core.Optimizer\", \"keras_core.optimizers.Optimizer\"])\n-class Optimizer:\n-    def __init__(\n-        self,\n-        learning_rate,\n-        weight_decay=None,\n-        clipnorm=None,\n-        clipvalue=None,\n-        global_clipnorm=None,\n-        use_ema=False,\n-        ema_momentum=0.99,\n-        ema_overwrite_frequency=None,\n-        name=None,\n-    ):\n-        self.name = name\n-        self.weight_decay = weight_decay\n-        self.clipnorm = clipnorm\n-        self.global_clipnorm = global_clipnorm\n-        self.clipvalue = clipvalue\n-        self.use_ema = use_ema\n-\n-        if use_ema:\n-            # Verify the arguments related to EMA.\n-            if ema_momentum > 1 or ema_momentum < 0:\n-                raise ValueError(\n-                    \"`ema_momentum` must be in the range [0, 1]. \"\n-                    f\"Received: ema_momentum={ema_momentum}\"\n-                )\n-            if ema_overwrite_frequency and (\n-                not isinstance(ema_overwrite_frequency, int)\n-                or ema_overwrite_frequency < 1\n-            ):\n-                raise ValueError(\n-                    \"`ema_overwrite_frequency` must be an integer >= 1 or \"\n-                    \"None. Received: ema_overwrite_frequency=\"\n-                    f\"{ema_overwrite_frequency}\"\n-                )\n-        self.ema_momentum = ema_momentum\n-        self.ema_overwrite_frequency = ema_overwrite_frequency\n-\n-        if self.clipnorm is not None and self.global_clipnorm is not None:\n-            raise ValueError(\n-                \"Only one of `clipnorm` and `global_clipnorm` can \"\n-                f\"be set. Received: clipnorm={self.clipnorm}, \"\n-                f\"global_clipnorm={self.global_clipnorm}\"\n-            )\n-\n-        self.built = False\n-        # Note: dtype=\"int\" will resolve to int32 in JAX\n-        # (since int64 is disallowed in JAX) and to int64 in TF.\n-        self.iterations = backend.Variable(\n-            0, name=\"iteration\", dtype=\"int\", trainable=False\n-        )\n-        if isinstance(\n-            learning_rate, learning_rate_schedule.LearningRateSchedule\n-        ):\n-            self._learning_rate = learning_rate\n-        elif callable(learning_rate):\n-            self._learning_rate = learning_rate\n+if backend.backend() == 'tensorflow':\n+    from keras_core.backend.tensorflow import optimizer as tf_optimizer\n+    Optimizer = tf_optimizer.TFOptimizer\n else:\n-            if not isinstance(learning_rate, float):\n-                raise ValueError(\n-                    \"Argument `learning_rate` should be float, or an instance \"\n-                    \"of LearningRateSchedule, or a callable \"\n-                    \"(that takes in the current iteration value \"\n-                    \"and returns the corresponding learning rate value). \"\n-                    f\"Received instead: learning_rate={learning_rate}\"\n-                )\n-            self._learning_rate = backend.Variable(\n-                learning_rate,\n-                name=\"learning_rate\",\n-                dtype=backend.floatx(),\n-                trainable=False,\n-            )\n-        self._variables = []\n-        self._trainable_variables = []\n-        self._tracker = Tracker(\n-            {\n-                \"variables\": (\n-                    lambda x: isinstance(x, backend.Variable),\n-                    self._variables,\n-                ),\n-            }\n-        )\n-        self._trainable_variables_indices = {}\n-\n-    def build(self, variables):\n-        for i, variable in enumerate(variables):\n-            self._trainable_variables_indices[id(variable)] = i\n-        self._trainable_variables = variables[:]\n-        self.built = True\n-\n-    @property\n-    def variables(self):\n-        return self._variables[:]\n-\n-    def _get_variable_index(self, variable):\n-        return self._trainable_variables_indices[id(variable)]\n-\n-    def add_variable(\n-        self,\n-        shape,\n-        initializer,\n-        dtype=None,\n-        name=None,\n-    ):\n-        self._check_super_called()\n-        initializer = initializers.get(initializer)\n-        variable = backend.Variable(\n-            initializer=initializer,\n-            shape=shape,\n-            dtype=dtype,\n-            trainable=False,\n-            name=name,\n-        )\n-        self._variables.append(variable)\n-        # Prevent double-tracking\n-        self._tracker.stored_ids[\"variables\"].add(id(variable))\n-        return variable\n-\n-    def add_variable_from_reference(self, reference_variable, name=None):\n-        \"\"\"Add an all-zeros variable with the shape and dtype of a reference\n-        variable.\n-        \"\"\"\n-        initializer = initializers.Zeros()\n-        name = name or auto_name(self.__class__.__name__)\n-        return self.add_variable(\n-            shape=reference_variable.shape,\n-            initializer=initializer,\n-            dtype=reference_variable.dtype,\n-            name=name,\n-        )\n-\n-    def _check_variables_are_known(self, variables):\n-        for v in variables:\n-            if id(v) not in self._trainable_variables_indices:\n-                raise ValueError(\n-                    f\"Unknown variable: {v}. This optimizer can only \"\n-                    \"be called for the variables it was originally built with. \"\n-                    \"When working with a new set of variables, you should \"\n-                    \"recreate a new optimizer instance.\"\n-                )\n-\n-    def update_step(self, gradient, variable, learning_rate):\n-        raise NotImplementedError\n-\n-    def apply_gradients(self, grads_and_vars):\n-        grads, trainable_variables = zip(*grads_and_vars)\n-        return self.apply(grads, trainable_variables)\n-\n-    def apply(self, grads, variables=None):\n-        \"\"\"\n-        `grads` should be a list of gradient tensors\n-        with 1:1 mapping to the list of variables the optimizer was built with.\n-\n-        `variables` can be provided on the first call to build the optimizer.\n-        \"\"\"\n-        grads = list(grads)\n-        if len(grads) == 0:\n-            # It is possible that the grad is empty. In this case,\n-            # `apply_gradients` is a no-op.\n-            return\n-\n-        if variables is None:\n-            if not self.built:\n-                raise ValueError(\n-                    \"When passing `grads` without `variables`, the optimizer \"\n-                    \"must already be built on a list of variables. \"\n-                    \"Call `optimizer.build(trainable_variables)` first. \"\n-                )\n-            if len(grads) != len(self._trainable_variables_indices):\n-                raise ValueError(\n-                    \"When passing `grads` as a list of gradient tensors, the \"\n-                    f\"gradients must match `optimizer.variables` one-to-on. \"\n-                    f\"Received a list of {len(grads)} gradients, but the \"\n-                    f\"optimizer is tracking {len(self._trainable_variables)} \"\n-                    \"trainable variables.\"\n-                )\n-            trainable_variables = self._trainable_variables\n-        else:\n-            trainable_variables = list(variables)\n-            # Optionally build optimizer.\n-            if not self.built:\n-                with ops.name_scope(self.name):\n-                    self.build(trainable_variables)\n-                self.built = True\n-            self._check_variables_are_known(trainable_variables)\n-\n-        grads_and_vars = list(zip(grads, self._trainable_variables))\n-\n-        with ops.name_scope(self.name):\n-            # Filter empty gradients.\n-            grads_and_vars = self._filter_empty_gradients(grads_and_vars)\n-            if len(list(grads_and_vars)) == 0:\n-                return\n-\n-            # Apply clipping and weight decay.\n-            grads, trainable_variables = zip(*grads_and_vars)\n-            grads = self._clip_gradients(grads)\n-            self._apply_weight_decay(trainable_variables)\n-\n-            # Apply gradient updates.\n-            learning_rate = self._get_current_learning_rate()\n-            for grad, var in zip(grads, trainable_variables):\n-                self.update_step(grad, var, learning_rate)\n-            self.iterations.assign(self.iterations + 1)\n-\n-            # Apply variable constraints after applying gradients.\n-            for variable in trainable_variables:\n-                if getattr(variable, \"constraint\", None) is not None:\n-                    variable.assign(variable.constraint(variable))\n-\n-    def stateless_apply(self, grads, trainable_variables, optimizer_variables):\n-        self._check_super_called()\n-\n-        if not self.built:\n-            raise ValueError(\n-                \"To call stateless_apply_gradients, {self.__class__.__name__} \"\n-                \"must be built (i.e. its variables must have been created). \"\n-                \"You can build it via `optimizer.build(trainable_variables)`.\"\n-            )\n-        if len(optimizer_variables) != len(self.variables):\n-            raise ValueError(\n-                \"Argument `optimizer_variables` must be a list of tensors \"\n-                f\"corresponding 1:1 to {self.__class__.__name__}().variables. \"\n-                f\"Received list with length {len(optimizer_variables)}, but \"\n-                f\"expected {len(self.variables)} variables.\"\n-            )\n-        if len(trainable_variables) != len(self._trainable_variables):\n-            raise ValueError(\n-                \"Argument `optimizer_variables` must be a list of tensors \"\n-                \"corresponding 1:1 to the trainable variables list that \"\n-                \"the optimizer was built with. Received \"\n-                f\"len(trainable_variables) == {len(trainable_variables)} \"\n-                \"whereas the optimizer was built with \"\n-                f\"{len(self._trainable_variables)} variables.\"\n-            )\n-\n-        # Gather variable mapping\n-        mapping = list(\n-            zip(self._trainable_variables, trainable_variables)\n-        ) + list(zip(self.variables, optimizer_variables))\n-\n-        # Call in stateless scope\n-        with backend.StatelessScope(state_mapping=mapping) as scope:\n-            self.apply(grads)\n-\n-        # Gather updated variables\n-        trainable_variables = []\n-        for v in self._trainable_variables:\n-            new_v = scope.get_current_value(v)\n-            if new_v is not None:\n-                trainable_variables.append(new_v)\n-            else:\n-                trainable_variables.append(v)\n-        optimizer_variables = []\n-        for v in self.variables:\n-            new_v = scope.get_current_value(v)\n-            if new_v is not None:\n-                optimizer_variables.append(new_v)\n-            else:\n-                optimizer_variables.append(v)\n-        return trainable_variables, optimizer_variables\n-\n-    @property\n-    def learning_rate(self):\n-        return self._get_current_learning_rate()\n-\n-    @learning_rate.setter\n-    def learning_rate(self, learning_rate):\n-        if isinstance(\n-            learning_rate, learning_rate_schedule.LearningRateSchedule\n-        ):\n-            self._learning_rate = learning_rate\n-        elif callable(learning_rate):\n-            self._learning_rate = learning_rate\n-        else:\n-            if isinstance(\n-                self._learning_rate, learning_rate_schedule.LearningRateSchedule\n-            ):\n-                raise TypeError(\n-                    \"This optimizer was created with a `LearningRateSchedule`\"\n-                    \" object as its `learning_rate` constructor argument, \"\n-                    \"hence its learning rate is not settable. If you need the\"\n-                    \" learning rate to be settable, you should instantiate \"\n-                    \"the optimizer with a float `learning_rate` argument.\"\n-                )\n-            self._learning_rate.assign(learning_rate)\n-\n-    def save_own_variables(self, store):\n-        \"\"\"Get the state of this optimizer object.\"\"\"\n-        for i, variable in enumerate(self.variables):\n-            store[str(i)] = np.array(variable)\n-\n-    def load_own_variables(self, store):\n-        \"\"\"Set the state of this optimizer object.\"\"\"\n-        if len(store.keys()) != len(self.variables):\n-            msg = (\n-                f\"Skipping variable loading for optimizer '{self.name}', \"\n-                f\"because it has {len(self.variables)} variables whereas \"\n-                f\"the saved optimizer has {len(store.keys())} variables. \"\n-            )\n-            if len(self.variables) == 0:\n-                msg += (\n-                    \"This is likely because the optimizer has not been \"\n-                    \"called/built yet.\"\n-                )\n-            warnings.warn(msg, stacklevel=2)\n-            return\n-        for i, variable in enumerate(self.variables):\n-            variable.assign(store[str(i)])\n-\n-    def _get_current_learning_rate(self):\n-        if isinstance(\n-            self._learning_rate, learning_rate_schedule.LearningRateSchedule\n-        ):\n-            return self._learning_rate(self.iterations)\n-        elif callable(self._learning_rate):\n-            return self._learning_rate(self.iterations)\n-        return self._learning_rate\n-\n-    def _filter_empty_gradients(self, grads_and_vars):\n-        filtered = [(g, v) for g, v in grads_and_vars if g is not None]\n-        if not filtered:\n-            raise ValueError(\"No gradients provided for any variable.\")\n-        if len(filtered) < len(grads_and_vars):\n-            missing_grad_vars = [v for g, v in grads_and_vars if g is None]\n-            warnings.warn(\n-                \"Gradients do not exist for variables \"\n-                f\"{[v.name for v in missing_grad_vars]} when minimizing the \"\n-                \"loss. If you're using `model.compile()`, did you forget to \"\n-                \"provide a `loss` argument?\"\n-            )\n-        return filtered\n-\n-    def _clip_gradients(self, grads):\n-        if self.clipnorm and self.clipnorm > 0:\n-            clipped_grads = []\n-            for g in grads:\n-                if g is None:\n-                    clipped_grads.append(g)\n-                else:\n-                    clipped_grads.append(clip_by_norm(g, self.clipnorm))\n-            return clipped_grads\n-\n-        if self.global_clipnorm and self.global_clipnorm > 0:\n-            return clip_by_global_norm(grads, self.global_clipnorm)\n-\n-        if self.clipvalue and self.clipvalue > 0:\n-            clipped_grads = []\n-            for g in grads:\n-                if g is None:\n-                    clipped_grads.append(g)\n-                else:\n-                    clipped_grads.append(\n-                        ops.clip(g, -self.clipvalue, self.clipvalue)\n-                    )\n-            return clipped_grads\n-        return grads\n-\n-    def exclude_from_weight_decay(self, var_list=None, var_names=None):\n-        \"\"\"Exclude variables from weight decay.\n-\n-        This method must be called before the optimizer's `build` method is\n-        called. You can set specific variables to exclude out, or set a list of\n-        strings as the anchor words, if any of which appear in a variable's\n-        name, then the variable is excluded.\n-\n-        Args:\n-            var_list: A list of `tf.Variable`s to exclude from weight decay.\n-            var_names: A list of strings. If any string in `var_names` appear\n-                in the model variable's name, then this model variable is\n-                excluded from weight decay. For example, `var_names=['bias']`\n-                excludes all bias variables from weight decay.\n-        \"\"\"\n-        if hasattr(self, \"_built\") and self._built:\n-            raise ValueError(\n-                \"`exclude_from_weight_decay()` can only be configued before \"\n-                \"the optimizer is built.\"\n-            )\n-\n-        if var_list:\n-            self._exclude_from_weight_decay = [\n-                id(variable) for variable in var_list\n-            ]\n-        else:\n-            self._exclude_from_weight_decay = []\n-        self._exclude_from_weight_decay_names = var_names or []\n-\n-    def _use_weight_decay(self, variable):\n-        exclude_from_weight_decay = getattr(\n-            self, \"_exclude_from_weight_decay\", []\n-        )\n-        exclude_from_weight_decay_names = getattr(\n-            self, \"_exclude_from_weight_decay_names\", []\n-        )\n-        variable_id = id(variable)\n-        for exclude_id in exclude_from_weight_decay:\n-            if variable_id == exclude_id:\n-                return False\n-        for name in exclude_from_weight_decay_names:\n-            if re.search(name, variable.name) is not None:\n-                return False\n-        return True\n-\n-    def _apply_weight_decay(self, variables):\n-        if self.weight_decay is None:\n-            return\n-        for variable in variables:\n-            if self._use_weight_decay(variable):\n-                lr = ops.cast(self._get_current_learning_rate(), variable.dtype)\n-                wd = ops.cast(self.weight_decay, variable.dtype)\n-                variable.assign(variable - variable * wd * lr)\n-\n-    def _check_super_called(self):\n-        if not hasattr(self, \"_tracker\"):\n-            raise RuntimeError(\n-                f\"In optimizer '{self.__class__.__name__}', you forgot to call \"\n-                \"`super().__init__()` in the `__init__()` method. \"\n-                \"Go add it!\"\n-            )\n-\n-    def _update_model_variables_moving_average(self, var_list):\n-        \"\"\"Update the stored moving average using the latest value.\"\"\"\n-        if self.use_ema:\n-            for var, average in zip(\n-                var_list, self._model_variables_moving_average\n-            ):\n-                average.assign(\n-                    self.ema_momentum * average + (1 - self.ema_momentum) * var\n-                )\n-\n-    def _overwrite_model_variables_with_average_value(self, var_list):\n-        \"\"\"Overwrite model variables with its moving average.\"\"\"\n-        if len(var_list) != len(self._model_variables_moving_average):\n-            raise ValueError(\n-                f\"The length of model variables ({len(var_list)}) to \"\n-                \"override does not match the length of model variables \"\n-                \"stored in the optimizer \"\n-                f\"({len(self._model_variables_moving_average)}). Please \"\n-                \"check if the optimizer was called on your model.\"\n-            )\n-        self._overwrite_model_variables_with_average_value_helper(var_list)\n-\n-    def _overwrite_model_variables_with_average_value_helper(self, var_list):\n-        \"\"\"Helper function that overwrites model variables.\"\"\"\n-        for var, average_var in zip(\n-            var_list, self._model_variables_moving_average\n-        ):\n-            var.assign(average_var)\n-\n-    def finalize_variable_values(self, var_list):\n-        \"\"\"Set the final value of model's trainable variables.\n-\n-        Sometimes there are some extra steps before ending the variable updates,\n-        such as overriding the model variables with its average value.\n-\n-        Args:\n-          var_list: list of model variables.\n-        \"\"\"\n-        if self.use_ema:\n-            # If the optimizer uses EMA, then when finalizing, we replace the\n-            # model variable value with its moving average stored inside\n-            # optimizer.\n-            self._overwrite_model_variables_with_average_value(var_list)\n-\n-    def get_config(self):\n-        \"\"\"Returns the config of the optimizer.\n-\n-        An optimizer config is a Python dictionary (serializable)\n-        containing the configuration of an optimizer.\n-        The same optimizer can be reinstantiated later\n-        (without any saved state) from this configuration.\n-\n-        Subclass optimizer should override this method to include other\n-        hyperparameters.\n-\n-        Returns:\n-            Python dictionary.\n-        \"\"\"\n-\n-        if isinstance(\n-            self._learning_rate, learning_rate_schedule.LearningRateSchedule\n-        ):\n-            learning_rate = learning_rate_schedule.serialize(\n-                self._learning_rate\n-            )\n-        elif isinstance(self._learning_rate, backend.Variable):\n-            learning_rate = float(self._learning_rate.numpy())\n-        elif ops.is_tensor(self._learning_rate):\n-            learning_rate = float(self._learning_rate)\n-        elif callable(self._learning_rate):\n-            learning_rate = serialization_lib.serialize_keras_object(\n-                self._learning_rate\n-            )\n-\n-        config = {\n-            \"name\": self.name,\n-            \"learning_rate\": learning_rate,\n-            \"weight_decay\": self.weight_decay,\n-            \"clipnorm\": self.clipnorm,\n-            \"global_clipnorm\": self.global_clipnorm,\n-            \"clipvalue\": self.clipvalue,\n-            \"use_ema\": self.use_ema,\n-            \"ema_momentum\": self.ema_momentum,\n-            \"ema_overwrite_frequency\": self.ema_overwrite_frequency,\n-        }\n-        return config\n-\n-    @classmethod\n-    def from_config(cls, config, custom_objects=None):\n-        \"\"\"Creates an optimizer from its config.\n-\n-        This method is the reverse of `get_config`, capable of instantiating the\n-        same optimizer from the config dictionary.\n-\n-        Args:\n-            config: A Python dictionary, typically the output of get_config.\n-            custom_objects: A Python dictionary mapping names to additional\n-              user-defined Python objects needed to recreate this optimizer.\n-\n-        Returns:\n-            An optimizer instance.\n-        \"\"\"\n-        if \"learning_rate\" in config:\n-            if isinstance(config[\"learning_rate\"], dict):\n-                config[\n-                    \"learning_rate\"\n-                ] = serialization_lib.deserialize_keras_object(\n-                    config[\"learning_rate\"], custom_objects=custom_objects\n-                )\n-        return cls(**config)\n+    Optimizer = base_optimizer.Optimizer\n \n \n-base_optimizer_keyword_args = \"\"\"name: String. The name to use\n-          for momentum accumulator weights created by\n-          the optimizer.\n-      weight_decay: Float, defaults to None. If set, weight decay is applied.\n-      clipnorm: Float. If set, the gradient of each weight is individually\n-          clipped so that its norm is no higher than this value.\n-      clipvalue: Float. If set, the gradient of each weight is clipped to be no\n-          higher than this value.\n-      global_clipnorm: Float. If set, the gradient of all weights is clipped so\n-          that their global norm is no higher than this value.\n-      use_ema: Boolean, defaults to False. If True, exponential moving average\n-          (EMA) is applied. EMA consists of computing an exponential moving\n-          average of the weights of the model (as the weight values change after\n-          each training batch), and periodically overwriting the weights with\n-          their moving average.\n-      ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n-          This is the momentum to use when computing\n-          the EMA of the model's weights:\n-          `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n-          current_variable_value`.\n-      ema_overwrite_frequency: Int or None, defaults to None. Only used if\n-          `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n-          we overwrite the model variable by its moving average.\n-          If None, the optimizer\n-          does not overwrite model variables in the middle of training, and you\n-          need to explicitly overwrite the variables at the end of training\n-          by calling `optimizer.finalize_variable_values()`\n-          (which updates the model\n-          variables in-place). When using the built-in `fit()` training loop,\n-          this happens automatically after the last epoch,\n-          and you don't need to do anything.\"\"\"\n+keras_core_export([\n+    \"keras_core.Optimizer\", \"keras_core.optimizers.Optimizer\"])(Optimizer)\n \n \n-def clip_by_norm(values, clip_norm, axes=None):\n-    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n-    l2sum = ops.sum(values * values, axes, keepdims=True)\n-    pred = l2sum > 0\n-    # Two-tap tf.where trick to bypass NaN gradients\n-    l2sum_safe = ops.where(pred, l2sum, ops.ones_like(l2sum))\n-    l2norm = ops.where(pred, ops.sqrt(l2sum_safe), l2sum)\n-    intermediate = values * clip_norm\n-    values_clip = intermediate / ops.maximum(l2norm, clip_norm)\n-    return values_clip\n-\n-\n-def global_norm(value_list):\n-    \"\"\"Computes the global norm of multiple tensors.\"\"\"\n-    squared_norms = []\n-    for v in value_list:\n-        if v is not None:\n-            squared_norms.append(ops.sum(ops.square(v)))\n-    squared_norm = ops.sum(ops.stack(squared_norms))\n-    return ops.sqrt(squared_norm)\n-\n-\n-def clip_by_global_norm(value_list, clip_norm):\n-    use_norm = global_norm(value_list)\n-    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n-    scale_for_finite = clip_norm * ops.minimum(1.0 / use_norm, 1.0 / clip_norm)\n-    # If use_norm is any finite number, this is a no-op. For inf/-inf/NaN,\n-    # this will make scale NaN.\n-    scale = scale_for_finite + (use_norm - use_norm)\n-    values_clipped = []\n-    for v in value_list:\n-        if v is None:\n-            values_clipped.append(None)\n-        else:\n-            values_clipped.append(v * scale)\n-    return values_clipped\n+base_optimizer_keyword_args = base_optimizer.base_optimizer_keyword_args\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
