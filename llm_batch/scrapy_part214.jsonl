{"custom_id": "scrapy#c55ff110a34d39be27bbd3d03fbf52caa271b4c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 421 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -200,7 +200,7 @@ class CsvItemExporter(BaseItemExporter):\n             try:\n                 yield to_native_str(s)\n             except TypeError:\n-                yield to_native_str(repr(s))\n+                yield to_native_str(str(s))\n \n     def _write_headers_and_set_fields_to_export(self, item):\n         if self.include_headers_line:\n\n@@ -271,6 +271,21 @@ class CsvItemExporterTest(BaseItemExporterTest):\n             expected='\"[4, 8]\",John\\r\\n',\n         )\n \n+    def test_other_python_types_item(self):\n+        from datetime import datetime\n+        now = datetime(2015, 1, 1, 1, 1, 1)\n+        item = {\n+            'boolean': False,\n+            'number': 22,\n+            'time': now,\n+            'float': 3.14,\n+        }\n+        self.assertExportResult(\n+            item=item,\n+            include_headers_line=False,\n+            expected='22,False,3.14,2015-01-01 01:01:01\\r\\n'\n+        )\n+\n \n class XmlItemExporterTest(BaseItemExporterTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27758f60ada4791c044bfe8bc86d267aa930c744", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 111 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -200,7 +200,7 @@ class CsvItemExporter(BaseItemExporter):\n             try:\n                 yield to_native_str(s)\n             except TypeError:\n-                yield to_native_str(str(s))\n+                yield s\n \n     def _write_headers_and_set_fields_to_export(self, item):\n         if self.include_headers_line:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e080c3c52720535519ba1be7dee472258b19647", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 366 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,12 @@ class TextResponse(Response):\n \n     def body_as_unicode(self):\n         \"\"\"Return body as unicode\"\"\"\n-        # check for self.encoding before _cached_ubody just in\n+        return self.text\n+\n+    @property\n+    def text(self):\n+        \"\"\" Body as unicode \"\"\"\n+        # access self.encoding before _cached_ubody to make sure\n         # _body_inferred_encoding is called\n         benc = self.encoding\n         if self._cached_ubody is None:\n@@ -67,11 +72,6 @@ class TextResponse(Response):\n             self._cached_ubody = html_to_unicode(charset, self.body)[1]\n         return self._cached_ubody\n \n-    @property\n-    def text(self):\n-        \"\"\" Body as unicode \"\"\"\n-        return self.body_as_unicode()\n-\n     def urljoin(self, url):\n         \"\"\"Join this Response's url with a possible relative url to form an\n         absolute interpretation of the latter.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90e3ae1c580875e4e68c9d7238d0fb4642306bf9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 83 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,9 +8,7 @@ import logging\n \n from six.moves.urllib import robotparser\n \n-from twisted.internet import reactor\n from twisted.internet.defer import Deferred, maybeDeferred\n-from twisted.internet.task import deferLater\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n@@ -59,13 +57,7 @@ class RobotsTxtMiddleware(object):\n                 priority=self.DOWNLOAD_PRIORITY,\n                 meta={'dont_obey_robotstxt': True}\n             )\n-            # engine.download() can return an already-called deferred, e.g. if a\n-            # middleware returns a response in process_request(). Using\n-            # deferLater() ensures that the error callback isn't called\n-            # immediately upon being added, so that it doesn't remove the key\n-            # before we check for it.\n-            dfd = deferLater(reactor, 0, self.crawler.engine.download,\n-                             robotsreq, spider)\n+            dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots, netloc)\n             dfd.addErrback(self._logerror, robotsreq, spider)\n             dfd.addErrback(self._robots_error, netloc)\n@@ -109,4 +101,6 @@ class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n \n     def _robots_error(self, failure, netloc):\n-        self._parsers.pop(netloc).callback(None)\n+        rp_dfd = self._parsers[netloc]\n+        self._parsers[netloc] = None\n+        rp_dfd.callback(None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cae268402d13a6d419c56024b3051b1cad3d82e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 29 | Files Changed: 3 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 55 | Churn Cumulative: 1273 | Contributors (this commit): 20 | Commits (past 90d): 20 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -3,40 +3,13 @@ Scrapy Shell\n \n See documentation in docs/topics/shell.rst\n \"\"\"\n-import re\n-from six.moves.urllib.parse import urlparse\n from threading import Thread\n-from w3lib.url import any_to_uri\n \n from scrapy.commands import ScrapyCommand\n from scrapy.shell import Shell\n from scrapy.http import Request\n-from scrapy.utils.url import add_http_if_no_scheme\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n-\n-\n-def guess_scheme(url):\n-    \"\"\"Given an URL as string,\n-    returns a FileURI if it looks like a file path,\n-    otherwise returns an HTTP URL\n-    \"\"\"\n-    parts = urlparse(url)\n-    if parts.scheme:\n-        return url\n-    # Note: this does not match Windows filepath\n-    if re.match(r'''^                   # start with...\n-                    (\n-                        \\.              # ...a single dot,\n-                        (\n-                            \\. | [^/\\.]+  # optionally followed by\n-                        )?                # either a second dot or some characters\n-                    )?      # optional match of \".\", \"..\" or \".blabla\"\n-                    /       # at least one \"/\" for a file path,\n-                    .       # and something after the \"/\"\n-                    ''', parts.path, flags=re.VERBOSE):\n-        return any_to_uri(url)\n-    else:\n-        return add_http_if_no_scheme(url)\n+from scrapy.utils.url import guess_scheme\n \n \n class Command(ScrapyCommand):\n\n@@ -122,3 +122,27 @@ def add_http_if_no_scheme(url):\n         url = scheme + url\n \n     return url\n+\n+\n+def guess_scheme(url):\n+    \"\"\"Given an URL as string,\n+    returns a FileURI if it looks like a file path,\n+    otherwise returns an HTTP URL\n+    \"\"\"\n+    parts = urlparse(url)\n+    if parts.scheme:\n+        return url\n+    # Note: this does not match Windows filepath\n+    if re.match(r'''^                   # start with...\n+                    (\n+                        \\.              # ...a single dot,\n+                        (\n+                            \\. | [^/\\.]+  # optionally followed by\n+                        )?                # either a second dot or some characters\n+                    )?      # optional match of \".\", \"..\" or \".blabla\"\n+                    /       # at least one \"/\" for a file path,\n+                    .       # and something after the \"/\"\n+                    ''', parts.path, flags=re.VERBOSE):\n+        return any_to_uri(url)\n+    else:\n+        return add_http_if_no_scheme(url)\n\n@@ -3,9 +3,9 @@ from os.path import join\n from twisted.trial import unittest\n from twisted.internet import defer\n \n-from scrapy.commands.shell import guess_scheme\n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n+from scrapy.utils.url import guess_scheme\n \n from tests import tests_datadir\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#481e251775a089a8e82c480a83181146d1bb6847", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 59 | Files Changed: 2 | Hunks: 3 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/4 | Churn Δ: 119 | Churn Cumulative: 836 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -10,64 +10,6 @@ from scrapy.utils.url import guess_scheme\n from tests import tests_datadir\n \n \n-class ShellURLTest(unittest.TestCase):\n-    pass\n-\n-def create_guess_scheme_t(args):\n-    def do_expected(self):\n-        url = guess_scheme(args[0])\n-        assert url.startswith(args[1]), \\\n-            'Wrong scheme guessed: for `%s` got `%s`, expected `%s...`' % (\n-                args[0], url, args[1])\n-    return do_expected\n-\n-def create_skipped_scheme_t(args):\n-    def do_expected(self):\n-        raise unittest.SkipTest(args[2])\n-        url = guess_scheme(args[0])\n-        assert url.startswith(args[1])\n-    return do_expected\n-\n-for k, args in enumerate ([\n-            ('/index',                              'file://'),\n-            ('/index.html',                         'file://'),\n-            ('./index.html',                        'file://'),\n-            ('../index.html',                       'file://'),\n-            ('../../index.html',                    'file://'),\n-            ('./data/index.html',                   'file://'),\n-            ('.hidden/data/index.html',             'file://'),\n-            ('/home/user/www/index.html',           'file://'),\n-            ('//home/user/www/index.html',          'file://'),\n-            ('file:///home/user/www/index.html',    'file://'),\n-\n-            ('index.html',                          'http://'),\n-            ('example.com',                         'http://'),\n-            ('www.example.com',                     'http://'),\n-            ('www.example.com/index.html',          'http://'),\n-            ('http://example.com',                  'http://'),\n-            ('http://example.com/index.html',       'http://'),\n-            ('localhost',                           'http://'),\n-            ('localhost/index.html',                'http://'),\n-\n-            # some corner cases (default to http://)\n-            ('/',                                   'http://'),\n-            ('.../test',                            'http://'),\n-\n-        ], start=1):\n-    t_method = create_guess_scheme_t(args)\n-    t_method.__name__ = 'test_uri_%03d' % k\n-    setattr (ShellURLTest, t_method.__name__, t_method)\n-\n-# TODO: the following tests do not pass with current implementation\n-for k, args in enumerate ([\n-            ('C:\\absolute\\path\\to\\a\\file.html',     'file://',\n-             'Windows filepath are not supported for scrapy shell'),\n-        ], start=1):\n-    t_method = create_skipped_scheme_t(args)\n-    t_method.__name__ = 'test_uri_skipped_%03d' % k\n-    setattr (ShellURLTest, t_method.__name__, t_method)\n-\n-\n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n \n     command = 'shell'\n\n@@ -4,7 +4,8 @@ import unittest\n import six\n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n-                              canonicalize_url, add_http_if_no_scheme)\n+                              canonicalize_url, add_http_if_no_scheme,\n+                              guess_scheme)\n \n __doctests__ = ['scrapy.utils.url']\n \n@@ -294,5 +295,63 @@ class AddHttpIfNoScheme(unittest.TestCase):\n                                                'ftp://www.example.com')\n \n \n+class GuessSchemeTest(unittest.TestCase):\n+    pass\n+\n+def create_guess_scheme_t(args):\n+    def do_expected(self):\n+        url = guess_scheme(args[0])\n+        assert url.startswith(args[1]), \\\n+            'Wrong scheme guessed: for `%s` got `%s`, expected `%s...`' % (\n+                args[0], url, args[1])\n+    return do_expected\n+\n+def create_skipped_scheme_t(args):\n+    def do_expected(self):\n+        raise unittest.SkipTest(args[2])\n+        url = guess_scheme(args[0])\n+        assert url.startswith(args[1])\n+    return do_expected\n+\n+for k, args in enumerate ([\n+            ('/index',                              'file://'),\n+            ('/index.html',                         'file://'),\n+            ('./index.html',                        'file://'),\n+            ('../index.html',                       'file://'),\n+            ('../../index.html',                    'file://'),\n+            ('./data/index.html',                   'file://'),\n+            ('.hidden/data/index.html',             'file://'),\n+            ('/home/user/www/index.html',           'file://'),\n+            ('//home/user/www/index.html',          'file://'),\n+            ('file:///home/user/www/index.html',    'file://'),\n+\n+            ('index.html',                          'http://'),\n+            ('example.com',                         'http://'),\n+            ('www.example.com',                     'http://'),\n+            ('www.example.com/index.html',          'http://'),\n+            ('http://example.com',                  'http://'),\n+            ('http://example.com/index.html',       'http://'),\n+            ('localhost',                           'http://'),\n+            ('localhost/index.html',                'http://'),\n+\n+            # some corner cases (default to http://)\n+            ('/',                                   'http://'),\n+            ('.../test',                            'http://'),\n+\n+        ], start=1):\n+    t_method = create_guess_scheme_t(args)\n+    t_method.__name__ = 'test_uri_%03d' % k\n+    setattr (GuessSchemeTest, t_method.__name__, t_method)\n+\n+# TODO: the following tests do not pass with current implementation\n+for k, args in enumerate ([\n+            ('C:\\absolute\\path\\to\\a\\file.html',     'file://',\n+             'Windows filepath are not supported for scrapy shell'),\n+        ], start=1):\n+    t_method = create_skipped_scheme_t(args)\n+    t_method.__name__ = 'test_uri_skipped_%03d' % k\n+    setattr (GuessSchemeTest, t_method.__name__, t_method)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9f6b98816c220129fb02e6c09f488ecc8d686bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 559 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -125,10 +125,7 @@ def add_http_if_no_scheme(url):\n \n \n def guess_scheme(url):\n-    \"\"\"Given an URL as string,\n-    returns a FileURI if it looks like a file path,\n-    otherwise returns an HTTP URL\n-    \"\"\"\n+    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or http:// otherwise.\"\"\"\n     parts = urlparse(url)\n     if parts.scheme:\n         return url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf2ebb0687b9198f017cab836806bae5a5f3009c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 70 | Churn Cumulative: 382 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ import marshal\n import tempfile\n import unittest\n from io import BytesIO\n+from datetime import datetime\n from six.moves import cPickle as pickle\n \n import lxml.etree\n@@ -42,6 +43,14 @@ class BaseItemExporterTest(unittest.TestCase):\n             exported_dict[k] = to_unicode(v)\n         self.assertEqual(self.i, exported_dict)\n \n+    def _get_nonstring_types_item(self):\n+        return {\n+            'boolean': False,\n+            'number': 22,\n+            'time': datetime(2015, 1, 1, 1, 1, 1),\n+            'float': 3.14,\n+        }\n+\n     def assertItemExportWorks(self, item):\n         self.ie.start_exporting()\n         try:\n@@ -134,15 +143,8 @@ class PythonItemExporterTest(BaseItemExporterTest):\n         expected = {b'name': b'John\\xc2\\xa3', b'age': b'22'}\n         self.assertEqual(expected, exporter.export_item(value))\n \n-    def test_other_python_types_item(self):\n-        from datetime import datetime\n-        now = datetime.now()\n-        item = {\n-            'boolean': False,\n-            'number': 22,\n-            'time': now,\n-            'float': 3.14,\n-        }\n+    def test_nonstring_types_item(self):\n+        item = self._get_nonstring_types_item()\n         ie = self._get_exporter()\n         exported = ie.export_item(item)\n         self.assertEqual(exported, item)\n@@ -178,6 +180,15 @@ class PickleItemExporterTest(BaseItemExporterTest):\n         self.assertEqual(pickle.load(f), i1)\n         self.assertEqual(pickle.load(f), i2)\n \n+    def test_nonstring_types_item(self):\n+        item = self._get_nonstring_types_item()\n+        fp = BytesIO()\n+        ie = PickleItemExporter(fp)\n+        ie.start_exporting()\n+        ie.export_item(item)\n+        ie.finish_exporting()\n+        self.assertEqual(pickle.loads(fp.getvalue()), item)\n+\n \n class MarshalItemExporterTest(BaseItemExporterTest):\n \n@@ -189,6 +200,17 @@ class MarshalItemExporterTest(BaseItemExporterTest):\n         self.output.seek(0)\n         self._assert_expected_item(marshal.load(self.output))\n \n+    def test_nonstring_types_item(self):\n+        item = self._get_nonstring_types_item()\n+        item.pop('time')  # datetime is not marshallable\n+        fp = tempfile.TemporaryFile()\n+        ie = MarshalItemExporter(fp)\n+        ie.start_exporting()\n+        ie.export_item(item)\n+        ie.finish_exporting()\n+        fp.seek(0)\n+        self.assertEqual(marshal.load(fp), item)\n+\n \n class CsvItemExporterTest(BaseItemExporterTest):\n     def _get_exporter(self, **kwargs):\n@@ -271,17 +293,9 @@ class CsvItemExporterTest(BaseItemExporterTest):\n             expected='\"[4, 8]\",John\\r\\n',\n         )\n \n-    def test_other_python_types_item(self):\n-        from datetime import datetime\n-        now = datetime(2015, 1, 1, 1, 1, 1)\n-        item = {\n-            'boolean': False,\n-            'number': 22,\n-            'time': now,\n-            'float': 3.14,\n-        }\n+    def test_nonstring_types_item(self):\n         self.assertExportResult(\n-            item=item,\n+            item=self._get_nonstring_types_item(),\n             include_headers_line=False,\n             expected='22,False,3.14,2015-01-01 01:01:01\\r\\n'\n         )\n@@ -390,6 +404,15 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n         self._check_output()\n         self.assertRaises(TypeError, self._get_exporter, foo_unknown_keyword_bar=True)\n \n+    def test_nonstring_types_item(self):\n+        item = self._get_nonstring_types_item()\n+        self.ie.start_exporting()\n+        self.ie.export_item(item)\n+        self.ie.finish_exporting()\n+        exported = json.loads(to_unicode(self.output.getvalue()))\n+        item['time'] = str(item['time'])\n+        self.assertEqual(exported, item)\n+\n \n class JsonItemExporterTest(JsonLinesItemExporterTest):\n \n@@ -438,6 +461,15 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         expected = {'name': u'Jesus', 'age': {'name': 'Maria', 'age': i1}}\n         self.assertEqual(exported, [expected])\n \n+    def test_nonstring_types_item(self):\n+        item = self._get_nonstring_types_item()\n+        self.ie.start_exporting()\n+        self.ie.export_item(item)\n+        self.ie.finish_exporting()\n+        exported = json.loads(to_unicode(self.output.getvalue()))\n+        item['time'] = str(item['time'])\n+        self.assertEqual(exported, [item])\n+\n \n class CustomItemExporterTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#78f00401cd284fd7bdcbb525ee94ea0bbedaa7cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 510 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,6 @@ from twisted.internet import defer\n \n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n-from scrapy.utils.url import guess_scheme\n \n from tests import tests_datadir\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a1ebff83d39e65cf4ed34281a1e81ea6cd108fe0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 22 | Churn Cumulative: 755 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -368,11 +368,25 @@ class BaseSettings(MutableMapping):\n     def __len__(self):\n         return len(self.attributes)\n \n-    def __str__(self):\n-        return str(self.attributes)\n+    def _to_dict(self):\n+        return {k: (v._to_dict() if isinstance(v, BaseSettings) else v)\n+                for k, v in six.iteritems(self)}\n \n-    def __repr__(self):\n-        return \"<%s %s>\" % (self.__class__.__name__, self.attributes)\n+    def copy_to_dict(self):\n+        \"\"\"\n+        Make a copy of current settings and convert to a dict.\n+\n+        This method returns a new dict populated with the same values\n+        and their priorities as the current settings.\n+\n+        Modifications to the returned dict won't be reflected on the original\n+        settings.\n+\n+        This method can be useful for example for printing settings\n+        in Scrapy shell.\n+        \"\"\"\n+        settings = self.copy()\n+        return settings._to_dict()\n \n     @property\n     def overrides(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa78758bc744b3264c330cc18e58a8b15315517d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 381 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -302,6 +302,21 @@ class BaseSettingsTest(unittest.TestCase):\n         self.assertListEqual(copy.get('TEST_LIST_OF_LISTS')[0],\n                              ['first_one', 'first_two'])\n \n+    def test_copy_to_dict(self):\n+        s = BaseSettings({'TEST_STRING': 'a string',\n+                          'TEST_LIST': [1, 2],\n+                          'TEST_BOOLEAN': False,\n+                          'TEST_BASE': BaseSettings({1: 1, 2: 2}, 'project'),\n+                          'TEST': BaseSettings({1: 10, 3: 30}, 'default'),\n+                          'HASNOBASE': BaseSettings({3: 3000}, 'default')})\n+        self.assertDictEqual(s.copy_to_dict(),\n+                            {'HASNOBASE': {3: 3000},\n+                             'TEST': {1: 10, 3: 30},\n+                             'TEST_BASE': {1: 1, 2: 2},\n+                             'TEST_BOOLEAN': False,\n+                             'TEST_LIST': [1, 2],\n+                             'TEST_STRING': 'a string'})\n+\n     def test_freeze(self):\n         self.settings.freeze()\n         with self.assertRaises(TypeError) as cm:\n@@ -343,14 +358,6 @@ class BaseSettingsTest(unittest.TestCase):\n             self.assertEqual(self.settings.defaults.get('BAR'), 'foo')\n             self.assertIn('BAR', self.settings.defaults)\n \n-    def test_repr(self):\n-        settings = BaseSettings()\n-        self.assertEqual(repr(settings), \"<BaseSettings {}>\")\n-        attr = SettingsAttribute('testval', 15)\n-        settings['testkey'] = attr\n-        self.assertEqual(repr(settings),\n-                         \"<BaseSettings {'testkey': %s}>\" % repr(attr))\n-\n \n class SettingsTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d843a0aae862a84b54f18f9d43ae957c182197b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 140 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,8 @@\n from __future__ import print_function\n+import json\n+\n from scrapy.commands import ScrapyCommand\n+from scrapy.settings import BaseSettings\n \n class Command(ScrapyCommand):\n \n@@ -28,7 +31,11 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         settings = self.crawler_process.settings\n         if opts.get:\n-            print(settings.get(opts.get))\n+            s = settings.get(opts.get)\n+            if isinstance(s, BaseSettings):\n+                print(json.dumps(s.copy_to_dict()))\n+            else:\n+                print(s)\n         elif opts.getbool:\n             print(settings.getbool(opts.getbool))\n         elif opts.getint:\n\n@@ -68,4 +68,4 @@ class CmdlineTest(unittest.TestCase):\n             settingsstr = settingsstr.replace(char, '\"')\n         settingsdict = json.loads(settingsstr)\n         six.assertCountEqual(self, settingsdict.keys(), EXTENSIONS.keys())\n-        self.assertIn('value=200', settingsdict[EXT_PATH])\n+        self.assertEquals(200, settingsdict[EXT_PATH])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f9dc02e23ae35e5ba9dce74dcc2571d02e90eb22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 756 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -194,6 +194,6 @@ class Downloader(object):\n \n     def _slot_gc(self, age=60):\n         mintime = time() - age\n-        for key, slot in self.slots.items():\n+        for key, slot in list(self.slots.items()):\n             if not slot.active and slot.lastseen + slot.delay < mintime:\n                 self.slots.pop(key).close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb2cf7c0d7199fffe0aa100e5c8a51c6b4b82fc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 18 | Churn Cumulative: 511 | Contributors (this commit): 5 | Commits (past 90d): 23 | Contributors (cumulative): 9 | DMM Complexity: 0.13333333333333333\n\nDIFF:\n@@ -144,8 +144,10 @@ class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n-        else:\n+        elif isinstance(serialized_value, six.text_type):\n             self._xg_characters(serialized_value)\n+        else:\n+            self._xg_characters(str(serialized_value))\n         self.xg.endElement(name)\n \n     # Workaround for http://bugs.python.org/issue17606\n\n@@ -376,6 +376,20 @@ class XmlItemExporterTest(BaseItemExporterTest):\n             b'</items>'\n         )\n \n+    def test_nonstring_types_item(self):\n+        item = self._get_nonstring_types_item()\n+        self.assertExportResult(item,\n+            b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n+            b'<items>'\n+               b'<item>'\n+                   b'<float>3.14</float>'\n+                   b'<boolean>False</boolean>'\n+                   b'<number>22</number>'\n+                   b'<time>2015-01-01 01:01:01</time>'\n+               b'</item>'\n+            b'</items>'\n+        )\n+\n \n class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#268e912273dcb7bacc5a7102d5a3f90a868f035f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 762 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import copy\n import warnings\n from collections import MutableMapping\n from importlib import import_module\n+from pprint import pformat\n \n from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -388,6 +389,12 @@ class BaseSettings(MutableMapping):\n         settings = self.copy()\n         return settings._to_dict()\n \n+    def _repr_pretty_(self, p, cycle):\n+        if cycle:\n+            p.text(repr(self))\n+        else:\n+            p.text(pformat(self.copy_to_dict()))\n+\n     @property\n     def overrides(self):\n         warnings.warn(\"`Settings.overrides` attribute is deprecated and won't \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65fb67f2dbdb36763832b7b332bdbff0fd2bd9db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 1466 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -189,7 +189,7 @@ class CrawlerRunner(object):\n \n         Returns a deferred that is fired when they all have ended.\n         \"\"\"\n-        return defer.DeferredList([c.stop() for c in self.crawlers])\n+        return defer.DeferredList([c.stop() for c in list(self.crawlers)])\n \n     @defer.inlineCallbacks\n     def join(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6591b5c9f46271731ee711a23da952037a15388", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 2204 | Contributors (this commit): 18 | Commits (past 90d): 2 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -177,12 +177,23 @@ class ExecutionEngine(object):\n         return d\n \n     def spider_is_idle(self, spider):\n-        scraper_idle = self.scraper.slot.is_idle()\n-        pending = self.slot.scheduler.has_pending_requests()\n-        downloading = bool(self.downloader.active)\n-        pending_start_requests = self.slot.start_requests is not None\n-        idle = scraper_idle and not (pending or downloading or pending_start_requests)\n-        return idle\n+        if not self.scraper.slot.is_idle():\n+            # scraper is not idle\n+            return False\n+\n+        if self.downloader.active:\n+            # downloader has pending requests\n+            return False\n+\n+        if self.slot.start_requests is not None:\n+            # not all start requests are handled\n+            return False\n+\n+        if self.slot.scheduler.has_pending_requests():\n+            # scheduler has pending requests\n+            return False\n+\n+        return True\n \n     @property\n     def open_spiders(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#061c63592a3678e059fdeb964e6878663698237b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 57 | Churn Cumulative: 269 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: 0.9512195121951219\n\nDIFF:\n@@ -43,7 +43,7 @@ class MailSender(object):\n             settings['MAIL_PASS'], settings.getint('MAIL_PORT'),\n             settings.getbool('MAIL_TLS'), settings.getbool('MAIL_SSL'))\n \n-    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', _callback=None):\n+    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):\n         if attachs:\n             msg = MIMEMultipart()\n         else:\n@@ -57,8 +57,11 @@ class MailSender(object):\n             rcpts.extend(cc)\n             msg['Cc'] = COMMASPACE.join(cc)\n \n+        if charset:\n+            msg.set_charset(charset)\n+\n         if attachs:\n-            msg.attach(MIMEText(body))\n+            msg.attach(MIMEText(body, 'plain', charset or 'us-ascii'))\n             for attach_name, mimetype, f in attachs:\n                 part = MIMEBase(*mimetype.split('/'))\n                 part.set_payload(f.read())\n\n@@ -1,5 +1,8 @@\n+# coding=utf-8\n+\n import unittest\n from io import BytesIO\n+from email.charset import Charset\n \n from scrapy.mail import MailSender\n \n@@ -54,11 +57,58 @@ class MailSenderTest(unittest.TestCase):\n \n         text, attach = payload\n         self.assertEqual(text.get_payload(decode=True), b'body')\n+        self.assertEqual(text.get_charset(), Charset('us-ascii'))\n         self.assertEqual(attach.get_payload(decode=True), b'content')\n \n     def _catch_mail_sent(self, **kwargs):\n         self.catched_msg = dict(**kwargs)\n \n+    def test_send_utf8(self):\n+        subject = u'sübjèçt'\n+        body = u'bödÿ-àéïöñß'\n+        mailsender = MailSender(debug=True)\n+        mailsender.send(to=['test@scrapy.org'], subject=subject, body=body,\n+                        charset='utf-8', _callback=self._catch_mail_sent)\n+\n+        assert self.catched_msg\n+        self.assertEqual(self.catched_msg['subject'], subject)\n+        self.assertEqual(self.catched_msg['body'], body)\n+\n+        msg = self.catched_msg['msg']\n+        self.assertEqual(msg['subject'], subject)\n+        self.assertEqual(msg.get_payload(), body)\n+        self.assertEqual(msg.get_charset(), Charset('utf-8'))\n+        self.assertEqual(msg.get('Content-Type'), 'text/plain; charset=\"utf-8\"')\n+\n+    def test_send_attach_utf8(self):\n+        subject = u'sübjèçt'\n+        body = u'bödÿ-àéïöñß'\n+        attach = BytesIO()\n+        attach.write(body.encode('utf-8'))\n+        attach.seek(0)\n+        attachs = [('attachment', 'text/plain', attach)]\n+\n+        mailsender = MailSender(debug=True)\n+        mailsender.send(to=['test@scrapy.org'], subject=subject, body=body,\n+                        attachs=attachs, charset='utf-8', _callback=self._catch_mail_sent)\n+\n+        assert self.catched_msg\n+        self.assertEqual(self.catched_msg['subject'], subject)\n+        self.assertEqual(self.catched_msg['body'], body)\n+\n+        msg = self.catched_msg['msg']\n+        self.assertEqual(msg['subject'], subject)\n+        self.assertEqual(msg.get_charset(), Charset('utf-8'))\n+        self.assertEqual(msg.get('Content-Type'), 'multipart/mixed; charset=\"utf-8\"')\n+\n+        payload = msg.get_payload()\n+        assert isinstance(payload, list)\n+        self.assertEqual(len(payload), 2)\n+\n+        text, attach = payload\n+        self.assertEqual(text.get_payload(decode=True).decode('utf-8'), body)\n+        self.assertEqual(text.get_charset(), Charset('utf-8'))\n+        self.assertEqual(attach.get_payload(decode=True).decode('utf-8'), body)\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#25c56159b86288311630cc0cf6db9d755aeeff1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 187 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,7 +32,7 @@ class SitemapSpider(Spider):\n \n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n-            for url in sitemap_urls_from_robots(response.body):\n+            for url in sitemap_urls_from_robots(response.text):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)\n\n@@ -328,6 +328,18 @@ class SitemapSpiderTest(SpiderTest):\n         r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.GZBODY)\n         self.assertSitemapBody(r, self.BODY)\n \n+    def test_get_sitemap_urls_from_robotstxt(self):\n+        robots = b\"\"\"# Sitemap files\n+Sitemap: http://example.com/sitemap.xml\n+Sitemap: http://example.com/sitemap-product-index.xml\n+\"\"\"\n+\n+        r = TextResponse(url=\"http://www.example.com/robots.txt\", body=robots)\n+        spider = self.spider_class(\"example.com\")\n+        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n+                         ['http://example.com/sitemap.xml',\n+                          'http://example.com/sitemap-product-index.xml'])\n+\n \n class BaseSpiderDeprecationTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1cb841bfb3c51ed0f6d5408b79af2086c771da86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 38 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ import logging\n import posixpath\n from tempfile import TemporaryFile\n from datetime import datetime\n+import six\n from six.moves.urllib.parse import urlparse\n from ftplib import FTP\n \n@@ -57,7 +58,9 @@ class BlockingFeedStorage(object):\n @implementer(IFeedStorage)\n class StdoutFeedStorage(object):\n \n-    def __init__(self, uri, _stdout=sys.stdout):\n+    def __init__(self, uri, _stdout=None):\n+        if not _stdout:\n+            _stdout = sys.stdout if six.PY2 else sys.stdout.buffer\n         self._stdout = _stdout\n \n     def open(self, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f743996ff00a7b728d59b93d0967e1eb50072f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 26 | Churn Cumulative: 180 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -137,13 +137,29 @@ class WrappedRequest(object):\n         \"\"\"\n         return self.request.meta.get('is_unverifiable', False)\n \n-    # python3 uses request.unverifiable\n+    def get_origin_req_host(self):\n+        return urlparse_cached(self.request).hostname\n+\n+    # python3 uses attributes instead of methods\n+    @property\n+    def full_url(self):\n+        return self.get_full_url()\n+\n+    @property\n+    def host(self):\n+        return self.get_host()\n+\n+    @property\n+    def type(self):\n+        return self.get_type()\n+\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n \n-    def get_origin_req_host(self):\n-        return urlparse_cached(self.request).hostname\n+    @property\n+    def origin_req_host(self):\n+        return self.get_origin_req_host()\n \n     def has_header(self, name):\n         return name in self.request.headers\n\n@@ -14,12 +14,15 @@ class WrappedRequestTest(TestCase):\n \n     def test_get_full_url(self):\n         self.assertEqual(self.wrapped.get_full_url(), self.request.url)\n+        self.assertEqual(self.wrapped.full_url, self.request.url)\n \n     def test_get_host(self):\n         self.assertEqual(self.wrapped.get_host(), urlparse(self.request.url).netloc)\n+        self.assertEqual(self.wrapped.host, urlparse(self.request.url).netloc)\n \n     def test_get_type(self):\n         self.assertEqual(self.wrapped.get_type(), urlparse(self.request.url).scheme)\n+        self.assertEqual(self.wrapped.type, urlparse(self.request.url).scheme)\n \n     def test_is_unverifiable(self):\n         self.assertFalse(self.wrapped.is_unverifiable())\n@@ -32,6 +35,7 @@ class WrappedRequestTest(TestCase):\n \n     def test_get_origin_req_host(self):\n         self.assertEqual(self.wrapped.get_origin_req_host(), 'www.example.com')\n+        self.assertEqual(self.wrapped.origin_req_host, 'www.example.com')\n \n     def test_has_header(self):\n         self.assertTrue(self.wrapped.has_header('content-type'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#164493df2e9f63fd7895ca50677c9f805c89201f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 31 | Churn Cumulative: 31 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,19 @@\n+from __future__ import absolute_import\n+\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n+from pydispatch import (\n+    dispatcher,\n+    errors,\n+    robust,\n+    robustapply,\n+    saferef,\n+)\n+\n+warnings.warn(\"Importing from scrapy.xlib.pydispatch is deprecated and will\"\n+              \" no longer be supported in future Scrapy versions.\"\n+              \" If you just want to connect signals use the from_crawler class method,\"\n+              \" otherwise import pydispatch directly if needed.\"\n+              \" See: https://github.com/scrapy/scrapy/issues/1762\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -0,0 +1,12 @@\n+import unittest\n+import warnings\n+from six.moves import reload_module\n+\n+\n+class DeprecatedPydispatchTest(unittest.TestCase):\n+    def test_import_xlib_pydispatch_show_warning(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            from scrapy.xlib import pydispatch\n+            reload_module(pydispatch)\n+        self.assertIn('Importing from scrapy.xlib.pydispatch is deprecated',\n+                      str(w[0].message))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#467553cc2922d1d844fa136a036716eef3158eb9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 577 | Contributors (this commit): 9 | Commits (past 90d): 21 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -47,9 +47,10 @@ class S3DownloadHandler(object):\n         # If no credentials could be found anywhere,\n         # consider this an anonymous connection request by default;\n         # unless 'anon' was set explicitly (True/False).\n-        anon = kw.get('anon', None)\n+        anon = kw.get('anon')\n         if anon is None and not aws_access_key_id and not aws_secret_access_key:\n             kw['anon'] = True\n+        self.anon = kw.get('anon')\n \n         self._signer = None\n         try:\n@@ -80,7 +81,9 @@ class S3DownloadHandler(object):\n         bucket = p.hostname\n         path = p.path + '?' + p.query if p.query else p.path\n         url = '%s://%s.s3.amazonaws.com%s' % (scheme, bucket, path)\n-        if self._signer is not None:\n+        if self.anon:\n+            request = request.replace(url=url)\n+        elif self._signer is not None:\n             import botocore.awsrequest\n             from botocore.vendored.requests.structures import CaseInsensitiveDict\n             print(url, request.headers)\n\n@@ -458,8 +458,10 @@ class S3AnonTestCase(BaseS3TestCase):\n     def test_anon_request(self):\n         req = Request('s3://aws-publicdatasets/')\n         httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(hasattr(self.s3reqh.conn, 'anon'), True)\n-        self.assertEqual(self.s3reqh.conn.anon, True)\n+        self.assertEqual(hasattr(self.s3reqh, 'anon'), True)\n+        self.assertEqual(self.s3reqh.anon, True)\n+        self.assertEqual(\n+            httpreq.url, 'http://aws-publicdatasets.s3.amazonaws.com/')\n \n \n class S3TestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b1092b7d073320531986b35461ca5633dc2829a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 14 | Churn Cumulative: 243 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -85,15 +85,10 @@ class S3DownloadHandler(object):\n             request = request.replace(url=url)\n         elif self._signer is not None:\n             import botocore.awsrequest\n-            from botocore.vendored.requests.structures import CaseInsensitiveDict\n-            print(url, request.headers)\n             awsrequest = botocore.awsrequest.AWSRequest(\n                 method=request.method,\n                 url='%s://s3.amazonaws.com/%s%s' % (scheme, bucket, path),\n-                # TODO - move to a header method\n-                headers=CaseInsensitiveDict(\n-                    (to_unicode(key), to_unicode(b','.join(value)))\n-                    for key, value in request.headers.items()),\n+                headers=request.headers.to_native_string_dict(),\n                 data=request.body)\n             self._signer.add_auth(awsrequest)\n             request = request.replace(\n\n@@ -1,6 +1,7 @@\n import six\n from w3lib.http import headers_dict_to_raw\n from scrapy.utils.datatypes import CaselessDict\n+from scrapy.utils.python import to_unicode\n \n \n class Headers(CaselessDict):\n@@ -78,6 +79,12 @@ class Headers(CaselessDict):\n     def to_string(self):\n         return headers_dict_to_raw(self)\n \n+    def to_native_string_dict(self):\n+        return CaselessDict(\n+            (to_unicode(key, encoding=self.encoding),\n+             to_unicode(b','.join(value), encoding=self.encoding))\n+            for key, value in self.items())\n+\n     def __copy__(self):\n         return self.__class__(self)\n     copy = __copy__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
