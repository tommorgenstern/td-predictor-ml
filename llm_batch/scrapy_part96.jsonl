{"custom_id": "scrapy#075b59f4af6ad96406c03fba1e529ef1e34cc73a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 14 | Methods Changed: 13 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 66 | Churn Cumulative: 126 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import sys, os\n \n from twisted.python import log\n-from twisted.internet import reactor, protocol, error\n+from twisted.internet import reactor, defer, protocol, error\n from twisted.application.service import Service\n \n from scrapy.utils.py26 import cpu_count\n@@ -11,50 +11,58 @@ from scrapy.conf import settings\n class ScrapyService(Service):\n \n     def startService(self):\n-        reactor.callWhenRunning(self.start_processes)\n+        reactor.callWhenRunning(self.spawn_processes)\n \n-    def start_processes(self):\n-        for i in range(cpu_count()):\n-            self.start_process(i+1)\n+    def spawn_processes(self):\n+        for settings_module, count in settings['PROJECTS'].items():\n+            for i in range(count or cpu_count()):\n+                self.spawn_process(settings_module, i)\n \n-    def start_process(self, id):\n-        args = [sys.executable, '-m', 'scrapy.service']\n+    def spawn_process(self, settings_module, position):\n+        args = [sys.executable, '-m', 'scrapy.cmdline', 'start']\n         env = os.environ.copy()\n-        self.set_log_file(env, id)\n-        pp = ScrapyProcessProtocol(self, id, env.get('SCRAPY_LOG_FILE'))\n+        botname = self.get_bot_name(settings_module)\n+        logfile = self.get_log_file(settings_module, position)\n+        env['SCRAPY_SETTINGS_MODULE'] = settings_module\n+        env['SCRAPY_LOG_FILE'] = logfile\n+        pp = ScrapyProcessProtocol(self, botname, settings_module, logfile)\n         reactor.spawnProcess(pp, sys.executable, args=args, env=env)\n+        pp.deferred.addCallback(self.respawn_process, args, env)\n \n-    def set_log_file(self, env, suffix):\n-        logfile = settings['LOG_FILE']\n-        if logfile:\n-            file, ext = os.path.splitext(logfile)\n-            env['SCRAPY_LOG_FILE'] = \"%s-%s%s\" % (file, suffix, ext)\n+    def respawn_process(self, pp, args, env):\n+        reactor.callLater(5, reactor.spawnProcess, pp, sys.executable, \\\n+            args=args, env=env)\n \n+    def get_log_file(self, settings_module, position):\n+        botname = self.get_bot_name(settings_module)\n+        basename = \"%s-%s\" % (botname, position) if position else botname\n+        return os.path.join(settings['LOG_DIR'], \"%s.log\" % basename)\n+\n+    def get_bot_name(self, settings_module):\n+        mod = __import__(settings_module, {}, {}, [''], -1)\n+        return mod.BOT_NAME\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n-    def __init__(self, service, id, logfile):\n+    def __init__(self, service, botname, settings_module, logfile):\n         self.service = service\n-        self.id = id\n+        self.botname = botname\n+        self.settings_module = settings_module\n         self.logfile = logfile\n         self.pid = None\n+        self.deferred = defer.Deferred()\n \n     def connectionMade(self):\n         self.pid = self.transport.pid\n-        log.msg(\"Process %r started: pid=%r logfile=%r\" % (self.id, self.pid, \\\n-            self.logfile))\n+        log.msg(\"Crawler %r started: pid=%r settings=%r log=%r\" % \\\n+            (self.botname, self.pid, self.settings_module, self.logfile))\n \n     def processEnded(self, status):\n         if isinstance(status.value, error.ProcessDone):\n-            log.msg(\"Process %r finished: pid=%r logfile=%r\" % (self.id, \\\n-                self.pid, self.logfile))\n+            log.msg(\"Crawler %r finished: pid=%r settings=%r log=%r\" % \\\n+                (self.botname, self.pid, self.settings_module, self.logfile))\n         else:\n-            log.msg(\"Process %r died: exitstatus=%r pid=%r logfile=%r\" % \\\n-                (self.id, status.value.exitCode, self.pid, self.logfile))\n-        reactor.callLater(5, self.service.start_process, self.id)\n-\n-\n-if __name__ == '__main__':\n-    from scrapy.core.manager import scrapymanager\n-    scrapymanager.configure()\n-    scrapymanager.start(keep_alive=True)\n+            log.msg(\"Crawler %r died: exitstatus=%r pid=%r settings=%r log=%r\" % \\\n+                (self.botname, status.value.exitCode, self.pid, self.settings_module, \\\n+                self.logfile))\n+        self.deferred.callback(self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a33e8b507f5d9721fd2c014bd4bbb8299d6f05c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 12 | Churn Cumulative: 138 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -25,13 +25,10 @@ class ScrapyService(Service):\n         logfile = self.get_log_file(settings_module, position)\n         env['SCRAPY_SETTINGS_MODULE'] = settings_module\n         env['SCRAPY_LOG_FILE'] = logfile\n-        pp = ScrapyProcessProtocol(self, botname, settings_module, logfile)\n+        pp = ScrapyProcessProtocol(botname, settings_module, logfile)\n+        pp.deferred.addCallback(lambda _: reactor.callLater(5, \\\n+            self.spawn_process, settings_module, position))\n         reactor.spawnProcess(pp, sys.executable, args=args, env=env)\n-        pp.deferred.addCallback(self.respawn_process, args, env)\n-\n-    def respawn_process(self, pp, args, env):\n-        reactor.callLater(5, reactor.spawnProcess, pp, sys.executable, \\\n-            args=args, env=env)\n \n     def get_log_file(self, settings_module, position):\n         botname = self.get_bot_name(settings_module)\n@@ -44,8 +41,7 @@ class ScrapyService(Service):\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n-    def __init__(self, service, botname, settings_module, logfile):\n-        self.service = service\n+    def __init__(self, botname, settings_module, logfile):\n         self.botname = botname\n         self.settings_module = settings_module\n         self.logfile = logfile\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8b80f3e2f675fda2c06427db3fca4d6baa0bd15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 31 | Churn Cumulative: 169 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,12 +41,22 @@ class ScrapyService(Service):\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n+    TAIL_LINES = 100\n+\n     def __init__(self, botname, settings_module, logfile):\n         self.botname = botname\n         self.settings_module = settings_module\n         self.logfile = logfile\n         self.pid = None\n         self.deferred = defer.Deferred()\n+        self.outdata = ''\n+        self.errdata = ''\n+\n+    def outReceived(self, data):\n+        self.outdata = self._tail(self.outdata + data)\n+\n+    def errReceived(self, data):\n+        self.errdata = self._tail(self.errdata + data)\n \n     def connectionMade(self):\n         self.pid = self.transport.pid\n@@ -55,10 +65,23 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n     def processEnded(self, status):\n         if isinstance(status.value, error.ProcessDone):\n-            log.msg(\"Crawler %r finished: pid=%r settings=%r log=%r\" % \\\n-                (self.botname, self.pid, self.settings_module, self.logfile))\n+            msg = \"Crawler %r finished: pid=%r settings=%r log=%r\" % \\\n+                (self.botname, self.pid, self.settings_module, self.logfile)\n         else:\n-            log.msg(\"Crawler %r died: exitstatus=%r pid=%r settings=%r log=%r\" % \\\n+            msg = \"Crawler %r died: exitstatus=%r pid=%r settings=%r log=%r\" % \\\n                 (self.botname, status.value.exitCode, self.pid, self.settings_module, \\\n-                self.logfile))\n+                self.logfile)\n+        self._log_ended(msg)\n         self.deferred.callback(self)\n+\n+    def _log_ended(self, msg):\n+        if self.outdata:\n+            msg += \"\\n>>> stdout (last %d lines) <<<\\n%s\" % (self.TAIL_LINES, \\\n+                self.outdata)\n+        if self.errdata:\n+            msg += \"\\n>>> stderr (last %d lines) <<<\\n%s\" % (self.TAIL_LINES, \\\n+                self.errdata)\n+        log.msg(msg)\n+\n+    def _tail(self, data, lines=TAIL_LINES):\n+        return os.linesep.join(data.splitlines()[-lines:])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d76276408e3d0a4ad5fee8bd6e6b151667136a9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 182 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,13 +75,14 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.deferred.callback(self)\n \n     def _log_ended(self, msg):\n+        tolog = [msg]\n         if self.outdata:\n-            msg += \"\\n>>> stdout (last %d lines) <<<\\n%s\" % (self.TAIL_LINES, \\\n-                self.outdata)\n+            tolog += [\">>> stdout (last %d lines) <<<\" % self.TAIL_LINES]\n+            tolog += [self.outdata]\n         if self.errdata:\n-            msg += \"\\n>>> stderr (last %d lines) <<<\\n%s\" % (self.TAIL_LINES, \\\n-                self.errdata)\n-        log.msg(msg)\n+            tolog += [\">>> stderr (last %d lines) <<<\" % self.TAIL_LINES]\n+            tolog += [self.errdata]\n+        log.msg(os.linesep.join(tolog))\n \n     def _tail(self, data, lines=TAIL_LINES):\n-        return os.linesep.join(data.splitlines()[-lines:])\n+        return os.linesep.join(data.split(os.linesep)[-lines:])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d13b50a234d1045fe916c68ebea1b73b32c29bb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 13 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,13 @@\n+# Scrapy service configuration\n+\n+# Directory where logs will be stored (one per crawler)\n+LOG_DIR = '/var/log/scrapy'\n+\n+# A dict containing the Scrapy projects that will be run by this service\n+# * Keys are paths to project settings modules\n+# * Values are number of processes that should be started for each project, or\n+#   zero if you want to use the number of cores available.\n+PROJECTS = {\n+#        'mybot.settings': 0,\n+}\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea8b5ddfd56471c987424b55092bfc1bf9b165ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 89 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -92,7 +92,7 @@ setup_args = {\n     'packages': packages,\n     'cmdclass': cmdclasses,\n     'data_files': data_files,\n-    'scripts': ['bin/scrapy-ctl.py'],\n+    'scripts': ['bin/scrapy-ctl.py', 'bin/scrapy-ws.py'],\n     'classifiers': [\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 2.5',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#efe9811d92c03b1f6f18c37fc36dc4768885225e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 699 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -111,7 +111,7 @@ class InstanceBasedLearningExtractor(object):\n             correctly_extracted = [c for c in correctly_extracted if \\\n                 extra_required.intersection(c.keys()) == extra_required ]\n             if len(correctly_extracted) > 0:\n-                return correctly_extracted, extraction_tree.template.id\n+                return correctly_extracted, extraction_tree.template\n         return None, None\n \n     def __str__(self):\n\n@@ -190,10 +190,11 @@ class AnnotationTag(object):\n                          for each item to be extracted from a tag attribute\n         annotation_text - text prefix and suffix for the attribute to be extracted\n         match_common_prefix - use this annotation for calculating across-template prefixes\n+        metadata - dict with annotation data not used by IBL extractor\n     \"\"\"\n     __slots__ = ('surrounds_attribute', 'start_index', 'end_index',\n             'tag_attributes', 'annotation_text', 'variant_id', \n-            'surrounds_variant','match_common_prefix')\n+            'surrounds_variant','match_common_prefix', 'metadata')\n     \n     def __init__(self, start_index, end_index, surrounds_attribute=None, \n             annotation_text=None, tag_attributes=None, variant_id=None,\n@@ -206,6 +207,7 @@ class AnnotationTag(object):\n         self.variant_id = variant_id\n         self.surrounds_variant = surrounds_variant\n         self.match_common_prefix = match_common_prefix\n+        self.metadata = {}\n \n     def __str__(self):\n         return \"AnnotationTag(%s)\" % \", \".join(\n\n@@ -125,7 +125,7 @@ class TemplatePageParser(InstanceLearningParser):\n                 self._close_unpaired_tag()\n                 \n             annotation = AnnotationTag(self.next_tag_index, self.next_tag_index + 1)\n-            attribute_annotations = jannotation.get('annotations', {}).items()\n+            attribute_annotations = jannotation.pop('annotations', {}).items()\n             for extract_attribute, tag_value in attribute_annotations:\n                 if extract_attribute == 'content':\n                     annotation.surrounds_attribute = tag_value\n@@ -133,10 +133,11 @@ class TemplatePageParser(InstanceLearningParser):\n                 else:\n                     annotation.tag_attributes.append((extract_attribute, tag_value))\n             self.annotations.append(annotation)\n-            if jannotation.get('common_prefix', False):\n+            if jannotation.pop('common_prefix', False):\n                 annotation.match_common_prefix = True\n \n-            self.extra_required_attrs.extend(jannotation.get('required', []))\n+            self.extra_required_attrs.extend(jannotation.pop('required', []))\n+            annotation.metadata = jannotation\n \n         self.next_tag_index += 1\n \n@@ -183,7 +184,7 @@ class TemplatePageParser(InstanceLearningParser):\n             return\n         \n         annotation = AnnotationTag(self.next_tag_index, None)\n-        if jannotation.get('generated', False):\n+        if jannotation.pop('generated', False):\n             self.token_list.pop()\n             annotation.start_index -= 1\n             if self.previous_element_class == HtmlTag:\n@@ -195,22 +196,24 @@ class TemplatePageParser(InstanceLearningParser):\n                 ignored = self.ignored_regions.pop()\n                 self.ignored_regions.append((ignored[0]-1, ignored[1]))\n                 \n-        if jannotation.get('common_prefix', False):\n+        if jannotation.pop('common_prefix', False):\n             annotation.match_common_prefix = True\n                 \n-        self.extra_required_attrs.extend(jannotation.get('required', []))\n+        self.extra_required_attrs.extend(jannotation.pop('required', []))\n         \n-        variant_id = jannotation.get('variant', 0)\n+        variant_id = jannotation.pop('variant', 0)\n         if variant_id > 0:\n             self.variant_stack.append(variant_id)\n             annotation.surrounds_variant = variant_id\n-        attribute_annotations = jannotation.get('annotations', {}).items()\n+        attribute_annotations = jannotation.pop('annotations', {}).items()\n         for extract_attribute, tag_value in attribute_annotations:\n             if extract_attribute == 'content':\n                 annotation.surrounds_attribute = tag_value\n             else:\n                 annotation.tag_attributes.append((extract_attribute, tag_value))\n         \n+        annotation.metadata = jannotation\n+\n         if annotation.annotation_text is None:\n             self.next_tag_index += 1\n         if self.variant_stack:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ed5d7561f9c4087f825ac7aa03e287182d87b35d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 4 | Files Changed: 6 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 12/10 | Churn Δ: 95 | Churn Cumulative: 517 | Contributors (this commit): 4 | Commits (past 90d): 15 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,30 @@\n+#!/usr/bin/env python\n+\n+import sys\n+\n+from boto.sqs.connection import SQSConnection\n+from boto.sqs.message import Message\n+\n+from scrapy.utils.py26 import json\n+from scrapy.conf import settings\n+\n+qname = settings['SQS_QUEUE']\n+\n+if len(sys.argv) <= 1:\n+    print \"usage: %s <command> [args]\" % sys.argv[0]\n+    print\n+    print \"available commands:\"\n+    print \"  put <spider_name> - append spider to queue\"\n+    print\n+    print \"SQS queue: %s\" % qname\n+    print\n+    sys.exit()\n+\n+cmd, args = sys.argv[1], sys.argv[2:]\n+\n+if cmd == 'put':\n+    conn = SQSConnection(settings['AWS_ACCESS_KEY_ID'], \\\n+        settings['AWS_SECRET_ACCESS_KEY'])\n+    q = conn.create_queue(qname)\n+    msg = Message(body=json.dumps({'name': args[0]}))\n+    q.write(msg)\n\n@@ -1,6 +1,7 @@\n-from scrapy.core.queue import KeepAliveExecutionQueue\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n \n class Command(ScrapyCommand):\n \n@@ -10,6 +11,6 @@ class Command(ScrapyCommand):\n         return \"Start the Scrapy manager but don't run any spider (idle mode)\"\n \n     def run(self, args, opts):\n-        q = KeepAliveExecutionQueue()\n-        scrapymanager.queue = q\n+        queue_class = load_object(settings['SERVICE_QUEUE'])\n+        scrapymanager.queue = queue_class()\n         scrapymanager.start()\n\n@@ -185,6 +185,8 @@ SCHEDULER_MIDDLEWARES_BASE = {\n \n SCHEDULER_ORDER = 'DFO'\n \n+SERVICE_QUEUE = 'scrapy.core.queue.KeepAliveExecutionQueue'\n+\n SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.TwistedPluginSpiderManager'\n \n SPIDER_MIDDLEWARES = {}\n@@ -205,6 +207,11 @@ SPIDER_MODULES = []\n \n SPIDERPROFILER_ENABLED = False\n \n+SQS_QUEUE = 'scrapy'\n+SQS_VISIBILITY_TIMEOUT = 7200\n+SQS_POLLING_DELAY = 30\n+SQS_REGION = 'us-east-1'\n+\n STATS_CLASS = 'scrapy.stats.collector.MemoryStatsCollector'\n STATS_ENABLED = True\n STATS_DUMP = False\n\n\n@@ -0,0 +1,49 @@\n+import threading\n+\n+from twisted.internet import threads\n+from boto.sqs.connection import SQSConnection\n+from boto.sqs import regions\n+\n+from scrapy.core.queue import ExecutionQueue\n+from scrapy.utils.py26 import json\n+from scrapy.conf import settings\n+\n+class SQSExecutionQueue(ExecutionQueue):\n+\n+    polling_delay = settings.getint('SQS_POLLING_DELAY')\n+    queue_name = settings['SQS_QUEUE']\n+    region_name = settings['SQS_REGION']\n+    visibility_timeout = settings.getint('SQS_VISIBILITY_TIMEOUT')\n+    aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n+    aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n+\n+    def __init__(self, *a, **kw):\n+        super(SQSExecutionQueue, self).__init__(*a, **kw)\n+        self.region = self._get_region()\n+        self._tls = threading.local()\n+\n+    def _append_next(self):\n+        return threads.deferToThread(self._append_next_from_sqs)\n+\n+    def _append_next_from_sqs(self):\n+        q = self._get_sqs_queue()\n+        msgs = q.get_messages(1, visibility_timeout=self.visibility_timeout)\n+        if msgs:\n+            msg = msgs[0]\n+            msg.delete()\n+            spargs = json.loads(msg.get_body())\n+            spname = spargs.pop('name')\n+            self.append_spider_name(spname, **spargs)\n+\n+    def _get_sqs_queue(self):\n+        if not hasattr(self._tls, 'queue'):\n+            c = SQSConnection(self.aws_access_key_id, self.aws_secret_access_key, \\\n+                region=self.region)\n+            self._tls.queue = c.create_queue(self.queue_name)\n+        return self._tls.queue\n+\n+    def _get_region(self, name=region_name):\n+        return [r for r in regions() if r.name == name][0]\n+\n+    def is_finished(self):\n+        return False\n\n@@ -15,7 +15,7 @@ class ExecutionQueue(object):\n         self._spiders = _spiders\n \n     def _append_next(self):\n-        \"\"\"Called when there are no more itemsl left in self.spider_requests.\n+        \"\"\"Called when there are no more items left in self.spider_requests.\n         This method is meant to be overriden in subclasses to add new (spider,\n         requests) tuples to self.spider_requests. It can return a Deferred.\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b083911e64e3a19413f9fbc5e54418518772970", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 119 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ def get_commands():\n     return {\n         'help': cmd_help,\n         'run': cmd_run,\n+        'stop': cmd_stop,\n         'list-available': cmd_list_available,\n         'list-running': cmd_list_running,\n         'list-resources': cmd_list_resources,\n@@ -33,6 +34,10 @@ def cmd_run(args, opts):\n     \"\"\"run <spider_name> - schedule spider for running\"\"\"\n     jsonrpc_call(opts, 'manager/queue', 'append_spider_name', args[0])\n \n+def cmd_stop(args, opts):\n+    \"\"\"stop <spider> - stop a running spider\"\"\"\n+    jsonrpc_call(opts, 'manager/engine', 'close_spider', args[0])\n+\n def cmd_list_running(args, opts):\n     \"\"\"list-running - list running spiders\"\"\"\n     for x in json_get(opts, 'manager/engine/open_spiders'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#495f23dea2d414ddb2710a29d0f58998cf4c3fa4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 258 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.6\n\nDIFF:\n@@ -2,6 +2,8 @@ import unittest\n import datetime\n from decimal import Decimal\n \n+from twisted.internet import defer\n+\n from scrapy.utils.serialize import SpiderReferencer, ScrapyJSONEncoder, ScrapyJSONDecoder\n from scrapy.utils.py26 import json\n from scrapy.spider import BaseSpider\n@@ -45,6 +47,10 @@ class SpiderReferencerTestCase(BaseTestCase):\n         assert sp1 is not sp2\n         assert sp1 is sp1_\n \n+        # referring to spiders by name\n+        assert sp1 is self.spref.get_spider_from_reference('spider::name1')\n+        assert sp2 is self.spref.get_spider_from_reference('spider::name2')\n+\n         # must return string as-is if spider id not found\n         assert 'lala' == self.spref.get_spider_from_reference('lala')\n         # must raise RuntimeError if spider id is not found and spider is not running\n@@ -107,6 +113,8 @@ class JsonEncoderTestCase(BaseTestCase):\n         for spiders, refs in examples_encode_only:\n             self.assertEqual(self.encoder.encode(spiders), json.dumps(refs))\n \n+        assert 'Deferred' in self.encoder.encode(defer.Deferred())\n+\n     def test_encode_request(self):\n         r = Request(\"http://www.example.com/lala\")\n         rs = self.encoder.encode(r)\n\n@@ -2,6 +2,8 @@ import re\n import datetime\n import decimal\n \n+from twisted.internet import defer\n+\n from scrapy.core.manager import scrapymanager\n from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n@@ -16,7 +18,7 @@ class SpiderReferencer(object):\n     ScrapyJSONEncoder.default() with a custom encoding mechanism.\n     \"\"\"\n \n-    spider_ref_re = re.compile('^spider:([0-9a-f]+)(:.*)?$')\n+    spider_ref_re = re.compile('^spider:([0-9a-f]+)?:?(.+)?$')\n \n     def __init__(self, manager=None):\n         self.manager = manager or scrapymanager\n@@ -31,9 +33,9 @@ class SpiderReferencer(object):\n         \"\"\"\n         m = self.spider_ref_re.search(ref)\n         if m:\n-            spid = int(m.group(1), 16)\n+            spid, spname = m.groups()\n             for spider in self.manager.engine.open_spiders:\n-                if id(spider) == spid:\n+                if \"%x\" % id(spider) == spid or spider.name == spname:\n                     return spider\n             raise RuntimeError(\"Spider not running: %s\" % ref)\n         return ref\n@@ -94,6 +96,8 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n             return o.strftime(self.TIME_FORMAT)\n         elif isinstance(o, decimal.Decimal):\n             return str(o)\n+        elif isinstance(o, defer.Deferred):\n+            return str(o)\n         elif isinstance(o, Request):\n             return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n         elif isinstance(o, Response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd16d1cd488eca45eeac079bb4644388fd9d6214", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 29 | Churn Cumulative: 418 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -138,7 +138,10 @@ LOG_FILE = None\n \n MAIL_DEBUG = False\n MAIL_HOST = 'localhost'\n+MAIL_PORT = 25\n MAIL_FROM = 'scrapy@localhost'\n+MAIL_PASS = None\n+MAIL_USER = None\n \n MEMDEBUG_ENABLED = False        # enable memory debugging\n MEMDEBUG_NOTIFY = []            # send memory debugging report by mail at engine shutdown\n\n@@ -12,7 +12,7 @@ from email.Utils import COMMASPACE, formatdate\n from email import Encoders\n \n from twisted.internet import defer, reactor\n-from twisted.mail.smtp import SMTPSenderFactory\n+from twisted.mail.smtp import ESMTPSenderFactory\n \n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n@@ -27,9 +27,13 @@ mail_sent = object()\n \n class MailSender(object):\n \n-    def __init__(self, smtphost=None, mailfrom=None):\n-        self.smtphost = smtphost if smtphost else settings['MAIL_HOST']\n-        self.mailfrom = mailfrom if mailfrom else settings['MAIL_FROM']\n+    def __init__(self, smtphost=None, mailfrom=None, smtpuser=None, smtppass=None, \\\n+            smtpport=None):\n+        self.smtphost = smtphost or settings['MAIL_HOST']\n+        self.smtpport = smtpport or settings.getint('MAIL_PORT')\n+        self.smtpuser = smtpuser or settings['MAIL_USER']\n+        self.smtppass = smtppass or settings['MAIL_PASS']\n+        self.mailfrom = mailfrom or settings['MAIL_FROM']\n \n         if not self.smtphost or not self.mailfrom:\n             raise NotConfigured(\"MAIL_HOST and MAIL_FROM settings are required\")\n@@ -68,11 +72,12 @@ class MailSender(object):\n                 (to, cc, subject, len(attachs)), level=log.DEBUG)\n             return\n \n-        dfd = self._sendmail(self.smtphost, self.mailfrom, rcpts, msg.as_string())\n+        dfd = self._sendmail(rcpts, msg.as_string())\n         dfd.addCallbacks(self._sent_ok, self._sent_failed,\n             callbackArgs=[to, cc, subject, len(attachs)],\n             errbackArgs=[to, cc, subject, len(attachs)])\n         reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)\n+        return dfd\n \n     def _sent_ok(self, result, to, cc, subject, nattachs):\n         log.msg('Mail sent OK: To=%s Cc=%s Subject=\"%s\" Attachs=%d' % \\\n@@ -83,13 +88,12 @@ class MailSender(object):\n         log.msg('Unable to send mail: To=%s Cc=%s Subject=\"%s\" Attachs=%d - %s' % \\\n             (to, cc, subject, nattachs, errstr), level=log.ERROR)\n \n-    def _sendmail(self, smtphost, from_addr, to_addrs, msg, port=25):\n-        \"\"\" This is based on twisted.mail.smtp.sendmail except that it\n-        instantiates a quiet (noisy=False) SMTPSenderFactory \"\"\"\n+    def _sendmail(self, to_addrs, msg):\n         msg = StringIO(msg)\n         d = defer.Deferred()\n-        factory = SMTPSenderFactory(from_addr, to_addrs, msg, d)\n+        factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, \\\n+            to_addrs, msg, d, heloFallback=True, requireAuthentication=False, \\\n+            requireTransportSecurity=False)\n         factory.noisy = False\n-        reactor.connectTCP(smtphost, port, factory)\n+        reactor.connectTCP(self.smtphost, self.smtpport, factory)\n         return d\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91e1e0aff30c57b4cfb2ab72a42467d9a77763ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 79 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ class FilterWordsPipeline(object):\n     # put all words in lowercase\n     words_to_filter = ['politics', 'religion']\n \n-    def process_item(self, domain, item):\n+    def process_item(self, spider, item):\n         for word in self.words_to_filter:\n             if word in unicode(item['description']).lower():\n                 raise DropItem(\"Contains forbidden word: %s\" % word)\n\n@@ -7,7 +7,7 @@ from googledir.items import GoogledirItem\n class GoogleDirectorySpider(CrawlSpider):\n \n     name = 'directory.google.com'\n-    allow_domains = ['directory.google.com']\n+    allowed_domains = ['directory.google.com']\n     start_urls = ['http://directory.google.com/']\n \n     rules = (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#09182efaffe042991ecf9c7649af1df396159375", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 91 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -92,7 +92,7 @@ setup_args = {\n     'packages': packages,\n     'cmdclass': cmdclasses,\n     'data_files': data_files,\n-    'scripts': ['bin/scrapy-ctl.py', 'bin/scrapy-ws.py'],\n+    'scripts': ['bin/scrapy-ctl.py', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py'],\n     'classifiers': [\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 2.5',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ede1df4b4f4d6833bb4151126bf1695ce056cf9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 25 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ master_doc = 'index'\n \n # General information about the project.\n project = u'Scrapy'\n-copyright = u'2008-2009, Insophia'\n+copyright = u'2008-2010, Insophia'\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68e8e6ac114bd84157544976c4bde1a73758fa11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 93 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -76,8 +76,6 @@ if len(sys.argv) > 1 and sys.argv[1] == 'bdist_wininst':\n \n # Dynamically calculate the version based on scrapy.__version__\n version = __import__('scrapy').__version__\n-if u'SVN' in version:\n-    version = ' '.join(version.split(' ')[:-1])\n \n setup_args = {\n     'name': 'Scrapy',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d3d5b6abab6e0d03bb491103ad6a0cc4a9c660d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 100 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -76,6 +76,13 @@ if len(sys.argv) > 1 and sys.argv[1] == 'bdist_wininst':\n \n # Dynamically calculate the version based on scrapy.__version__\n version = __import__('scrapy').__version__\n+if 'dev' in version:\n+    try:\n+        from subprocess import Popen, PIPE\n+        hgrev = Popen(['hg', 'tip', '--template={rev}'], stdout=PIPE).communicate()[0]\n+        version = version.replace('-dev', '-r%s' % hgrev)\n+    except:\n+        pass\n \n setup_args = {\n     'name': 'Scrapy',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8ac4857a50064476713270d10b2ea0098fb85a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 7 | Churn Cumulative: 256 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.75\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import os\n import unittest\n import urlparse\n \n@@ -147,7 +148,10 @@ class ResponseUtilsTest(unittest.TestCase):\n         url = \"http:///www.example.com/some/page.html\"\n         body = \"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n         def browser_open(burl):\n-            bbody = open(urlparse.urlparse(burl).path).read()\n+            path = urlparse.urlparse(burl).path\n+            if not os.path.exists(path):\n+                path = burl.replace('file://', '')\n+            bbody = open(path).read()\n             assert '<base href=\"%s\">' % url in bbody, \"<base> tag not added\"\n             return True\n         response = HtmlResponse(url, body=body)\n\n@@ -67,6 +67,7 @@ def jsonrpc_server_call(target, jsonrpc_request, json_decoder=None):\n \n     params = req.get('params', [])\n     a, kw = ([], params) if isinstance(params, dict) else (params, {})\n+    kw = dict([(str(k), v) for k, v in kw.items()]) # convert kw keys to str\n     try:\n         return jsonrpc_result(id, method(*a, **kw))\n     except Exception, e:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f3d2ee41ccd6b48fc3ffe7c27d3f642030465828", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 67 | Files Changed: 2 | Hunks: 14 | Methods Changed: 18 | Complexity Δ (Sum/Max): 11/9 | Churn Δ: 168 | Churn Cumulative: 303 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,9 @@ from scrapy.utils.misc import arg_to_iter\n \n \n class MediaPipeline(object):\n+\n     DOWNLOAD_PRIORITY = 1000\n+    LOG_FAILED_RESULTS = True\n \n     class SpiderInfo(object):\n         def __init__(self, spider):\n@@ -33,26 +35,20 @@ class MediaPipeline(object):\n     def process_item(self, spider, item):\n         info = self.spiderinfo[spider]\n         requests = arg_to_iter(self.get_media_requests(item, info))\n-        dlist = []\n-        for request in requests:\n-            dfd = self._enqueue(request, info)\n-            dfd.addCallbacks(\n-                    callback=self.item_media_downloaded,\n-                    callbackArgs=(item, request, info),\n-                    errback=self.item_media_failed,\n-                    errbackArgs=(item, request, info),)\n-            dlist.append(dfd)\n-\n-        return DeferredList(dlist, consumeErrors=1).addCallback(self.item_completed, item, info)\n+        dlist = [self._enqueue(r, info) for r in requests]\n+        dfd = DeferredList(dlist, consumeErrors=1)\n+        return dfd.addCallback(self.item_completed, item, info)\n \n     def _enqueue(self, request, info):\n-        wad = Deferred().addCallbacks(request.callback, request.errback)\n         fp = request_fingerprint(request)\n+        cb = request.callback or (lambda _: _)\n+        eb = request.errback\n \n         # if already downloaded, return cached result.\n         if fp in info.downloaded:\n-            return defer_result(info.downloaded[fp]).chainDeferred(wad)\n+            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n \n+        wad = Deferred().addCallbacks(cb, eb)\n         # add to pending list for this request, and wait for result like the others.\n         info.waiting.setdefault(fp, []).append(wad)\n \n@@ -93,73 +89,31 @@ class MediaPipeline(object):\n \n     ### Overradiable Interface\n     def download(self, request, info):\n-        \"\"\" Defines how to request the download of media\n-\n-        Default gives high priority to media requests and use scheduler,\n-        shouldn't be necessary to override.\n-\n-        This methods is called only if result for request isn't cached,\n-        request fingerprint is used as cache key.\n-\n-        \"\"\"\n+        \"\"\"Defines how to download the media request\"\"\"\n         request.priority = self.DOWNLOAD_PRIORITY\n         return scrapymanager.engine.download(request, info.spider)\n \n     def media_to_download(self, request, info):\n-        \"\"\" Ongoing request hook pre-cache\n-\n-        This method is called every time a media is requested for download, and\n-        only once for the same request because return value is cached as media\n-        result.\n-\n-        returning a non-None value implies:\n-            - the return value is cached and piped into `item_media_downloaded` or `item_media_failed`\n-            - prevents downloading, this means calling `download` method.\n-            - `media_downloaded` or `media_failed` isn't called.\n-\n-        \"\"\"\n+        \"\"\"Check request before starting download\"\"\"\n+        pass\n \n     def get_media_requests(self, item, info):\n+        \"\"\"Returns the media requests to download\"\"\"\n         pass\n \n     def media_downloaded(self, response, request, info):\n-        \"\"\" Method called on success download of media request\n-\n-        Return value is cached and used as input for `item_media_downloaded` method.\n-        Default implementation returns None.\n-\n-        WARNING: returning the response object can eat your memory.\n-\n-        \"\"\"\n+        \"\"\"Handler for success downloads\"\"\"\n+        return response\n \n     def media_failed(self, failure, request, info):\n-        \"\"\" Method called when media request failed due to any kind of download error.\n-\n-        Return value is cached and used as input for `item_media_failed` method.\n-        Default implementation returns same Failure object.\n-        \"\"\"\n-        return failure\n-\n-    def item_media_downloaded(self, result, item, request, info):\n-        \"\"\" Method to handle result of requested media for item.\n-\n-        result is the return value of `media_downloaded` hook, or the non-Failure instance\n-        returned by `media_failed` hook.\n-\n-        return value of this method is used for results parameter of item_completed hook\n-        \"\"\"\n-        return result\n-\n-    def item_media_failed(self, failure, item, request, info):\n-        \"\"\" Method to handle failed result of requested media for item.\n-\n-        result is the returned Failure instance of `media_failed` hook, or Failure instance\n-        of an exception raised by `media_downloaded` hook.\n-\n-        return value of this method is used for results parameter of item_completed hook\n-        \"\"\"\n+        \"\"\"Handler for failed downloads\"\"\"\n         return failure\n \n     def item_completed(self, results, item, info):\n+        \"\"\"Called per item when all media requests has been processed\"\"\"\n+        if self.LOG_FAILED_RESULTS:\n+            for success, result in results:\n+                if not success:\n+                    log.err(result, '%s found errors proessing %s' % (self.__class__.__name__, item))\n         return item\n \n\n@@ -0,0 +1,80 @@\n+from twisted.trial import unittest\n+from twisted.python import failure\n+from twisted.internet import defer, reactor\n+\n+from scrapy.http import Request, Response\n+from scrapy.spider import BaseSpider\n+from scrapy.utils.request import request_fingerprint\n+from scrapy.contrib.pipeline.media import MediaPipeline\n+\n+\n+class _MockedMediaPipeline(MediaPipeline):\n+\n+    def download(self, request, info):\n+        delay = request.meta.get('delay')\n+        response = request.meta.get('response')\n+        if delay is None:\n+            return response\n+        else:\n+            dfd = defer.Deferred()\n+            reactor.callLater(delay, dfd.callback, None)\n+            return dfd.addCallback(lambda _: response)\n+\n+    def get_media_requests(self, item, info):\n+        return item.get('requests')\n+\n+\n+class MediaPipelineTestCase(unittest.TestCase):\n+\n+    pipeline_class = _MockedMediaPipeline\n+\n+    def setUp(self):\n+        self.spider = BaseSpider('media.com')\n+        self.pipe = self.pipeline_class()\n+        self.pipe.spider_opened(self.spider)\n+\n+    def tearDown(self):\n+        self.pipe.spider_closed(self.spider)\n+\n+    @defer.inlineCallbacks\n+    def test_return_item_by_default(self):\n+        item = dict(name='sofa')\n+        new_item = yield self.pipe.process_item(self.spider, item)\n+        assert new_item is item\n+\n+    @defer.inlineCallbacks\n+    def test_get_media_requests(self):\n+        # returns single Request (without callback)\n+        info = self.pipe.spiderinfo[self.spider]\n+        req = Request('http://media.com/2.gif')\n+        item = dict(requests=req) # pass a single item\n+        new_item = yield self.pipe.process_item(self.spider, item)\n+        assert new_item is item\n+        assert request_fingerprint(req) in info.downloaded\n+\n+        # returns iterable of Requests\n+        req1 = Request('http://media.com/1.gif')\n+        req2 = Request('http://media.com/1.jpg')\n+        item = dict(requests=iter([req1, req2]))\n+        new_item = yield self.pipe.process_item(self.spider, item)\n+        assert new_item is item\n+        assert info.downloaded.get(request_fingerprint(req1)) is None\n+        assert info.downloaded.get(request_fingerprint(req2)) is None\n+\n+    @defer.inlineCallbacks\n+    def test_requets_callback_is_called(self):\n+        collected = []\n+        response = Response('http://media.com/2.gif')\n+        request = Request('http://media.com/2.gif', meta=dict(response=response), callback=collected.append)\n+        item = dict(requests=request) # pass a single item\n+        yield self.pipe.process_item(self.spider, item)\n+        assert collected == [response]\n+\n+    @defer.inlineCallbacks\n+    def test_requets_errback_is_called(self):\n+        collected = []\n+        fail = failure.Failure(Exception())\n+        req = Request('http://media.com/2.gif', meta=dict(response=fail), callback=lambda _:_, errback=collected.append)\n+        item = dict(requests=req) # pass a single item\n+        yield self.pipe.process_item(self.spider, item)\n+        assert collected == [fail]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#61e374eb56a74822b3e248c04d62cf6c570ad373", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 57 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 9, 0, 'dev')\n-__version__ = \"0.9-dev\"\n+version_info = (0, 9, 0, 'rc1')\n+__version__ = \"0.9-rc1\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e665e5abb70eef0081bfad48abe7efe3a66344f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 61 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 9, 0, 'rc1')\n-__version__ = \"0.9-rc1\"\n+version_info = (0, 10, 0, 'dev')\n+__version__ = \"0.10-dev\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#baa523055f77087a2add4a8725f2ac4553e72bae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 23 | Files Changed: 4 | Hunks: 10 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 29 | Churn Cumulative: 1851 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,9 +8,5 @@ Based Learning (IBL) algorithm, as described in the following papers:\n     Extracting web data using instance based learning\n     http://portal.acm.org/citation.cfm?id=1265174 \n \n-This code has some additional dependencies too:\n-\n-* numpy\n-* nltk\n-\n+This code requires the numpy library.\n \"\"\"\n\n@@ -3,13 +3,13 @@ Region Extract\n \n Custom extraction for regions in a document\n \"\"\"\n+import re\n import operator\n import copy\n import pprint\n import cStringIO\n from itertools import groupby\n \n-import nltk\n from numpy import array\n \n from scrapy.contrib.ibl.descriptor import FieldDescriptor\n@@ -572,7 +572,8 @@ class TemplatePageExtractor(object):\n     def __str__(self):\n         return str(self.extractor)\n \n-_tokenize = nltk.tokenize.WordPunctTokenizer().tokenize\n+# Based on nltk's WordPunctTokenizer\n+_tokenize = re.compile(r'\\w+|[^\\w\\s]+', re.UNICODE | re.MULTILINE | re.DOTALL).findall\n \n class TextRegionDataExtractor(object):\n     \"\"\"Data Extractor for extracting text fragments from within a larger\n\n@@ -11,17 +11,12 @@ from scrapy.contrib.ibl.descriptor import (FieldDescriptor as A,\n from scrapy.contrib.ibl.extractors import (contains_any_numbers,\n         image_url)\n \n-try:\n-    import nltk\n-except ImportError:\n-    nltk = None\n-\n try:\n     import numpy\n except ImportError:\n     numpy = None\n \n-if nltk and numpy:\n+if numpy:\n     from scrapy.contrib.ibl.extraction import InstanceBasedLearningExtractor\n \n # simple page with all features\n@@ -791,8 +786,6 @@ TEST_DATA = [\n class TestExtraction(TestCase):\n \n     def setUp(self):\n-        if not nltk:\n-            raise SkipTest(\"nltk not available\")\n         if not numpy:\n             raise SkipTest(\"numpy not available\")\n \n\n@@ -12,17 +12,12 @@ from scrapy.utils.py26 import json\n from scrapy.contrib.ibl.htmlpage import HtmlPage\n from scrapy.tests.test_contrib_ibl import path\n \n-try:\n-    import nltk\n-except ImportError:\n-    nltk = None\n-\n try:\n     import numpy\n except ImportError:\n     numpy = None\n \n-if nltk and numpy:\n+if numpy:\n     from scrapy.contrib.ibl.extraction.pageparsing import (\n         InstanceLearningParser, TemplatePageParser, ExtractionPageParser)\n     from scrapy.contrib.ibl.extraction.pageobjects import TokenDict, TokenType\n@@ -174,8 +169,6 @@ def _tags(pp, predicate):\n class TestPageParsing(TestCase):\n \n     def setUp(self):\n-        if not nltk:\n-            raise SkipTest(\"nltk not available\")\n         if not numpy:\n             raise SkipTest(\"numpy not available\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
