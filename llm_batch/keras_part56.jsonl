{"custom_id": "keras#b372bd9dd427ff90792a2db007d2764eb1b2989d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#829dff1866e71c3afa992054212ec66a8d467cfc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 880 | Contributors (this commit): 13 | Commits (past 90d): 23 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -348,7 +348,6 @@ class Graph(Layer):\n                 sh = SiameseHead(i)\n                 sh.previous = s\n                 sh_name = outputs[i]\n-                sh.set_name(sh_name)\n                 self.namespace.add(sh_name)\n                 self.nodes[sh_name] = sh\n                 self.node_config.append({'name': sh_name,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cee4f72a8ee98468146a4c97f53592b77127654d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5662 | Contributors (this commit): 32 | Commits (past 90d): 91 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -1381,7 +1381,7 @@ class Siamese(Layer):\n     def get_config(self):\n \n         config = {\"name\": self.__class__.__name__,\n-                  \"layer\": self.layer.get_config,\n+                  \"layer\": self.layer.get_config(),\n                   \"inputs\": [m.get_config() for m in self.inputs],\n                   \"merge_mode\": self.merge_mode,\n                   \"concat_axis\": self.concat_axis,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93af5e95fd621fbb1bbabca51e3f08484ebf91ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 1953 | Contributors (this commit): 13 | Commits (past 90d): 32 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -243,7 +243,8 @@ class Convolution2D(Layer):\n class Pooling1D(Layer):\n     input_dim = 3\n \n-    def __init__(self, pool_length=2, stride=None, border_mode='valid', **kwargs):\n+    def __init__(self, pool_length=2, stride=None,\n+                 border_mode='valid', **kwargs):\n         super(Pooling1D, self).__init__(**kwargs)\n         if stride is None:\n             stride = pool_length\n@@ -262,14 +263,15 @@ class Pooling1D(Layer):\n                                     self.border_mode, self.stride)\n         return (input_shape[0], length, input_shape[2])\n \n-    def pooling_function(self, back_end, inputs, pool_size, strides, border_mode, dim_ordering):\n+    def pooling_function(self, back_end, inputs, pool_size, strides,\n+                         border_mode, dim_ordering):\n         raise NotImplementedError\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n         X = K.expand_dims(X, -1)   # add dummy last dimension\n         X = K.permute_dimensions(X, (0, 2, 1, 3))\n-        output = self.pooling_function(back_end=K, inputs=X, pool_size=self.pool_size,\n+        output = self.pooling_function(inputs=X, pool_size=self.pool_size,\n                                        strides=self.st,\n                                        border_mode=self.border_mode,\n                                        dim_ordering='th')\n@@ -286,21 +288,23 @@ class Pooling1D(Layer):\n \n \n class MaxPooling1D(Pooling1D):\n-    def __init__(self, **kwargs):\n-        super(MaxPooling1D, self).__init__(**kwargs)\n+    def __init__(self, *args, **kwargs):\n+        super(MaxPooling1D, self).__init__(*args, **kwargs)\n \n-    def pooling_function(self, back_end, inputs, pool_size, strides, border_mode, dim_ordering):\n-        output = back_end.pool2d(inputs, pool_size, strides,\n+    def pooling_function(self, inputs, pool_size, strides,\n+                         border_mode, dim_ordering):\n+        output = K.pool2d(inputs, pool_size, strides,\n                           border_mode, dim_ordering, pool_mode='max')\n         return output\n \n \n class AveragePooling1D(Pooling1D):\n-    def __init__(self, **kwargs):\n-        super(AveragePooling1D, self).__init__(**kwargs)\n+    def __init__(self, *args, **kwargs):\n+        super(AveragePooling1D, self).__init__(*args, **kwargs)\n \n-    def pooling_function(self, back_end, inputs, pool_size, strides, border_mode, dim_ordering):\n-        output = back_end.pool2d(inputs, pool_size, strides,\n+    def pooling_function(self, inputs, pool_size, strides,\n+                         border_mode, dim_ordering):\n+        output = K.pool2d(inputs, pool_size, strides,\n                           border_mode, dim_ordering, pool_mode='avg')\n         return output\n \n@@ -345,12 +349,13 @@ class Pooling2D(Layer):\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n-    def pooling_function(self, back_end, inputs, pool_size, strides, border_mode, dim_ordering):\n+    def pooling_function(self, inputs, pool_size, strides,\n+                         border_mode, dim_ordering):\n         raise NotImplementedError\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        output = self.pooling_function(back_end=K, inputs=X, pool_size=self.pool_size,\n+        output = self.pooling_function(inputs=X, pool_size=self.pool_size,\n                                        strides=self.strides,\n                                        border_mode=self.border_mode,\n                                        dim_ordering=self.dim_ordering)\n@@ -367,21 +372,23 @@ class Pooling2D(Layer):\n \n \n class MaxPooling2D(Pooling2D):\n-    def __init__(self, **kwargs):\n-        super(MaxPooling2D, self).__init__(**kwargs)\n+    def __init__(self, *args, **kwargs):\n+        super(MaxPooling2D, self).__init__(*args, **kwargs)\n \n-    def pooling_function(self, back_end, inputs, pool_size, strides, border_mode, dim_ordering):\n-        output = back_end.pool2d(inputs, pool_size, strides,\n+    def pooling_function(self, inputs, pool_size, strides,\n+                         border_mode, dim_ordering):\n+        output = K.pool2d(inputs, pool_size, strides,\n                           border_mode, dim_ordering, pool_mode='max')\n         return output\n \n \n class AveragePooling2D(Pooling2D):\n-    def __init__(self, **kwargs):\n-        super(AveragePooling2D, self).__init__(**kwargs)\n+    def __init__(self, *args, **kwargs):\n+        super(AveragePooling2D, self).__init__(*args, **kwargs)\n \n-    def pooling_function(self, back_end, inputs, pool_size, strides, border_mode, dim_ordering):\n-        output = back_end.pool2d(inputs, pool_size, strides,\n+    def pooling_function(self, inputs, pool_size, strides,\n+                         border_mode, dim_ordering):\n+        output = K.pool2d(inputs, pool_size, strides,\n                           border_mode, dim_ordering, pool_mode='avg')\n         return output\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d6481fedd18acc9a2de548f8516fa10ed7d2269", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 5669 | Contributors (this commit): 32 | Commits (past 90d): 92 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -98,7 +98,8 @@ class Layer(object):\n         if hasattr(self, 'input_ndim') and self.input_ndim:\n             if self.input_ndim != len(input_shape):\n                 raise Exception('Invalid input shape - Layer expects input ndim=' +\n-                                str(self.input_ndim) + ', was provided with input shape ' + str(input_shape))\n+                                str(self.input_ndim) +\n+                                ', was provided with input shape ' + str(input_shape))\n         self._input_shape = input_shape\n         self.input = K.placeholder(shape=self._input_shape)\n         self.build()\n@@ -117,8 +118,8 @@ class Layer(object):\n         elif hasattr(self, 'input'):\n             return self.input\n         else:\n-            raise Exception('Layer is not connected\\\n-                and is not an input layer.')\n+            raise Exception('Layer is not connected' +\n+                            'and is not an input layer.')\n \n     def supports_masked_input(self):\n         ''' Whether or not this layer respects the output mask of its previous layer in its calculations. If you try\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07ffc76b9318ab1197ce61838992960e9abb3c22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 62 | Churn Cumulative: 2525 | Contributors (this commit): 11 | Commits (past 90d): 20 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n-import numpy as np\n \n from .. import backend as K\n from .. import activations, initializations\n@@ -80,7 +79,7 @@ class Recurrent(MaskedLayer):\n                                              masking=masking)\n         if self.stateful:\n             for i in range(len(states)):\n-                K.set_value(self.states[i], states[i])\n+                K.set_value(self.states[i], K.eval(states[i]))\n \n         if self.return_sequences:\n             return outputs\n@@ -123,11 +122,7 @@ class SimpleRNN(Recurrent):\n     def build(self):\n         input_shape = self.input_shape\n         if self.stateful:\n-            if not input_shape[0]:\n-                raise Exception('If a RNN is stateful, a complete ' +\n-                                'input_shape must be provided ' +\n-                                '(including batch size).')\n-            self.states = [K.zeros((input_shape[0], self.output_dim))]\n+            self.reset_states()\n         else:\n             # initial states: all-zero tensor of shape (output_dim)\n             self.states = [None]\n@@ -143,6 +138,15 @@ class SimpleRNN(Recurrent):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n+    def reset_states(self):\n+        assert self.stateful, 'Layer must be stateful.'\n+        input_shape = self.input_shape\n+        if not input_shape[0]:\n+            raise Exception('If a RNN is stateful, a complete ' +\n+                            'input_shape must be provided ' +\n+                            '(including batch size).')\n+        self.states = [K.zeros((input_shape[0], self.output_dim))]\n+\n     def step(self, x, states):\n         # states only contains the previous output.\n         assert len(states) == 1\n@@ -216,11 +220,7 @@ class GRU(Recurrent):\n                        self.W_h, self.U_h, self.b_h]\n \n         if self.stateful:\n-            if not input_shape[0]:\n-                raise Exception('If a RNN is stateful, a complete ' +\n-                                'input_shape must be provided ' +\n-                                '(including batch size).')\n-            self.states = [K.zeros((input_shape[0], self.output_dim))]\n+            self.reset_states()\n         else:\n             # initial states: all-zero tensor of shape (output_dim)\n             self.states = [None]\n@@ -229,6 +229,15 @@ class GRU(Recurrent):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n+    def reset_states(self):\n+        assert self.stateful, 'Layer must be stateful.'\n+        input_shape = self.input_shape\n+        if not input_shape[0]:\n+            raise Exception('If a RNN is stateful, a complete ' +\n+                            'input_shape must be provided ' +\n+                            '(including batch size).')\n+        self.states = [K.zeros((input_shape[0], self.output_dim))]\n+\n     def step(self, x, states):\n         assert len(states) == 1\n         x_z = K.dot(x, self.W_z) + self.b_z\n@@ -295,12 +304,7 @@ class LSTM(Recurrent):\n         self.input = K.placeholder(input_shape)\n \n         if self.stateful:\n-            if not input_shape[0]:\n-                raise Exception('If a RNN is stateful, a complete ' +\n-                                'input_shape must be provided ' +\n-                                '(including batch size).')\n-            self.states = [K.zeros((input_shape[0], self.output_dim)),\n-                           K.zeros((input_shape[0], self.output_dim))]\n+            self.reset_states()\n         else:\n             # initial states: 2 all-zero tensor of shape (output_dim)\n             self.states = [None, None]\n@@ -330,6 +334,16 @@ class LSTM(Recurrent):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n+    def reset_states(self):\n+        assert self.stateful, 'Layer must be stateful.'\n+        input_shape = self.input_shape\n+        if not input_shape[0]:\n+            raise Exception('If a RNN is stateful, a complete ' +\n+                            'input_shape must be provided ' +\n+                            '(including batch size).')\n+        self.states = [K.zeros((input_shape[0], self.output_dim)),\n+                       K.zeros((input_shape[0], self.output_dim))]\n+\n     def step(self, x, states):\n         assert len(states) == 2\n         h_tm1 = states[0]\n\n@@ -1,5 +1,6 @@\n import unittest\n import numpy as np\n+from numpy.testing import assert_allclose\n \n from keras.layers import recurrent\n from keras import backend as K\n@@ -29,11 +30,18 @@ def _runner(layer_class):\n             mask = layer.get_output_mask(train)\n \n     # check statefulness\n-    layer = layer_class(output_dim, return_sequences=False,\n+    layer = layer_class(output_dim, return_sequences=False, stateful=True,\n                         weights=None, batch_input_shape=(nb_samples, timesteps, input_dim))\n     layer.input = K.variable(np.ones((nb_samples, timesteps, input_dim)))\n-    out = K.eval(layer.get_output(train))\n+    out1 = K.eval(layer.get_output(train))\n     assert(out.shape == (nb_samples, output_dim))\n+    out2 = K.eval(layer.get_output(train))\n+    # if the state is not reset, output should be different\n+    assert(out1.max() != out2.max())\n+    # check that output stays the same when state is reset\n+    layer.reset_states()\n+    out3 = K.eval(layer.get_output(train))\n+    assert_allclose(out1, out3, atol=1e-5)\n \n \n class TestRNNS(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#96f3404a574faa97b1895753fed7a602f853baa7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 50 | Churn Cumulative: 5711 | Contributors (this commit): 32 | Commits (past 90d): 31 | Contributors (cumulative): 38 | DMM Complexity: 0.9523809523809523\n\nDIFF:\n@@ -1,5 +1,6 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n+import numpy as np\n \n from .. import backend as K\n from .. import activations, initializations\n@@ -78,8 +79,9 @@ class Recurrent(MaskedLayer):\n                                              go_backwards=self.go_backwards,\n                                              masking=masking)\n         if self.stateful:\n+            self.updates = []\n             for i in range(len(states)):\n-                K.set_value(self.states[i], K.eval(states[i]))\n+                self.updates.append((self.states[i], states[i]))\n \n         if self.return_sequences:\n             return outputs\n@@ -145,6 +147,10 @@ class SimpleRNN(Recurrent):\n             raise Exception('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided ' +\n                             '(including batch size).')\n+        if hasattr(self, 'states'):\n+            K.set_value(self.states[0],\n+                        np.zeros((input_shape[0], self.output_dim)))\n+        else:\n             self.states = [K.zeros((input_shape[0], self.output_dim))]\n \n     def step(self, x, states):\n@@ -236,6 +242,10 @@ class GRU(Recurrent):\n             raise Exception('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided ' +\n                             '(including batch size).')\n+        if hasattr(self, 'states'):\n+            K.set_value(self.states[0],\n+                        np.zeros((input_shape[0], self.output_dim)))\n+        else:\n             self.states = [K.zeros((input_shape[0], self.output_dim))]\n \n     def step(self, x, states):\n@@ -341,6 +351,12 @@ class LSTM(Recurrent):\n             raise Exception('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided ' +\n                             '(including batch size).')\n+        if hasattr(self, 'states'):\n+            K.set_value(self.states[0],\n+                        np.zeros((input_shape[0], self.output_dim)))\n+            K.set_value(self.states[1],\n+                        np.zeros((input_shape[0], self.output_dim)))\n+        else:\n             self.states = [K.zeros((input_shape[0], self.output_dim)),\n                            K.zeros((input_shape[0], self.output_dim))]\n \n\n@@ -75,8 +75,10 @@ def weighted_objective(fn):\n \n         mask: binary\n         '''\n+        # score_array has ndim >= 2\n         score_array = fn(y_true, y_pred)\n         if mask is not None:\n+            # mask should have the same shape as score_array\n             score_array *= mask\n             #  the loss per batch should be proportional\n             #  to the number of unmasked sampled.\n\n@@ -4,6 +4,7 @@ from numpy.testing import assert_allclose\n \n from keras.layers import recurrent\n from keras import backend as K\n+from keras.models import Sequential\n \n nb_samples, timesteps, input_dim, output_dim = 3, 3, 10, 5\n \n@@ -30,18 +31,29 @@ def _runner(layer_class):\n             mask = layer.get_output_mask(train)\n \n     # check statefulness\n-    layer = layer_class(output_dim, return_sequences=False, stateful=True,\n-                        weights=None, batch_input_shape=(nb_samples, timesteps, input_dim))\n-    layer.input = K.variable(np.ones((nb_samples, timesteps, input_dim)))\n-    out1 = K.eval(layer.get_output(train))\n-    assert(out.shape == (nb_samples, output_dim))\n-    out2 = K.eval(layer.get_output(train))\n+    layer = layer_class(output_dim, return_sequences=False,\n+                        stateful=True,\n+                        weights=None,\n+                        batch_input_shape=(nb_samples, timesteps, input_dim))\n+    model = Sequential()\n+    model.add(layer)\n+    model.compile(optimizer='sgd', loss='mse')\n+    out1 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n+    assert(out1.shape == (nb_samples, output_dim))\n+\n+    # train once so that the states change\n+    model.train_on_batch(np.ones((nb_samples, timesteps, input_dim)),\n+                         np.ones((nb_samples, output_dim)))\n+    out2 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n+\n     # if the state is not reset, output should be different\n     assert(out1.max() != out2.max())\n-    # check that output stays the same when state is reset\n+\n+    # check that output changes after states are reset\n+    # (even though the model itself didn't change)\n     layer.reset_states()\n-    out3 = K.eval(layer.get_output(train))\n-    assert_allclose(out1, out3, atol=1e-5)\n+    out3 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n+    assert(out2.max() != out3.max())\n \n \n class TestRNNS(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#81787dd2bb6b0f0afd3e4f55ead8fa6c854bbc05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 213 | Lines Deleted: 234 | Files Changed: 15 | Hunks: 72 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 447 | Churn Cumulative: 2888 | Contributors (this commit): 17 | Commits (past 90d): 59 | Contributors (cumulative): 48 | DMM Complexity: None\n\nDIFF:\n@@ -1,13 +1,5 @@\n # -*- coding: utf-8 -*-\n-from __future__ import print_function\n-from keras.models import Sequential, slice_X\n-from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n-from keras.layers import recurrent\n-import numpy as np\n-from six.moves import range\n-\n-\"\"\"\n-An implementation of sequence to sequence learning for performing addition\n+'''An implementation of sequence to sequence learning for performing addition\n Input: \"535+61\"\n Output: \"596\"\n Padding is handled by using a repeated sentinel character (space)\n@@ -32,16 +24,23 @@ Four digits inverted:\n Five digits inverted:\n + One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n \n-\"\"\"\n+'''\n+\n+from __future__ import print_function\n+from keras.models import Sequential, slice_X\n+from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n+from keras.layers import recurrent\n+import numpy as np\n+from six.moves import range\n \n \n class CharacterTable(object):\n-    \"\"\"\n+    '''\n     Given a set of characters:\n     + Encode them to a one hot integer representation\n     + Decode the one hot integer representation to their character output\n     + Decode a vector of probabilties to their character output\n-    \"\"\"\n+    '''\n     def __init__(self, chars, maxlen):\n         self.chars = sorted(set(chars))\n         self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n@@ -150,7 +149,8 @@ for iteration in range(1, 200):\n     print()\n     print('-' * 50)\n     print('Iteration', iteration)\n-    model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=1, validation_data=(X_val, y_val), show_accuracy=True)\n+    model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=1,\n+              validation_data=(X_val, y_val), show_accuracy=True)\n     ###\n     # Select 10 samples from the validation set at random so we can visualize errors\n     for i in range(10):\n\n@@ -1,17 +1,4 @@\n-from __future__ import print_function\n-from keras.models import Sequential\n-from keras.layers.embeddings import Embedding\n-from keras.layers.core import Activation, Dense, Merge, Permute, Dropout\n-from keras.layers.recurrent import LSTM\n-from keras.datasets.data_utils import get_file\n-from keras.preprocessing.sequence import pad_sequences\n-from functools import reduce\n-import tarfile\n-import numpy as np\n-import re\n-\n-\"\"\"\n-Train a memory network on the bAbI dataset.\n+'''Train a memory network on the bAbI dataset.\n \n References:\n - Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n@@ -24,7 +11,19 @@ References:\n \n Reaches 93% accuracy on task 'single_supporting_fact_10k' after 70 epochs.\n Time per epoch: 3s on CPU (core i7).\n-\"\"\"\n+'''\n+\n+from __future__ import print_function\n+from keras.models import Sequential\n+from keras.layers.embeddings import Embedding\n+from keras.layers.core import Activation, Dense, Merge, Permute, Dropout\n+from keras.layers.recurrent import LSTM\n+from keras.datasets.data_utils import get_file\n+from keras.preprocessing.sequence import pad_sequences\n+from functools import reduce\n+import tarfile\n+import numpy as np\n+import re\n \n \n def tokenize(sent):\n@@ -200,4 +199,3 @@ answer.fit([inputs_train, queries_train, inputs_train], answers_train,\n            nb_epoch=70,\n            show_accuracy=True,\n            validation_data=([inputs_test, queries_test, inputs_test], answers_test))\n-\n\n@@ -1,21 +1,4 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-from functools import reduce\n-import re\n-import tarfile\n-\n-import numpy as np\n-np.random.seed(1337)  # for reproducibility\n-\n-from keras.datasets.data_utils import get_file\n-from keras.layers.embeddings import Embedding\n-from keras.layers.core import Dense, Merge\n-from keras.layers import recurrent\n-from keras.models import Sequential\n-from keras.preprocessing.sequence import pad_sequences\n-\n-'''\n-Trains two recurrent neural networks based upon a story and a question.\n+'''Trains two recurrent neural networks based upon a story and a question.\n The resulting merged vector is then queried to answer a range of bAbI tasks.\n \n The results are comparable to those for an LSTM model provided in Weston et al.:\n@@ -73,6 +56,21 @@ noise to find the relevant statements, improving performance substantially.\n This becomes especially obvious on QA2 and QA3, both far longer than QA1.\n '''\n \n+from __future__ import print_function\n+from functools import reduce\n+import re\n+import tarfile\n+\n+import numpy as np\n+np.random.seed(1337)  # for reproducibility\n+\n+from keras.datasets.data_utils import get_file\n+from keras.layers.embeddings import Embedding\n+from keras.layers.core import Dense, Merge\n+from keras.layers import recurrent\n+from keras.models import Sequential\n+from keras.preprocessing.sequence import pad_sequences\n+\n \n def tokenize(sent):\n     '''Return the tokens of a sentence including punctuation.\n\n@@ -1,16 +1,4 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-from keras.datasets import cifar10\n-from keras.preprocessing.image import ImageDataGenerator\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation, Flatten\n-from keras.layers.convolutional import Convolution2D, MaxPooling2D\n-from keras.optimizers import SGD, Adadelta, Adagrad\n-from keras.utils import np_utils, generic_utils\n-from six.moves import range\n-\n-'''\n-    Train a (fairly simple) deep CNN on the CIFAR10 small images dataset.\n+'''Train a simple deep CNN on the CIFAR10 small images dataset.\n \n GPU run command:\n     THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10_cnn.py\n@@ -23,6 +11,16 @@ from six.moves import range\n save it in a different format, load it in Python 3 and repickle it.\n '''\n \n+from __future__ import print_function\n+from keras.datasets import cifar10\n+from keras.preprocessing.image import ImageDataGenerator\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation, Flatten\n+from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.optimizers import SGD, Adadelta, Adagrad\n+from keras.utils import np_utils, generic_utils\n+from six.moves import range\n+\n batch_size = 32\n nb_classes = 10\n nb_epoch = 200\n@@ -71,19 +69,19 @@ model.add(Activation('softmax'))\n sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n model.compile(loss='categorical_crossentropy', optimizer=sgd)\n \n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n+X_train = X_train.astype('float32')\n+X_test = X_test.astype('float32')\n X_train /= 255\n X_test /= 255\n \n if not data_augmentation:\n-    print(\"Not using data augmentation or normalization\")\n+    print('Not using data augmentation or normalization')\n     model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n     score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n     print('Test score:', score)\n \n else:\n-    print(\"Using real time data augmentation\")\n+    print('Using real time data augmentation')\n \n     # this will do preprocessing and realtime data augmentation\n     datagen = ImageDataGenerator(\n@@ -106,16 +104,16 @@ else:\n         print('-'*40)\n         print('Epoch', e)\n         print('-'*40)\n-        print(\"Training...\")\n+        print('Training...')\n         # batch train with realtime data augmentation\n         progbar = generic_utils.Progbar(X_train.shape[0])\n         for X_batch, Y_batch in datagen.flow(X_train, Y_train):\n             loss = model.train_on_batch(X_batch, Y_batch)\n-            progbar.add(X_batch.shape[0], values=[(\"train loss\", loss[0])])\n+            progbar.add(X_batch.shape[0], values=[('train loss', loss[0])])\n \n-        print(\"Testing...\")\n+        print('Testing...')\n         # test time!\n         progbar = generic_utils.Progbar(X_test.shape[0])\n         for X_batch, Y_batch in datagen.flow(X_test, Y_test):\n             score = model.test_on_batch(X_batch, Y_batch)\n-            progbar.add(X_batch.shape[0], values=[(\"test loss\", score[0])])\n+            progbar.add(X_batch.shape[0], values=[('test loss', score[0])])\n\n@@ -1,4 +1,12 @@\n-from __future__ import absolute_import\n+'''Train a Bidirectional LSTM on the IMDB sentiment classification task.\n+\n+GPU command:\n+    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_bidirectional_lstm.py\n+\n+Output after 4 epochs on CPU: ~0.8146\n+Time per epoch on CPU (Core i7): ~150s.\n+'''\n+\n from __future__ import print_function\n import numpy as np\n np.random.seed(1337)  # for reproducibility\n@@ -11,21 +19,12 @@ from keras.layers.embeddings import Embedding\n from keras.layers.recurrent import LSTM\n from keras.datasets import imdb\n \n-'''\n-    Train a Bidirectional LSTM on the IMDB sentiment classification task.\n-\n-    GPU command:\n-        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_bidirectional_lstm.py\n-\n-    Output after 4 epochs on CPU: ~0.8146\n-    Time per epoch on CPU (Core i7): ~150s.\n-'''\n \n max_features = 20000\n maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n batch_size = 32\n \n-print(\"Loading data...\")\n+print('Loading data...')\n (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n                                                       test_split=0.2)\n print(len(X_train), 'train sequences')\n@@ -53,7 +52,7 @@ model.add_output(name='output', input='sigmoid')\n # try using different optimizers and different optimizer configs\n model.compile('adam', {'output': 'binary_crossentropy'})\n \n-print(\"Train...\")\n+print('Train...')\n model.fit({'input': X_train, 'output': y_train},\n           batch_size=batch_size,\n           nb_epoch=4)\n\n@@ -1,4 +1,10 @@\n-from __future__ import absolute_import\n+'''This example demonstrates the use of Convolution1D for text classification.\n+\n+Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_cnn.py\n+\n+Get to 0.835 test accuracy after 2 epochs. 100s/epoch on K520 GPU.\n+'''\n+\n from __future__ import print_function\n import numpy as np\n np.random.seed(1337)  # for reproducibility\n@@ -10,14 +16,6 @@ from keras.layers.embeddings import Embedding\n from keras.layers.convolutional import Convolution1D, MaxPooling1D\n from keras.datasets import imdb\n \n-'''\n-    This example demonstrates the use of Convolution1D\n-    for text classification.\n-\n-    Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_cnn.py\n-\n-    Get to 0.835 test accuracy after 2 epochs. 100s/epoch on K520 GPU.\n-'''\n \n # set parameters:\n max_features = 5000\n@@ -29,13 +27,13 @@ filter_length = 3\n hidden_dims = 250\n nb_epoch = 2\n \n-print(\"Loading data...\")\n+print('Loading data...')\n (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n                                                       test_split=0.2)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n-print(\"Pad sequences (samples x time)\")\n+print('Pad sequences (samples x time)')\n X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n print('X_train shape:', X_train.shape)\n@@ -53,8 +51,8 @@ model.add(Dropout(0.25))\n # word group filters of size filter_length:\n model.add(Convolution1D(nb_filter=nb_filter,\n                         filter_length=filter_length,\n-                        border_mode=\"valid\",\n-                        activation=\"relu\",\n+                        border_mode='valid',\n+                        activation='relu',\n                         subsample_length=1))\n # we use standard max pooling (halving the output of the previous layer):\n model.add(MaxPooling1D(pool_length=2))\n@@ -74,7 +72,7 @@ model.add(Activation('sigmoid'))\n \n model.compile(loss='binary_crossentropy',\n               optimizer='rmsprop',\n-              class_mode=\"binary\")\n+              class_mode='binary')\n model.fit(X_train, y_train, batch_size=batch_size,\n           nb_epoch=nb_epoch, show_accuracy=True,\n           validation_data=(X_test, y_test))\n\n@@ -1,11 +1,17 @@\n-from __future__ import absolute_import\n+'''Train a recurrent convolutional network on the IMDB sentiment\n+classification task.\n+\n+GPU command:\n+    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n+\n+Get to 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU.\n+'''\n+\n from __future__ import print_function\n import numpy as np\n np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n-from keras.optimizers import SGD, RMSprop, Adagrad\n-from keras.utils import np_utils\n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation\n from keras.layers.embeddings import Embedding\n@@ -13,14 +19,6 @@ from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n from keras.layers.convolutional import Convolution1D, MaxPooling1D\n from keras.datasets import imdb\n \n-'''\n-    Train a recurrent convolutional network on the IMDB sentiment classification task.\n-\n-    GPU command:\n-        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n-\n-    Get to 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU.\n-'''\n \n # Embedding\n max_features = 20000\n@@ -45,12 +43,12 @@ batch_size is highly sensitive.\n Only 2 epochs are needed as the dataset is very small.\n '''\n \n-print(\"Loading data...\")\n+print('Loading data...')\n (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n-print(\"Pad sequences (samples x time)\")\n+print('Pad sequences (samples x time)')\n X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n print('X_train shape:', X_train.shape)\n@@ -63,8 +61,8 @@ model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n model.add(Dropout(0.25))\n model.add(Convolution1D(nb_filter=nb_filter,\n                         filter_length=filter_length,\n-                        border_mode=\"valid\",\n-                        activation=\"relu\",\n+                        border_mode='valid',\n+                        activation='relu',\n                         subsample_length=1))\n model.add(MaxPooling1D(pool_length=pool_length))\n model.add(LSTM(lstm_output_size))\n@@ -73,9 +71,9 @@ model.add(Activation('sigmoid'))\n \n model.compile(loss='binary_crossentropy',\n               optimizer='adam',\n-              class_mode=\"binary\")\n+              class_mode='binary')\n \n-print(\"Train...\")\n+print('Train...')\n model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n           validation_data=(X_test, y_test), show_accuracy=True)\n score, acc = model.evaluate(X_test, y_test, batch_size=batch_size,\n\n@@ -1,18 +1,4 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import numpy as np\n-np.random.seed(1337)  # for reproducibility\n-\n-from keras.preprocessing import sequence\n-from keras.utils import np_utils\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation\n-from keras.layers.embeddings import Embedding\n-from keras.layers.recurrent import LSTM\n-from keras.datasets import imdb\n-\n-'''\n-    Train a LSTM on the IMDB sentiment classification task.\n+'''Train a LSTM on the IMDB sentiment classification task.\n \n The dataset is actually too small for LSTM to be of any advantage\n compared to simpler, much faster methods such as TF-IDF+LogReg.\n@@ -30,11 +16,23 @@ from keras.datasets import imdb\n     THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n '''\n \n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337)  # for reproducibility\n+\n+from keras.preprocessing import sequence\n+from keras.utils import np_utils\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation\n+from keras.layers.embeddings import Embedding\n+from keras.layers.recurrent import LSTM\n+from keras.datasets import imdb\n+\n max_features = 20000\n maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n batch_size = 32\n \n-print(\"Loading data...\")\n+print('Loading data...')\n (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n                                                       test_split=0.2)\n print(len(X_train), 'train sequences')\n\n@@ -1,24 +1,7 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-\n-import numpy as np\n-import pandas as pd\n-np.random.seed(1337)  # for reproducibility\n-\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation\n-from keras.layers.normalization import BatchNormalization\n-from keras.layers.advanced_activations import PReLU\n-from keras.utils import np_utils, generic_utils\n-\n-from sklearn.preprocessing import LabelEncoder\n-from sklearn.preprocessing import StandardScaler\n-\n-'''\n-    This demonstrates how to reach a score of 0.4890 (local validation)\n+'''This demonstrates how to reach a score of 0.4890 (local validation)\n on the Kaggle Otto challenge, with a deep net using Keras.\n \n-    Compatible Python 2.7-3.4. Requires Scikit-Learn and Pandas.\n+Requires Scikit-Learn and Pandas.\n \n Recommended to run on GPU:\n     Command: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python kaggle_otto_nn.py\n@@ -33,9 +16,24 @@ from sklearn.preprocessing import StandardScaler\n     - with more layers, less layers\n     - with different optimizers (SGD+momentum+decay is probably better than Adam!)\n \n-    Get the data from Kaggle: https://www.kaggle.com/c/otto-group-product-classification-challenge/data\n+Get the data from Kaggle:\n+https://www.kaggle.com/c/otto-group-product-classification-challenge/data\n '''\n \n+from __future__ import print_function\n+import numpy as np\n+import pandas as pd\n+np.random.seed(1337)  # for reproducibility\n+\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation\n+from keras.layers.normalization import BatchNormalization\n+from keras.layers.advanced_activations import PReLU\n+from keras.utils import np_utils, generic_utils\n+\n+from sklearn.preprocessing import LabelEncoder\n+from sklearn.preprocessing import StandardScaler\n+\n \n def load_data(path, train=True):\n     df = pd.read_csv(path)\n@@ -76,9 +74,9 @@ def make_submission(y_prob, ids, encoder, fname):\n             probas = ','.join([i] + [str(p) for p in probs.tolist()])\n             f.write(probas)\n             f.write('\\n')\n-    print(\"Wrote submission to file {}.\".format(fname))\n+    print('Wrote submission to file {}.'.format(fname))\n \n-print(\"Loading data...\")\n+print('Loading data...')\n X, labels = load_data('train.csv', train=True)\n X, scaler = preprocess_data(X)\n y, encoder = preprocess_labels(labels)\n@@ -92,7 +90,7 @@ print(nb_classes, 'classes')\n dims = X.shape[1]\n print(dims, 'dims')\n \n-print(\"Building model...\")\n+print('Building model...')\n \n model = Sequential()\n model.add(Dense(512, input_shape=(dims,)))\n@@ -113,11 +111,11 @@ model.add(Dropout(0.5))\n model.add(Dense(nb_classes))\n model.add(Activation('softmax'))\n \n-model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n+model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n-print(\"Training model...\")\n+print('Training model...')\n model.fit(X, y, nb_epoch=20, batch_size=128, validation_split=0.15)\n \n-print(\"Generating submission...\")\n+print('Generating submission...')\n proba = model.predict_proba(X_test)\n make_submission(proba, ids, encoder, fname='keras-otto.csv')\n\n@@ -1,14 +1,4 @@\n-from __future__ import print_function\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Dropout\n-from keras.layers.recurrent import LSTM\n-from keras.datasets.data_utils import get_file\n-import numpy as np\n-import random\n-import sys\n-\n-'''\n-    Example script to generate text from Nietzsche's writings.\n+'''Example script to generate text from Nietzsche's writings.\n \n At least 20 epochs are required before the generated text\n starts sounding coherent.\n@@ -20,6 +10,15 @@ import sys\n has at least ~100k characters. ~1M is better.\n '''\n \n+from __future__ import print_function\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, Dropout\n+from keras.layers.recurrent import LSTM\n+from keras.datasets.data_utils import get_file\n+import numpy as np\n+import random\n+import sys\n+\n path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n text = open(path).read().lower()\n print('corpus length:', len(text))\n\n@@ -1,4 +1,11 @@\n-from __future__ import absolute_import\n+'''Train a simple convnet on the MNIST dataset.\n+\n+Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n+\n+Get to 99.25% test accuracy after 12 epochs (there is still a lot of margin for parameter tuning).\n+16 seconds per epoch on a GRID K520 GPU.\n+'''\n+\n from __future__ import print_function\n import numpy as np\n np.random.seed(1337)  # for reproducibility\n@@ -9,15 +16,6 @@ from keras.layers.core import Dense, Dropout, Activation, Flatten\n from keras.layers.convolutional import Convolution2D, MaxPooling2D\n from keras.utils import np_utils\n \n-'''\n-    Train a simple convnet on the MNIST dataset.\n-\n-    Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n-\n-    Get to 99.25% test accuracy after 12 epochs (there is still a lot of margin for parameter tuning).\n-    16 seconds per epoch on a GRID K520 GPU.\n-'''\n-\n batch_size = 128\n nb_classes = 10\n nb_epoch = 12\n@@ -36,8 +34,8 @@ nb_conv = 3\n \n X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n+X_train = X_train.astype('float32')\n+X_test = X_test.astype('float32')\n X_train /= 255\n X_test /= 255\n print('X_train shape:', X_train.shape)\n@@ -68,7 +66,8 @@ model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n \n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n+          show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n print('Test score:', score[0])\n print('Test accuracy:', score[1])\n\n@@ -1,18 +1,4 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import numpy as np\n-np.random.seed(1337)  # for reproducibility\n-\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n-from keras.initializations import normal, identity\n-from keras.layers.recurrent import SimpleRNN, LSTM\n-from keras.optimizers import RMSprop\n-from keras.utils import np_utils\n-\n-'''\n-    This is a reproduction of the IRNN experiment\n+'''This is a reproduction of the IRNN experiment\n with pixel-by-pixel sequential MNIST in\n \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\"\n by Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\n@@ -27,6 +13,19 @@ from keras.utils import np_utils\n (which roughly corresponds to 1687500 steps in the original paper.)\n '''\n \n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337)  # for reproducibility\n+\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation\n+from keras.initializations import normal, identity\n+from keras.layers.recurrent import SimpleRNN, LSTM\n+from keras.optimizers import RMSprop\n+from keras.utils import np_utils\n+\n+\n batch_size = 32\n nb_classes = 10\n nb_epochs = 200\n@@ -40,8 +39,8 @@ clip_norm = 1.0\n \n X_train = X_train.reshape(X_train.shape[0], -1, 1)\n X_test = X_test.reshape(X_test.shape[0], -1, 1)\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n+X_train = X_train.astype('float32')\n+X_test = X_test.astype('float32')\n X_train /= 255\n X_test /= 255\n print('X_train shape:', X_train.shape)\n\n@@ -1,4 +1,10 @@\n-from __future__ import absolute_import\n+'''Train a simple deep NN on the MNIST dataset.\n+\n+Get to 98.40% test accuracy after 20 epochs\n+(there is *a lot* of margin for parameter tuning).\n+2 seconds per epoch on a K520 GPU.\n+'''\n+\n from __future__ import print_function\n import numpy as np\n np.random.seed(1337)  # for reproducibility\n@@ -9,13 +15,6 @@ from keras.layers.core import Dense, Dropout, Activation\n from keras.optimizers import SGD, Adam, RMSprop\n from keras.utils import np_utils\n \n-'''\n-    Train a simple deep NN on the MNIST dataset.\n-\n-    Get to 98.40% test accuracy after 20 epochs\n-    (there is *a lot* of margin for parameter tuning).\n-    2 seconds per epoch on a K520 GPU.\n-'''\n \n batch_size = 128\n nb_classes = 10\n@@ -26,8 +25,8 @@ nb_epoch = 20\n \n X_train = X_train.reshape(60000, 784)\n X_test = X_test.reshape(10000, 784)\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n+X_train = X_train.astype('float32')\n+X_test = X_test.astype('float32')\n X_train /= 255\n X_test /= 255\n print(X_train.shape[0], 'train samples')\n\n@@ -1,4 +1,16 @@\n-from __future__ import absolute_import\n+'''Transfer learning toy example:\n+\n+1- Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n+2- Freeze convolutional layers and fine-tune dense layers\n+   for the classification of digits [5..9].\n+\n+Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n+\n+Get to 99.8% test accuracy after 5 epochs\n+for the first five digits classifier\n+and 99.2% for the last five digits after transfer + fine-tuning.\n+'''\n+\n from __future__ import print_function\n import numpy as np\n import datetime\n@@ -11,18 +23,6 @@ from keras.layers.core import Dense, Dropout, Activation, Flatten\n from keras.layers.convolutional import Convolution2D, MaxPooling2D\n from keras.utils import np_utils\n \n-'''\n-    Transfer learning toy example:\n-        1- Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n-        2- Freeze convolutional layers and fine-tune dense layers\n-           for the classification of digits [5..9].\n-\n-    Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n-\n-    Get to 99.8% test accuracy after 5 epochs\n-    for the first five digits classifier\n-    and 99.2% for the last five digits after transfer + fine-tuning.\n-'''\n \n now = datetime.datetime.now\n \n@@ -43,8 +43,8 @@ nb_conv = 3\n def train_model(model, train, test, nb_classes):\n     X_train = train[0].reshape(train[0].shape[0], 1, img_rows, img_cols)\n     X_test = test[0].reshape(test[0].shape[0], 1, img_rows, img_cols)\n-    X_train = X_train.astype(\"float32\")\n-    X_test = X_test.astype(\"float32\")\n+    X_train = X_train.astype('float32')\n+    X_test = X_test.astype('float32')\n     X_train /= 255\n     X_test /= 255\n     print('X_train shape:', X_train.shape)\n\n@@ -1,4 +1,10 @@\n-from __future__ import absolute_import\n+'''Train and evaluate a simple MLP on the Reuters newswire topic classification task.\n+GPU run command:\n+    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python examples/reuters_mlp.py\n+CPU run command:\n+    python examples/reuters_mlp.py\n+'''\n+\n from __future__ import print_function\n import numpy as np\n np.random.seed(1337)  # for reproducibility\n@@ -10,19 +16,11 @@ from keras.layers.normalization import BatchNormalization\n from keras.utils import np_utils\n from keras.preprocessing.text import Tokenizer\n \n-'''\n-    Train and evaluate a simple MLP on the Reuters newswire topic classification task.\n-    GPU run command:\n-        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python examples/reuters_mlp.py\n-    CPU run command:\n-        python examples/reuters_mlp.py\n-'''\n-\n max_words = 1000\n batch_size = 32\n nb_epoch = 5\n \n-print(\"Loading data...\")\n+print('Loading data...')\n (X_train, y_train), (X_test, y_test) = reuters.load_data(nb_words=max_words, test_split=0.2)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n@@ -30,20 +28,20 @@ print(len(X_test), 'test sequences')\n nb_classes = np.max(y_train)+1\n print(nb_classes, 'classes')\n \n-print(\"Vectorizing sequence data...\")\n+print('Vectorizing sequence data...')\n tokenizer = Tokenizer(nb_words=max_words)\n-X_train = tokenizer.sequences_to_matrix(X_train, mode=\"binary\")\n-X_test = tokenizer.sequences_to_matrix(X_test, mode=\"binary\")\n+X_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n+X_test = tokenizer.sequences_to_matrix(X_test, mode='binary')\n print('X_train shape:', X_train.shape)\n print('X_test shape:', X_test.shape)\n \n-print(\"Convert class vector to binary class matrix (for use with categorical_crossentropy)\")\n+print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n print('Y_train shape:', Y_train.shape)\n print('Y_test shape:', Y_test.shape)\n \n-print(\"Building model...\")\n+print('Building model...')\n model = Sequential()\n model.add(Dense(512, input_shape=(max_words,)))\n model.add(Activation('relu'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
