{"custom_id": "keras#dea05ad430aa03e5a60c37496e562749c6f9210f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 987 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 10 | Methods Changed: 69 | Complexity Δ (Sum/Max): 178/133 | Churn Δ: 992 | Churn Cumulative: 6694 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: 0.8957219251336899\n\nDIFF:\n@@ -124,10 +124,7 @@ tri\n tril\n triu\n true_divide\n-unique\n-unrival_index\n vdot\n-vectorize\n vstack\n where\n zeros\n@@ -216,9 +213,9 @@ def shape_equal(shape1, shape2, axis=None, allow_none=True):\n     if allow_none:\n         for i in range(len(shape1)):\n             if shape1[i] is None:\n-                shape1[i] = -1\n+                shape1[i] = shape2[i]\n             if shape2[i] is None:\n-                shape2[i] = -1\n+                shape2[i] = shape1[i]\n \n     return shape1 == shape2\n \n@@ -2165,6 +2162,534 @@ def roll(x, shift, axis=None):\n     return backend.execute(\"roll\", x, shift, axis=axis)\n \n \n+class Round(Operation):\n+    def __init__(self, decimal=0):\n+        super().__init__()\n+        self.decimal = decimal\n+\n+    def call(self, x):\n+        return backend.execute(\"round\", x, self.decimal)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+class Shape(Operation):\n+    def call(self, x):\n+        return backend.execute(\"shape\", x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor([len(x.shape)], dtype=\"int32\")\n+\n+\n+def shape(x):\n+    if any_symbolic_tensors((x,)):\n+        return Shape().symbolic_call(x)\n+    return backend.execute(\"shape\", x)\n+\n+\n+class Sign(Operation):\n+    def call(self, x):\n+        return backend.execute(\"sign\", x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=\"int32\")\n+\n+\n+def sign(x):\n+    if any_symbolic_tensors((x,)):\n+        return Sign().symbolic_call(x)\n+    return backend.execute(\"sign\", x)\n+\n+\n+class Sin(Operation):\n+    def call(self, x):\n+        return backend.execute(\"sin\", x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def sin(x):\n+    if any_symbolic_tensors((x,)):\n+        return Sin().symbolic_call(x)\n+    return backend.execute(\"sin\", x)\n+\n+\n+class Size(Operation):\n+    def call(self, x):\n+        return backend.execute(\"size\", x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor([], dtype=\"int32\")\n+\n+\n+def size(x):\n+    if any_symbolic_tensors((x,)):\n+        return Size().symbolic_call(x)\n+    return backend.execute(\"size\", x)\n+\n+\n+class Sort(Operation):\n+    def __init__(self, axis=-1):\n+        super().__init__()\n+        self.axis = axis\n+\n+    def call(self, x):\n+        return backend.execute(\"sort\", x, axis=self.axis)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, x.dtype)\n+\n+\n+def sort(x, axis=-1):\n+    if any_symbolic_tensors((x,)):\n+        return Sort(axis=axis).symbolic_call(x)\n+    return backend.execute(\"sort\", x, axis=axis)\n+\n+\n+class Split(Operation):\n+    def __init__(self, indices_or_sections, axis=0):\n+        super().__init__()\n+        self.indices_or_sections = indices_or_sections\n+        self.axis = axis\n+\n+    def call(self, x):\n+        return backend.execute(\n+            \"split\", x, self.indices_or_sections, axis=self.axis\n+        )\n+\n+    def compute_output_spec(self, x):\n+        x_shape = list(x.shape)\n+        x_size_on_axis = x_shape[self.axis]\n+        if isinstance(self.indices_or_sections, int):\n+            if x_size_on_axis is None:\n+                x_shape[self.axis] = None\n+                return [\n+                    KerasTensor(x_shape, dtype=x.dtype)\n+                    for _ in range(self.indices_or_sections)\n+                ]\n+            if np.mod(x_size_on_axis, self.indices_or_sections) != 0:\n+                raise ValueError(\n+                    \"`x` size on given `axis` must be dividible by \"\n+                    \"`indices_or_sections` when `indices_or_sections` is an \"\n+                    f\"int. But received {x_size_on_axis} and \"\n+                    f\"{self.indices_or_sections}.\"\n+                )\n+            size = x_size_on_axis // self.indices_or_sections\n+            x_shape[self.axis] = size\n+            return [\n+                KerasTensor(x_shape, dtype=x.dtype)\n+                for _ in range(self.indices_or_sections)\n+            ]\n+\n+        indices_or_sections = [0] + self.indices_or_sections\n+        output_size = np.diff(indices_or_sections)\n+        outputs = []\n+        for i in range(len(output_size)):\n+            output_shape = list(x_shape)\n+            output_shape[self.axis] = int(output_size[i])\n+            outputs.append(KerasTensor(output_shape, dtype=x.dtype))\n+        return outputs\n+\n+\n+def split(x, indices_or_sections, axis=0):\n+    if any_symbolic_tensors((x,)):\n+        return Split(indices_or_sections, axis=axis).symbolic_call(x)\n+    return backend.execute(\"split\", x, indices_or_sections, axis=axis)\n+\n+\n+class Stack(Operation):\n+    def __init__(self, axis=0):\n+        super().__init__()\n+        self.axis = axis\n+\n+    def call(self, xs):\n+        return backend.execute(\"stack\", xs, axis=self.axis)\n+\n+    def compute_output_spec(self, xs):\n+        first_shape = xs[0].shape\n+        for x in xs:\n+            if not shape_equal(x.shape, first_shape, axis=[], allow_none=True):\n+                raise ValueError(\n+                    \"Every value in `xs` must have the same shape. But found \"\n+                    f\"element of shape {x.shape},  which is different from the \"\n+                    f\"first element's shape {first_shape}.\"\n+                )\n+\n+        size_on_axis = len(xs)\n+        output_shape = list(first_shape)\n+        if self.axis == -1:\n+            output_shape = output_shape + [size_on_axis]\n+        elif self.axis >= 0:\n+            output_shape.insert(self.axis, size_on_axis)\n+        else:\n+            output_shape.insert(self.axis + 1, size_on_axis)\n+        return KerasTensor(output_shape, dtype=x.dtype)\n+\n+\n+def stack(x, axis=0):\n+    if any_symbolic_tensors((x,)):\n+        return Stack(axis=axis).symbolic_call(x)\n+    return backend.execute(\"stack\", x, axis=axis)\n+\n+\n+class Std(Operation):\n+    def __init__(self, axis=None, keepdims=False):\n+        super().__init__()\n+        if isinstance(axis, int):\n+            self.axis = [axis]\n+        else:\n+            self.axis = axis\n+        self.keepdims = keepdims\n+\n+    def call(self, x):\n+        return backend.execute(\"std\", x, axis=self.axis, keepdims=self.keepdims)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(\n+            reduce_shape(x.shape, axis=self.axis, keepdims=self.keepdims),\n+        )\n+\n+\n+def std(x, axis=None, keepdims=False):\n+    if any_symbolic_tensors((x,)):\n+        return Std(axis=axis, keepdims=keepdims).symbolic_call(x)\n+    return backend.execute(\"std\", x, axis=axis, keepdims=keepdims)\n+\n+\n+class Swapaxes(Operation):\n+    def __init__(self, axis1, axis2):\n+        super().__init__()\n+\n+        self.axis1 = axis1\n+        self.axis2 = axis2\n+\n+    def call(self, x):\n+        return backend.execute(\"swapaxes\", x, self.axis1, self.axis2)\n+\n+    def compute_output_spec(self, x):\n+        x_shape = list(x.shape)\n+        tmp = x_shape[self.axis1]\n+        x_shape[self.axis1] = x_shape[self.axis2]\n+        x_shape[self.axis2] = tmp\n+        return KerasTensor(x_shape, dtype=x.dtype)\n+\n+\n+def swapaxes(x, axis1, axis2):\n+    if any_symbolic_tensors((x,)):\n+        return Swapaxes(axis1, axis2).symbolic_call(x)\n+    return backend.execute(\"swapaxes\", x, axis1=axis1, axis2=axis2)\n+\n+\n+class Take(Operation):\n+    def __init__(self, axis=None):\n+        super().__init__()\n+        self.axis = axis\n+\n+    def call(self, x, indices):\n+        return backend.execute(\"take\", x, indices, axis=self.axis)\n+\n+    def compute_output_spec(self, x, indices):\n+        x_shape = list(x.shape)\n+        indices_shape = list(getattr(np.array(indices), \"shape\", []))\n+        if self.axis is None:\n+            return KerasTensor(indices_shape, dtype=x.dtype)\n+\n+        if self.axis == -1:\n+            output_shape = x_shape[:-1] + indices_shape\n+        else:\n+            output_shape = (\n+                x_shape[: self.axis] + indices_shape + x_shape[self.axis + 1 :]\n+            )\n+        return KerasTensor(output_shape, dtype=x.dtype)\n+\n+\n+def take(x, indices, axis=None):\n+    if any_symbolic_tensors((x, indices)):\n+        return Take(axis=axis).symbolic_call(x, indices)\n+    return backend.execute(\"take\", x, indices, axis=axis)\n+\n+\n+class TakeAlongAxis(Operation):\n+    def __init__(self, axis=None):\n+        super().__init__()\n+        self.axis = axis\n+\n+    def call(self, x, indices):\n+        return backend.execute(\"take_along_axis\", x, indices, axis=self.axis)\n+\n+    def compute_output_spec(self, x, indices):\n+        x_shape = list(x.shape)\n+        indices_shape = list(indices.shape)\n+        if self.axis is None:\n+            x_shape = [None] if None in x_shape else [int(np.prod(x_shape))]\n+\n+        if len(x_shape) != len(indices_shape):\n+            raise ValueError(\n+                \"`x` and `indices` must have the same number of dimensions, \"\n+                f\"but receive shape {x_shape} and {indices_shape}.\"\n+            )\n+\n+        del x_shape[self.axis]\n+        del indices_shape[self.axis]\n+        output_shape = broadcast_shapes(x_shape, indices_shape)\n+        size_on_axis = indices.shape[self.axis]\n+        if self.axis == -1:\n+            output_shape = output_shape + [size_on_axis]\n+        elif self.axis >= 0:\n+            output_shape.insert(self.axis, size_on_axis)\n+        else:\n+            output_shape.insert(self.axis + 1, size_on_axis)\n+\n+        return KerasTensor(output_shape, dtype=x.dtype)\n+\n+\n+def take_along_axis(x, indices, axis=None):\n+    if any_symbolic_tensors((x, indices)):\n+        return TakeAlongAxis(axis=axis).symbolic_call(x, indices)\n+    return backend.execute(\"take_along_axis\", x, indices, axis=axis)\n+\n+\n+class Tan(Operation):\n+    def call(self, x):\n+        return backend.execute(\"tan\", x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def tan(x):\n+    if any_symbolic_tensors((x,)):\n+        return Tan().symbolic_call(x)\n+    return backend.execute(\"tan\", x)\n+\n+\n+class Tensordot(Operation):\n+    def __init__(self, axes=2):\n+        super().__init__()\n+        self.axes = axes\n+\n+    def call(self, x1, x2):\n+        return backend.execute(\"tensordot\", x1, x2, axes=self.axes)\n+\n+    def compute_output_spec(self, x1, x2):\n+        x1_shape = list(getattr(x1, \"shape\", []))\n+        x2_shape = list(getattr(x2, \"shape\", []))\n+        if not isinstance(self.axes, int):\n+            x1_select_shape = [x1_shape[ax] for ax in self.axes[0]]\n+            x2_select_shape = [x2_shape[ax] for ax in self.axes[1]]\n+            if not shape_equal(\n+                x1_select_shape, x2_select_shape, allow_none=True\n+            ):\n+                raise ValueError(\n+                    \"Shape mismatch on `x1[axes[0]]` and `x2[axes[1]]`, \"\n+                    f\"received {x1_select_shape} and {x2_select_shape}.\"\n+                )\n+\n+            for ax in self.axes[0]:\n+                x1_shape[ax] = -1\n+            for ax in self.axes[1]:\n+                x2_shape[ax] = -1\n+\n+            x1_shape = list(filter((-1).__ne__, x1_shape))\n+            x2_shape = list(filter((-1).__ne__, x2_shape))\n+\n+            output_shape = x1_shape + x2_shape\n+            return KerasTensor(output_shape, dtype=x1.dtype)\n+\n+        if self.axes <= 0:\n+            output_shape = x1_shape + x2_shape\n+        else:\n+            output_shape = x1_shape[: -self.axes] + x2_shape[self.axes :]\n+\n+        return KerasTensor(output_shape, dtype=x1.dtype)\n+\n+\n+def tensordot(x1, x2, axes=2):\n+    if any_symbolic_tensors((x1, x2)):\n+        return Tensordot(axes=axes).symbolic_call(x1, x2)\n+    return backend.execute(\"tensordot\", x1, x2, axes=axes)\n+\n+\n+def round(x):\n+    if any_symbolic_tensors((x,)):\n+        return Round().symbolic_call(x)\n+    return backend.execute(\"round\", x)\n+\n+\n+class Tile(Operation):\n+    def __init__(self, repeats):\n+        super().__init__()\n+        self.repeats = repeats\n+\n+    def call(self, x):\n+        return backend.execute(\"tile\", x, self.repeats)\n+\n+    def compute_output_spec(self, x):\n+        x_shape = list(x.shape)\n+        repeats = self.repeats\n+        if len(x_shape) > len(repeats):\n+            repeats = [1] * (len(x_shape) - len(repeats)) + repeats\n+        else:\n+            x_shape = [1] * (len(repeats) - len(x_shape)) + x_shape\n+\n+        output_shape = []\n+        for x_size, repeat in zip(x_shape, repeats):\n+            if x_size is None:\n+                output_shape.append(None)\n+            else:\n+                output_shape.append(x_size * repeat)\n+        return KerasTensor(output_shape, dtype=x.dtype)\n+\n+\n+def tile(x, repeats):\n+    if any_symbolic_tensors((x,)):\n+        return Tile(\n+            repeats,\n+        ).symbolic_call(x)\n+    return backend.execute(\"tile\", x, repeats)\n+\n+\n+class Trace(Operation):\n+    def __init__(self, offset=0, axis1=0, axis2=1):\n+        super().__init__()\n+        self.offset = offset\n+        self.axis1 = axis1\n+        self.axis2 = axis2\n+\n+    def call(self, x):\n+        return backend.execute(\n+            \"trace\", x, offset=self.offset, axis1=self.axis1, axis2=self.axis2\n+        )\n+\n+    def compute_output_spec(self, x):\n+        x_shape = list(x.shape)\n+        x_shape[self.axis1] = -1\n+        x_shape[self.axis2] = -1\n+        output_shape = list(filter((-1).__ne__, x_shape))\n+        return KerasTensor(output_shape, dtype=x.dtype)\n+\n+\n+def trace(x, offset=0, axis1=0, axis2=1):\n+    if any_symbolic_tensors((x,)):\n+        return Trace(offset, axis1, axis2).symbolic_call(x)\n+    return backend.execute(\"trace\", x, offset=offset, axis1=axis1, axis2=axis2)\n+\n+\n+class Tri(Operation):\n+    def call(self, N, M=None, k=0, dtype=\"float32\"):\n+        return backend.execute(\"tri\", N, M=M, k=k, dtype=dtype)\n+\n+    def compute_output_spec(self, N, M=None, k=0, dtype=\"float32\"):\n+        if M is None:\n+            M = N\n+        return KerasTensor((N, M), dtype=dtype)\n+\n+\n+def tri(N, M=None, k=0, dtype=\"float32\"):\n+    return backend.execute(\"tri\", N, M=M, k=k, dtype=dtype)\n+\n+\n+class Tril(Operation):\n+    def __init__(self, k=0):\n+        super().__init__()\n+        self.k = k\n+\n+    def call(self, x):\n+        return backend.execute(\"tril\", x, k=self.k)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+def tril(x, k=0):\n+    if any_symbolic_tensors((x,)):\n+        return Tril(k=k).symbolic_call(x)\n+    return backend.execute(\"tril\", x, k=k)\n+\n+\n+class Triu(Operation):\n+    def __init__(self, k=0):\n+        super().__init__()\n+        self.k = k\n+\n+    def call(self, x):\n+        return backend.execute(\"triu\", x, k=self.k)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+def triu(x, k=0):\n+    if any_symbolic_tensors((x,)):\n+        return Triu(k=k).symbolic_call(x)\n+    return backend.execute(\"triu\", x, k=k)\n+\n+\n+class Vdot(Operation):\n+    def call(self, x1, x2):\n+        return backend.execute(\"vdot\", x1, x2)\n+\n+    def compute_output_spec(self, x1, x2):\n+        return KerasTensor([], dtype=x1.dtype)\n+\n+\n+def vdot(x1, x2):\n+    if any_symbolic_tensors((x1, x2)):\n+        return Vdot().symbolic_call(x1, x2)\n+    return backend.execute(\"vdot\", x1, x2)\n+\n+\n+class Vstack(Operation):\n+    def call(self, xs):\n+        return backend.execute(\"vstack\", xs)\n+\n+    def compute_output_spec(self, xs):\n+        first_shape = xs[0].shape\n+        total_size_on_axis = 0\n+        for x in xs:\n+            if not shape_equal(x.shape, first_shape, axis=[0], allow_none=True):\n+                raise ValueError(\n+                    \"Every value in `xs` must have the same shape except on \"\n+                    f\"the `axis` dim. But found element of shape {x.shape}, \"\n+                    f\"which is different from the first element's \"\n+                    f\"shape {first_shape}.\"\n+                )\n+            if total_size_on_axis is None or x.shape[0] is None:\n+                total_size_on_axis = None\n+            else:\n+                total_size_on_axis += x.shape[0]\n+        output_shape = list(first_shape)\n+        output_shape[0] = total_size_on_axis\n+        return KerasTensor(output_shape)\n+\n+\n+def vstack(xs):\n+    if any_symbolic_tensors((xs,)):\n+        return Vstack().symbolic_call(xs)\n+    return backend.execute(\"vstack\", xs)\n+\n+\n+class Where(Operation):\n+    def call(self, condition, x1, x2):\n+        return backend.execute(\"where\", condition, x1, x2)\n+\n+    def compute_output_spec(self, condition, x1, x2):\n+        condition_shape = getattr(condition, \"shape\", [])\n+        x1_shape = getattr(x1, \"shape\", [])\n+        x2_shape = getattr(x2, \"shape\", [])\n+        output_shape = broadcast_shapes(condition_shape, x1_shape)\n+        output_shape = broadcast_shapes(output_shape, x2_shape)\n+        return KerasTensor(output_shape, dtype=x1.dtype)\n+\n+\n+def where(condition, x1, x2):\n+    if any_symbolic_tensors((condition, x1, x2)):\n+        return Where().symbolic_call(condition, x1, x2)\n+    return backend.execute(\"where\", condition, x1, x2)\n+\n+\n class Subtract(Operation):\n     def call(self, x1, x2):\n         return backend.execute(\"subtract\", x1, x2)\n\n@@ -438,6 +438,86 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.outer(x, 2).shape, (6, 1))\n \n+    def test_take(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.take(x, 1).shape, ())\n+        self.assertEqual(knp.take(x, [1, 2]).shape, (2,))\n+        self.assertEqual(knp.take(x, [[1, 2], [1, 2]], axis=1).shape, (2, 2, 2))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.take(x, 1).shape, ())\n+        self.assertEqual(knp.take(x, [1, 2]).shape, (2,))\n+        self.assertEqual(\n+            knp.take(x, [[1, 2], [1, 2]], axis=1).shape, (None, 2, 2)\n+        )\n+\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.take(x, 1, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.take(x, [1, 2]).shape, (2,))\n+        self.assertEqual(\n+            knp.take(x, [[1, 2], [1, 2]], axis=1).shape, (None, 2, 2, 3)\n+        )\n+\n+    def test_take_along_axis(self):\n+        x = KerasTensor([2, 3])\n+        indices = KerasTensor([1, 3])\n+        self.assertEqual(knp.take_along_axis(x, indices, axis=0).shape, (1, 3))\n+        self.assertEqual(knp.take_along_axis(x, indices, axis=1).shape, (2, 3))\n+\n+        x = KerasTensor([None, 3])\n+        indices = KerasTensor([1, 3])\n+        self.assertEqual(knp.take_along_axis(x, indices, axis=0).shape, (1, 3))\n+        self.assertEqual(knp.take_along_axis(x, indices, axis=1).shape, (1, 3))\n+\n+        x = KerasTensor([None, 3, 3])\n+        indices = KerasTensor([1, 3, None])\n+        self.assertEqual(\n+            knp.take_along_axis(x, indices, axis=1).shape, (1, 3, 3)\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([2, 3])\n+            indices = KerasTensor([1, 4])\n+            knp.take_along_axis(x, indices, axis=0)\n+\n+    def test_tensordot(self):\n+        x = KerasTensor([2, 3, 3])\n+        y = KerasTensor([3, 3, 4])\n+        self.assertEqual(knp.tensordot(x, y, axes=1).shape, (2, 3, 3, 4))\n+        self.assertEqual(knp.tensordot(x, y, axes=2).shape, (2, 4))\n+        self.assertEqual(\n+            knp.tensordot(x, y, axes=[[1, 2], [0, 1]]).shape, (2, 4)\n+        )\n+\n+        x = KerasTensor([None, 3, 4])\n+        y = KerasTensor([3, 4])\n+        self.assertEqual(knp.tensordot(x, y, axes=1).shape, (None, 3, 4))\n+        self.assertEqual(knp.tensordot(x, y, axes=[[0, 1], [1, 0]]).shape, (4,))\n+\n+    def test_vdot(self):\n+        x = KerasTensor([2, 3])\n+        y = KerasTensor([2, 3])\n+        self.assertEqual(knp.vdot(x, y).shape, ())\n+\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.vdot(x, y).shape, ())\n+\n+        x = KerasTensor([None, 3, 3])\n+        y = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.vdot(x, y).shape, ())\n+\n+    def test_where(self):\n+        condition = KerasTensor([2, 3])\n+        x = KerasTensor([2, 3])\n+        y = KerasTensor([2, 3])\n+        self.assertEqual(knp.where(condition, x, y).shape, (2, 3))\n+\n+        condition = KerasTensor([2, None, 1])\n+        x = KerasTensor([None, 1])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.where(condition, x, y).shape, (2, None, 3))\n+\n \n class NumpyOneInputOpsShapeTest(testing.TestCase):\n     def test_mean(self):\n@@ -1066,6 +1146,170 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.roll(x, 1, axis=1).shape, (None, 3))\n         self.assertEqual(knp.roll(x, 1, axis=0).shape, (None, 3))\n \n+    def test_round(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.round(x).shape, (2, 3))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.round(x).shape, (None, 3))\n+\n+    def test_shape(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.shape(x).shape, (2,))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.shape(x).shape, (2,))\n+\n+    def test_sign(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.sign(x).shape, (2, 3))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.sign(x).shape, (None, 3))\n+\n+    def test_sin(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.sin(x).shape, (2, 3))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.sin(x).shape, (None, 3))\n+\n+    def test_size(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.size(x).shape, ())\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.size(x).shape, ())\n+\n+    def test_sort(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.sort(x).shape, (2, 3))\n+        self.assertEqual(knp.sort(x, axis=1).shape, (2, 3))\n+        self.assertEqual(knp.sort(x, axis=0).shape, (2, 3))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.sort(x).shape, (None, 3))\n+        self.assertEqual(knp.sort(x, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.sort(x, axis=0).shape, (None, 3))\n+\n+    def test_split(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(len(knp.split(x, 2)), 2)\n+        self.assertEqual(knp.split(x, 2)[0].shape, (1, 3))\n+        self.assertEqual(knp.split(x, 3, axis=1)[0].shape, (2, 1))\n+        self.assertEqual(knp.split(x, [1, 3], axis=1)[1].shape, (2, 2))\n+\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.split(x, 2)[0].shape, (None, 3, 3))\n+        self.assertEqual(knp.split(x, 3, axis=1)[0].shape, (None, 1, 3))\n+        self.assertEqual(knp.split(x, [1, 3], axis=1)[1].shape, (None, 2, 3))\n+\n+        with self.assertRaises(ValueError):\n+            knp.split(x, 2, axis=1)\n+\n+    def test_sqrt(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.sqrt(x).shape, (2, 3))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.sqrt(x).shape, (None, 3))\n+\n+    def test_stack(self):\n+        x = KerasTensor([2, 3])\n+        y = KerasTensor([2, 3])\n+        self.assertEqual(knp.stack([x, y]).shape, (2, 2, 3))\n+        self.assertEqual(knp.stack([x, y], axis=-1).shape, (2, 3, 2))\n+\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.stack([x, y]).shape, (2, None, 3))\n+        self.assertEqual(knp.stack([x, y], axis=-1).shape, (None, 3, 2))\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([2, 3])\n+            y = KerasTensor([3, 3])\n+            knp.stack([x, y])\n+\n+    def test_std(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.std(x).shape, ())\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.std(x).shape, ())\n+\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.std(x, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.std(x, axis=1, keepdims=True).shape, (None, 1, 3))\n+\n+    def test_swapaxes(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.swapaxes(x, 0, 1).shape, (3, 2))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.swapaxes(x, 0, 1).shape, (3, None))\n+\n+    def test_tan(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.tan(x).shape, (2, 3))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.tan(x).shape, (None, 3))\n+\n+    def test_tile(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.tile(x, [2]).shape, (2, 6))\n+        self.assertEqual(knp.tile(x, [1, 2]).shape, (2, 6))\n+        self.assertEqual(knp.tile(x, [2, 1, 2]).shape, (2, 2, 6))\n+\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.tile(x, [2]).shape, (None, 6))\n+        self.assertEqual(knp.tile(x, [1, 2]).shape, (None, 6))\n+        self.assertEqual(knp.tile(x, [2, 1, 2]).shape, (2, None, 6))\n+\n+    def test_trace(self):\n+        x = KerasTensor([2, 3, 4, 5])\n+        self.assertEqual(knp.trace(x).shape, (4, 5))\n+        self.assertEqual(knp.trace(x, axis1=2, axis2=3).shape, (2, 3))\n+\n+        x = KerasTensor([None, 3, None, 5])\n+        self.assertEqual(knp.trace(x).shape, (None, 5))\n+        self.assertEqual(knp.trace(x, axis1=2, axis2=3).shape, (None, 3))\n+\n+    def test_tril(self):\n+        x = KerasTensor([2, 3, 4, 5])\n+        self.assertEqual(knp.tril(x).shape, (2, 3, 4, 5))\n+        self.assertEqual(knp.tril(x, k=1).shape, (2, 3, 4, 5))\n+        self.assertEqual(knp.tril(x, k=-1).shape, (2, 3, 4, 5))\n+\n+        x = KerasTensor([None, 3, None, 5])\n+        self.assertEqual(knp.tril(x).shape, (None, 3, None, 5))\n+        self.assertEqual(knp.tril(x, k=1).shape, (None, 3, None, 5))\n+        self.assertEqual(knp.tril(x, k=-1).shape, (None, 3, None, 5))\n+\n+    def test_triu(self):\n+        x = KerasTensor([2, 3, 4, 5])\n+        self.assertEqual(knp.triu(x).shape, (2, 3, 4, 5))\n+        self.assertEqual(knp.triu(x, k=1).shape, (2, 3, 4, 5))\n+        self.assertEqual(knp.triu(x, k=-1).shape, (2, 3, 4, 5))\n+\n+        x = KerasTensor([None, 3, None, 5])\n+        self.assertEqual(knp.triu(x).shape, (None, 3, None, 5))\n+        self.assertEqual(knp.triu(x, k=1).shape, (None, 3, None, 5))\n+        self.assertEqual(knp.triu(x, k=-1).shape, (None, 3, None, 5))\n+\n+    def test_vstack(self):\n+        x = KerasTensor([2, 3])\n+        y = KerasTensor([2, 3])\n+        self.assertEqual(knp.vstack([x, y]).shape, (4, 3))\n+\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.vstack([x, y]).shape, (None, 3))\n+\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, None])\n+        self.assertEqual(knp.vstack([x, y]).shape, (None, 3))\n+\n \n class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n     def test_add(self):\n@@ -1481,6 +1725,81 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n         self.assertAllClose(np.array(knp.outer(x, y)), np.outer(x, y))\n         self.assertAllClose(np.array(knp.Outer()(x, y)), np.outer(x, y))\n \n+    def test_take(self):\n+        x = np.arange(24).reshape([1, 2, 3, 4])\n+        indices = np.array([0, 1])\n+        self.assertAllClose(np.array(knp.take(x, indices)), np.take(x, indices))\n+        self.assertAllClose(np.array(knp.take(x, 0)), np.take(x, 0))\n+        self.assertAllClose(\n+            np.array(knp.take(x, 0, axis=1)), np.take(x, 0, axis=1)\n+        )\n+\n+        self.assertAllClose(\n+            np.array(knp.Take()(x, indices)), np.take(x, indices)\n+        )\n+        self.assertAllClose(np.array(knp.Take()(x, 0)), np.take(x, 0))\n+        self.assertAllClose(\n+            np.array(knp.Take(axis=1)(x, 0)), np.take(x, 0, axis=1)\n+        )\n+\n+    def test_take_along_axis(self):\n+        x = np.arange(24).reshape([1, 2, 3, 4])\n+        indices = np.ones([1, 4, 1, 1], dtype=np.int32)\n+        self.assertAllClose(\n+            np.array(knp.take_along_axis(x, indices, axis=1)),\n+            np.take_along_axis(x, indices, axis=1),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.TakeAlongAxis(axis=1)(x, indices)),\n+            np.take_along_axis(x, indices, axis=1),\n+        )\n+\n+        x = np.arange(12).reshape([1, 1, 3, 4])\n+        indices = np.ones([1, 4, 1, 1], dtype=np.int32)\n+        self.assertAllClose(\n+            np.array(knp.take_along_axis(x, indices, axis=2)),\n+            np.take_along_axis(x, indices, axis=2),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.TakeAlongAxis(axis=2)(x, indices)),\n+            np.take_along_axis(x, indices, axis=2),\n+        )\n+\n+    def test_tensordot(self):\n+        x = np.arange(24).reshape([1, 2, 3, 4])\n+        y = np.arange(24).reshape([3, 4, 1, 2])\n+        self.assertAllClose(\n+            np.array(knp.tensordot(x, y, axes=2)), np.tensordot(x, y, axes=2)\n+        )\n+        self.assertAllClose(\n+            np.array(knp.tensordot(x, y, axes=([0, 1], [2, 3]))),\n+            np.tensordot(x, y, axes=([0, 1], [2, 3])),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.Tensordot(axes=2)(x, y)),\n+            np.tensordot(x, y, axes=2),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.Tensordot(axes=([0, 1], [2, 3]))(x, y)),\n+            np.tensordot(x, y, axes=([0, 1], [2, 3])),\n+        )\n+\n+    def test_vdot(self):\n+        x = np.array([1, 2, 3])\n+        y = np.array([4, 5, 6])\n+        self.assertAllClose(np.array(knp.vdot(x, y)), np.vdot(x, y))\n+        self.assertAllClose(np.array(knp.Vdot()(x, y)), np.vdot(x, y))\n+\n+    def test_where(self):\n+        x = np.array([1, 2, 3])\n+        y = np.array([4, 5, 6])\n+        self.assertAllClose(\n+            np.array(knp.where(x > 1, x, y)), np.where(x > 1, x, y)\n+        )\n+        self.assertAllClose(\n+            np.array(knp.Where()(x > 1, x, y)), np.where(x > 1, x, y)\n+        )\n+\n \n class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_mean(self):\n@@ -2233,6 +2552,135 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n             np.array(knp.Roll(-1, axis=0)(x)), np.roll(x, -1, axis=0)\n         )\n \n+    def test_round(self):\n+        x = np.array([[1.1, 2.5, 3.9], [3.2, 2.3, 1.8]])\n+        self.assertAllClose(np.array(knp.round(x)), np.round(x))\n+        self.assertAllClose(np.array(knp.Round()(x)), np.round(x))\n+\n+    def test_shape(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(np.array(knp.shape(x)), np.shape(x))\n+        self.assertAllClose(np.array(knp.Shape()(x)), np.shape(x))\n+\n+    def test_sign(self):\n+        x = np.array([[1, -2, 3], [-3, 2, -1]])\n+        self.assertAllClose(np.array(knp.sign(x)), np.sign(x))\n+        self.assertAllClose(np.array(knp.Sign()(x)), np.sign(x))\n+\n+    def test_sin(self):\n+        x = np.array([[1, -2, 3], [-3, 2, -1]])\n+        self.assertAllClose(np.array(knp.sin(x)), np.sin(x))\n+        self.assertAllClose(np.array(knp.Sin()(x)), np.sin(x))\n+\n+    def test_size(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(np.array(knp.size(x)), np.size(x))\n+        self.assertAllClose(np.array(knp.Size()(x)), np.size(x))\n+\n+    def test_sort(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(np.array(knp.sort(x)), np.sort(x))\n+        self.assertAllClose(np.array(knp.Sort()(x)), np.sort(x))\n+        self.assertAllClose(np.array(knp.sort(x, axis=0)), np.sort(x, axis=0))\n+        self.assertAllClose(np.array(knp.Sort(axis=0)(x)), np.sort(x, axis=0))\n+\n+    def test_split(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(np.array(knp.split(x, 2)), np.split(x, 2))\n+        self.assertAllClose(np.array(knp.Split(2)(x)), np.split(x, 2))\n+        self.assertAllClose(\n+            np.array(knp.split(x, [1, 2], axis=1)),\n+            np.split(x, [1, 2], axis=1),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.Split([1, 2], axis=1)(x)),\n+            np.split(x, [1, 2], axis=1),\n+        )\n+\n+    def test_sqrt(self):\n+        x = np.array([[1, 4, 9], [16, 25, 36]])\n+        self.assertAllClose(np.array(knp.sqrt(x)), np.sqrt(x))\n+        self.assertAllClose(np.array(knp.Sqrt()(x)), np.sqrt(x))\n+\n+    def test_stack(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        y = np.array([[4, 5, 6], [6, 5, 4]])\n+        self.assertAllClose(np.array(knp.stack([x, y])), np.stack([x, y]))\n+        self.assertAllClose(\n+            np.array(knp.stack([x, y], axis=1)), np.stack([x, y], axis=1)\n+        )\n+        self.assertAllClose(np.array(knp.Stack()([x, y])), np.stack([x, y]))\n+        self.assertAllClose(\n+            np.array(knp.Stack(axis=1)([x, y])), np.stack([x, y], axis=1)\n+        )\n+\n+    def test_std(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(np.array(knp.std(x)), np.std(x))\n+        self.assertAllClose(np.array(knp.std(x, axis=1)), np.std(x, axis=1))\n+        self.assertAllClose(\n+            np.array(knp.std(x, axis=1, keepdims=True)),\n+            np.std(x, axis=1, keepdims=True),\n+        )\n+\n+        self.assertAllClose(np.array(knp.Std()(x)), np.std(x))\n+        self.assertAllClose(np.array(knp.Std(axis=1)(x)), np.std(x, axis=1))\n+        self.assertAllClose(\n+            np.array(knp.Std(axis=1, keepdims=True)(x)),\n+            np.std(x, axis=1, keepdims=True),\n+        )\n+\n+    def test_swapaxes(self):\n+        x = np.arange(24).reshape([1, 2, 3, 4])\n+        self.assertAllClose(\n+            np.array(knp.swapaxes(x, 0, 1)),\n+            np.swapaxes(x, 0, 1),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.Swapaxes(0, 1)(x)),\n+            np.swapaxes(x, 0, 1),\n+        )\n+\n+    def test_tan(self):\n+        x = np.array([[1, -2, 3], [-3, 2, -1]])\n+        self.assertAllClose(np.array(knp.tan(x)), np.tan(x))\n+        self.assertAllClose(np.array(knp.Tan()(x)), np.tan(x))\n+\n+    def test_tile(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(np.array(knp.tile(x, [2, 3])), np.tile(x, [2, 3]))\n+        self.assertAllClose(np.array(knp.Tile([2, 3])(x)), np.tile(x, [2, 3]))\n+\n+    def test_trace(self):\n+        x = np.arange(24).reshape([1, 2, 3, 4])\n+        self.assertAllClose(np.array(knp.trace(x)), np.trace(x))\n+        self.assertAllClose(\n+            np.array(knp.trace(x, axis1=2, axis2=3)),\n+            np.trace(x, axis1=2, axis2=3),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.Trace(axis1=2, axis2=3)(x)),\n+            np.trace(x, axis1=2, axis2=3),\n+        )\n+\n+    def test_tril(self):\n+        x = np.arange(24).reshape([1, 2, 3, 4])\n+        self.assertAllClose(np.array(knp.tril(x)), np.tril(x))\n+        self.assertAllClose(np.array(knp.tril(x, -1)), np.tril(x, -1))\n+        self.assertAllClose(np.array(knp.Tril(-1)(x)), np.tril(x, -1))\n+\n+    def test_triu(self):\n+        x = np.arange(24).reshape([1, 2, 3, 4])\n+        self.assertAllClose(np.array(knp.triu(x)), np.triu(x))\n+        self.assertAllClose(np.array(knp.triu(x, -1)), np.triu(x, -1))\n+        self.assertAllClose(np.array(knp.Triu(-1)(x)), np.triu(x, -1))\n+\n+    def test_vstack(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        y = np.array([[4, 5, 6], [6, 5, 4]])\n+        self.assertAllClose(np.array(knp.vstack([x, y])), np.vstack([x, y]))\n+        self.assertAllClose(np.array(knp.Vstack()([x, y])), np.vstack([x, y]))\n+\n \n class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n     def test_ones(self):\n@@ -2281,3 +2729,12 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n     def test_identity(self):\n         self.assertAllClose(np.array(knp.identity(3)), np.identity(3))\n         self.assertAllClose(np.array(knp.Identity()(3)), np.identity(3))\n+\n+    def test_tri(self):\n+        self.assertAllClose(np.array(knp.tri(3)), np.tri(3))\n+        self.assertAllClose(np.array(knp.tri(3, 4)), np.tri(3, 4))\n+        self.assertAllClose(np.array(knp.tri(3, 4, 1)), np.tri(3, 4, 1))\n+\n+        self.assertAllClose(np.array(knp.Tri()(3)), np.tri(3))\n+        self.assertAllClose(np.array(knp.Tri()(3, 4)), np.tri(3, 4))\n+        self.assertAllClose(np.array(knp.Tri()(3, 4, 1)), np.tri(3, 4, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67c42632fe4cc12e01b7a6eef21067fe44dfa615", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 24 | Files Changed: 3 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 26 | Churn Cumulative: 6755 | Contributors (this commit): 3 | Commits (past 90d): 24 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,10 +20,10 @@ class TestTFTrainer(testing.TestCase):\n         x = np.random.random((100, 4))\n         y = np.random.random((100, 3))\n         batch_size = 16\n-        epochs = 3\n+        epochs = 10\n \n         model.compile(optimizer=optimizers.SGD(), loss=losses.MeanSquaredError(), run_eagerly=run_eagerly, jit_compile=jit_compile)\n-        model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+        history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n \n     def test_basic_flow_eager(self):\n         self._test_basic_flow(run_eagerly=True, jit_compile=False)\n\n@@ -99,7 +99,6 @@ repeat\n reshape\n roll\n round\n-shape\n sign\n sin\n size\n@@ -2174,20 +2173,6 @@ class Round(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-class Shape(Operation):\n-    def call(self, x):\n-        return backend.execute(\"shape\", x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor([len(x.shape)], dtype=\"int32\")\n-\n-\n-def shape(x):\n-    if any_symbolic_tensors((x,)):\n-        return Shape().symbolic_call(x)\n-    return backend.execute(\"shape\", x)\n-\n-\n class Sign(Operation):\n     def call(self, x):\n         return backend.execute(\"sign\", x)\n\n@@ -1153,13 +1153,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.round(x).shape, (None, 3))\n \n-    def test_shape(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.shape(x).shape, (2,))\n-\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.shape(x).shape, (2,))\n-\n     def test_sign(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.sign(x).shape, (2, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b276c92b0b8b37ab597e879e3924fe4d656347d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 435 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,3 @@\n-import copy\n-\n-import numpy as np\n import tensorflow as tf\n \n from keras_core import callbacks as callbacks_module\n@@ -28,10 +25,6 @@ class TensorFlowTrainer(trainer.Trainer):\n         trainable_weights = [v.value for v in self.trainable_weights]\n         gradients = tape.gradient(loss, trainable_weights)\n \n-        print(\"loss\", loss)\n-        print(\"trainable_weights\", trainable_weights)\n-        print(\"gradients\", gradients)\n-\n         # Update weights\n         self.optimizer.apply_gradients(zip(gradients, trainable_weights))\n         return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n@@ -60,10 +53,6 @@ class TensorFlowTrainer(trainer.Trainer):\n         else:\n             train_function = step_function\n \n-        print(\"run eagerly:\", self.run_eagerly)\n-        print(\"train_function:\", train_function)\n-        print(\"self._compile_loss\", self._compile_loss)\n-\n         self.train_function = train_function\n \n     def make_test_function(self, force=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f4108a9e9dce9877bcf7cd6b1298627d224e3376", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 10 | Files Changed: 6 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 41 | Churn Cumulative: 2302 | Contributors (this commit): 3 | Commits (past 90d): 44 | Contributors (cumulative): 9 | DMM Complexity: 0.8888888888888888\n\nDIFF:\n@@ -143,7 +143,7 @@ class TensorFlowTrainer(trainer.Trainer):\n                     break\n \n             # Override with model metrics instead of last step logs\n-            epoch_logs = self.get_metrics_result()\n+            epoch_logs = self._process_logs(self.get_metrics_result())\n \n             # Run validation.\n             if validation_data and self._should_eval(epoch, validation_freq):\n@@ -169,7 +169,7 @@ class TensorFlowTrainer(trainer.Trainer):\n                 val_logs = {\n                     \"val_\" + name: val for name, val in val_logs.items()\n                 }\n-                epoch_logs.update(val_logs)\n+                epoch_logs.update(self._process_logs(val_logs))\n \n             callbacks.on_epoch_end(epoch, epoch_logs)\n             training_logs = epoch_logs\n@@ -206,3 +206,13 @@ class TensorFlowTrainer(trainer.Trainer):\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n         raise NotImplementedError\n+    \n+    def _process_logs(self, logs):\n+        result = {}\n+        for key, value in logs.items():\n+            try:\n+                value = float(value)\n+            except:\n+                pass\n+            result[key] = value\n+        return result\n\n@@ -1,6 +1,9 @@\n from keras_core.api_export import keras_core_export\n from keras_core.metrics.metric import Metric\n from keras_core.metrics.regression_metrics import MeanSquaredError\n+from keras_core.metrics.reduction_metrics import Mean\n+from keras_core.metrics.reduction_metrics import Sum\n+from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n \n \n @keras_core_export(\"keras_core.metrics.get\")\n\n@@ -114,12 +114,14 @@ class Mean(Metric):\n             values, sample_weight, reduce_fn=ops.mean, dtype=self.dtype\n         )\n         self.total.assign(self.total + ops.sum(values))\n+        if len(values.shape) >= 1:\n+            num_samples = ops.shape(values)[0]\n+        else:\n+            num_samples = 1 \n         if sample_weight is not None:\n             num_samples = ops.sum(\n-                ops.ones(shape=(ops.shape(values)[0],)) * sample_weight\n+                ops.ones(shape=(num_samples,)) * sample_weight\n             )\n-        else:\n-            num_samples = ops.shape(values)[0]\n         self.count.assign(self.count + ops.cast(num_samples, dtype=\"int64\"))\n \n     def reset_state(self):\n\n@@ -8,7 +8,7 @@ def mean_squared_error(y_true, y_pred):\n \n \n class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n-    def __init__(self, name=\"mean_square_error\", dtype=None):\n+    def __init__(self, name=\"mean_squared_error\", dtype=None):\n         super().__init__(fn=mean_squared_error, name=name, dtype=dtype)\n \n     def get_config(self):\n\n@@ -546,6 +546,8 @@ class CompileLoss(losses_module.Loss):\n                         flat_loss_weights.append(1.0)\n         self.flat_losses = flat_losses\n         self.flat_loss_weights = flat_loss_weights\n+        self.loss_tracker = metrics_module.Mean(name=\"loss\")\n+        self.built = True\n \n     def call(self, y_true, y_pred):\n         if not self.built:\n@@ -561,7 +563,9 @@ class CompileLoss(losses_module.Loss):\n                 value = w * ops.cast(loss(y_t, y_p), dtype=backend.floatx())\n                 loss_values.append(value)\n         if loss_values:\n-            return ops.sum(loss_values)\n+            total_loss = ops.sum(loss_values)\n+            self.loss_tracker.update_state(total_loss)\n+            return total_loss\n         return None\n \n     def get_config(self):\n\n@@ -60,10 +60,12 @@ class Trainer:\n \n     @property\n     def metrics(self):\n-        metrics = self._metrics[:]\n-        if self._compile_metrics is not None:\n+        metrics = []\n+        if self._compile_loss is not None and self._compile_loss.built:\n+            metrics.append(self._compile_loss.loss_tracker)\n+        metrics.extend(self._metrics[:])\n+        if self._compile_metrics is not None and self._compile_metrics.built:\n             metrics += [self._compile_metrics]\n-        # TODO: also track mean of compile_loss.\n         return metrics\n \n     def reset_metrics(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc0f3467fc66d9fa2243c58af126a78074136899", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 121 | Lines Deleted: 68 | Files Changed: 12 | Hunks: 34 | Methods Changed: 22 | Complexity Δ (Sum/Max): 12/14 | Churn Δ: 189 | Churn Cumulative: 3875 | Contributors (this commit): 3 | Commits (past 90d): 76 | Contributors (cumulative): 17 | DMM Complexity: 0.4406779661016949\n\nDIFF:\n@@ -1,5 +1,6 @@\n import jax\n \n+from keras_core import backend\n from keras_core import callbacks as callbacks_module\n from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n@@ -8,8 +9,49 @@ from keras_core.trainers.epoch_iterator import EpochIterator\n \n \n class Trainer(base_trainer.Trainer):\n-    def train_step(self, data):\n-        raise NotImplementedError\n+\n+    def compute_loss_and_updates(\n+        self, trainable_variables, non_trainable_variables, data\n+    ):\n+        x, y, sample_weight = data_adapters_utils.unpack_x_y_sample_weight(data)\n+        y_pred, non_trainable_variables = self.stateless_call(\n+            trainable_variables, non_trainable_variables, x\n+        )\n+        loss = self.compute_loss(x, y, y_pred, sample_weight=sample_weight)\n+        return loss, (non_trainable_variables, y_pred)\n+    \n+    def _get_gradient_fn(self):\n+        return jax.value_and_grad(self.compute_loss_and_updates, has_aux=True)\n+    \n+    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n+        \"\"\"Stateless loss computation.\"\"\"\n+        del x  # The default implementation does not use `x`.\n+        losses = []\n+        if self._compile_loss is not None:\n+            loss = self._compile_loss(y, y_pred, sample_weight)\n+            if loss is not None:\n+                losses.append(loss)\n+        for l in self.losses:\n+            losses.append(l.astype(backend.floatx()))\n+        if len(losses) == 0:\n+            raise ValueError(\n+                \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n+            )\n+        if len(losses) == 1:\n+            total_loss = losses[0]\n+        else:\n+            total_loss = sum(losses)\n+        return total_loss\n+\n+    def train_step(self, trainable_variables, non_trainable_variables, optimizer_variables, data):\n+        grad_fn = self._get_gradient_fn()\n+        (loss, (non_trainable_variables, y_pred)), grads = grad_fn(\n+            trainable_variables, non_trainable_variables, data\n+        )\n+        trainable_variables, optimizer_variables = self.optimizer.stateless_apply(\n+            grads, optimizer_variables\n+        )\n+        return trainable_variables, non_trainable_variables, optimizer_variables, y_pred, loss\n \n     def test_step(self, data):\n         raise NotImplementedError\n@@ -18,7 +60,16 @@ class Trainer(base_trainer.Trainer):\n         raise NotImplementedError\n \n     def make_train_function(self, force=False):\n-        raise NotImplementedError\n+        if self.run_eagerly or not self.jit_compile:\n+            self.train_function = self.train_step\n+            return self.train_step\n+\n+        @jax.jit\n+        def train_fn(trainable_variables, non_trainable_variables, optimizer_variables, data):\n+            return self.train_step(trainable_variables, non_trainable_variables, optimizer_variables, data)\n+        \n+        self.train_function = train_fn\n+        return train_fn\n \n     def make_test_function(self, force=False):\n         raise NotImplementedError\n@@ -49,6 +100,7 @@ class Trainer(base_trainer.Trainer):\n             raise ValueError(\n                 \"You must call `compile()` before calling `fit()`.\"\n             )\n+\n         # TODO: respect compiled trainable state\n         if validation_split and validation_data is None:\n             # Create the validation data using the training data. Only supported\n@@ -68,7 +120,7 @@ class Trainer(base_trainer.Trainer):\n                 val_sample_weight,\n             ) = data_adapters_utils.unpack_x_y_sample_weight(validation_data)\n \n-        # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n+        # Create an iterator that yields batches for one epoch.\n         epoch_iterator = EpochIterator(\n             x=x,\n             y=y,\n@@ -79,6 +131,14 @@ class Trainer(base_trainer.Trainer):\n             class_weight=class_weight,\n         )\n \n+        if not self.built:\n+            # Build the model on one batch of data.\n+            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+                x, y, _ = data_adapters_utils.unpack_x_y_sample_weight(data)\n+                # Build model\n+                self(x)\n+                break\n+\n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n@@ -96,18 +156,43 @@ class Trainer(base_trainer.Trainer):\n         callbacks.on_train_begin()\n         training_logs = None\n         logs = None\n+\n+        trainable_variables = self.trainable_variables\n+        if not self.optimizer.built:\n+            self.optimizer.build(trainable_variables)\n+        non_trainable_variables = self.non_trainable_variables\n+        optimizer_variables = self.optimizer.variables\n+\n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n-            for step, batch in epoch_iterator.enumerate_epoch():\n+            for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+                # Callbacks\n                 callbacks.on_train_batch_begin(step)\n-                logs = self.train_function(batch)\n+\n+                # Train step (in JAX context)\n+                trainable_variables, non_trainable_variables, optimizer_variables, y_pred, loss = self.train_function(trainable_variables, non_trainable_variables, optimizer_variables, data)\n+\n+                # Run variable updates (back in eager context)\n+                x, y, sample_weight = data_adapters_utils.unpack_x_y_sample_weight(data)\n+                logs = self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n+                # for variable, value in zip(self.trainable_variables, trainable_variables):\n+                #     variable.assign(value)\n+                # for variable, value in zip(\n+                #     self.non_trainable_variables, non_trainable_variables\n+                # ):\n+                #     variable.assign(value)\n+                # for variable, value in zip(self.optimizer.variables, optimizer_variables):\n+                #     variable.assign(value)\n+                self._loss_tracker.update_state(loss)\n+\n+                # Callbacks\n                 callbacks.on_train_batch_end(step, logs)\n                 if self.stop_training:\n                     break\n \n             # Override with model metrics instead of last step logs\n-            epoch_logs = self._process_logs(self.get_metrics_result())\n+            epoch_logs = {\"loss\": self._loss_tracker.result()} # self._process_logs(self.get_metrics_result())\n \n             # Run validation.\n             if validation_data and self._should_eval(epoch, validation_freq):\n\n@@ -104,7 +104,7 @@ class Trainer(base_trainer.Trainer):\n                 val_sample_weight,\n             ) = data_adapters_utils.unpack_x_y_sample_weight(validation_data)\n \n-        # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n+        # Create an iterator that yields batches for one epoch.\n         epoch_iterator = EpochIterator(\n             x=x,\n             y=y,\n@@ -135,7 +135,7 @@ class Trainer(base_trainer.Trainer):\n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n-            for step, batch in epoch_iterator.enumerate_epoch():\n+            for step, batch in epoch_iterator.enumerate_epoch(return_type=\"tf\"):\n                 callbacks.on_train_batch_begin(step)\n                 logs = self.train_function(batch)\n                 callbacks.on_train_batch_end(step, logs)\n\n@@ -1,5 +1,7 @@\n import numpy as np\n+import pytest\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n@@ -15,6 +17,7 @@ class TFModel(layers.Dense, Trainer):\n         Trainer.__init__(self)\n \n \n+@pytest.mark.skipif(backend.backend() != \"tensorflow\", reason=\"Target the TF backend only.\")\n class TestTFTrainer(testing.TestCase):\n     def _test_basic_flow(self, run_eagerly, jit_compile):\n         model = TFModel(units=3)\n\n@@ -36,7 +36,6 @@ class InitializersTest(testing.TestCase):\n         self.assertEqual(initializer.seed, seed)\n         self.assertEqual(values.shape, shape)\n         self.assert_idempotent_config(initializer, external_config)\n-        values = values.numpy()\n         self.assertGreaterEqual(np.min(values), minval)\n         self.assertLess(np.max(values), maxval)\n \n\n@@ -106,7 +106,7 @@ class Mean(Metric):\n             shape=(), initializer=initializers.Zeros(), dtype=self.dtype\n         )\n         self.count = self.add_variable(\n-            shape=(), initializer=initializers.Zeros(), dtype=\"int64\"\n+            shape=(), initializer=initializers.Zeros(), dtype=\"int32\"\n         )\n \n     def update_state(self, values, sample_weight=None):\n@@ -122,7 +122,7 @@ class Mean(Metric):\n             num_samples = ops.sum(\n                 ops.ones(shape=(num_samples,)) * sample_weight\n             )\n-        self.count.assign(self.count + ops.cast(num_samples, dtype=\"int64\"))\n+        self.count.assign(self.count + ops.cast(num_samples, dtype=\"int32\"))\n \n     def reset_state(self):\n         self.total.assign(0.0)\n\n@@ -3,9 +3,10 @@ from keras_core.layers.layer import Layer\n from keras_core.utils import summary_utils\n from keras_core import backend\n \n-# TODO: clean up\n if backend.backend() == \"tensorflow\":\n     from keras_core.backend.tensorflow.trainer import Trainer\n+elif backend.backend() == \"jax\":\n+    from keras_core.backend.jax.trainer import Trainer\n else:\n     Trainer = None\n \n\n@@ -60,7 +60,7 @@ class Optimizer:\n \n         self.built = False\n         self.iterations = backend.Variable(\n-            0, name=\"iteration\", dtype=\"int64\", trainable=False\n+            0, name=\"iteration\", dtype=\"int32\", trainable=False\n         )\n         if isinstance(\n             learning_rate, learning_rate_schedule.LearningRateSchedule\n\n@@ -1,43 +1,3 @@\n-\"\"\"\n-## Simple case:\n-\n-metrics = [\"m1', \"m2\"]\n-outputs = x\n-\n-## Nested list case:\n-\n-metrics = [[\"m1', \"m2\"], [\"m3\"]]\n-outputs [x1, x2]\n-\n-## Nested dict case:\n-\n-metrics = {\"out_1\": [\"m1', \"m2\"], \"out_2\": [\"m3\"]]\n-outputs {\"out_1\": x1, \"out_2\": x2}\n-\n-## Process\n-\n-Case 1: known symbolic outputs\n-\n--> Flatten metrics to standard order at build time\n--> Flatten arrays to standard order at computation time\n-\n-Case 2: only arrays are known\n-\n--> expect exact matches between arrays and metrics\n--> Give array elements names based on dict keys or order,\n-used to uniquify public-facing metric names\n-\n-\n-\n-General process:\n-\n-1. At build time\n-- Check structure match\n-- Flatten everything\n-\n-\n-\n-\"\"\"\n from tensorflow import nest\n \n from keras_core import backend\n@@ -546,7 +506,6 @@ class CompileLoss(losses_module.Loss):\n                         flat_loss_weights.append(1.0)\n         self.flat_losses = flat_losses\n         self.flat_loss_weights = flat_loss_weights\n-        self.loss_tracker = metrics_module.Mean(name=\"loss\")\n         self.built = True\n \n     def call(self, y_true, y_pred):\n@@ -563,8 +522,7 @@ class CompileLoss(losses_module.Loss):\n                 value = w * ops.cast(loss(y_t, y_p), dtype=backend.floatx())\n                 loss_values.append(value)\n         if loss_values:\n-            total_loss = ops.sum(loss_values)\n-            self.loss_tracker.update_state(total_loss)\n+            total_loss = sum(loss_values)\n             return total_loss\n         return None\n \n\n@@ -177,7 +177,7 @@ def make_sample_weight_from_class_weight(y, class_weight):\n     if len(y.shape) > 1 and y.shape[-1] != 1:\n         y = np.argmax(y, axis=-1)\n     else:\n-        y = np.round(np.squeeze(y, axis=-1)).astype(\"int64\")\n+        y = np.round(np.squeeze(y, axis=-1)).astype(\"int32\")\n     for i in range(y.shape[0]):\n         sample_weight[i] = class_weight.get(int(y[i]), 1.0)\n     return sample_weight\n\n@@ -147,7 +147,7 @@ class TestArrayDataAdapter(testing.TestCase):\n     def _test_class_weights(self, target_encoding=\"int\"):\n         x = np.random.random((4, 2))\n         if target_encoding == \"int\":\n-            y = np.array([[0], [1], [2], [3]], dtype=\"int64\")\n+            y = np.array([[0], [1], [2], [3]], dtype=\"int32\")\n         else:\n             y = np.array(\n                 [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n\n@@ -84,11 +84,11 @@ def make_class_weight_map_fn(class_weight):\n             y_classes = tf.__internal__.smart_cond.smart_cond(\n                 tf.shape(y)[-1] > 1,\n                 lambda: tf.argmax(y, axis=-1),\n-                lambda: tf.cast(tf.round(tf.squeeze(y, axis=-1)), tf.int64),\n+                lambda: tf.cast(tf.round(tf.squeeze(y, axis=-1)), tf.int32),\n             )\n         else:\n             # Special casing for rank 1, where we can guarantee sparse encoding.\n-            y_classes = tf.cast(tf.round(y), tf.int64)\n+            y_classes = tf.cast(tf.round(y), tf.int32)\n \n         cw = tf.gather(class_weight_tensor, y_classes)\n         return x, y, cw\n\n@@ -1,5 +1,7 @@\n+import warnings\n from keras_core import backend\n from keras_core import operations as ops\n+from keras_core import metrics as metrics_module\n from keras_core.trainers.compile_utils import CompileLoss\n from keras_core.trainers.compile_utils import CompileMetrics\n \n@@ -34,13 +36,17 @@ class Trainer:\n         else:\n             self._compile_metrics = None\n         if jit_compile and run_eagerly:\n-            raise ValueError(\n-                \"If `jit_compile` is True, then `run_eagerly` cannot also be True.\"\n+            jit_compile = False\n+            warnings.warn(\n+                \"If `run_eagerly` is True, then `jit_compile` cannot also be True. \"\n+                \"Disabling `jit_compile`.\",\n+                stacklevel=2,\n             )\n         self.jit_compile = jit_compile\n         self.run_eagerly = run_eagerly\n         self.stop_training = False\n         self.compiled = True\n+        self._loss_tracker = metrics_module.Mean(name=\"loss\")\n \n     @property\n     def jit_compile(self):\n@@ -60,9 +66,7 @@ class Trainer:\n \n     @property\n     def metrics(self):\n-        metrics = []\n-        if self._compile_loss is not None and self._compile_loss.built:\n-            metrics.append(self._compile_loss.loss_tracker)\n+        metrics = [self._loss_tracker]\n         metrics.extend(self._metrics[:])\n         if self._compile_metrics is not None and self._compile_metrics.built:\n             metrics += [self._compile_metrics]\n@@ -134,8 +138,11 @@ class Trainer:\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n             )\n         if len(losses) == 1:\n-            return losses[0]\n-        return ops.sum(losses)\n+            total_loss = losses[0]\n+        else:\n+            total_loss = ops.sum(losses)\n+        self._loss_tracker.update_state(total_loss)\n+        return total_loss\n \n     def compute_metrics(self, x, y, y_pred, sample_weight):\n         \"\"\"Update metric states and collect all metrics to be returned.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
