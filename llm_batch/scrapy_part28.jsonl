{"custom_id": "scrapy#86aa3bcb6515902cb9e218dc10081985b2e12fee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 31 | Files Changed: 2 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): -2/6 | Churn Δ: 57 | Churn Cumulative: 470 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,6 +5,8 @@ Functions for dealing with markup text\n import re\n import htmlentitydefs\n \n+from scrapy.utils.python import str_to_unicode\n+\n _ent_re = re.compile(r'&(#?)([^&;]+);')\n _tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n \n@@ -45,16 +47,10 @@ def remove_entities(text, keep=(), remove_illegal=True):\n             else:\n                 return u'&%s;' % m.group(2)\n \n-    if not isinstance(text, unicode):\n-        text = text.decode('utf-8')\n-\n-    return _ent_re.sub(convert_entity, text)\n+    return _ent_re.sub(convert_entity, str_to_unicode(text))\n \n def has_entities(text):\n-    if not isinstance(text, unicode):\n-        text = text.decode('utf-8')\n-\n-    return bool(_ent_re.search(text))\n+    return bool(_ent_re.search(str_to_unicode(text)))\n \n def replace_tags(text, token=''):\n     \"\"\"Replace all markup tags found in the given text by the given token. By\n@@ -64,18 +60,12 @@ def replace_tags(text, token=''):\n \n     Always returns a unicode string.\n     \"\"\"\n-    if not isinstance(text, unicode):\n-        text = text.decode('utf-8')\n-\n-    return _tag_re.sub(token, text)\n+    return _tag_re.sub(token, str_to_unicode(text))\n \n \n def remove_comments(text):\n     \"\"\" Remove HTML Comments. \"\"\"\n-    if not isinstance(text, unicode):\n-        text = text.decode('utf-8')\n-\n-    return re.sub('<!--.*?-->', u'', text, re.DOTALL)\n+    return re.sub('<!--.*?-->', u'', str_to_unicode(text), re.DOTALL)\n       \n def remove_tags(text, which_ones=()):\n     \"\"\" Remove HTML Tags only. \n@@ -83,16 +73,13 @@ def remove_tags(text, which_ones=()):\n         which_ones -- is a tuple of which tags we want to remove.\n                       if is empty remove all tags.\n     \"\"\"\n-    if not isinstance(text, unicode):\n-        text = text.decode('utf-8')\n-\n     if len(which_ones) > 0:\n         tags = [ '<%s>|<%s .*?>|</%s>' % (tag,tag,tag) for tag in which_ones ]\n         reg_exp_remove_tags = '|'.join(tags)\n     else:\n         reg_exp_remove_tags = '<.*?>'\n     re_tags = re.compile(reg_exp_remove_tags, re.DOTALL)\n-    return re_tags.sub(u'', text)\n+    return re_tags.sub(u'', str_to_unicode(text))\n \n def remove_tags_with_content(text, which_ones=()):\n     \"\"\" Remove tags and its content.\n@@ -100,12 +87,9 @@ def remove_tags_with_content(text, which_ones=()):\n         which_ones -- is a tuple of which tags with its content we want to remove.\n                       if is empty do nothing.\n     \"\"\"\n-    if not isinstance(text, unicode):\n-        text = text.decode('utf-8')\n-\n     tags = [ '<%s.*?</%s>' % (tag,tag) for tag in which_ones ]\n     re_tags_remove = re.compile('|'.join(tags), re.DOTALL)\n-    return re_tags_remove.sub(u'', text)\n+    return re_tags_remove.sub(u'', str_to_unicode(text))\n \n def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n     \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n@@ -113,11 +97,8 @@ def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n         which_ones -- is a tuple of which escape chars we want to remove.\n                       By default removes \\n, \\t, \\r.\n     \"\"\"\n-    if not isinstance(text, unicode):\n-        text = text.decode('utf-8')\n-\n     re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n-    return re_escape_chars.sub(u'', text)\n+    return re_escape_chars.sub(u'', str_to_unicode(text))\n \n def unquote_markup(text, keep=(), remove_illegal=True):\n     \"\"\"\n@@ -139,9 +120,7 @@ def unquote_markup(text, keep=(), remove_illegal=True):\n         fragments.append(txt[offset:])\n         return fragments\n \n-    if not isinstance(text, unicode):\n-        text = text.decode('utf-8')\n-\n+    text = str_to_unicode(text)\n     ret_text = u''\n     for fragment in _get_fragments(text, _cdata_re):\n         if isinstance(fragment, basestring):\n\n@@ -53,6 +53,22 @@ def unique(list_):\n     return result\n \n \n+def str_to_unicode(text):\n+    if isinstance(text, str):\n+        return text.decode('utf-8')\n+    elif isinstance(text, unicode):\n+        return text\n+    else:\n+        raise TypeError('str_to_unicode can only receive a string object')\n+\n+def unicode_to_str(text):\n+    if isinstance(text, unicode):\n+        return text.encode('utf-8')\n+    elif isinstance(text, str):\n+        return text\n+    else:\n+        raise TypeError('unicode_to_str can only receive a unicode object')\n+\n def re_rsearch(pattern, text, chunk_size=1024):\n     \"\"\"\n     This function does a reverse search in a text using a regular expression\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b9360f659ebb02887ed8e7524b70391093bd2af7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 27 | Churn Cumulative: 27 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,27 @@\n+import unittest\n+\n+from scrapy.utils.python import str_to_unicode, unicode_to_str\n+\n+class UtilsPythonTestCase(unittest.TestCase):\n+    def test_str_to_unicode(self):\n+        # converting an utf-8 encoded string to unicode\n+        self.assertEqual(str_to_unicode('lel\\xc3\\xb1e'), u'lel\\xf1e')\n+\n+        # converting a unicode to unicode should return the same object\n+        self.assertEqual(str_to_unicode(u'\\xf1e\\xf1e\\xf1e'), u'\\xf1e\\xf1e\\xf1e')\n+\n+        # converting a strange object should raise TypeError\n+        self.assertRaises(TypeError, str_to_unicode, 423)\n+\n+    def test_unicode_to_str(self):\n+        # converting a unicode object to an utf-8 encoded string\n+        self.assertEqual(unicode_to_str(u'\\xa3 49'), '\\xc2\\xa3 49')\n+\n+        # converting a regular string to string should return the same object\n+        self.assertEqual(unicode_to_str('lel\\xf1e'), 'lel\\xf1e')\n+\n+        # converting a strange object should raise TypeError\n+        self.assertRaises(TypeError, unicode_to_str, unittest)\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ee3626763434cd6943dbf6d387be0fb718f41ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 131 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,18 +3,20 @@ from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n-from scrapy.xpath.selector import XmlXPathSelector\n-from scrapy.core.exceptions import UsageError, NotConfigured\n+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n+from scrapy.core.exceptions import UsageError, NotConfigured, NotSupported\n \n class XMLFeedSpider(BaseSpider):\n     \"\"\"\n     This class intends to be the base class for spiders that scrape\n     from XML feeds.\n \n-    You can choose whether to parse the file using the iternodes tool,\n-    or not using it (which just splits the tags using xpath)\n+    You can choose whether to parse the file using the 'iternodes' iterator,\n+    an 'xml' selector, or an 'html' selector.\n+    In most cases, it's convenient to use iternodes, since it's a faster and\n+    cleaner.\n     \"\"\"\n-    iternodes = True\n+    iterator = 'iternodes'\n     itertag = 'item'\n \n     def process_results(self, results, response):\n@@ -46,10 +48,14 @@ class XMLFeedSpider(BaseSpider):\n             raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n \n         response = self.adapt_response(response)\n-        if self.iternodes:\n+        if self.iterator == 'iternodes':\n             nodes = xmliter(response, self.itertag)\n-        else:\n+        elif self.iterator == 'xml':\n             nodes = XmlXPathSelector(response).x('//%s' % self.itertag)\n+        elif self.iterator == 'html':\n+            nodes = HtmlXPathSelector(response).x('//%s' % self.itertag)\n+        else:\n+            raise NotSupported('Unsupported node iterator')\n \n         return self.parse_nodes(response, nodes)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f59f1c8bc07dc378e7898ee94079d1def964d2fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 11/10 | Churn Δ: 68 | Churn Cumulative: 493 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 0.6595744680851063\n\nDIFF:\n@@ -1,7 +1,9 @@\n import unittest\n+from cStringIO import StringIO\n \n-from scrapy.utils.misc import hash_values\n+from scrapy.utils.misc import hash_values, items_to_csv\n from scrapy.core.exceptions import UsageError\n+from scrapy.item import ScrapedItem\n \n class UtilsMiscTestCase(unittest.TestCase):\n     def test_hash_values(self):\n@@ -10,5 +12,41 @@ class UtilsMiscTestCase(unittest.TestCase):\n \n         self.assertRaises(UsageError, hash_values, 'some', None, 'value')\n \n+    def test_items_to_csv(self):\n+        item_1 = ScrapedItem()\n+        item_1.attribute('_hidden', '543565')\n+        item_1.attribute('id', '3213')\n+        item_1.attribute('name', 'Item 1')\n+        item_1.attribute('description', 'Really cute item')\n+        item_1.attribute('url', 'http://dummyurl.com')\n+\n+        item_2 = ScrapedItem()\n+        item_2.attribute('_hidden', 'lala')\n+        item_2.attribute('id', '1234')\n+        item_2.attribute('name', 'Item 2')\n+        item_2.attribute('description', 'This item rocks!')\n+        item_2.attribute('url', 'http://dummyurl.com/2')\n+        item_2.attribute('supplier', 'A random supplier')\n+\n+        file = StringIO()\n+        items_to_csv(file, [item_1, item_2])\n+        file.reset()\n+        self.assertEqual(file.read(),\n+            '\"description\";\"id\";\"name\";\"url\"\\r\\n' +\n+            '\"Really cute item\";\"3213\";\"Item 1\";\"http://dummyurl.com\"\\r\\n' +\n+            '\"This item rocks!\";\"1234\";\"Item 2\";\"http://dummyurl.com/2\"\\r\\n')\n+\n+        file = StringIO()\n+        items_to_csv(file, [item_2, item_1])\n+        file.reset()\n+        self.assertEqual(file.read(),\n+            '\"description\";\"id\";\"name\";\"supplier\";\"url\"\\r\\n' +\n+            '\"This item rocks!\";\"1234\";\"Item 2\";\"A random supplier\";\"http://dummyurl.com/2\"\\r\\n' +\n+            '\"Really cute item\";\"3213\";\"Item 1\";\"\";\"http://dummyurl.com\"\\r\\n')\n+\n+        file = StringIO()\n+        items_to_csv(file, [])\n+        self.assertEqual(file.tell(), 0)\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -6,11 +6,12 @@ from __future__ import with_statement\n import re\n import string\n import hashlib\n+import csv\n \n from twisted.internet import defer\n \n from scrapy.core.exceptions import UsageError\n-from scrapy.utils.python import flatten\n+from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.markup import remove_entities\n from scrapy.utils.defer import defer_succeed\n \n@@ -127,3 +128,28 @@ def render_templatefile(path, **kwargs):\n         file.write(content)\n \n \n+def items_to_csv(file, items, delimiter=';'):\n+    \"\"\"\n+    This function takes a list of items and stores their attributes\n+    in a csv file given in 'file' (which can be either a descriptor, or a filename).\n+    The attributes are the ones found in the first item of the list, so\n+    if it lacks any attribute that other item has, that attribute will be missing.\n+    The written file will be encoded as utf-8.\n+    \"\"\"\n+    if not items or not hasattr(items, '__iter__'):\n+        return\n+\n+    if isinstance(file, basestring):\n+        file = open(file, 'a+')\n+    csv_file = csv.writer(file, delimiter=delimiter, quoting=csv.QUOTE_ALL)\n+    header = sorted([key for key in items[0].__dict__.keys() if not key.startswith('_')])\n+    if not file.tell():\n+        csv_file.writerow(header)\n+\n+    for item in items:\n+        row = []\n+        for attrib in header:\n+            value = getattr(item, attrib, None)\n+            value = unicode_to_str(value) if isinstance(value, basestring) else value\n+            row.append(value)\n+        csv_file.writerow(row)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5b159d0f040125a15e58386b69983d4c4c374f9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 885 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,6 +44,14 @@ class UtilsMiscTestCase(unittest.TestCase):\n             '\"This item rocks!\";\"1234\";\"Item 2\";\"A random supplier\";\"http://dummyurl.com/2\"\\r\\n' +\n             '\"Really cute item\";\"3213\";\"Item 1\";\"\";\"http://dummyurl.com\"\\r\\n')\n \n+        file = StringIO()\n+        items_to_csv(file, [item_1, item_2], headers=['id', 'name'], delimiter=',')\n+        file.reset()\n+        self.assertEqual(file.read(),\n+            '\"id\",\"name\"\\r\\n' +\n+            '\"3213\",\"Item 1\"\\r\\n' +\n+            '\"1234\",\"Item 2\"\\r\\n')\n+\n         file = StringIO()\n         items_to_csv(file, [])\n         self.assertEqual(file.tell(), 0)\n\n@@ -110,15 +110,13 @@ def unquote_markup(text, keep=(), remove_illegal=True):\n     _cdata_re = re.compile(r'((?P<cdata_s><!\\[CDATA\\[)(?P<cdata_d>.*?)(?P<cdata_e>\\]\\]>))', re.DOTALL)\n \n     def _get_fragments(txt, pattern):\n-        fragments = []\n         offset = 0\n         for match in pattern.finditer(txt):\n             match_s, match_e = match.span(1)\n-            fragments.append(txt[offset:match_s])\n-            fragments.append(match)\n+            yield txt[offset:match_s]\n+            yield match\n             offset = match_e\n-        fragments.append(txt[offset:])\n-        return fragments\n+        yield txt[offset:]\n \n     text = str_to_unicode(text)\n     ret_text = u''\n\n@@ -128,12 +128,12 @@ def render_templatefile(path, **kwargs):\n         file.write(content)\n \n \n-def items_to_csv(file, items, delimiter=';'):\n+def items_to_csv(file, items, delimiter=';', headers=None):\n     \"\"\"\n     This function takes a list of items and stores their attributes\n     in a csv file given in 'file' (which can be either a descriptor, or a filename).\n-    The attributes are the ones found in the first item of the list, so\n-    if it lacks any attribute that other item has, that attribute will be missing.\n+    The saved attributes are either the ones found in the 'headers' parameter\n+    (if specified) or the first item's list of public attributes.\n     The written file will be encoded as utf-8.\n     \"\"\"\n     if not items or not hasattr(items, '__iter__'):\n@@ -142,7 +142,7 @@ def items_to_csv(file, items, delimiter=';'):\n     if isinstance(file, basestring):\n         file = open(file, 'a+')\n     csv_file = csv.writer(file, delimiter=delimiter, quoting=csv.QUOTE_ALL)\n-    header = sorted([key for key in items[0].__dict__.keys() if not key.startswith('_')])\n+    header = headers or sorted([key for key in items[0].__dict__.keys() if not key.startswith('_')])\n     if not file.tell():\n         csv_file.writerow(header)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9557e242037f430441e35d5c23288e56ad6fc63f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 51 | Files Changed: 2 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 97 | Churn Cumulative: 416 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.4\n\nDIFF:\n@@ -108,10 +108,6 @@ class ExecutionManager(object):\n                 sites.add(a)\n \n         perdomain = {}\n-        def _add(domain, request):\n-            if domain not in perdomain:\n-                perdomain[domain] = []\n-            perdomain[domain] += [request]\n \n         # sites\n         for domain in sites:\n@@ -119,15 +115,15 @@ class ExecutionManager(object):\n             if not spider:\n                 log.msg('Could not find spider for %s' % domain, log.ERROR)\n                 continue\n-            for url in self._start_urls(spider):\n-                request = Request(url, callback=spider.parse, dont_filter=True)\n-                _add(domain, request)\n+            reqs = spider.start_requests()\n+            perdomain.setdefault(domain, []).extend(reqs)\n+\n         # urls\n         for url in urls:\n             spider = spiders.fromurl(url)\n             if spider:\n-                request = Request(url=url, callback=spider.parse, dont_filter=True)\n-                _add(spider.domain_name, request)\n+                reqs = spider.start_requests([url])\n+                perdomain.setdefault(domain, []).extend(reqs)\n             else:\n                 log.msg('Could not find spider for <%s>' % url, log.ERROR)\n \n@@ -140,10 +136,7 @@ class ExecutionManager(object):\n             if not spider:\n                 log.msg('Could not find spider for %s' % request, log.ERROR)\n                 continue\n-            _add(spider.domain_name, request)\n+            perdomain.setdefault(domain, []).append(request)\n         return perdomain\n \n-    def _start_urls(self, spider):\n-        return spider.start_urls if hasattr(spider.start_urls, '__iter__') else [spider.start_urls]\n-        \n scrapymanager = ExecutionManager()\n\n@@ -5,13 +5,9 @@ from zope.interface import Interface, Attribute, invariant, implements\n from twisted.plugin import IPlugin\n \n from scrapy import log\n+from scrapy.http import Request\n from scrapy.core.exceptions import UsageError\n \n-def _valid_start_urls(obj):\n-    \"\"\"Check the start urls specified are valid\"\"\"\n-    if not obj.start_urls:\n-        raise UsageError(\"A start url is required\")\n-\n def _valid_domain_name(obj):\n     \"\"\"Check the domain name specified is valid\"\"\"\n     if not obj.domain_name:\n@@ -28,10 +24,6 @@ def _valid_download_delay(obj):\n class ISpider(Interface, IPlugin) :\n     \"\"\"Interface to be implemented by site-specific web spiders\"\"\"\n     \n-    start_urls = Attribute(\n-        \"\"\"A sequence of URLs to retrieve to initiate the spider for this \n-        site. A single URL may also be provided here.\"\"\")\n-    \n     domain_name = Attribute(\n          \"\"\"The domain name of the site to be scraped.\"\"\")\n \n@@ -43,37 +35,9 @@ class ISpider(Interface, IPlugin) :\n     user_agent = Attribute(\n          \"\"\"Optional User-Agent to use for this domain\"\"\")\n \n-    invariant(_valid_start_urls)\n     invariant(_valid_domain_name)\n     invariant(_valid_download_delay)\n \n-    def parse(self, pagedata) :\n-        \"\"\"This is first called with the data corresponding to start_url. It\n-        must return a (possibly empty) sequence where each element is either:\n-         * A Request object for further processing.\n-         * An object that extends ScrapedItem (defined in scrapeditem module)\n-         * or None (this will be ignored)\n-\n-        When a Request object is returned, the Request is scheduled, then\n-        downloaded and finally its results is handled to the Request callback.\n-        That callback must behave the same way as this function.\n-\n-        When a ScrapedItem is returned, it is passed to the transformation pipeline\n-        and finally the destination systems are updated.\n-\n-        The simplest way to use this is to have a method in your class for each\n-        page type. So each function knows the layout and how to extract data\n-        for a single page (or set of similar pages). A typical class might work\n-        like:\n-        * parse() parses the landing page and returns Requests\n-          for a category() function to parse category pages.\n-        * category() parses the category pages and returns links and callbacks\n-          for a item() function to parse item pages.\n-        * item() parses the item details page and returns objects that\n-          extend ScrapedItem\n-        \"\"\"\n-        pass\n-\n     def init_domain(self):\n         \"\"\"This is first called to initialize domain specific quirks, like \n         session cookies or login stuff\n@@ -83,9 +47,13 @@ class ISpider(Interface, IPlugin) :\n \n class BaseSpider(object):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n-    class.\"\"\"\n+    class.\n+    \n+    \"\"\"\n     \n     implements(ISpider)\n+\n+    start_urls = []\n     domain_name = None\n     extra_domain_names = []\n \n@@ -94,3 +62,37 @@ class BaseSpider(object):\n         method to send log messages from your spider\n         \"\"\"\n         log.msg(message, domain=self.domain_name, level=level)\n+\n+    def start_requests(self, urls=None):\n+        \"\"\"Return the requests to crawl when this spider is opened for\n+        scraping. urls contain the urls passed from command line (if any),\n+        otherwise None if the enrie domain was requested for scraping.\n+\n+        This function must return a list of Requests to be crawled, based on\n+        the given urls. The Requests must include a callback function which\n+        must return a list of:\n+         * Request's for further crawling\n+         * ScrapedItem's for processing\n+         * Both\n+        Or None (which will be treated the same way as an empty list)\n+\n+        When a Request object is returned, the Request is scheduled, then\n+        downloaded and finally its results is handled to the Request callback.\n+\n+        When a ScrapedItem is returned, it is passed to the item pipeline.\n+\n+        Unless this method is overrided, the start_urls attribute will be used\n+        to create the initial requests (when urls is None).\n+        \"\"\"\n+        if urls is None:\n+            urls = self.start_urls\n+        requests = [Request(url, callback=self.parse, dont_filter=True) for url in urls]\n+        return requests\n+        \n+        return []\n+\n+    def parse(self, response):\n+        \"\"\"This is the default callback function used to parse the start\n+        requests, although it can be overrided in descendant spiders.\n+        \"\"\"\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e89ad0be8c49a861414bb1f41883478589a5ab02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 232 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -123,7 +123,7 @@ class ExecutionManager(object):\n             spider = spiders.fromurl(url)\n             if spider:\n                 reqs = spider.start_requests([url])\n-                perdomain.setdefault(domain, []).extend(reqs)\n+                perdomain.setdefault(spider.domain_name, []).extend(reqs)\n             else:\n                 log.msg('Could not find spider for <%s>' % url, log.ERROR)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6fa33f44630c5862849b28f3d7f68abdf676dabf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 234 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -136,7 +136,7 @@ class ExecutionManager(object):\n             if not spider:\n                 log.msg('Could not find spider for %s' % request, log.ERROR)\n                 continue\n-            perdomain.setdefault(domain, []).append(request)\n+            perdomain.setdefault(spider.domain_name, []).append(request)\n         return perdomain\n \n scrapymanager = ExecutionManager()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f932a3d842de6b3662bfa0b81ee4035cb37d0885", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 602 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from twisted.python.failure import Failure\n from pydispatch import dispatcher\n \n from scrapy import log\n+from scrapy.stats import stats\n from scrapy.conf import settings\n from scrapy.core import signals\n from scrapy.core.scheduler import Scheduler\n@@ -259,6 +260,7 @@ class ExecutionEngine(object):\n                 if not isinstance(_failure.value, IgnoreRequest):\n                     referer = None if not isinstance(response, Response) else response.request.headers.get('Referer', None)\n                     log.msg(\"Error while spider was processing <%s> from <%s>: %s\" % (request.url, referer, _failure), log.ERROR, domain=domain)\n+                    stats.incpath(\"%s/spider_exceptions/%s\" % (domain, _failure.value.__class__.__name__))\n \n             def _bugtrap(_failure):\n                 log.msg('FRAMEWORK BUG processing %s: %s' % (request, _failure), log.ERROR, domain=domain)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e5b85ca9edeb0ba62864dd9a37dcfcd951f225b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a593af5f530500040cbece93f2e0221ef885bd46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 87 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ Function for dealing with databases\n import re\n from scrapy.conf import settings\n from scrapy import log\n-from scrapy.core.engine import scrapyengine\n \n class URIValidationError(Exception):\n     pass\n@@ -28,6 +27,7 @@ def mysql_connect(db_uri_or_dict, **kwargs):\n     Connects to a MySQL DB given a mysql URI\n     \"\"\"\n     import MySQLdb\n+    from scrapy.core.engine import scrapyengine\n \n     if isinstance(db_uri_or_dict, dict):\n         d = db_uri_or_dict\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5eb7e5ba89dfbcc4fd56b3e14716d34806388a8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 99 | Lines Deleted: 35 | Files Changed: 6 | Hunks: 31 | Methods Changed: 15 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 134 | Churn Cumulative: 1131 | Contributors (this commit): 2 | Commits (past 90d): 38 | Contributors (cumulative): 9 | DMM Complexity: 0.8181818181818182\n\nDIFF:\n@@ -4,7 +4,7 @@ import re\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n from scrapy.utils.url import canonicalize_url\n from scrapy.utils.misc import extract_regex\n-from scrapy.utils.python import flatten\n+from scrapy.utils.python import flatten, str_to_unicode\n from scrapy.item.adaptors import adaptize\n \n def to_unicode(value):\n@@ -19,7 +19,7 @@ def to_unicode(value):\n     Output: list of unicodes\n     \"\"\"\n     if hasattr(value, '__iter__'):\n-        return [ unicode(v) for v in value ]\n+        return [ str_to_unicode(v) if isinstance(v, basestring) else str_to_unicode(str(v)) for v in value ]\n     else:\n         raise TypeError('to_unicode must receive an iterable.')\n \n\n@@ -1,21 +1,82 @@\n # -*- coding: utf8 -*-\n+import os\n import unittest\n+import re\n+\n from scrapy.contrib import adaptors\n-from scrapy.http import Response, ResponseBody\n-from scrapy.xpath.selector import XmlXPathSelector\n+from scrapy.http import Response, ResponseBody, Headers\n+from scrapy.xpath.selector import HtmlXPathSelector, XmlXPathSelector\n \n class AdaptorsTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.samplesdir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'sample_data', 'adaptors'))\n+\n+    def get_selector(self, domain, url, sample_filename, headers=None, selector=HtmlXPathSelector):\n+        sample_filename = os.path.join(self.samplesdir, sample_filename)\n+        body = file(sample_filename).read()\n+        response = Response(domain=domain, url=url, headers=Headers(headers), status='200', body=body)\n+        return selector(response)\n+\n     def test_extract(self):\n-        sample_xsel = XmlXPathSelector(text='<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>')\n-        self.assertEqual(adaptors.extract(sample_xsel.x('/')),\n-                         ['<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('xml/*')),\n-                         ['<tag1>foo<tag2>bar</tag2></tag1>', '<tag3 value=\"mytag\">test</tag3>'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('xml/@id')), ['2'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1')), ['<tag1>foo<tag2>bar</tag2></tag1>'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1//text()')), ['foo', 'bar'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//text()')), ['foo', 'bar', 'test'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag3/@value')), ['mytag'])\n+        def check_extractor(x, pound=True, euro=True):\n+            poundre = re.compile(r'<span class=\"pound\" .*?>(.*?)</span>')\n+            eurore = re.compile(r'<span class=\"euro\" .*?>(.*?)</span>')\n+\n+            if pound:\n+                self.assertEqual(adaptors.extract(x.x(\"//span[@class='pound']/text()\")), [u'\\xa3'])\n+                self.assertEqual(adaptors.extract(x.x(\"//span[@class='pound']/@value\")), [u'\\xa3'])\n+                self.assertEqual(adaptors.extract(x.re(poundre)), [u'\\xa3'])\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundent']/text()\")), [u'\\xa3'])\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundent']/@value\")), [u'\\xa3'])\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundnum']/text()\")), [u'\\xa3'])\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundnum']/@value\")), [u'\\xa3'])\n+            if euro:\n+                self.assertEqual(adaptors.extract(x.x(\"//span[@class='euro']/text()\")), [u'\\u20ac'])\n+                self.assertEqual(adaptors.extract(x.x(\"//span[@class='euro']/@value\")), [u'\\u20ac'])\n+                self.assertEqual(adaptors.extract(x.re(eurore)), [u'\\u20ac'])\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euroent']/text()\")), [u'\\u20ac'])\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euroent']/@value\")), [u'\\u20ac'])\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euronum']/text()\")), [u'\\u20ac'])\n+            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euronum']/@value\")), [u'\\u20ac'])\n+\n+        x = self.get_selector('example.com',\n+                         'http://www.example.com/test/utf8',\n+                         'enc-utf8.html',\n+                         {})\n+        check_extractor(x)\n+\n+        x = self.get_selector('example.com',\n+                         'http://www.example.com/test/latin1',\n+                         'enc-latin1.html',\n+                         {})\n+        check_extractor(x, euro=False)\n+\n+        x = self.get_selector('example.com',\n+                         'http://www.example.com/test/cp1252',\n+                         'enc-cp1252.html',\n+                         {})\n+        check_extractor(x)\n+\n+        # HTTP utf-8 | Meta latin1 | Content ascii | using entities\n+        x = self.get_selector('example.com',\n+                         'http://www.example.com/test/ascii',\n+                         'enc-ascii.html',\n+                         {'Content-Type': ['text/html; charset=utf-8']})\n+        check_encoding(x, pound=False, euro=False)\n+\n+        # Test for inconsistencies between HTTP header encoding and \n+        # META header encoding. It must prefer HTTP header like browsers do\n+        x = self.get_selector('example.com',\n+                         'http://www.example.com/test/utf8-meta-latin1',\n+                         'enc-utf8-meta-latin1.html',\n+                         {'Content-Type': ['text/html; charset=utf-8']})\n+        check_extractor(x)\n+\n+\n+    def test_extract_unquoted(self):\n+        x = self.get_selector('example.com', 'http://www.example.com/test_unquoted', 'extr_unquoted.xml', selector=XmlXPathSelector)\n+        self.assertEqual(adaptors.extract_unquoted(x.x('//tag1/text()')), [u'test text & &', u'more test text &amp; &gt;', u'blah&blah'])\n+        self.assertEqual(adaptors.extract_unquoted(x.x('//tag2/text()')), [u'blaheawfds<'])\n \n     def test_extract_links(self):\n         test_data = \"\"\"<html><body>\n@@ -44,44 +105,54 @@ class AdaptorsTestCase(unittest.TestCase):\n         self.assertEqual(sample_adaptor(sample_xsel.x('//a[@onclick]').re(r'opensomething\\(\\'(.*?)\\'\\)')),\n                          [u'http://foobar.com/my_html1.html', u'http://foobar.com/dummy/my_html2.html'])\n \n+        \n     def test_to_unicode(self):\n-        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'luluñ', 1, 'áé']),\n+        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'lulu\\xc3\\xb1', 1, '\\xc3\\xa1\\xc3\\xa9']),\n                          [u'lala', u'lele', u'lulu\\xf1', u'1', u'\\xe1\\xe9'])\n \n+        \n     def test_regex(self):\n         adaptor = adaptors.Regex(regex=r'href=\"(.*?)\"')\n         self.assertEqual(adaptor(['<a href=\"lala.com\">dsa</a><a href=\"pepe.co.uk\"></a>',\n                                   '<a href=\"das.biz\">href=\"lelelel.net\"</a>']),\n                                   ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n \n+        \n     def test_unquote_all(self):\n         self.assertEqual(adaptors.Unquote(keep=[])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n \n+        \n     def test_unquote(self):\n         self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n \n+        \n     def test_remove_tags(self):\n         test_data = ['<a href=\"lala\">adsaas<br /></a>', '<div id=\"1\"><table>dsadasf</table></div>']\n         self.assertEqual(adaptors.remove_tags(test_data), ['adsaas', 'dsadasf'])\n \n+        \n     def test_remove_root(self):\n         self.assertEqual(adaptors.remove_root(['<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>']),\n                          ['lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br />'])\n \n+        \n     def test_remove_multispaces(self):\n         self.assertEqual(adaptors.clean_spaces(['  hello,  whats     up?', 'testing testingtesting      testing']),\n                          [' hello, whats up?', 'testing testingtesting testing'])\n \n+        \n     def test_strip(self):\n         self.assertEqual(adaptors.strip([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n                          ['hi there, sweety ;D', 'I CAN HAZ TEST??'])\n         self.assertEqual(adaptors.strip('      hello there, this is my test     '),\n                          'hello there, this is my test')\n \n+        \n     def test_drop_empty_elements(self):\n         self.assertEqual(adaptors.drop_empty([1, 2, None, 5, 0, 6, False, 'hi']),\n                          [1, 2, 5, 6, 'hi'])\n \n+        \n     def test_delist(self):\n         self.assertEqual(adaptors.Delist()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n                          'hi there fellas. this is my test.')\n\n@@ -1,9 +0,0 @@\n-import sys\n-from unittest import TestCase, main\n-\n-class DefaultEncodingTest(TestCase):\n-    def test_defaultencoding(self):\n-        self.assertEqual(sys.getdefaultencoding(), 'utf-8')\n-\n-if __name__ == \"__main__\":\n-    main()\n\n@@ -3,16 +3,16 @@ import re, csv\n from scrapy.xpath import XmlXPathSelector\n from scrapy.http import Response\n from scrapy import log\n-from scrapy.utils.python import re_rsearch\n+from scrapy.utils.python import re_rsearch, str_to_unicode, unicode_to_str\n \n-def _normalize_input(obj):\n+def _normalize_input(obj, unicode=True):\n     assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n     if isinstance(obj, Response):\n-        return obj.body.to_unicode()\n+        return obj.body.to_unicode() if unicode else obj.body.to_string()\n     elif isinstance(obj, str):\n-        return obj.decode('utf-8')\n+        return obj.decode('utf-8') if unicode else obj\n     else:\n-        return obj\n+        return obj if unicode else obj.encode('utf-8')\n \n def xmliter(obj, nodename):\n     \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n@@ -51,9 +51,9 @@ def csviter(obj, delimiter=None, headers=None):\n     for the returned dictionaries, if not the first row is used.\n     \"\"\"\n     def _getrow(csv_r):\n-        return [field.decode() for field in csv_r.next()]\n+        return [str_to_unicode(field) for field in csv_r.next()]\n \n-    lines = _normalize_input(obj).splitlines(True)\n+    lines = _normalize_input(obj, unicode=False).splitlines(True)\n     if delimiter:\n         csv_r = csv.reader(lines, delimiter=delimiter)\n     else:\n\n@@ -9,6 +9,8 @@ import urllib\n import posixpath\n import cgi\n \n+from scrapy.utils.python import unicode_to_str\n+\n def url_is_from_any_domain(url, domains):\n     \"\"\"Return True if the url belongs to the given domain\"\"\"\n     host = urlparse.urlparse(url).hostname\n@@ -141,7 +143,7 @@ def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n     For examples see the tests in scrapy.tests.test_utils_url\n     \"\"\"\n \n-    url = url.encode('utf-8')\n+    url = unicode_to_str(url)\n     parts = list(urlparse.urlparse(url))\n     keyvals = cgi.parse_qsl(parts[4], keep_blank_values)\n     keyvals.sort()\n\n@@ -65,13 +65,13 @@ class XPathSelector(object):\n                 text = unicode(data, 'utf-8', errors='ignore') if data else u''\n             elif isinstance(self.xmlNode, libxml2.xmlAttr): \n                 # serialization doesn't work sometimes for xmlAttr types\n-                text = unicode(self.xmlNode.content, errors='ignore')\n+                text = unicode(self.xmlNode.content, 'utf-8', errors='ignore')\n             else:\n                 data = self.xmlNode.serialize('utf-8')\n                 text = unicode(data, 'utf-8', errors='ignore') if data else u''\n         else:\n             try:\n-                text = unicode(self.xmlNode, errors='ignore')\n+                text = unicode(self.xmlNode, 'utf-8', errors='ignore')\n             except TypeError:  # catched when self.xmlNode is a float - see tests\n                 text = unicode(self.xmlNode)\n         return text\n@@ -79,7 +79,7 @@ class XPathSelector(object):\n     def extract_unquoted(self):\n         \"\"\"Get unescaped contents from the text node (no entities, no CDATA)\"\"\"\n         if self.x('self::text()'):\n-            return unicode(self.xmlNode.getContent(), errors='ignore')\n+            return unicode(self.xmlNode.getContent(), 'utf-8', errors='ignore')\n         else:\n             return u''\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bbd2a9d6f089a52600e2e64f2b652cd47ef3a28e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 314 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -62,7 +62,7 @@ class AdaptorsTestCase(unittest.TestCase):\n                          'http://www.example.com/test/ascii',\n                          'enc-ascii.html',\n                          {'Content-Type': ['text/html; charset=utf-8']})\n-        check_encoding(x, pound=False, euro=False)\n+        check_extractor(x, pound=False, euro=False)\n \n         # Test for inconsistencies between HTTP header encoding and \n         # META header encoding. It must prefer HTTP header like browsers do\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#25a38e53d5d026da56e4791fa579cb1df623d771", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 191 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ import re\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n from scrapy.utils.url import canonicalize_url\n from scrapy.utils.misc import extract_regex\n-from scrapy.utils.python import flatten, str_to_unicode\n+from scrapy.utils.python import flatten, str_to_unicode, unicode_to_str\n from scrapy.item.adaptors import adaptize\n \n def to_unicode(value):\n@@ -35,7 +35,7 @@ def clean_spaces(value):\n     Output: list of unicodes\n     \"\"\"\n     _clean_spaces_re = re.compile(\"\\s+\", re.U)\n-    return [ _clean_spaces_re.sub(' ', v.decode('utf-8')) for v in value ]\n+    return [ _clean_spaces_re.sub(' ', str_to_unicode(v)) for v in value ]\n \n def strip(value):\n     \"\"\"\n@@ -78,9 +78,9 @@ def canonicalize_urls(value):\n     Output: list of unicodes(urls)\n     \"\"\"\n     if hasattr(value, '__iter__'):\n-        return [canonicalize_url(str(url)) for url in value]\n+        return [canonicalize_url(unicode_to_str(url)) for url in value]\n     elif isinstance(value, basestring):\n-        return canonicalize_url(str(value))\n+        return canonicalize_url(unicode_to_str(value))\n     return ''\n \n class Delist(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#48682189b4f1bdb12ce12ed68869649c90d939d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 536 | Lines Deleted: 0 | Files Changed: 13 | Hunks: 12 | Methods Changed: 28 | Complexity Δ (Sum/Max): 44/16 | Churn Δ: 536 | Churn Cumulative: 673 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n\n@@ -0,0 +1,17 @@\n+from django.contrib import admin\n+from scrapyorg.blog.models import *\n+\n+\n+class CategoryAdmin(admin.ModelAdmin):\n+    prepopulated_fields = {'slug': ('title',)}\n+\n+admin.site.register(Category, CategoryAdmin)\n+\n+\n+class PostAdmin(admin.ModelAdmin):\n+    list_display  = ('title', 'publish', 'status')\n+    list_filter   = ('publish', 'categories', 'status')\n+    search_fields = ('title', 'body')\n+    prepopulated_fields = {'slug': ('title',)}\n+\n+admin.site.register(Post, PostAdmin)\n\\ No newline at end of file\n\n@@ -0,0 +1,42 @@\n+from django.contrib.syndication.feeds import FeedDoesNotExist\n+from django.core.exceptions import ObjectDoesNotExist\n+from django.contrib.sites.models import Site\n+from django.contrib.syndication.feeds import Feed\n+from django.core.urlresolvers import reverse\n+from scrapyorg.blog.models import Post, Category\n+\n+\n+class BlogPostsFeed(Feed):\n+    _site = Site.objects.get_current()\n+    title = '%s feed' % _site.name\n+    description = '%s posts feed.' % _site.name\n+\n+    def link(self):\n+        return reverse('blog_index')\n+\n+    def items(self):\n+        return Post.objects.published()[:10]\n+\n+    def item_pubdate(self, obj):\n+        return obj.publish\n+\n+\n+class BlogPostsByCategory(Feed):\n+    _site = Site.objects.get_current()\n+    title = '%s posts category feed' % _site.name\n+    \n+    def get_object(self, bits):\n+        if len(bits) != 1:\n+            raise ObjectDoesNotExist\n+        return Category.objects.get(slug__exact=bits[0])\n+\n+    def link(self, obj):\n+        if not obj:\n+            raise FeedDoesNotExist\n+        return obj.get_absolute_url()\n+\n+    def description(self, obj):\n+        return \"Posts recently categorized as %s\" % obj.title\n+    \n+    def items(self, obj):\n+        return obj.post_set.published()[:10]\n\\ No newline at end of file\n\n@@ -0,0 +1,9 @@\n+from django.db.models import Manager\n+import datetime\n+\n+\n+class PublicManager(Manager):\n+    \"\"\"Returns published posts that are not in the future.\"\"\"\n+    \n+    def published(self):\n+        return self.get_query_set().filter(status__gte=2, publish__lte=datetime.datetime.now())\n\\ No newline at end of file\n\n@@ -0,0 +1,81 @@\n+from django.db import models\n+from django.utils.translation import ugettext_lazy as _\n+from django.db.models import permalink\n+from django.contrib.auth.models import User\n+#from tagging.fields import TagField\n+from scrapyorg.blog.managers import PublicManager\n+\n+#import tagging\n+\n+\n+class Category(models.Model):\n+    \"\"\"Category model.\"\"\"\n+    title       = models.CharField(_('title'), max_length=100)\n+    slug        = models.SlugField(_('slug'), unique=True)\n+\n+    class Meta:\n+        verbose_name = _('category')\n+        verbose_name_plural = _('categories')\n+        db_table = 'blog_categories'\n+        ordering = ('title',)\n+\n+    class Admin:\n+        pass\n+\n+    def __unicode__(self):\n+        return u'%s' % self.title\n+\n+    @permalink\n+    def get_absolute_url(self):\n+        return ('blog_category_detail', None, {'slug': self.slug})\n+\n+\n+class Post(models.Model):\n+    \"\"\"Post model.\"\"\"\n+    STATUS_CHOICES = (\n+        (1, _('Draft')),\n+        (2, _('Public')),\n+    )\n+    title           = models.CharField(_('title'), max_length=200)\n+    slug            = models.SlugField(_('slug'), unique_for_date='publish')\n+    author          = models.ForeignKey(User, blank=True, null=True)\n+    body            = models.TextField(_('body'))\n+    tease           = models.TextField(_('tease'), blank=True)\n+    status          = models.IntegerField(_('status'), choices=STATUS_CHOICES, default=2)\n+    allow_comments  = models.BooleanField(_('allow comments'), default=True)\n+    publish         = models.DateTimeField(_('publish'))\n+    created         = models.DateTimeField(_('created'), auto_now_add=True)\n+    modified        = models.DateTimeField(_('modified'), auto_now=True)\n+    categories      = models.ManyToManyField(Category, blank=True)\n+#    tags            = TagField()\n+    objects         = PublicManager()\n+\n+    class Meta:\n+        verbose_name = _('post')\n+        verbose_name_plural = _('posts')\n+        db_table  = 'blog_posts'\n+        ordering  = ('-publish',)\n+        get_latest_by = 'publish'\n+\n+    class Admin:\n+        list_display  = ('title', 'publish', 'status')\n+        list_filter   = ('publish', 'categories', 'status')\n+        search_fields = ('title', 'body')\n+\n+    def __unicode__(self):\n+        return u'%s' % self.title\n+\n+    @permalink\n+    def get_absolute_url(self):\n+        return ('blog_detail', None, {\n+            'year': self.publish.year,\n+            'month': self.publish.strftime('%b').lower(),\n+            'day': self.publish.day,\n+            'slug': self.slug\n+        })\n+    \n+    def get_previous_post(self):\n+        return self.get_previous_by_publish(status__gte=2)\n+    \n+    def get_next_post(self):\n+        return self.get_next_by_publish(status__gte=2)\n\n@@ -0,0 +1,13 @@\n+from django.contrib.sitemaps import Sitemap\n+from scrapyorg.blog.models import Post\n+\n+\n+class BlogSitemap(Sitemap):\n+    changefreq = \"never\"\n+    priority = 0.5\n+\n+    def items(self):\n+        return Post.objects.published()\n+\n+        def lastmod(self, obj):\n+            return obj.publish\n\\ No newline at end of file\n\n\n@@ -0,0 +1,103 @@\n+from django import template\n+from django.conf import settings\n+from django.db import models\n+\n+import re\n+\n+Post = models.get_model('blog', 'post')\n+Category = models.get_model('blog', 'category')\n+\n+register = template.Library()\n+\n+\n+class LatestPosts(template.Node):\n+    def __init__(self, limit, var_name):\n+        self.limit = limit\n+        self.var_name = var_name\n+\n+    def render(self, context):\n+        posts = Post.objects.published()[:int(self.limit)]\n+        if posts and (int(self.limit) == 1):\n+            context[self.var_name] = posts[0]\n+        else:\n+            context[self.var_name] = posts\n+        return ''\n+\n+@register.tag\n+def get_latest_posts(parser, token):\n+    \"\"\"\n+    Gets any number of latest posts and stores them in a varable.\n+\n+    Syntax::\n+\n+        {% get_latest_posts [limit] as [var_name] %}\n+\n+    Example usage::\n+\n+        {% get_latest_posts 10 as latest_post_list %}\n+    \"\"\"\n+    try:\n+        tag_name, arg = token.contents.split(None, 1)\n+    except ValueError:\n+        raise template.TemplateSyntaxError, \"%s tag requires arguments\" % token.contents.split()[0]\n+    m = re.search(r'(.*?) as (\\w+)', arg)\n+    if not m:\n+        raise template.TemplateSyntaxError, \"%s tag had invalid arguments\" % tag_name\n+    format_string, var_name = m.groups()\n+    return LatestPosts(format_string, var_name)\n+\n+\n+class BlogCategories(template.Node):\n+    def __init__(self, var_name):\n+        self.var_name = var_name\n+\n+    def render(self, context):\n+        categories = Category.objects.all()\n+        context[self.var_name] = categories\n+        return ''\n+\n+@register.tag\n+def get_blog_categories(parser, token):\n+    \"\"\"\n+    Gets all blog categories.\n+\n+    Syntax::\n+\n+        {% get_blog_categories as [var_name] %}\n+\n+    Example usage::\n+\n+        {% get_blog_categories as category_list %}\n+    \"\"\"\n+    try:\n+        tag_name, arg = token.contents.split(None, 1)\n+    except ValueError:\n+        raise template.TemplateSyntaxError, \"%s tag requires arguments\" % token.contents.split()[0]\n+    m = re.search(r'as (\\w+)', arg)\n+    if not m:\n+        raise template.TemplateSyntaxError, \"%s tag had invalid arguments\" % tag_name\n+    var_name = m.groups()[0]\n+    return BlogCategories(var_name)\n+\n+\n+@register.filter\n+def get_links(value):\n+    \"\"\"\n+    Extracts links from a ``Post`` body and returns a list.\n+\n+    Template Syntax::\n+\n+        {{ post.body|markdown:\"safe\"|get_links }}\n+\n+    \"\"\"\n+    try:\n+        try:\n+            from BeautifulSoup import BeautifulSoup\n+        except ImportError:\n+            from beautifulsoup import BeautifulSoup\n+        soup = BeautifulSoup(value)\n+        return soup.findAll('a')\n+    except ImportError:\n+        if settings.DEBUG:\n+            raise template.TemplateSyntaxError, \"Error in 'get_links' filter: BeautifulSoup isn't installed.\"\n+    return value\n\n@@ -0,0 +1,66 @@\n+\"\"\"\n+>>> from django.test import Client\n+>>> from scrapyorg.blog.models import Post, Category\n+>>> import datetime\n+>>> from django.core.urlresolvers import reverse\n+>>> client = Client()\n+\n+>>> category = Category(title='Django', slug='django')\n+>>> category.save()\n+>>> category2 = Category(title='Rails', slug='rails')\n+>>> category2.save()\n+\n+>>> post = Post(title='DJ Ango', slug='dj-ango', body='Yo DJ! Turn that music up!', status=2, publish=datetime.datetime(2008,5,5,16,20))\n+>>> post.save()\n+\n+>>> post2 = Post(title='Where my grails at?', slug='where', body='I Can haz Holy plez?', status=2, publish=datetime.datetime(2008,4,2,11,11))\n+>>> post2.save()\n+\n+>>> post.categories.add(category)\n+>>> post2.categories.add(category2)\n+\n+>>> response = client.get(reverse('blog_index'))\n+>>> response.context[-1]['object_list']\n+[<Post: DJ Ango>, <Post: Where my grails at?>]\n+>>> response.status_code\n+200\n+\n+>>> response = client.get(reverse('blog_category_list'))\n+>>> response.context[-1]['object_list']\n+[<Category: Django>, <Category: Rails>]\n+>>> response.status_code\n+200\n+\n+>>> response = client.get(category.get_absolute_url())\n+>>> response.context[-1]['object_list']\n+[<Post: DJ Ango>]\n+>>> response.status_code\n+200\n+\n+>>> response = client.get(post.get_absolute_url())\n+>>> response.context[-1]['object']\n+<Post: DJ Ango>\n+>>> response.status_code\n+200\n+\n+>>> response = client.get(reverse('blog_search'), {'q': 'DJ'})\n+>>> response.context[-1]['object_list']\n+[<Post: DJ Ango>]\n+>>> response.status_code\n+200\n+>>> response = client.get(reverse('blog_search'), {'q': 'Holy'})\n+>>> response.context[-1]['object_list']\n+[<Post: Where my grails at?>]\n+>>> response.status_code\n+200\n+>>> response = client.get(reverse('blog_search'), {'q': ''})\n+>>> response.context[-1]['message']\n+'Search term was too vague. Please try again.'\n+\n+>>> response = client.get(reverse('blog_detail', args=[2008, 'apr', 2, 'where']))\n+>>> response.context[-1]['object']\n+<Post: Where my grails at?>\n+>>> response.status_code\n+200\n+\"\"\"\n+\n\n@@ -0,0 +1,41 @@\n+from django.conf.urls.defaults import *\n+from scrapyorg.blog import views as blog_views\n+\n+\n+urlpatterns = patterns('',\n+    url(r'^(?P<year>\\d{4})/(?P<month>\\w{3})/(?P<day>\\d{1,2})/(?P<slug>[-\\w]+)/$',\n+        view=blog_views.post_detail,\n+        name='blog_detail'),\n+\n+    url(r'^(?P<year>\\d{4})/(?P<month>\\w{3})/(?P<day>\\d{1,2})/$',\n+        view=blog_views.post_archive_day,\n+        name='blog_archive_day'),\n+\n+    url(r'^(?P<year>\\d{4})/(?P<month>\\w{3})/$',\n+        view=blog_views.post_archive_month,\n+        name='blog_archive_month'),\n+\n+    url(r'^(?P<year>\\d{4})/$',\n+        view=blog_views.post_archive_year,\n+        name='blog_archive_year'),\n+\n+    url(r'^categories/(?P<slug>[-\\w]+)/$',\n+        view=blog_views.category_detail,\n+        name='blog_category_detail'),\n+\n+    url (r'^categories/$',\n+        view=blog_views.category_list,\n+        name='blog_category_list'),\n+\n+    url (r'^search/$',\n+        view=blog_views.search,\n+        name='blog_search'),\n+\n+    url(r'^page/(?P<page>\\w)/$',\n+        view=blog_views.post_list,\n+        name='blog_index_paginated'),\n+\n+    url(r'^$',\n+        view=blog_views.post_list,\n+        name='blog_index'),\n+)\n\\ No newline at end of file\n\n@@ -0,0 +1,160 @@\n+from django.shortcuts import render_to_response, get_object_or_404\n+from django.template import RequestContext\n+from django.http import Http404\n+from django.views.generic import date_based, list_detail\n+from django.db.models import Q\n+from scrapyorg.blog.models import *\n+\n+import datetime\n+import re\n+\n+\n+def post_list(request, page=0, **kwargs):\n+    return list_detail.object_list(\n+        request,\n+        queryset = Post.objects.published(),\n+        paginate_by = 20,\n+        page = page,\n+        **kwargs\n+    )\n+post_list.__doc__ = list_detail.object_list.__doc__\n+\n+\n+def post_archive_year(request, year, **kwargs):\n+    return date_based.archive_year(\n+        request,\n+        year = year,\n+        date_field = 'publish',\n+        queryset = Post.objects.published(),\n+        make_object_list = True,\n+        **kwargs\n+    )\n+post_archive_year.__doc__ = date_based.archive_year.__doc__\n+\n+\n+def post_archive_month(request, year, month, **kwargs):\n+    return date_based.archive_month(\n+        request,\n+        year = year,\n+        month = month,\n+        date_field = 'publish',\n+        queryset = Post.objects.published(),\n+        **kwargs\n+    )\n+post_archive_month.__doc__ = date_based.archive_month.__doc__\n+\n+\n+def post_archive_day(request, year, month, day, **kwargs):\n+    return date_based.archive_day(\n+        request,\n+        year = year,\n+        month = month,\n+        day = day,\n+        date_field = 'publish',\n+        queryset = Post.objects.published(),\n+        **kwargs\n+    )\n+post_archive_day.__doc__ = date_based.archive_day.__doc__\n+\n+\n+def post_detail(request, slug, year, month, day, **kwargs):\n+    return date_based.object_detail(\n+        request,\n+        year = year,\n+        month = month,\n+        day = day,\n+        date_field = 'publish',\n+        slug = slug,\n+        queryset = Post.objects.published(),\n+        **kwargs\n+    )\n+post_detail.__doc__ = date_based.object_detail.__doc__\n+\n+\n+def category_list(request, template_name = 'blog/category_list.html', **kwargs):\n+    \"\"\"\n+    Category list\n+\n+    Template: ``blog/category_list.html``\n+    Context:\n+        object_list\n+            List of categories.\n+    \"\"\"\n+    return list_detail.object_list(\n+        request,\n+        queryset = Category.objects.all(),\n+        template_name = template_name,\n+        **kwargs\n+    )\n+\n+def category_detail(request, slug, template_name = 'blog/category_detail.html', **kwargs):\n+    \"\"\"\n+    Category detail\n+\n+    Template: ``blog/category_detail.html``\n+    Context:\n+        object_list\n+            List of posts specific to the given category.\n+        category\n+            Given category.\n+    \"\"\"\n+    category = get_object_or_404(Category, slug__iexact=slug)\n+\n+    return list_detail.object_list(\n+        request,\n+        queryset = category.post_set.published(),\n+        extra_context = {'category': category},\n+        template_name = template_name,\n+        **kwargs\n+    )\n+\n+\n+# Stop Words courtesy of http://www.dcs.gla.ac.uk/idom/ir_resources/linguistic_utils/stop_words\n+STOP_WORDS = r\"\"\"\\b(a|about|above|across|after|afterwards|again|against|all|almost|alone|along|already|also|\n+although|always|am|among|amongst|amoungst|amount|an|and|another|any|anyhow|anyone|anything|anyway|anywhere|are|\n+around|as|at|back|be|became|because|become|becomes|becoming|been|before|beforehand|behind|being|below|beside|\n+besides|between|beyond|bill|both|bottom|but|by|call|can|cannot|cant|co|computer|con|could|couldnt|cry|de|describe|\n+detail|do|done|down|due|during|each|eg|eight|either|eleven|else|elsewhere|empty|enough|etc|even|ever|every|everyone|\n+everything|everywhere|except|few|fifteen|fify|fill|find|fire|first|five|for|former|formerly|forty|found|four|from|\n+front|full|further|get|give|go|had|has|hasnt|have|he|hence|her|here|hereafter|hereby|herein|hereupon|hers|herself|\n+him|himself|his|how|however|hundred|i|ie|if|in|inc|indeed|interest|into|is|it|its|itself|keep|last|latter|latterly|\n+least|less|ltd|made|many|may|me|meanwhile|might|mill|mine|more|moreover|most|mostly|move|much|must|my|myself|name|\n+namely|neither|never|nevertheless|next|nine|no|nobody|none|noone|nor|not|nothing|now|nowhere|of|off|often|on|once|\n+one|only|onto|or|other|others|otherwise|our|ours|ourselves|out|over|own|part|per|perhaps|please|put|rather|re|same|\n+see|seem|seemed|seeming|seems|serious|several|she|should|show|side|since|sincere|six|sixty|so|some|somehow|someone|\n+something|sometime|sometimes|somewhere|still|such|system|take|ten|than|that|the|their|them|themselves|then|thence|\n+there|thereafter|thereby|therefore|therein|thereupon|these|they|thick|thin|third|this|those|though|three|through|\n+throughout|thru|thus|to|together|too|top|toward|towards|twelve|twenty|two|un|under|until|up|upon|us|very|via|was|\n+we|well|were|what|whatever|when|whence|whenever|where|whereafter|whereas|whereby|wherein|whereupon|wherever|whether|\n+which|while|whither|who|whoever|whole|whom|whose|why|will|with|within|without|would|yet|you|your|yours|yourself|\n+yourselves)\\b\"\"\"\n+\n+\n+def search(request, template_name='blog/post_search.html'):\n+    \"\"\"\n+    Search for blog posts.\n+\n+    This template will allow you to setup a simple search form that will try to return results based on\n+    given search strings. The queries will be put through a stop words filter to remove words like\n+    'the', 'a', or 'have' to help imporve the result set.\n+\n+    Template: ``blog/post_search.html``\n+    Context:\n+        object_list\n+            List of blog posts that match given search term(s).\n+        search_term\n+            Given search term.\n+    \"\"\"\n+    context = {}\n+    if request.GET:\n+        stop_word_list = re.compile(STOP_WORDS, re.IGNORECASE)\n+        search_term = '%s' % request.GET['q']\n+        cleaned_search_term = stop_word_list.sub('', search_term)\n+        cleaned_search_term = cleaned_search_term.strip()\n+        if len(cleaned_search_term) != 0:\n+            post_list = Post.objects.published().filter(Q(body__icontains=cleaned_search_term) | Q(tags__icontains=cleaned_search_term) | Q(categories__title__icontains=cleaned_search_term))\n+            context = {'object_list': post_list, 'search_term':search_term}\n+        else:\n+            message = 'Search term was too vague. Please try again.'\n+            context = {'message':message}\n+    return render_to_response(template_name, context, context_instance=RequestContext(request))\n\n@@ -77,9 +77,11 @@ INSTALLED_APPS = (\n     'django.contrib.sites',\n     'django.contrib.admin',\n     'django.contrib.markup',\n+    'django.contrib.comments',\n     'scrapyorg.article',\n     'scrapyorg.download',\n     'scrapyorg.docs',\n+    'scrapyorg.blog',\n )\n \n DOC_PICKLE_ROOT = os.path.join(PROJECT_ROOT, 'docs', 'pickle')\n\n@@ -13,6 +13,8 @@ urlpatterns = patterns('',\n \n     # docs\n     url(r\"^docs/\", include(\"scrapyorg.docs.urls\")),\n+    # blog\n+    url(r\"^blog/\", include(\"scrapyorg.blog.urls\")),\n )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
