{"custom_id": "keras#33f7c97b6452abff8866c8d55c23fd323ddc1aee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 82 | Files Changed: 29 | Hunks: 69 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 164 | Churn Cumulative: 4633 | Contributors (this commit): 2 | Commits (past 90d): 76 | Contributors (cumulative): 56 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import timeit\n import numpy as np\n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n from keras.benchmarks import benchmark_util\n \n\n@@ -22,7 +22,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import os\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import time\n import six\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import numpy as np\n import six\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n from keras.benchmarks import benchmark_util\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n from keras.benchmarks import benchmark_util\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n from keras.benchmarks import benchmark_util\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import numpy as np\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import timeit\n import numpy as np\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n from keras.benchmarks import benchmark_util\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n from keras.benchmarks import benchmark_util\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import numpy as np\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n from keras.benchmarks import benchmark_util\n \n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import functools\n import numpy as np\n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import time\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import time\n \n\n@@ -23,7 +23,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n from absl import app\n from absl import flags\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n from keras.benchmarks import benchmark_util\n from keras.optimizer_v2 import adam\n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n from keras.benchmarks.saved_model_benchmarks import saved_model_benchmark_util\n \n \n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n from keras.benchmarks.saved_model_benchmarks import saved_model_benchmark_util\n \n \n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n from keras.benchmarks.saved_model_benchmarks import saved_model_benchmark_util\n \n \n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n from keras.benchmarks.saved_model_benchmarks import saved_model_benchmark_util\n \n \n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n from keras.benchmarks.saved_model_benchmarks import saved_model_benchmark_util\n \n \n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n from keras.benchmarks.saved_model_benchmarks import saved_model_benchmark_util\n \n \n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import tempfile\n import time\n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n from keras.benchmarks.saved_model_benchmarks import saved_model_benchmark_util\n \n \n\n@@ -18,7 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n from keras.benchmarks.saved_model_benchmarks import saved_model_benchmark_util\n \n \n\n@@ -17,17 +17,17 @@ from __future__ import division\n from __future__ import print_function\n \n import numpy as np\n-import tensorflow.compat.v1 as tf\n+import tensorflow as tf\n \n-tf.disable_eager_execution()\n+tf.compat.v1.disable_eager_execution()\n \n \n class KerasNetworkTFRNNs(tf.keras.Model):\n \n   def __init__(self, name=None):\n     super(KerasNetworkTFRNNs, self).__init__(name=name)\n-    self._cell = tf.nn.rnn_cell.MultiRNNCell(\n-        [tf.nn.rnn_cell.LSTMCell(1) for _ in range(2)])\n+    self._cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(\n+        [tf.compat.v1.nn.rnn_cell.LSTMCell(1) for _ in range(2)])\n \n   def call(self, inputs):\n     return self._cell(inputs, self._cell.get_initial_state(inputs))\n@@ -44,7 +44,7 @@ class KerasNetworkKerasRNNs(tf.keras.Model):\n     return self._cell(inputs, self._cell.get_initial_state(inputs))\n \n \n-class LegacyRNNTest(tf.test.TestCase):\n+class LegacyRNNTest(tf.compat.v1.test.TestCase):\n \n   def setUp(self):\n     super(LegacyRNNTest, self).setUp()\n@@ -65,19 +65,19 @@ class LegacyRNNTest(tf.test.TestCase):\n       y_train = tf.keras.utils.to_categorical(y_train)\n       cell = tf.keras.layers.SimpleRNNCell(output_shape)\n \n-      inputs = tf.placeholder(\n+      inputs = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      predict = tf.placeholder(\n+      predict = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.nn.dynamic_rnn(\n+      outputs, state = tf.compat.v1.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(outputs.shape.as_list(), [None, timestep, output_shape])\n       self.assertEqual(state.shape.as_list(), [None, output_shape])\n-      loss = tf.losses.softmax_cross_entropy(predict, state)\n-      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state)\n+      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.global_variables_initializer()])\n+      sess.run([tf.compat.v1.global_variables_initializer()])\n       _, outputs, state = sess.run(\n           [train_op, outputs, state], {inputs: x_train, predict: y_train})\n \n@@ -98,19 +98,19 @@ class LegacyRNNTest(tf.test.TestCase):\n       y_train = tf.keras.utils.to_categorical(y_train)\n       cell = tf.keras.layers.GRUCell(output_shape)\n \n-      inputs = tf.placeholder(\n+      inputs = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      predict = tf.placeholder(\n+      predict = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.nn.dynamic_rnn(\n+      outputs, state = tf.compat.v1.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(outputs.shape.as_list(), [None, timestep, output_shape])\n       self.assertEqual(state.shape.as_list(), [None, output_shape])\n-      loss = tf.losses.softmax_cross_entropy(predict, state)\n-      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state)\n+      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.global_variables_initializer()])\n+      sess.run([tf.compat.v1.global_variables_initializer()])\n       _, outputs, state = sess.run(\n           [train_op, outputs, state], {inputs: x_train, predict: y_train})\n \n@@ -131,21 +131,21 @@ class LegacyRNNTest(tf.test.TestCase):\n       y_train = tf.keras.utils.to_categorical(y_train)\n       cell = tf.keras.layers.LSTMCell(output_shape)\n \n-      inputs = tf.placeholder(\n+      inputs = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      predict = tf.placeholder(\n+      predict = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.nn.dynamic_rnn(\n+      outputs, state = tf.compat.v1.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(outputs.shape.as_list(), [None, timestep, output_shape])\n       self.assertEqual(len(state), 2)\n       self.assertEqual(state[0].shape.as_list(), [None, output_shape])\n       self.assertEqual(state[1].shape.as_list(), [None, output_shape])\n-      loss = tf.losses.softmax_cross_entropy(predict, state[0])\n-      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state[0])\n+      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.global_variables_initializer()])\n+      sess.run([tf.compat.v1.global_variables_initializer()])\n       _, outputs, state = sess.run(\n           [train_op, outputs, state], {inputs: x_train, predict: y_train})\n \n@@ -170,25 +170,25 @@ class LegacyRNNTest(tf.test.TestCase):\n           [tf.keras.layers.LSTMCell(2 * output_shape),\n            tf.keras.layers.LSTMCell(output_shape)])\n \n-      inputs = tf.placeholder(\n+      inputs = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      predict = tf.placeholder(\n+      predict = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.nn.dynamic_rnn(\n+      outputs, state = tf.compat.v1.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(outputs.shape.as_list(), [None, timestep, output_shape])\n       self.assertEqual(len(state), 2)\n-      state = tf.nest.flatten(state)\n+      state = tf.compat.v1.nest.flatten(state)\n       self.assertEqual(len(state), 4)\n       self.assertEqual(state[0].shape.as_list(), [None, 2 * output_shape])\n       self.assertEqual(state[1].shape.as_list(), [None, 2 * output_shape])\n       self.assertEqual(state[2].shape.as_list(), [None, output_shape])\n       self.assertEqual(state[3].shape.as_list(), [None, output_shape])\n-      loss = tf.losses.softmax_cross_entropy(predict, state[2])\n-      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state[2])\n+      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.global_variables_initializer()])\n+      sess.run([tf.compat.v1.global_variables_initializer()])\n       _, outputs, state = sess.run(\n           [train_op, outputs, state], {inputs: x_train, predict: y_train})\n \n@@ -212,20 +212,20 @@ class LegacyRNNTest(tf.test.TestCase):\n       y_train = tf.keras.utils.to_categorical(y_train)\n       cell = tf.keras.layers.SimpleRNNCell(output_shape)\n \n-      inputs = [tf.placeholder(\n+      inputs = [tf.compat.v1.placeholder(\n           tf.float32, shape=(None, input_shape))] * timestep\n-      predict = tf.placeholder(\n+      predict = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.nn.static_rnn(\n+      outputs, state = tf.compat.v1.nn.static_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(len(outputs), timestep)\n       self.assertEqual(outputs[0].shape.as_list(), [None, output_shape])\n       self.assertEqual(state.shape.as_list(), [None, output_shape])\n-      loss = tf.losses.softmax_cross_entropy(predict, state)\n-      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state)\n+      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.global_variables_initializer()])\n+      sess.run([tf.compat.v1.global_variables_initializer()])\n       feed_dict = {i: d for i, d in zip(inputs, x_train)}\n       feed_dict[predict] = y_train\n       _, outputs, state = sess.run(\n@@ -250,10 +250,10 @@ class LegacyRNNTest(tf.test.TestCase):\n     weights = fix_weights_generator.get_weights()\n \n     with self.session(graph=tf.Graph()) as sess:\n-      inputs = tf.placeholder(\n+      inputs = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n       cell = tf.keras.layers.SimpleRNNCell(output_shape)\n-      tf_out, tf_state = tf.nn.dynamic_rnn(\n+      tf_out, tf_state = tf.compat.v1.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       cell.set_weights(weights)\n       [tf_out, tf_state] = sess.run([tf_out, tf_state], {inputs: x_train})\n@@ -289,18 +289,18 @@ class LegacyRNNTest(tf.test.TestCase):\n     tf_weights = [np.concatenate((kernel, recurrent_kernel)), bias]\n \n     with self.session(graph=tf.Graph()) as sess:\n-      inputs = tf.placeholder(\n+      inputs = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n       cell = tf.keras.layers.SimpleRNNCell(output_shape)\n-      k_out, k_state = tf.nn.dynamic_rnn(\n+      k_out, k_state = tf.compat.v1.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       cell.set_weights(keras_weights)\n       [k_out, k_state] = sess.run([k_out, k_state], {inputs: x_train})\n     with self.session(graph=tf.Graph()) as sess:\n-      inputs = tf.placeholder(\n+      inputs = tf.compat.v1.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      cell = tf.nn.rnn_cell.BasicRNNCell(output_shape)\n-      tf_out, tf_state = tf.nn.dynamic_rnn(\n+      cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(output_shape)\n+      tf_out, tf_state = tf.compat.v1.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       cell.set_weights(tf_weights)\n       [tf_out, tf_state] = sess.run([tf_out, tf_state], {inputs: x_train})\n@@ -310,10 +310,10 @@ class LegacyRNNTest(tf.test.TestCase):\n \n   def testRNNCellSerialization(self):\n     for cell in [\n-        tf.nn.rnn_cell.LSTMCell(32, use_peepholes=True, cell_clip=True),\n-        tf.nn.rnn_cell.BasicLSTMCell(32, dtype=tf.float32),\n-        tf.nn.rnn_cell.BasicRNNCell(32, activation=\"relu\", dtype=tf.float32),\n-        tf.nn.rnn_cell.GRUCell(32, dtype=tf.float32)\n+        tf.compat.v1.nn.rnn_cell.LSTMCell(32, use_peepholes=True, cell_clip=True),\n+        tf.compat.v1.nn.rnn_cell.BasicLSTMCell(32, dtype=tf.float32),\n+        tf.compat.v1.nn.rnn_cell.BasicRNNCell(32, activation=\"relu\", dtype=tf.float32),\n+        tf.compat.v1.nn.rnn_cell.GRUCell(32, dtype=tf.float32)\n     ]:\n       with self.cached_session():\n         x = tf.keras.Input((None, 5))\n@@ -333,10 +333,10 @@ class LegacyRNNTest(tf.test.TestCase):\n         layer = tf.keras.layers.RNN.from_config(\n             config,\n             custom_objects={\n-                \"BasicRNNCell\": tf.nn.rnn_cell.BasicRNNCell,\n-                \"GRUCell\": tf.nn.rnn_cell.GRUCell,\n-                \"LSTMCell\": tf.nn.rnn_cell.LSTMCell,\n-                \"BasicLSTMCell\": tf.nn.rnn_cell.BasicLSTMCell\n+                \"BasicRNNCell\": tf.compat.v1.nn.rnn_cell.BasicRNNCell,\n+                \"GRUCell\": tf.compat.v1.nn.rnn_cell.GRUCell,\n+                \"LSTMCell\": tf.compat.v1.nn.rnn_cell.LSTMCell,\n+                \"BasicLSTMCell\": tf.compat.v1.nn.rnn_cell.BasicLSTMCell\n             })\n         y = layer(x)\n         model = tf.keras.models.Model(x, y)\n@@ -345,7 +345,7 @@ class LegacyRNNTest(tf.test.TestCase):\n         self.assertAllClose(y_np, y_np_2, atol=1e-4)\n \n   def testRNNCellActsLikeKerasRNNCellInProperScope(self):\n-    with tf.layers.experimental.keras_style_scope():\n+    with tf.compat.v1.layers.experimental.keras_style_scope():\n       kn1 = KerasNetworkTFRNNs(name=\"kn1\")\n       kn2 = KerasNetworkKerasRNNs(name=\"kn2\")\n \n@@ -358,7 +358,7 @@ class LegacyRNNTest(tf.test.TestCase):\n     self.assertTrue(all(\"kn1\" in v.name for v in kn1._cell.variables))\n     self.assertTrue(all(\"kn2\" in v.name for v in kn2._cell.variables))\n \n-    with tf.layers.experimental.keras_style_scope():\n+    with tf.compat.v1.layers.experimental.keras_style_scope():\n       kn1_new = KerasNetworkTFRNNs(name=\"kn1_new\")\n       kn2_new = KerasNetworkKerasRNNs(name=\"kn2_new\")\n \n@@ -385,4 +385,4 @@ def get_test_data(train_samples,\n \n \n if __name__ == \"__main__\":\n-  tf.test.main()\n+  tf.compat.v1.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f8c62b90274f9c5a261984c098312ff8fab3d66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 134 | Lines Deleted: 32 | Files Changed: 19 | Hunks: 31 | Methods Changed: 24 | Complexity Δ (Sum/Max): 17/7 | Churn Δ: 166 | Churn Cumulative: 51039 | Contributors (this commit): 231 | Commits (past 90d): 97 | Contributors (cumulative): 315 | DMM Complexity: 0.9491525423728814\n\nDIFF:\n@@ -2123,7 +2123,6 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     self.embeddings_freq = embeddings_freq\n     self.embeddings_metadata = embeddings_metadata\n     self._init_profile_batch(profile_batch)\n-    self._epoch = 0\n     self._global_train_batch = 0\n     self._previous_epoch_iterations = 0\n     self._train_accumulated_time = 0\n@@ -2397,7 +2396,6 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n \n   def on_epoch_begin(self, epoch, logs=None):\n     # Keeps track of epoch for profiling.\n-    self._epoch = epoch\n     if self.write_steps_per_second:\n       self._previous_epoch_iterations = self.model.optimizer.iterations.numpy()\n       self._train_accumulated_time = 0\n\n@@ -377,7 +377,6 @@ class TensorBoard(callbacks.TensorBoard):\n \n     # check if histogram summary should be run for this epoch\n     if self.histogram_freq and epoch % self.histogram_freq == 0:\n-      self._epoch = epoch\n       # pylint: disable=protected-access\n       # add the histogram summary op if it should run this epoch\n       self.model._make_test_function()\n\n@@ -2836,7 +2836,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                                     None)\n             if subtrackables:\n               deque.extendleft(reversed(subtrackables))\n-        elif isinstance(trackable_obj, data_structures.TrackableDataStructure):\n+        elif isinstance(trackable_obj, tf.__internal__.tracking.TrackableDataStructure):\n           # Data structures are introspected even with `recursive=False`.\n           tracked_values = trackable_obj._values\n           if tracked_values:\n\n@@ -56,7 +56,6 @@ from keras.utils.mode_keys import ModeKeys\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training import checkpoint_management\n from tensorflow.python.training.tracking import base as trackable\n-from tensorflow.python.training.tracking import data_structures\n from tensorflow.python.training.tracking import util as trackable_utils\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n@@ -318,7 +317,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     if all(\n         isinstance(v, (base_layer.Layer,\n-                       data_structures.TrackableDataStructure)) or\n+                       tf.__internal__.tracking.TrackableDataStructure)) or\n         base_layer_utils.has_weights(v) for v in tf.nest.flatten(value)):\n       try:\n         self._base_model_initialized\n@@ -935,13 +934,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             noise and dropout.\n             `validation_data` will override `validation_split`.\n             `validation_data` could be:\n-              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n-              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n-              - dataset\n-            For the first two cases, `batch_size` must be provided.\n-            For the last case, `validation_steps` could be provided.\n-            Note that `validation_data` does not support all the data types that\n-            are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n+              - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n+              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n+              - A `tf.data.Dataset`.\n+              - A Python generator or `keras.utils.Sequence` returning\n+              `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n         shuffle: Boolean (whether to shuffle the training data\n             before each epoch) or str (for 'batch'). This argument is ignored\n             when `x` is a generator or an object of tf.data.Dataset.\n\n@@ -1073,7 +1073,8 @@ class Dense(Layer):\n   where `activation` is the element-wise activation function\n   passed as the `activation` argument, `kernel` is a weights matrix\n   created by the layer, and `bias` is a bias vector created by the layer\n-  (only applicable if `use_bias` is `True`).\n+  (only applicable if `use_bias` is `True`). These are all attributes of \n+  `Dense`.\n \n   Note: If the input to the layer has a rank greater than 2, then `Dense`\n   computes the dot product between the `inputs` and the `kernel` along the\n@@ -1086,6 +1087,9 @@ class Dense(Layer):\n \n   Besides, layer attributes cannot be modified after the layer has been called\n   once (except the `trainable` attribute).\n+  When a popular kwarg `input_shape` is passed, then keras will create\n+  an input layer to insert before the current layer. This can be treated\n+  equivalent to explicitly defining an `InputLayer`.\n \n   Example:\n \n\n@@ -198,7 +198,7 @@ class BatchNormalization(normalization.BatchNormalizationBase):\n   with the argument `training=True`), the layer normalizes its output using\n   the mean and standard deviation of the current batch of inputs. That is to\n   say, for each channel being normalized, the layer returns\n-  `(batch - mean(batch)) / (var(batch) + epsilon) * gamma + beta`, where:\n+  `gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta`, where:\n \n   - `epsilon` is small constant (configurable as part of the constructor\n   arguments)\n@@ -212,7 +212,7 @@ class BatchNormalization(normalization.BatchNormalizationBase):\n   default), the layer normalizes its output using a moving average of the\n   mean and standard deviation of the batches it has seen during training. That\n   is to say, it returns\n-  `(batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta`.\n+  `gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta`.\n \n   `self.moving_mean` and `self.moving_var` are non-trainable variables that\n   are updated each time the layer in called in training mode, as such:\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import itertools\n import time\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import time\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import time\n \n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import itertools\n import random\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import functools\n import time\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import collections\n import itertools\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import os\n import random\n\n@@ -17,7 +17,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import tensorflow.compat.v2 as tf\n+import tensorflow as tf\n \n import time\n \n\n@@ -914,6 +914,27 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n     else:\n       super(LossScaleOptimizer, self).__setattr__(name, value)\n \n+  # Explicitly delegate learning_rate. Normally hyperparameters are delegated in\n+  # __getattribute__, but if a hyperparameter is not in self._optimizer._hyper\n+  # (e.g. because self._optimizer itself wraps another optimizer), then it won't\n+  # be delegated. Since learning_rate is a very commonly accessed\n+  # hyperparameter, we delegate it here.\n+  @property\n+  def learning_rate(self):\n+    return self._optimizer.learning_rate\n+\n+  @learning_rate.setter\n+  def learning_rate(self, value):\n+    self._optimizer.learning_rate = value\n+\n+  @property\n+  def lr(self):\n+    return self._optimizer.learning_rate\n+\n+  @lr.setter\n+  def lr(self, value):\n+    self._optimizer.lr = value\n+\n   # We do not override some OptimizerV2 methods. For each, we describe why we do\n   # not delegate them to self._optimizer:\n   # * get_updates: get_updates() calls get_gradients(). Since we override\n@@ -933,8 +954,8 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n class LossScaleOptimizerV1(LossScaleOptimizer):\n   \"\"\"An deprecated optimizer that applies loss scaling.\n \n-  Warning: This class is deprecated and will be removed in TensorFlow 2.5.\n-  Please use the non-experimental class\n+  Warning: This class is deprecated and will be removed in a future version of\n+  TensorFlow. Please use the non-experimental class\n   `tf.keras.mixed_precision.LossScaleOptimizer` instead.\n \n   This class is identical to the non-experimental\n\n@@ -31,6 +31,7 @@ from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import test_util as mp_test_util\n from keras.optimizer_v2 import adam\n from keras.optimizer_v2 import gradient_descent\n+from keras.optimizer_v2 import optimizer_v2\n \n # Disable not-callable lint error, as the linter is unable to detect that\n # LossScale instances are callable.\n@@ -641,6 +642,43 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                    'DynamicLossScale is no longer supported. Got:'):\n       loss_scale_optimizer.LossScaleOptimizerV1(opt, MyLossScale())\n \n+  def testLossScaleDelegationWithWrapper(self):\n+    # Test learning_rate is exposed when LossScaleOptimizer wraps another\n+    # wrapper.\n+\n+    class MyOptimizer(optimizer_v2.OptimizerV2):\n+\n+      def __init__(self):\n+        super().__init__('MyOptimizer')\n+        self.inner_optimizer = adam.Adam(learning_rate=1.0)\n+\n+      @property\n+      def learning_rate(self):\n+        return self.inner_optimizer.learning_rate\n+\n+      @learning_rate.setter\n+      def learning_rate(self, value):\n+        self.inner_optimizer.learning_rate = value\n+\n+      def get_config(self):\n+        return {}\n+\n+    with self.cached_session():\n+      opt = MyOptimizer()\n+      opt = loss_scale_optimizer.LossScaleOptimizer(opt)\n+\n+      # Force hyperparameters to be created\n+      opt.learning_rate  # pylint: disable=pointless-statement\n+      self.evaluate(tf.compat.v1.global_variables_initializer())\n+\n+      self.assertEqual(self.evaluate(opt.learning_rate), 1.0)\n+      self.assertEqual(\n+          self.evaluate(opt.inner_optimizer.inner_optimizer.learning_rate), 1.0)\n+      opt.learning_rate = 2.0\n+      self.assertEqual(self.evaluate(opt.learning_rate), 2.0)\n+      self.assertEqual(self.evaluate(\n+          opt.inner_optimizer.inner_optimizer.learning_rate), 2.0)\n+\n   @parameterized.named_parameters({\n       'testcase_name': 'SaveAndRestoreBase',\n       'strategy_fn': default_strategy_fn,\n\n@@ -24,6 +24,7 @@ import tensorflow.compat.v2 as tf\n import abc\n import contextlib\n import functools\n+import warnings\n \n import six\n from tensorflow.python.distribute import values as ds_values\n@@ -360,6 +361,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n       # checks that all keyword arguments are non-negative.\n       if kwargs[k] is not None and kwargs[k] < 0:\n         raise ValueError(\"Expected {} >= 0, received: {}\".format(k, kwargs[k]))\n+      if k == \"lr\":\n+        warnings.warn(\n+            \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n \n     self._use_locking = True\n     self._init_set_name(name)\n\n@@ -43,7 +43,6 @@ from tensorflow.python.saved_model import loader_impl\n from tensorflow.python.saved_model import nested_structure_coder\n from tensorflow.python.saved_model import revived_types\n from tensorflow.python.training.tracking import base as trackable\n-from tensorflow.python.training.tracking import data_structures\n \n # To avoid circular dependencies between keras/engine and keras/saving,\n # code in keras/saving must delay imports.\n@@ -350,7 +349,7 @@ class KerasObjectLoader(object):\n           child_proto.variable.name):\n         obj_child._handle_name = child_proto.variable.name + ':0'  # pylint: disable=protected-access\n \n-      if isinstance(obj_child, data_structures.TrackableDataStructure):\n+      if isinstance(obj_child, tf.__internal__.tracking.TrackableDataStructure):\n         setter = lambda *args: None\n \n       child_path = '{}.{}'.format(parent_path, child_name)\n\n@@ -34,7 +34,6 @@ from keras.utils.generic_utils import LazyLoader\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.saved_model import builder as saved_model_builder\n from tensorflow.python.saved_model import constants\n-from tensorflow.python.saved_model import utils_impl as saved_model_utils\n from tensorflow.python.training.tracking import graph_view\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -143,7 +142,7 @@ def _export_model_json(model, saved_model_path):\n   \"\"\"Saves model configuration as a json string under assets folder.\"\"\"\n   model_json = model.to_json()\n   model_json_filepath = os.path.join(\n-      saved_model_utils.get_or_create_assets_dir(saved_model_path),\n+      _get_or_create_assets_dir(saved_model_path),\n       tf.compat.as_text(constants.SAVED_MODEL_FILENAME_JSON))\n   with tf.io.gfile.GFile(model_json_filepath, 'w') as f:\n     f.write(model_json)\n@@ -151,8 +150,8 @@ def _export_model_json(model, saved_model_path):\n \n def _export_model_variables(model, saved_model_path):\n   \"\"\"Saves model weights in checkpoint format under variables folder.\"\"\"\n-  saved_model_utils.get_or_create_variables_dir(saved_model_path)\n-  checkpoint_prefix = saved_model_utils.get_variables_path(saved_model_path)\n+  _get_or_create_variables_dir(saved_model_path)\n+  checkpoint_prefix = _get_variables_path(saved_model_path)\n   model.save_weights(checkpoint_prefix, save_format='tf', overwrite=True)\n   return checkpoint_prefix\n \n@@ -422,3 +421,46 @@ def load_from_saved_model(saved_model_path, custom_objects=None):\n       tf.compat.as_text(tf.saved_model.VARIABLES_FILENAME))\n   model.load_weights(checkpoint_prefix)\n   return model\n+\n+\n+#### Directory / path helpers\n+\n+\n+def _get_or_create_variables_dir(export_dir):\n+  \"\"\"Return variables sub-directory, or create one if it doesn't exist.\"\"\"\n+  variables_dir = _get_variables_dir(export_dir)\n+  if not tf.compat.v1.gfile.Exists(variables_dir):\n+    tf.compat.v1.gfile.MakeDirs(variables_dir)\n+  return variables_dir\n+\n+\n+def _get_variables_dir(export_dir):\n+  \"\"\"Return variables sub-directory in the SavedModel.\"\"\"\n+  return os.path.join(\n+      tf.compat.as_text(export_dir),\n+      tf.compat.as_text(tf.saved_model.VARIABLES_DIRECTORY))\n+\n+\n+def _get_variables_path(export_dir):\n+  \"\"\"Return the variables path, used as the prefix for checkpoint files.\"\"\"\n+  return os.path.join(\n+      tf.compat.as_text(_get_variables_dir(export_dir)),\n+      tf.compat.as_text(tf.saved_model.VARIABLES_FILENAME))\n+\n+\n+def _get_or_create_assets_dir(export_dir):\n+  \"\"\"Return assets sub-directory, or create one if it doesn't exist.\"\"\"\n+  assets_destination_dir = _get_assets_dir(export_dir)\n+\n+  if not tf.compat.v1.gfile.Exists(assets_destination_dir):\n+    tf.compat.v1.gfile.MakeDirs(assets_destination_dir)\n+\n+  return assets_destination_dir\n+\n+\n+def _get_assets_dir(export_dir):\n+  \"\"\"Return path to asset directory in the SavedModel.\"\"\"\n+  return os.path.join(\n+      tf.compat.as_text(export_dir),\n+      tf.compat.as_text(tf.saved_model.ASSETS_DIRECTORY))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7adb536018baf47b9dade95d99f66f4c130df6bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1784 | Lines Deleted: 1725 | Files Changed: 58 | Hunks: 409 | Methods Changed: 362 | Complexity Δ (Sum/Max): -30/27 | Churn Δ: 3509 | Churn Cumulative: 64891 | Contributors (this commit): 129 | Commits (past 90d): 261 | Contributors (cumulative): 254 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,7 +48,6 @@ from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training import moving_averages\n-from tensorflow.python.training.tracking import util as tracking_util\n from tensorflow.python.util import keras_deps\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n@@ -721,7 +720,7 @@ keras_deps.register_get_session_function(get_session)\n \n # Inject the get_session function to tracking_util to avoid the backward\n # dependency from TF to Keras.\n-tracking_util.register_session_provider(get_session)\n+tf.__internal__.tracking.register_session_provider(get_session)\n \n \n def get_graph():\n\n@@ -0,0 +1,206 @@\n+# Lint as: python3\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for `DatasetCreator` with `Model.fit` across usages and strategies.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow.compat.v2 as tf\n+from absl import logging\n+from absl.testing import parameterized\n+import numpy as np\n+import keras\n+from keras import callbacks as callbacks_lib\n+from keras.distribute import multi_worker_testing_utils\n+from keras.distribute import strategy_combinations\n+from keras.engine import sequential\n+from keras.layers import core as core_layers\n+from keras.optimizer_v2 import gradient_descent\n+from keras.utils import dataset_creator\n+\n+\n+class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n+\n+  def _model_compile(self,\n+                     strategy,\n+                     steps_per_execution=1,\n+                     run_eagerly=False,\n+                     with_normalization_layer=False):\n+\n+    class ResultAssertingCallback(callbacks_lib.Callback):\n+\n+      def __init__(self):\n+        self._prev_epoch = -1\n+        self._loss_to_compare_against = 2  # Empirical initial value\n+\n+      def on_epoch_end(self, epoch, logs=None):\n+        logging.info(\"testModelFit: epoch=%r, logs=%r\", epoch, logs)\n+        if epoch <= self._prev_epoch:\n+          raise RuntimeError(\"Epoch is supposed to be larger than previous.\")\n+        self._prev_epoch = epoch\n+        is_loss_float = (\n+            logs.get(\"loss\", None) is not None and\n+            isinstance(logs[\"loss\"], (float, np.floating)))\n+        if not is_loss_float:\n+          raise RuntimeError(\"loss is supposed to be in the logs and float.\")\n+        if epoch == 0 or epoch == 9:\n+          # Making sure the loss of first epoch is below 1, and that of last\n+          # epoch is smaller than the first epoch.\n+          if logs[\"loss\"] > self._loss_to_compare_against:\n+            raise RuntimeError(\n+                \"loss at epoch {} is larger than previous.\".format(epoch))\n+          self._loss_to_compare_against = logs[\"loss\"]\n+\n+      def on_train_end(self, logs=None):\n+        if self._prev_epoch != 9:\n+          raise RuntimeError(\"Unexpected last epoch: {}\".format(\n+              self._prev_epoch))\n+\n+    # TODO(b/182193218): Use ParameterServerStrategy as a proper strategy\n+    # combination.\n+    if strategy == \"ParameterServerStrategy\":\n+      gpu_devices = tf.config.list_physical_devices(\"GPU\")\n+      if len(gpu_devices) > 1:\n+        self.skipTest(\"b/178452835: Multi-GPUs not supported in \"\n+                      \"ParameterServerStrategy.\")\n+      strategy = tf.distribute.experimental.ParameterServerStrategy(\n+          multi_worker_testing_utils.make_parameter_server_cluster(3, 2),\n+          variable_partitioner=tf.distribute.experimental.partitioners.FixedShardsPartitioner(2))\n+\n+    with strategy.scope():\n+      model = sequential.Sequential([core_layers.Dense(10)])\n+      if with_normalization_layer:\n+        norm = keras.layers.BatchNormalization(\n+            axis=-1, input_shape=(4, 4, 3), momentum=0.8)\n+        model.add(norm)\n+\n+    model.compile(\n+        gradient_descent.SGD(),\n+        loss=\"mse\",\n+        steps_per_execution=steps_per_execution,\n+        run_eagerly=run_eagerly)\n+    return model, [ResultAssertingCallback()]\n+\n+  def _model_fit(self,\n+                 strategy,\n+                 steps_per_execution=1,\n+                 validation_data=None,\n+                 x=None,\n+                 steps_per_epoch=10,\n+                 run_eagerly=False,\n+                 with_normalization_layer=False):\n+    model, callbacks = self._model_compile(strategy, steps_per_execution,\n+                                           run_eagerly,\n+                                           with_normalization_layer)\n+\n+    def dataset_fn(input_context):\n+      del input_context\n+      x = tf.random.uniform((10, 10))\n+      y = tf.random.uniform((10,))\n+      return tf.data.Dataset.from_tensor_slices(\n+          (x, y)).shuffle(10).repeat().batch(2)\n+\n+    x = x or dataset_creator.DatasetCreator(dataset_fn)\n+\n+    model.fit(\n+        x,\n+        epochs=10,\n+        steps_per_epoch=steps_per_epoch,\n+        verbose=0,\n+        callbacks=callbacks,\n+        validation_data=validation_data)\n+    return model\n+\n+\n+@tf.__internal__.distribute.combinations.generate(\n+    tf.__internal__.test.combinations.combine(\n+        strategy=strategy_combinations.all_strategies +\n+        strategy_combinations.multi_worker_mirrored_strategies +\n+        [\"ParameterServerStrategy\"],\n+        mode=\"eager\"))\n+class DatasetCreatorModelFitTest(DatasetCreatorModelFitTestBase):\n+\n+  def testModelFit(self, strategy):\n+    model = self._model_fit(strategy)\n+    self.assertEqual(model.optimizer.iterations, 100)\n+    return model\n+\n+  def testModelFitWithNormalizationLayer(self, strategy):\n+    model = self._model_fit(strategy, with_normalization_layer=True)\n+    self.assertEqual(model.optimizer.iterations, 100)\n+\n+  def testModelFitWithStepsPerExecution(self, strategy):\n+    model = self._model_fit(strategy, steps_per_execution=10)\n+    self.assertEqual(model.optimizer.iterations, 100)\n+\n+\n+@tf.__internal__.distribute.combinations.generate(\n+    tf.__internal__.test.combinations.combine(strategy=[\"ParameterServerStrategy\"], mode=\"eager\"))\n+class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n+    DatasetCreatorModelFitTestBase):\n+\n+  def testModelFitWithNoStepsPerEpoch(self, strategy):\n+    with self.assertRaisesRegex(\n+        ValueError, \"`steps_per_epoch` must be specified with \"\n+        \"`ParameterServerStrategy`.\"):\n+      self._model_fit(strategy, steps_per_epoch=None)\n+\n+  def testModelFitWithRunEagerly(self, strategy):\n+    with self.assertRaisesRegex(\n+        ValueError, \"When using `Model` with `ParameterServerStrategy`, \"\n+        \"`run_eagerly` is not supported.\"):\n+      self._model_fit(strategy, run_eagerly=True)\n+\n+  def testModelFitWithValidationData(self, strategy):\n+    with self.assertRaisesRegex(\n+        NotImplementedError, \"Evaluation in `model.fit` with \"\n+        \"`ParameterServerStrategy` is not yet supported.\"):\n+      self._model_fit(\n+          strategy,\n+          validation_data=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  def testModelFitWithDatasetInstance(self, strategy):\n+    with self.assertRaisesRegex(\n+        NotImplementedError, \"Only `DatasetCreator` input is supported in \"\n+        \"`ParameterServerStrategy` at this time.\"):\n+      self._model_fit(\n+          strategy, x=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  def testModelEvaluate(self, strategy):\n+    model, _ = self._model_compile(strategy)\n+    with self.assertRaisesRegex(\n+        NotImplementedError, \"`model.evaluate` is not yet supported with \"\n+        \"`ParameterServerStrategy`.\"):\n+      model.evaluate(x=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  def testModelPredict(self, strategy):\n+    model, _ = self._model_compile(strategy)\n+    with self.assertRaisesRegex(\n+        NotImplementedError, \"`model.predict` is not yet supported with \"\n+        \"`ParameterServerStrategy`.\"):\n+      model.predict(x=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  def testClusterCoordinatorSingleInstance(self, strategy):\n+    model = self._model_fit(strategy)\n+    strategy = model.distribute_strategy\n+    self.assertIs(strategy._cluster_coordinator,\n+                  tf.distribute.experimental.coordinator.ClusterCoordinator(strategy))\n+\n+\n+if __name__ == \"__main__\":\n+  tf.compat.v1.enable_v2_behavior()\n+  tf.__internal__.distribute.multi_process_runner.test_main()\n\n@@ -25,7 +25,6 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from tensorflow.python.distribute import distribute_utils\n from tensorflow.python.distribute import multi_worker_test_base\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n from keras import backend\n@@ -2666,7 +2665,8 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n       model.load_weights(temp_dir)\n       self.assertNotEmpty(model.optimizer.weights)\n       self.assertTrue(\n-          distribute_utils.is_distributed_variable(model.optimizer.weights[0]))\n+          distributed_training_utils.is_distributed_variable(\n+              model.optimizer.weights[0]))\n \n     with distribution.scope():\n       model = create_model()\n@@ -2674,7 +2674,8 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n     model.load_weights(temp_dir)\n     self.assertNotEmpty(model.optimizer.weights)\n     self.assertTrue(\n-        distribute_utils.is_distributed_variable(model.optimizer.weights[0]))\n+        distributed_training_utils.is_distributed_variable(\n+            model.optimizer.weights[0]))\n \n \n if __name__ == '__main__':\n\n@@ -58,3 +58,9 @@ def call_replica_local_fn(fn, *args, **kwargs):\n     with strategy.scope():\n       return strategy.extended.call_for_each_replica(fn, args, kwargs)\n   return fn(*args, **kwargs)\n+\n+\n+def is_distributed_variable(v):\n+  \"\"\"Returns whether `v` is a distributed variable.\"\"\"\n+  return (isinstance(v, tf.distribute.DistributedValues) and\n+          isinstance(v, tf.Variable))\n\n@@ -20,7 +20,10 @@ from __future__ import print_function\n \n import tensorflow.compat.v2 as tf\n import keras\n+from tensorflow.python.distribute import multi_worker_test_base\n+from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n from keras.optimizer_v2 import gradient_descent\n+from tensorflow.python.training.server_lib import ClusterSpec\n \n \n def mnist_synthetic_dataset(batch_size, steps_per_epoch):\n@@ -71,3 +74,12 @@ def get_mnist_model(input_shape):\n       optimizer=gradient_descent.SGD(learning_rate=0.001),\n       metrics=[\"accuracy\"])\n   return model\n+\n+\n+def make_parameter_server_cluster(num_workers, num_ps):\n+  cluster_def = multi_worker_test_base.create_in_process_cluster(\n+      num_workers=num_workers, num_ps=num_ps, rpc_layer=\"grpc\")\n+  cluster_def[\"chief\"] = [\n+      \"localhost:%d\" % multi_worker_test_base.pick_unused_port()\n+  ]\n+  return SimpleClusterResolver(ClusterSpec(cluster_def), rpc_layer=\"grpc\")\n\n@@ -25,22 +25,13 @@ import random\n import tempfile\n \n from absl.testing import parameterized\n-import numpy as np\n \n import keras\n-from tensorflow.python.distribute import multi_worker_test_base\n-from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n-from keras import callbacks as callbacks_lib\n+from keras.distribute import multi_worker_testing_utils\n from keras.engine import base_layer\n-from keras.engine import sequential\n-from keras.layers import core as core_layers\n from keras.layers.preprocessing import string_lookup\n-from keras.optimizer_v2 import gradient_descent\n from keras.optimizer_v2 import rmsprop\n-from keras.utils import dataset_creator\n from keras.utils import losses_utils\n-from tensorflow.python.platform import tf_logging as logging\n-from tensorflow.python.training.server_lib import ClusterSpec\n \n \n # These vocabularies usually come from TFT or a Beam pipeline.\n@@ -51,19 +42,11 @@ FEATURE_VOCAB = [\n LABEL_VOCAB = [\"yes\", \"no\"]\n \n \n-def make_cluster(num_workers, num_ps):\n-  cluster_def = multi_worker_test_base.create_in_process_cluster(\n-      num_workers=num_workers, num_ps=num_ps, rpc_layer=\"grpc\")\n-  cluster_def[\"chief\"] = [\n-      \"localhost:%d\" % multi_worker_test_base.pick_unused_port()\n-  ]\n-  return SimpleClusterResolver(ClusterSpec(cluster_def), rpc_layer=\"grpc\")\n-\n-\n def make_coordinator(num_workers, num_ps, variable_partitioner=None):\n   return tf.distribute.experimental.coordinator.ClusterCoordinator(\n       tf.distribute.experimental.ParameterServerStrategy(\n-          make_cluster(num_workers, num_ps),\n+          multi_worker_testing_utils.make_parameter_server_cluster(\n+              num_workers, num_ps),\n           variable_partitioner=variable_partitioner))\n \n \n@@ -231,148 +214,13 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertIn(prediction1, (\"yes\", \"no\"))\n \n \n-class ModelFitTest(tf.test.TestCase, parameterized.TestCase):\n-\n-  def _model_compile(self,\n-                     steps_per_execution=1,\n-                     run_eagerly=False,\n-                     with_normalization_layer=False):\n-\n-    class ResultAssertingCallback(callbacks_lib.Callback):\n-\n-      def __init__(self):\n-        self._prev_epoch = -1\n-\n-      def on_epoch_end(self, epoch, logs=None):\n-        logging.info(\"testModelFit: epoch=%r, logs=%r\", epoch, logs)\n-        if epoch <= self._prev_epoch:\n-          raise RuntimeError(\"Epoch is supposed to be larger than previous.\")\n-        self._prev_epoch = epoch\n-        if (logs.get(\"loss\", None) is None or\n-            not isinstance(logs[\"loss\"], np.floating)):\n-          raise RuntimeError(\"loss is supposed to be in the logs and float.\")\n-\n-    strategy = tf.distribute.experimental.ParameterServerStrategy(\n-        make_cluster(3, 2),\n-        variable_partitioner=tf.distribute.experimental.partitioners.FixedShardsPartitioner(2))\n-    with strategy.scope():\n-      model = sequential.Sequential([core_layers.Dense(10)])\n-      if with_normalization_layer:\n-        norm = keras.layers.BatchNormalization(\n-            axis=-1, input_shape=(4, 4, 3), momentum=0.8)\n-        model.add(norm)\n-\n-    model.compile(\n-        gradient_descent.SGD(),\n-        loss=\"mse\",\n-        steps_per_execution=steps_per_execution,\n-        run_eagerly=run_eagerly)\n-    return model, [ResultAssertingCallback()]\n-\n-  def _model_fit(self,\n-                 steps_per_execution=1,\n-                 validation_data=None,\n-                 x=None,\n-                 steps_per_epoch=10,\n-                 run_eagerly=False,\n-                 with_normalization_layer=False):\n-    model, callbacks = self._model_compile(steps_per_execution, run_eagerly,\n-                                           with_normalization_layer)\n-\n-    def dataset_fn(input_context):\n-      del input_context\n-      x = tf.random.uniform((10, 10))\n-      y = tf.random.uniform((10,))\n-      return tf.data.Dataset.from_tensor_slices(\n-          (x, y)).shuffle(10).repeat().batch(2)\n-\n-    x = x or dataset_creator.DatasetCreator(dataset_fn)\n-\n-    model.fit(\n-        x,\n-        epochs=10,\n-        steps_per_epoch=steps_per_epoch,\n-        verbose=0,\n-        callbacks=callbacks,\n-        validation_data=validation_data)\n-    return model\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testModelFit(self):\n-    model = self._model_fit()\n-    self.assertEqual(model.optimizer.iterations, 100)\n-    return model\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testModelFitWithNormalizationLayer(self):\n-    model = self._model_fit(with_normalization_layer=True)\n-    self.assertEqual(model.optimizer.iterations, 100)\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testModelFitWithStepsPerExecution(self):\n-    model = self._model_fit(steps_per_execution=10)\n-    self.assertEqual(model.optimizer.iterations, 100)\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testModelFitWithNoStepsPerEpoch(self):\n-    with self.assertRaisesRegex(\n-        ValueError, \"`steps_per_epoch` must be specified with \"\n-        \"`ParameterServerStrategy`.\"):\n-      self._model_fit(steps_per_epoch=None)\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testModelFitWithRunEagerly(self):\n-    with self.assertRaisesRegex(\n-        ValueError, \"When using `Model` with `ParameterServerStrategy`, \"\n-        \"`run_eagerly` is not supported.\"):\n-      self._model_fit(run_eagerly=True)\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testModelFitWithValidationData(self):\n-    with self.assertRaisesRegex(\n-        NotImplementedError, \"Evaluation in `model.fit` with \"\n-        \"`ParameterServerStrategy` is not yet supported.\"):\n-      self._model_fit(\n-          validation_data=tf.data.Dataset.from_tensor_slices([1, 1]))\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testModelFitWithDatasetInstance(self):\n-    with self.assertRaisesRegex(\n-        NotImplementedError, \"Only `DatasetCreator` input is supported in \"\n-        \"`ParameterServerStrategy` at this time.\"):\n-      self._model_fit(x=tf.data.Dataset.from_tensor_slices([1, 1]))\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testModelEvaluate(self):\n-    model, _ = self._model_compile()\n-    with self.assertRaisesRegex(\n-        NotImplementedError, \"`model.evaluate` is not yet supported with \"\n-        \"`ParameterServerStrategy`.\"):\n-      model.evaluate(x=tf.data.Dataset.from_tensor_slices([1, 1]))\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testModelPredict(self):\n-    model, _ = self._model_compile()\n-    with self.assertRaisesRegex(\n-        NotImplementedError, \"`model.predict` is not yet supported with \"\n-        \"`ParameterServerStrategy`.\"):\n-      model.predict(x=tf.data.Dataset.from_tensor_slices([1, 1]))\n-\n-  @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=[\"eager\"]))\n-  def testClusterCoordinatorSingleInstance(self):\n-    model = self._model_fit()\n-    strategy = model.distribute_strategy\n-    self.assertIs(strategy._cluster_coordinator,\n-                  tf.distribute.experimental.coordinator.ClusterCoordinator(strategy))\n-\n-\n class ShardedVariableTest(tf.test.TestCase):\n \n   @classmethod\n   def setUpClass(cls):\n     super().setUpClass()\n     cls.strategy = tf.distribute.experimental.ParameterServerStrategy(\n-        make_cluster(3, 2),\n+        multi_worker_testing_utils.make_parameter_server_cluster(3, 2),\n         variable_partitioner=tf.distribute.experimental.partitioners.FixedShardsPartitioner(2))\n \n   def test_keras_layer_setattr(self):\n\n@@ -57,7 +57,6 @@ from keras.utils.generic_utils import to_snake_case  # pylint: disable=unused-im\n from keras.utils.tf_utils import is_tensor_or_tensor_list  # pylint: disable=unused-import\n from tensorflow.python.platform import tf_logging\n from tensorflow.python.training.tracking import base as trackable\n-from tensorflow.python.training.tracking import data_structures\n from tensorflow.python.util.tf_export import get_canonical_name_for_symbol\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n@@ -2726,7 +2725,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       return\n \n     # Wraps data structures in `Trackable`, unwraps `NoDependency` objects.\n-    value = data_structures.sticky_attribute_assignment(\n+    value = tf.__internal__.tracking.sticky_attribute_assignment(\n         trackable=self, value=value, name=name)\n \n     reference_counts = self._obj_reference_counts\n\n@@ -50,7 +50,6 @@ from keras.utils.generic_utils import to_snake_case  # pylint: disable=unused-im\n from keras.utils.tf_utils import is_tensor_or_tensor_list  # pylint: disable=unused-import\n from tensorflow.python.platform import tf_logging\n from tensorflow.python.training.tracking import base as trackable\n-from tensorflow.python.training.tracking import data_structures\n from tensorflow.tools.docs import doc_controls\n \n \n@@ -2189,7 +2188,7 @@ class Layer(base_layer.Layer):\n       return\n \n     # Keep track of trackable objects, for the needs of `Network.save_weights`.\n-    value = data_structures.sticky_attribute_assignment(\n+    value = tf.__internal__.tracking.sticky_attribute_assignment(\n         trackable=self, value=value, name=name)\n \n     reference_counts = self._obj_reference_counts\n\n@@ -30,6 +30,7 @@ from keras import backend as K\n from keras.engine import data_adapter\n from keras.engine.base_layer import Layer\n from keras.utils import tf_utils\n+from keras.utils import version_utils\n from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -202,6 +203,8 @@ class PreprocessingLayer(Layer):\n           throw if 'reset_state' is set to False.\n     \"\"\"\n     _disallow_inside_tf_function('adapt')\n+    if not version_utils.should_use_v2():\n+      raise RuntimeError('`adapt` is only supported in tensorflow v2.')  # pylint: disable=g-doc-exception\n     if not self.stateful:\n       return\n     if not self.streaming and self._is_adapted and not reset_state:\n@@ -284,6 +287,7 @@ class CombinerPreprocessingLayer(PreprocessingLayer):\n   def reset_state(self):\n     self._adapt_accumulator = None\n \n+  @trackable.no_automatic_dependency_tracking\n   def update_state(self, data):\n     if self._adapt_accumulator is None:\n       self._adapt_accumulator = self._get_accumulator()\n\n@@ -30,7 +30,6 @@ import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import base_preprocessing_layer\n-from keras.engine import base_preprocessing_layer_v1\n \n \n # Define a test-only implementation of CombinerPreprocessingLayer to validate\n@@ -110,27 +109,14 @@ class AddingPreprocessingLayer(\n       return json.loads(tf.compat.as_text(encoded_accumulator))\n \n \n-class AddingPreprocessingLayerV1(\n-    AddingPreprocessingLayer,\n-    base_preprocessing_layer_v1.CombinerPreprocessingLayer):\n-  pass\n-\n-\n-def get_layer(**kwargs):\n-  if tf.executing_eagerly():\n-    return AddingPreprocessingLayer(**kwargs)\n-  else:\n-    return AddingPreprocessingLayerV1(**kwargs)\n-\n-\n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class PreprocessingLayerTest(keras_parameterized.TestCase):\n \n   def test_adapt_bad_input_fails(self):\n     \"\"\"Test that non-Dataset/Numpy inputs cause a reasonable error.\"\"\"\n     input_dataset = {\"foo\": 0}\n \n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     if tf.executing_eagerly():\n       with self.assertRaisesRegex(ValueError, \"Failed to find data adapter\"):\n         layer.adapt(input_dataset)\n@@ -143,7 +129,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     input_dataset = tf.data.Dataset.from_tensor_slices(\n         np.array([[1], [2], [3], [4], [5], [0]])).repeat()\n \n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     if tf.executing_eagerly():\n       with self.assertRaisesRegex(ValueError, \"infinite dataset\"):\n         layer.adapt(input_dataset)\n@@ -156,7 +142,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     \"\"\"Test external update injection before build() is called fails.\"\"\"\n     input_dataset = np.array([1, 2, 3, 4, 5])\n \n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     combiner = layer._combiner\n     updates = combiner.extract(combiner.compute(input_dataset))\n \n@@ -166,7 +152,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n   def test_setter_update(self):\n     \"\"\"Test the prototyped setter method.\"\"\"\n     input_data = keras.Input(shape=(1,))\n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n     model._run_eagerly = testing_utils.should_run_eagerly()\n@@ -179,7 +165,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     \"\"\"Test that preproc layers can adapt() before build() is called.\"\"\"\n     input_dataset = np.array([1, 2, 3, 4, 5])\n \n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     layer.adapt(input_dataset)\n \n     input_data = keras.Input(shape=(1,))\n@@ -194,7 +180,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     input_dataset = np.array([1, 2, 3, 4, 5])\n \n     input_data = keras.Input(shape=(1,))\n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n     model._run_eagerly = testing_utils.should_run_eagerly()\n@@ -207,7 +193,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     \"\"\"Test external update injection before build() is called.\"\"\"\n     input_dataset = np.array([1, 2, 3, 4, 5])\n \n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     combiner = layer._combiner\n     updates = combiner.extract(combiner.compute(input_dataset))\n \n@@ -225,7 +211,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     \"\"\"Test external update injection after build() is called.\"\"\"\n     input_dataset = np.array([1, 2, 3, 4, 5])\n     input_data = keras.Input(shape=(1,))\n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n     model._run_eagerly = testing_utils.should_run_eagerly()\n@@ -241,7 +227,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     input_dataset = tf.data.Dataset.from_tensor_slices(\n         np.array([[1], [2], [3], [4], [5], [0]]))\n \n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     layer.adapt(input_dataset)\n \n     input_data = keras.Input(shape=(1,))\n@@ -257,7 +243,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n         np.array([[1], [2], [3], [4], [5], [0]]))\n \n     input_data = keras.Input(shape=(1,))\n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n     model._run_eagerly = testing_utils.should_run_eagerly()\n@@ -271,7 +257,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n \n     input_dataset = np.array([1, 2, 3, 4, 5])\n \n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     layer.adapt(input_dataset)\n \n     input_data = keras.Input(shape=(1,))\n@@ -289,7 +275,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n \n     input_dataset = np.array([1, 2, 3, 4, 5])\n \n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n \n     input_data = keras.Input(shape=(1,))\n     output = layer(input_data)\n@@ -309,7 +295,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n \n     def get_model():\n       input_data = keras.Input(shape=(1,))\n-      layer = get_layer()\n+      layer = AddingPreprocessingLayer()\n       output = layer(input_data)\n       model = keras.Model(input_data, output)\n       model._run_eagerly = testing_utils.should_run_eagerly()\n@@ -334,7 +320,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n \n     def get_model():\n       input_data = keras.Input(shape=(1,))\n-      layer = get_layer()\n+      layer = AddingPreprocessingLayer()\n       output = layer(input_data)\n       model = keras.Model(input_data, output)\n       model._run_eagerly = testing_utils.should_run_eagerly()\n@@ -356,7 +342,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n \n   def test_loading_without_providing_class_fails(self):\n     input_data = keras.Input(shape=(1,))\n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n \n@@ -376,7 +362,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n                               [[3., 4.]],\n                               [[5., 6.]]], dtype=np.float32)\n \n-    layer = get_layer()\n+    layer = AddingPreprocessingLayer()\n     layer.adapt(adapt_dataset)\n \n     input_dataset = np.array([[[1., 2.], [3., 4.]],\n@@ -394,7 +380,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n                               [[3., 4.]],\n                               [[5., 6.]]], dtype=np.float32)\n \n-    layer = get_layer(input_shape=[1, 2])\n+    layer = AddingPreprocessingLayer(input_shape=[1, 2])\n     layer.adapt(adapt_dataset)\n \n     model = keras.Sequential([layer])\n@@ -402,7 +388,20 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     self.assertEqual(model.input_shape, (None, 1, 2))\n \n \n-@keras_parameterized.run_all_keras_modes\n+class PreprocessingLayerV1Test(keras_parameterized.TestCase):\n+\n+  def test_adapt_fails(self):\n+    \"\"\"Test that calling adapt leads to a runtime error.\"\"\"\n+    input_dataset = {\"foo\": 0}\n+\n+    with tf.Graph().as_default():\n+      layer = AddingPreprocessingLayer()\n+      with self.assertRaisesRegex(RuntimeError,\n+                                  \"`adapt` is only supported in tensorflow v2\"):\n+        layer.adapt(input_dataset)\n+\n+\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class ConvertToListTest(keras_parameterized.TestCase):\n \n   # Note: We need the inputs to be lambdas below to avoid some strangeness with\n\n@@ -1,171 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Contains the base ProcessingLayer and a subclass that uses Combiners.\"\"\"\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n-from keras import backend as K\n-from keras.engine import base_preprocessing_layer\n-from keras.engine import training_generator_v1\n-from keras.utils import tf_utils\n-\n-\n-class CombinerPreprocessingLayer(\n-    base_preprocessing_layer.CombinerPreprocessingLayer):\n-  \"\"\"V1-compatible CombinerPreprocessingLayer.\n-\n-  This class overrides several methods of the CombinerPreprocessingLayer to\n-  make it compatible with V1 execution. End users should not need to worry about\n-  the implementation details here; Keras will export the appropriate class under\n-  the 'CombinerPreprocessingLayer' symbol. (Users should not directly\n-  instantiate engine.base_preprocessing_layer/_v1.CombinerPreprocessingLayer).\n-\n-  When creating a subclass of PreprocessingLayer, you can create a V1-compatible\n-  subclass as follows:\n-\n-  class MyProcLayer(MyProcLayer,\n-                    base_preprocessing_layer_v1.CombinerPreprocessingLayer):\n-    pass\n-\n-  Note that the same classname is required for serialization purposes.\n-\n-  This is only necessary for internal classes, since any class that inherits\n-  from tf.keras.[...].CombinerPreprocessingLayer will get the right symbol.\n-  \"\"\"\n-\n-  def __init__(self, combiner, **kwargs):\n-    super(CombinerPreprocessingLayer, self).__init__(combiner, **kwargs)\n-    self._previously_updated = False\n-\n-  def _restore_updates(self):\n-    \"\"\"Recreates a dict of updates from the layer's weights.\"\"\"\n-    data_dict = {}\n-    for name, var in self.state_variables.items():\n-      data_dict[name] = K.get_session().run(var)\n-    return data_dict\n-\n-  def _get_dataset_iterator(self, dataset):\n-    \"\"\"Gets an iterator from a tf.data.Dataset.\"\"\"\n-    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n-    session = K.get_session()\n-    session.run(iterator.initializer)\n-    next_element = iterator.get_next()\n-    return lambda: session.run(next_element)\n-\n-  def _set_state_variables(self, updates):\n-    \"\"\"Directly update the internal state of this Layer. V1 compatible.\"\"\"\n-    # TODO(momernick): Do we need to do any more input sanitization?\n-    if not self.built:\n-      raise RuntimeError('_set_state_variables() must be called after build().')\n-\n-    assignments = []\n-    for var_name, value in updates.items():\n-      assignments.append(\n-          tf.compat.v1.assign(self.state_variables[var_name], value))\n-    K.get_session().run(assignments)\n-\n-  def adapt(self, data, reset_state=True):\n-    \"\"\"Fits the state of the preprocessing layer to the data being passed.\n-\n-    Args:\n-      data: The data to train on. It can be passed either as a tf.data Dataset,\n-        or as a numpy array.\n-      reset_state: Optional argument specifying whether to clear the state of\n-        the layer at the start of the call to `adapt`, or whether to start from\n-        the existing state. Subclasses may choose to throw if reset_state is set\n-        to 'False'.\n-    \"\"\"\n-    if reset_state:\n-      accumulator = None\n-    else:\n-      accumulator = self._combiner.restore(self._restore_updates())\n-    if isinstance(data, (list, tuple)):\n-      data = tf.convert_to_tensor(data)\n-    if not isinstance(data, (tf.data.Dataset, np.ndarray, tf.Tensor,\n-                             tf.RaggedTensor)):\n-      raise ValueError('`adapt()` requires a batched Dataset, a Tensor, '\n-                       'or a Numpy array as input, '\n-                       'got {}'.format(type(data)))\n-\n-    if isinstance(data, tf.data.Dataset):\n-      # Validate that the dataset only contains single-tensor elements.\n-      if not isinstance(data.element_spec, tf.TypeSpec):\n-        raise TypeError(\n-            'The dataset should yield single-Tensor elements. Use `dataset.map`'\n-            'to select the element of interest.\\n'\n-            'Got dataset.element_spec=' + str(data.element_spec))\n-      # Validate the datasets to try and ensure we haven't been passed one with\n-      # infinite size. That would cause an infinite loop here.\n-      if tf_utils.dataset_is_infinite(data):\n-        raise ValueError(\n-            'The dataset passed to `adapt()` has an infinite number of '\n-            'elements. Please use `dataset.take(...)` to make the number '\n-            'of elements finite.')\n-      next_data = self._get_dataset_iterator(data)\n-      # TODO(fchollet): consider checking if the dataset is already batched\n-      # and otherwise batching it.\n-    elif isinstance(data, (tf.Tensor, tf.RaggedTensor)):\n-      next_data = self._get_dataset_iterator(\n-          tf.data.Dataset.from_tensor_slices(data).batch(512))\n-    else:\n-      generator, _ = training_generator_v1.convert_to_generator_like(\n-          data, batch_size=512)\n-      # If the data is not a dataset, we can iterate over it using next(foo);\n-      # here, we wrap that into a callable.\n-      next_data = lambda: next(generator)\n-\n-    # TODO(momernick): Some sort of status bar?\n-    # TODO(momernick): Implement parallel processing here?\n-    try:\n-      data_element = next_data()\n-\n-      # First, see if the layer is built or not. If it is not, then we must\n-      # build it.\n-      if not self.built:\n-        try:\n-          # If this is a Numpy array or tensor, we can get shape from .shape.\n-          # If not, an attribute error will be thrown.\n-          data_shape = data_element.shape\n-          data_shape_nones = tuple([None] * len(data_element.shape))\n-        except AttributeError:\n-          # The input has an unknown number of dimensions.\n-          data_shape = None\n-          data_shape_nones = None\n-\n-        # TODO (b/159261555): move this to base layer build.\n-        batch_input_shape = getattr(self, '_batch_input_shape', None)\n-        if batch_input_shape is None:\n-          # Set the number of dimensions.\n-          self._batch_input_shape = data_shape_nones\n-\n-        self.build(data_shape)\n-\n-      # Once we have built the Layer, we can process the input data. We do so\n-      # until we've gotten an exception indicating that we have no more data.\n-      while True:\n-        accumulator = self._combiner.compute(data_element, accumulator)\n-        data_element = next_data()\n-    # Note that this belongs to the outer indentation of 'try' - we need to\n-    # catch exceptions resulting from the first 'next_data()' invocation as\n-    # well.\n-    except (StopIteration, tf.errors.OutOfRangeError):\n-      pass\n-\n-    updates = self._combiner.extract(accumulator)\n-    self._set_state_variables(updates)\n\n@@ -502,8 +502,15 @@ class GenericArrayLikeDataAdapter(TensorLikeDataAdapter):\n class DatasetCreatorAdapter(DataAdapter):\n   \"\"\"Adapter that handles dataset functions.\"\"\"\n \n-  def __init__(self, *args, **kwargs):\n-    super(DatasetCreatorAdapter, self).__init__(*args, **kwargs)\n+  def __init__(self, x, *args, **kwargs):\n+    super(DatasetCreatorAdapter, self).__init__(x, *args, **kwargs)\n+\n+    if not isinstance(x, dataset_creator.DatasetCreator):\n+      raise TypeError(\"The input of a `DatasetCreatorAdapter` should be a \"\n+                      \"`DatasetCreator` but it received type {}.\".format(\n+                          type(x)))\n+    self.dataset_creator = x\n+    self.strategy = kwargs.get(\"distribution_strategy\", None)\n \n   @staticmethod\n   def can_handle(x, y=None):\n@@ -518,10 +525,10 @@ class DatasetCreatorAdapter(DataAdapter):\n     return False\n \n   def get_size(self):\n-    raise NotImplementedError()\n+    return None  # To be inferred by `DataHandler`.\n \n   def get_dataset(self):\n-    raise NotImplementedError()\n+    return self.strategy.distribute_datasets_from_function(self.dataset_creator)\n \n   def batch_size(self):\n     raise NotImplementedError()\n@@ -1165,9 +1172,7 @@ class DataHandler(object):\n                                                class_weight, distribute)\n \n   def _verify_data_adapter_compatibility(self, adapter_cls):\n-    if adapter_cls == DatasetCreatorAdapter:\n-      raise NotImplementedError(\"`DatasetCreator` input is only supported in \"\n-                                \"`ParameterServerStrategy` at this time.\")\n+    pass\n \n   def _configure_dataset_and_inferred_steps(self, strategy, x, steps_per_epoch,\n                                             class_weight, distribute):\n\n@@ -2246,7 +2246,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         session = backend.get_session()\n         # Restore existing variables (if any) immediately, and set up a\n         # streaming restore for any variables created in the future.\n-        trackable_utils.streaming_restore(status=status, session=session)\n+        tf.__internal__.tracking.streaming_restore(status=status, session=session)\n       status.assert_nontrivial_match()\n       return status\n     if h5py is None:\n\n@@ -17,17 +17,17 @@ from __future__ import division\n from __future__ import print_function\n \n import numpy as np\n-import tensorflow as tf\n+import tensorflow.compat.v1 as tf\n \n-tf.compat.v1.disable_eager_execution()\n+tf.disable_eager_execution()\n \n \n class KerasNetworkTFRNNs(tf.keras.Model):\n \n   def __init__(self, name=None):\n     super(KerasNetworkTFRNNs, self).__init__(name=name)\n-    self._cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(\n-        [tf.compat.v1.nn.rnn_cell.LSTMCell(1) for _ in range(2)])\n+    self._cell = tf.nn.rnn_cell.MultiRNNCell(\n+        [tf.nn.rnn_cell.LSTMCell(1) for _ in range(2)])\n \n   def call(self, inputs):\n     return self._cell(inputs, self._cell.get_initial_state(inputs))\n@@ -44,7 +44,7 @@ class KerasNetworkKerasRNNs(tf.keras.Model):\n     return self._cell(inputs, self._cell.get_initial_state(inputs))\n \n \n-class LegacyRNNTest(tf.compat.v1.test.TestCase):\n+class LegacyRNNTest(tf.test.TestCase):\n \n   def setUp(self):\n     super(LegacyRNNTest, self).setUp()\n@@ -65,19 +65,19 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n       y_train = tf.keras.utils.to_categorical(y_train)\n       cell = tf.keras.layers.SimpleRNNCell(output_shape)\n \n-      inputs = tf.compat.v1.placeholder(\n+      inputs = tf.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      predict = tf.compat.v1.placeholder(\n+      predict = tf.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.compat.v1.nn.dynamic_rnn(\n+      outputs, state = tf.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(outputs.shape.as_list(), [None, timestep, output_shape])\n       self.assertEqual(state.shape.as_list(), [None, output_shape])\n-      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state)\n-      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.losses.softmax_cross_entropy(predict, state)\n+      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.compat.v1.global_variables_initializer()])\n+      sess.run([tf.global_variables_initializer()])\n       _, outputs, state = sess.run(\n           [train_op, outputs, state], {inputs: x_train, predict: y_train})\n \n@@ -98,19 +98,19 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n       y_train = tf.keras.utils.to_categorical(y_train)\n       cell = tf.keras.layers.GRUCell(output_shape)\n \n-      inputs = tf.compat.v1.placeholder(\n+      inputs = tf.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      predict = tf.compat.v1.placeholder(\n+      predict = tf.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.compat.v1.nn.dynamic_rnn(\n+      outputs, state = tf.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(outputs.shape.as_list(), [None, timestep, output_shape])\n       self.assertEqual(state.shape.as_list(), [None, output_shape])\n-      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state)\n-      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.losses.softmax_cross_entropy(predict, state)\n+      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.compat.v1.global_variables_initializer()])\n+      sess.run([tf.global_variables_initializer()])\n       _, outputs, state = sess.run(\n           [train_op, outputs, state], {inputs: x_train, predict: y_train})\n \n@@ -131,21 +131,21 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n       y_train = tf.keras.utils.to_categorical(y_train)\n       cell = tf.keras.layers.LSTMCell(output_shape)\n \n-      inputs = tf.compat.v1.placeholder(\n+      inputs = tf.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      predict = tf.compat.v1.placeholder(\n+      predict = tf.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.compat.v1.nn.dynamic_rnn(\n+      outputs, state = tf.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(outputs.shape.as_list(), [None, timestep, output_shape])\n       self.assertEqual(len(state), 2)\n       self.assertEqual(state[0].shape.as_list(), [None, output_shape])\n       self.assertEqual(state[1].shape.as_list(), [None, output_shape])\n-      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state[0])\n-      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.losses.softmax_cross_entropy(predict, state[0])\n+      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.compat.v1.global_variables_initializer()])\n+      sess.run([tf.global_variables_initializer()])\n       _, outputs, state = sess.run(\n           [train_op, outputs, state], {inputs: x_train, predict: y_train})\n \n@@ -170,25 +170,25 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n           [tf.keras.layers.LSTMCell(2 * output_shape),\n            tf.keras.layers.LSTMCell(output_shape)])\n \n-      inputs = tf.compat.v1.placeholder(\n+      inputs = tf.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      predict = tf.compat.v1.placeholder(\n+      predict = tf.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.compat.v1.nn.dynamic_rnn(\n+      outputs, state = tf.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(outputs.shape.as_list(), [None, timestep, output_shape])\n       self.assertEqual(len(state), 2)\n-      state = tf.compat.v1.nest.flatten(state)\n+      state = tf.nest.flatten(state)\n       self.assertEqual(len(state), 4)\n       self.assertEqual(state[0].shape.as_list(), [None, 2 * output_shape])\n       self.assertEqual(state[1].shape.as_list(), [None, 2 * output_shape])\n       self.assertEqual(state[2].shape.as_list(), [None, output_shape])\n       self.assertEqual(state[3].shape.as_list(), [None, output_shape])\n-      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state[2])\n-      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.losses.softmax_cross_entropy(predict, state[2])\n+      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.compat.v1.global_variables_initializer()])\n+      sess.run([tf.global_variables_initializer()])\n       _, outputs, state = sess.run(\n           [train_op, outputs, state], {inputs: x_train, predict: y_train})\n \n@@ -212,20 +212,20 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n       y_train = tf.keras.utils.to_categorical(y_train)\n       cell = tf.keras.layers.SimpleRNNCell(output_shape)\n \n-      inputs = [tf.compat.v1.placeholder(\n+      inputs = [tf.placeholder(\n           tf.float32, shape=(None, input_shape))] * timestep\n-      predict = tf.compat.v1.placeholder(\n+      predict = tf.placeholder(\n           tf.float32, shape=(None, output_shape))\n \n-      outputs, state = tf.compat.v1.nn.static_rnn(\n+      outputs, state = tf.nn.static_rnn(\n           cell, inputs, dtype=tf.float32)\n       self.assertEqual(len(outputs), timestep)\n       self.assertEqual(outputs[0].shape.as_list(), [None, output_shape])\n       self.assertEqual(state.shape.as_list(), [None, output_shape])\n-      loss = tf.compat.v1.losses.softmax_cross_entropy(predict, state)\n-      train_op = tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss)\n+      loss = tf.losses.softmax_cross_entropy(predict, state)\n+      train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n \n-      sess.run([tf.compat.v1.global_variables_initializer()])\n+      sess.run([tf.global_variables_initializer()])\n       feed_dict = {i: d for i, d in zip(inputs, x_train)}\n       feed_dict[predict] = y_train\n       _, outputs, state = sess.run(\n@@ -250,10 +250,10 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n     weights = fix_weights_generator.get_weights()\n \n     with self.session(graph=tf.Graph()) as sess:\n-      inputs = tf.compat.v1.placeholder(\n+      inputs = tf.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n       cell = tf.keras.layers.SimpleRNNCell(output_shape)\n-      tf_out, tf_state = tf.compat.v1.nn.dynamic_rnn(\n+      tf_out, tf_state = tf.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       cell.set_weights(weights)\n       [tf_out, tf_state] = sess.run([tf_out, tf_state], {inputs: x_train})\n@@ -289,18 +289,18 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n     tf_weights = [np.concatenate((kernel, recurrent_kernel)), bias]\n \n     with self.session(graph=tf.Graph()) as sess:\n-      inputs = tf.compat.v1.placeholder(\n+      inputs = tf.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n       cell = tf.keras.layers.SimpleRNNCell(output_shape)\n-      k_out, k_state = tf.compat.v1.nn.dynamic_rnn(\n+      k_out, k_state = tf.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       cell.set_weights(keras_weights)\n       [k_out, k_state] = sess.run([k_out, k_state], {inputs: x_train})\n     with self.session(graph=tf.Graph()) as sess:\n-      inputs = tf.compat.v1.placeholder(\n+      inputs = tf.placeholder(\n           tf.float32, shape=(None, timestep, input_shape))\n-      cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(output_shape)\n-      tf_out, tf_state = tf.compat.v1.nn.dynamic_rnn(\n+      cell = tf.nn.rnn_cell.BasicRNNCell(output_shape)\n+      tf_out, tf_state = tf.nn.dynamic_rnn(\n           cell, inputs, dtype=tf.float32)\n       cell.set_weights(tf_weights)\n       [tf_out, tf_state] = sess.run([tf_out, tf_state], {inputs: x_train})\n@@ -310,10 +310,10 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n \n   def testRNNCellSerialization(self):\n     for cell in [\n-        tf.compat.v1.nn.rnn_cell.LSTMCell(32, use_peepholes=True, cell_clip=True),\n-        tf.compat.v1.nn.rnn_cell.BasicLSTMCell(32, dtype=tf.float32),\n-        tf.compat.v1.nn.rnn_cell.BasicRNNCell(32, activation=\"relu\", dtype=tf.float32),\n-        tf.compat.v1.nn.rnn_cell.GRUCell(32, dtype=tf.float32)\n+        tf.nn.rnn_cell.LSTMCell(32, use_peepholes=True, cell_clip=True),\n+        tf.nn.rnn_cell.BasicLSTMCell(32, dtype=tf.float32),\n+        tf.nn.rnn_cell.BasicRNNCell(32, activation=\"relu\", dtype=tf.float32),\n+        tf.nn.rnn_cell.GRUCell(32, dtype=tf.float32)\n     ]:\n       with self.cached_session():\n         x = tf.keras.Input((None, 5))\n@@ -333,10 +333,10 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n         layer = tf.keras.layers.RNN.from_config(\n             config,\n             custom_objects={\n-                \"BasicRNNCell\": tf.compat.v1.nn.rnn_cell.BasicRNNCell,\n-                \"GRUCell\": tf.compat.v1.nn.rnn_cell.GRUCell,\n-                \"LSTMCell\": tf.compat.v1.nn.rnn_cell.LSTMCell,\n-                \"BasicLSTMCell\": tf.compat.v1.nn.rnn_cell.BasicLSTMCell\n+                \"BasicRNNCell\": tf.nn.rnn_cell.BasicRNNCell,\n+                \"GRUCell\": tf.nn.rnn_cell.GRUCell,\n+                \"LSTMCell\": tf.nn.rnn_cell.LSTMCell,\n+                \"BasicLSTMCell\": tf.nn.rnn_cell.BasicLSTMCell\n             })\n         y = layer(x)\n         model = tf.keras.models.Model(x, y)\n@@ -345,7 +345,7 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n         self.assertAllClose(y_np, y_np_2, atol=1e-4)\n \n   def testRNNCellActsLikeKerasRNNCellInProperScope(self):\n-    with tf.compat.v1.layers.experimental.keras_style_scope():\n+    with tf.layers.experimental.keras_style_scope():\n       kn1 = KerasNetworkTFRNNs(name=\"kn1\")\n       kn2 = KerasNetworkKerasRNNs(name=\"kn2\")\n \n@@ -358,7 +358,7 @@ class LegacyRNNTest(tf.compat.v1.test.TestCase):\n     self.assertTrue(all(\"kn1\" in v.name for v in kn1._cell.variables))\n     self.assertTrue(all(\"kn2\" in v.name for v in kn2._cell.variables))\n \n-    with tf.compat.v1.layers.experimental.keras_style_scope():\n+    with tf.layers.experimental.keras_style_scope():\n       kn1_new = KerasNetworkTFRNNs(name=\"kn1_new\")\n       kn2_new = KerasNetworkKerasRNNs(name=\"kn2_new\")\n \n@@ -385,4 +385,4 @@ def get_test_data(train_samples,\n \n \n if __name__ == \"__main__\":\n-  tf.compat.v1.test.main()\n+  tf.test.main()\n\n@@ -45,36 +45,14 @@ from keras.layers.preprocessing.image_preprocessing import Resizing\n from keras.layers.preprocessing.image_preprocessing import Rescaling\n \n # Preprocessing layers.\n-if tf.__internal__.tf2.enabled():\n-  from keras.layers.preprocessing.integer_lookup import IntegerLookup\n-  from keras.layers.preprocessing.integer_lookup_v1 import IntegerLookup as IntegerLookupV1\n-  IntegerLookupV2 = IntegerLookup\n-  from keras.layers.preprocessing.normalization import Normalization\n-  from keras.layers.preprocessing.normalization_v1 import Normalization as NormalizationV1\n-  NormalizationV2 = Normalization\n-  from keras.layers.preprocessing.string_lookup import StringLookup\n-  from keras.layers.preprocessing.string_lookup_v1 import StringLookup as StringLookupV1\n-  StringLookupV2 = StringLookup\n-  from keras.layers.preprocessing.text_vectorization import TextVectorization\n-  from keras.layers.preprocessing.text_vectorization_v1 import TextVectorization as TextVectorizationV1\n-  TextVectorizationV2 = TextVectorization\n-else:\n-  from keras.layers.preprocessing.integer_lookup_v1 import IntegerLookup\n-  from keras.layers.preprocessing.integer_lookup import IntegerLookup as IntegerLookupV2\n-  IntegerLookupV1 = IntegerLookup\n-  from keras.layers.preprocessing.normalization_v1 import Normalization\n-  from keras.layers.preprocessing.normalization import Normalization as NormalizationV2\n-  NormalizationV1 = Normalization\n-  from keras.layers.preprocessing.string_lookup_v1 import StringLookup\n-  from keras.layers.preprocessing.string_lookup import StringLookup as StringLookupV2\n-  StringLookupV1 = StringLookup\n-  from keras.layers.preprocessing.text_vectorization_v1 import TextVectorization\n-  from keras.layers.preprocessing.text_vectorization import TextVectorization as TextVectorizationV2\n-  TextVectorizationV1 = TextVectorization\n from keras.layers.preprocessing.category_crossing import CategoryCrossing\n from keras.layers.preprocessing.category_encoding import CategoryEncoding\n from keras.layers.preprocessing.discretization import Discretization\n from keras.layers.preprocessing.hashing import Hashing\n+from keras.layers.preprocessing.integer_lookup import IntegerLookup\n+from keras.layers.preprocessing.normalization import Normalization\n+from keras.layers.preprocessing.string_lookup import StringLookup\n+from keras.layers.preprocessing.text_vectorization import TextVectorization\n \n # Advanced activations.\n from keras.layers.advanced_activations import LeakyReLU\n\n@@ -26,12 +26,11 @@ from keras import layers\n class LayersTest(tf.test.TestCase):\n \n   def test_keras_private_symbol(self):\n-    normalization_parent = layers.Normalization.__module__.split('.')[-1]\n     if tf.__internal__.tf2.enabled():\n+      normalization_parent = layers.Normalization.__module__.split('.')[-1]\n       self.assertEqual('normalization', normalization_parent)\n       self.assertTrue(layers.BatchNormalization._USE_V2_BEHAVIOR)\n     else:\n-      self.assertEqual('normalization_v1', normalization_parent)\n       self.assertFalse(layers.BatchNormalization._USE_V2_BEHAVIOR)\n \n \n\n@@ -0,0 +1,78 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of bucketized columns with dense inputs.\"\"\"\n+\n+import numpy as np\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing import discretization\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10  # The number of times to run each benchmark.\n+BATCH_SIZES = [32, 256]\n+\n+\n+### KPL AND FC IMPLEMENTATION BENCHMARKS ###\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+  max_value = 25.0\n+  bins = np.arange(1.0, max_value)\n+  data = fc_bm.create_data(\n+      max_length, batch_size * NUM_REPEATS, 100000, dtype=float)\n+\n+  # Keras implementation\n+  model = keras.Sequential()\n+  model.add(keras.Input(shape=(max_length,), name=\"data\", dtype=tf.float32))\n+  model.add(discretization.Discretization(bins))\n+\n+  # FC implementation\n+  fc = tf.feature_column.bucketized_column(\n+      tf.feature_column.numeric_column(\"data\"), boundaries=list(bins))\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\"data\": data.to_tensor(default_value=0.0)}\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\"data\": data.to_tensor(default_value=0.0)}\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"bucketized|dense|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,90 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of categorical cross hash columns with dense inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing import category_crossing\n+from keras.layers.preprocessing import hashing\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+\n+  num_buckets = 10000\n+  vocab = fc_bm.create_vocabulary(32768)\n+  data_a = fc_bm.create_string_data(\n+      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)\n+  data_b = fc_bm.create_string_data(\n+      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)\n+\n+  # Keras implementation\n+  input_1 = keras.Input(shape=(None,), name=\"data_a\", dtype=tf.string)\n+  input_2 = keras.Input(shape=(None,), name=\"data_b\", dtype=tf.string)\n+  crossed_data = category_crossing.CategoryCrossing()([input_1, input_2])\n+  hashed_data = hashing.Hashing(num_buckets)(crossed_data)\n+  model = keras.Model([input_1, input_2], hashed_data)\n+\n+  # FC implementation\n+  fc = tf.feature_column.crossed_column([\"data_a\", \"data_b\"], num_buckets)\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\n+      \"data_a\":\n+          data_a.to_tensor(default_value=\"\", shape=(batch_size, max_length)),\n+      \"data_b\":\n+          data_b.to_tensor(default_value=\"\", shape=(batch_size, max_length)),\n+  }\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\n+      \"data_a\":\n+          data_a.to_tensor(default_value=\"\", shape=(batch_size, max_length)),\n+      \"data_b\":\n+          data_b.to_tensor(default_value=\"\", shape=(batch_size, max_length)),\n+  }\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"cross_hash|dense|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,79 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of categorical hash columns with dense inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing import hashing\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+\n+  num_buckets = 10000\n+  vocab = fc_bm.create_vocabulary(32768)\n+  data = fc_bm.create_string_data(\n+      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)\n+\n+  # Keras implementation\n+  model = keras.Sequential()\n+  model.add(keras.Input(shape=(max_length,), name=\"data\", dtype=tf.string))\n+  model.add(hashing.Hashing(num_buckets))\n+\n+  # FC implementation\n+  fc = tf.feature_column.sequence_categorical_column_with_hash_bucket(\"data\", num_buckets)\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\n+      \"data\": data.to_tensor(default_value=\"\", shape=(batch_size, max_length))\n+  }\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\n+      \"data\": data.to_tensor(default_value=\"\", shape=(batch_size, max_length))\n+  }\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"hash|dense|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,77 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of categorical hash columns with varying-length inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing import hashing\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+\n+  num_buckets = 10000\n+  vocab = fc_bm.create_vocabulary(32768)\n+  data = fc_bm.create_string_data(\n+      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)\n+\n+  # Keras implementation\n+  model = keras.Sequential()\n+  model.add(\n+      keras.Input(\n+          shape=(max_length,), name=\"data\", ragged=True, dtype=tf.string))\n+  model.add(hashing.Hashing(num_buckets))\n+\n+  # FC implementation\n+  fc = tf.feature_column.categorical_column_with_hash_bucket(\"data\", num_buckets)\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\"data\": data}\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\"data\": data.to_sparse()}\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"hash|varlen|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,78 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of vocabulary columns from lists with dense inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing import string_lookup\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+  vocab = fc_bm.create_vocabulary(32768)\n+  data = fc_bm.create_string_data(\n+      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)\n+\n+  # Keras implementation\n+  model = keras.Sequential()\n+  model.add(keras.Input(shape=(max_length,), name=\"data\", dtype=tf.string))\n+  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))\n+\n+  # FC implementation\n+  fc = tf.feature_column.categorical_column_with_vocabulary_list(\n+      key=\"data\", vocabulary_list=vocab, num_oov_buckets=1)\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\n+      \"data\": data.to_tensor(default_value=\"\", shape=(batch_size, max_length))\n+  }\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\n+      \"data\": data.to_tensor(default_value=\"\", shape=(batch_size, max_length))\n+  }\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"vocab_list|dense|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,84 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of vocabulary columns + indicator from lists with dense inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing import category_encoding\n+from keras.layers.preprocessing import string_lookup\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+  vocab_size = 32768\n+  vocab = fc_bm.create_vocabulary(vocab_size)\n+  data = fc_bm.create_string_data(\n+      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)\n+\n+  # Keras implementation\n+  model = keras.Sequential()\n+  model.add(keras.Input(shape=(max_length,), name=\"data\", dtype=tf.string))\n+  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))\n+  model.add(\n+      category_encoding.CategoryEncoding(\n+          num_tokens=vocab_size + 1, output_mode=\"count\"))\n+\n+  # FC implementation\n+  fc = tf.feature_column.indicator_column(\n+      tf.feature_column.categorical_column_with_vocabulary_list(\n+          key=\"data\", vocabulary_list=vocab, num_oov_buckets=1))\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\n+      \"data\": data.to_tensor(default_value=\"\", shape=(batch_size, max_length))\n+  }\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\n+      \"data\": data.to_tensor(default_value=\"\", shape=(batch_size, max_length))\n+  }\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"vocab_list_indicator|dense|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,82 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of vocabulary columns + indicator from lists with varying-length inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing import category_encoding\n+from keras.layers.preprocessing import string_lookup\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+  vocab_size = 32768\n+  vocab = fc_bm.create_vocabulary(vocab_size)\n+  data = fc_bm.create_string_data(\n+      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)\n+\n+  # Keras implementation\n+  model = keras.Sequential()\n+  model.add(\n+      keras.Input(\n+          shape=(max_length,), name=\"data\", ragged=True, dtype=tf.string))\n+  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))\n+  model.add(\n+      category_encoding.CategoryEncoding(\n+          num_tokens=vocab_size + 1, output_mode=\"count\"))\n+\n+  # FC implementation\n+  fc = tf.feature_column.indicator_column(\n+      tf.feature_column.sequence_categorical_column_with_vocabulary_list(\n+          key=\"data\", vocabulary_list=vocab, num_oov_buckets=1))\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\"data\": data}\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\"data\": data.to_sparse()}\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"vocab_list_indicator|varlen|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,76 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of vocabulary columns from lists with varying-length inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing import string_lookup\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+  vocab = fc_bm.create_vocabulary(32768)\n+  data = fc_bm.create_string_data(\n+      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)\n+\n+  # Keras implementation\n+  model = keras.Sequential()\n+  model.add(\n+      keras.Input(\n+          shape=(max_length,), name=\"data\", ragged=True, dtype=tf.string))\n+  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))\n+\n+  # FC implementation\n+  fc = tf.feature_column.sequence_categorical_column_with_vocabulary_list(\n+      key=\"data\", vocabulary_list=vocab, num_oov_buckets=1)\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\"data\": data}\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\"data\": data.to_sparse()}\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"vocab_list|varlen|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,77 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of embedding column with dense inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+### KPL AND FC IMPLEMENTATION BENCHMARKS ###\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+  embedding_size = 32768\n+  data = fc_bm.create_data(\n+      max_length, batch_size * NUM_REPEATS, embedding_size - 1, dtype=int)\n+\n+  # Keras implementation\n+  model = keras.Sequential()\n+  model.add(keras.Input(shape=(None,), name=\"data\", dtype=tf.int64))\n+  model.add(keras.layers.Embedding(embedding_size, 256))\n+  model.add(keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1)))\n+\n+  # FC implementation\n+  fc = tf.feature_column.embedding_column(\n+      tf.feature_column.categorical_column_with_identity(\n+          \"data\", num_buckets=embedding_size - 1),\n+      dimension=256)\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\"data\": data.to_tensor(default_value=0)}\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\"data\": data.to_tensor(default_value=0)}\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"embedding|dense|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,78 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of embedding column with varying-length inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+### KPL AND FC IMPLEMENTATION BENCHMARKS ###\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+  embedding_size = 32768\n+  data = fc_bm.create_data(\n+      max_length, batch_size * NUM_REPEATS, embedding_size - 1, dtype=int)\n+\n+  # Keras implementation\n+  model = keras.Sequential()\n+  model.add(\n+      keras.Input(shape=(None,), ragged=True, name=\"data\", dtype=tf.int64))\n+  model.add(keras.layers.Embedding(embedding_size, 256))\n+  model.add(keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1)))\n+\n+  # FC implementation\n+  fc = tf.feature_column.embedding_column(\n+      tf.feature_column.categorical_column_with_identity(\n+          \"data\", num_buckets=embedding_size - 1),\n+      dimension=256)\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\"data\": data}\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\"data\": data.to_sparse()}\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"embedding|varlen|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -0,0 +1,147 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark suite for KPL and feature column implementations.\"\"\"\n+import itertools\n+import math\n+import random\n+import string\n+import time\n+\n+import numpy as np\n+\n+import keras\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+\n+class LayerBenchmark(tf.test.Benchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def report(self, name, keras_time, fc_time, iters):\n+    \"\"\"Calculate and report benchmark statistics.\"\"\"\n+    extras = {\n+        \"fc_avg_time\": fc_time,\n+        \"fc_vs_keras_sec\": fc_time - keras_time,\n+        \"fc_vs_keras_pct\": ((fc_time - keras_time) / fc_time) * 100,\n+        \"keras_faster_ratio\": fc_time / keras_time\n+    }\n+    self.report_benchmark(\n+        iters=iters, wall_time=keras_time, extras=extras, name=name)\n+\n+\n+class StepTimingCallback(keras.callbacks.Callback):\n+  \"\"\"A callback that times non-warmup steps of a Keras predict call.\"\"\"\n+\n+  def __init__(self):\n+    self.t0 = None\n+    self.steps = 0\n+\n+  def on_predict_batch_begin(self, batch_index, _):\n+    if batch_index == 2:\n+      self.t0 = time.time()\n+    elif batch_index > 2:\n+      self.steps += 1\n+\n+  def on_predict_end(self, _):\n+    self.tn = time.time()\n+    self.t_avg = (self.tn - self.t0) / self.steps\n+\n+\n+def create_data(length, num_entries, max_value, dtype):\n+  \"\"\"Create a ragged tensor with random data entries.\"\"\"\n+  lengths = (np.random.random(size=num_entries) * length).astype(int)\n+  total_length = np.sum(lengths)\n+  values = (np.random.random(size=total_length) * max_value).astype(dtype)\n+  return tf.RaggedTensor.from_row_lengths(values, lengths)\n+\n+\n+def create_string_data(length,\n+                       num_entries,\n+                       vocabulary,\n+                       pct_oov,\n+                       oov_string=\"__OOV__\"):\n+  \"\"\"Create a ragged tensor with random data entries.\"\"\"\n+  lengths = (np.random.random(size=num_entries) * length).astype(int)\n+  total_length = np.sum(lengths)\n+  num_oovs = int(pct_oov * total_length)\n+  values = []\n+  for _ in range(total_length):\n+    values.append(random.choice(vocabulary))\n+\n+  if pct_oov > 0:\n+    oov_cadence = int(total_length / num_oovs)\n+    idx = 0\n+    for _ in range(num_oovs):\n+      if idx < total_length:\n+        values[idx] = oov_string\n+      idx += oov_cadence\n+\n+  return tf.RaggedTensor.from_row_lengths(values, lengths)\n+\n+\n+def create_vocabulary(vocab_size):\n+  base = len(string.ascii_letters)\n+  n = math.ceil(math.log(vocab_size, base))\n+  vocab = []\n+  for i in range(1, n + 1):\n+    for item in itertools.product(string.ascii_letters, repeat=i):\n+      if len(vocab) >= vocab_size:\n+        break\n+      vocab.append(\"\".join(item))\n+  return vocab\n+\n+\n+def run_keras(data, model, batch_size, num_runs, steps_per_repeat=100):\n+  \"\"\"Benchmark a Keras model.\"\"\"\n+  ds = tf.data.Dataset.from_tensor_slices(data).repeat().prefetch(\n+      tf.data.AUTOTUNE).batch(batch_size).cache()\n+  steps = 0\n+  times = []\n+  for _ in range(num_runs):\n+    steps += steps_per_repeat\n+    timer = StepTimingCallback()\n+    # Benchmarked code begins here.\n+    model.predict(ds, steps=steps, callbacks=[timer])\n+    # Benchmarked code ends here.\n+    times.append(timer.t_avg)\n+  avg_time = np.mean(times)\n+  return avg_time\n+\n+\n+def run_fc(data, fc_fn, batch_size, num_runs, steps_per_repeat=100):\n+  \"\"\"Benchmark a Feature Column.\"\"\"\n+\n+  ds = tf.data.Dataset.from_tensor_slices(data).repeat().prefetch(\n+      tf.data.AUTOTUNE).batch(batch_size).cache()\n+\n+  # Trace the fc_fn\n+  ds_iter = ds.__iter__()\n+  fc_fn(next(ds_iter))\n+  fc_starts = []\n+  fc_ends = []\n+  for _ in range(num_runs):\n+    fc_starts.append(time.time())\n+    # Benchmarked code begins here.\n+    for _ in range(steps_per_repeat):\n+      _ = fc_fn(next(ds_iter))\n+    # Benchmarked code ends here.\n+    fc_ends.append(time.time())\n+  avg_per_step_time = (np.array(fc_ends) -\n+                       np.array(fc_starts)) / steps_per_repeat\n+  avg_time = np.mean(avg_per_step_time)\n+  return avg_time\n\n@@ -0,0 +1,85 @@\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import tensorflow as tf\n+\"\"\"Benchmark for KPL implementation of weighted embedding column with varying-length inputs.\"\"\"\n+\n+import keras\n+from tensorflow.python.eager.def_function import function as tf_function\n+from tensorflow.python.feature_column import feature_column_v2 as fcv2\n+from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n+\n+# This is required as of 3/2021 because otherwise we drop into graph mode.\n+tf.compat.v1.enable_v2_behavior()\n+\n+NUM_REPEATS = 10\n+BATCH_SIZES = [32, 256]\n+\n+\n+### KPL AND FC IMPLEMENTATION BENCHMARKS ###\n+def embedding_varlen(batch_size, max_length):\n+  \"\"\"Benchmark a variable-length embedding.\"\"\"\n+  # Data and constants.\n+  embedding_size = 32768\n+  data = fc_bm.create_data(\n+      max_length, batch_size * NUM_REPEATS, embedding_size - 1, dtype=int)\n+  weight = tf.ones_like(data, dtype=tf.float32)\n+\n+  # Keras implementation\n+  data_input = keras.Input(\n+      shape=(None,), ragged=True, name=\"data\", dtype=tf.int64)\n+  weight_input = keras.Input(\n+      shape=(None,), ragged=True, name=\"weight\", dtype=tf.float32)\n+  embedded_data = keras.layers.Embedding(embedding_size, 256)(data_input)\n+  weighted_embedding = tf.multiply(\n+      embedded_data, tf.compat.v1.expand_dims(weight_input, -1))\n+  reduced_embedding = tf.reduce_sum(weighted_embedding, axis=1)\n+  model = keras.Model([data_input, weight_input], reduced_embedding)\n+\n+  # FC implementation\n+  fc = tf.feature_column.embedding_column(\n+      tf.feature_column.weighted_categorical_column(\n+          tf.feature_column.categorical_column_with_identity(\n+              \"data\", num_buckets=embedding_size - 1),\n+          weight_feature_key=\"weight\"),\n+      dimension=256)\n+\n+  # Wrap the FC implementation in a tf.function for a fair comparison\n+  @tf_function()\n+  def fc_fn(tensors):\n+    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+\n+  # Benchmark runs\n+  keras_data = {\"data\": data, \"weight\": weight}\n+  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)\n+\n+  fc_data = {\"data\": data.to_sparse(), \"weight\": weight.to_sparse()}\n+  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)\n+\n+  return k_avg_time, fc_avg_time\n+\n+\n+class BenchmarkLayer(fc_bm.LayerBenchmark):\n+  \"\"\"Benchmark the layer forward pass.\"\"\"\n+\n+  def benchmark_layer(self):\n+    for batch in BATCH_SIZES:\n+      name = \"weighted_embedding|varlen|batch_%s\" % batch\n+      k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n+      self.report(name, k_time, f_time, NUM_REPEATS)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n@@ -28,17 +28,9 @@ import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers.preprocessing import discretization\n-from keras.layers.preprocessing import discretization_v1\n from keras.layers.preprocessing import preprocessing_test_utils\n \n \n-def get_layer_class():\n-  if tf.executing_eagerly():\n-    return discretization.Discretization\n-  else:\n-    return discretization_v1.Discretization\n-\n-\n @keras_parameterized.run_all_keras_modes\n class DiscretizationTest(keras_parameterized.TestCase,\n                          preprocessing_test_utils.PreprocessingLayerTest):\n@@ -50,7 +42,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     expected_output_shape = [None, 4]\n \n     input_data = keras.Input(shape=(4,))\n-    layer = get_layer_class()(bin_boundaries=[0., 1., 2.])\n+    layer = discretization.Discretization(bin_boundaries=[0., 1., 2.])\n     bucket_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())\n \n@@ -65,7 +57,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     expected_output_shape = [None, 4]\n \n     input_data = keras.Input(shape=(4,), dtype=tf.int64)\n-    layer = get_layer_class()(bin_boundaries=[-.5, 0.5, 1.5])\n+    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])\n     bucket_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())\n \n@@ -79,7 +71,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n         indices=indices, values=[-1.5, 1.0, 3.4], dense_shape=[2, 3])\n     expected_output = [0, 2, 3]\n     input_data = keras.Input(shape=(3,), dtype=tf.float32, sparse=True)\n-    layer = get_layer_class()(bin_boundaries=[-.5, 0.5, 1.5])\n+    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])\n     bucket_data = layer(input_data)\n \n     model = keras.Model(inputs=input_data, outputs=bucket_data)\n@@ -95,7 +87,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     expected_output_shape = [None, None]\n \n     input_data = keras.Input(shape=(None,), ragged=True)\n-    layer = get_layer_class()(bin_boundaries=[0., 1., 2.])\n+    layer = discretization.Discretization(bin_boundaries=[0., 1., 2.])\n     bucket_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())\n \n@@ -111,7 +103,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     expected_output_shape = [None, None]\n \n     input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.int64)\n-    layer = get_layer_class()(bin_boundaries=[-.5, 0.5, 1.5])\n+    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])\n     bucket_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())\n     model = keras.Model(inputs=input_data, outputs=bucket_data)\n@@ -124,7 +116,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n         indices=indices, values=[-1, 1, 3], dense_shape=[2, 3])\n     expected_output = [0, 2, 3]\n     input_data = keras.Input(shape=(3,), dtype=tf.int32, sparse=True)\n-    layer = get_layer_class()(bin_boundaries=[-.5, 0.5, 1.5])\n+    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])\n     bucket_data = layer(input_data)\n \n     model = keras.Model(inputs=input_data, outputs=bucket_data)\n@@ -134,13 +126,47 @@ class DiscretizationTest(keras_parameterized.TestCase,\n \n   def test_num_bins_negative_fails(self):\n     with self.assertRaisesRegex(ValueError, \"`num_bins` must be.*num_bins=-7\"):\n-      _ = get_layer_class()(num_bins=-7)\n+      _ = discretization.Discretization(num_bins=-7)\n \n   def test_num_bins_and_bins_set_fails(self):\n     with self.assertRaisesRegex(\n         ValueError,\n         r\"`num_bins` and `bin_boundaries` should not be set.*5.*\\[1, 2\\]\"):\n-      _ = get_layer_class()(num_bins=5, bins=[1, 2])\n+      _ = discretization.Discretization(num_bins=5, bins=[1, 2])\n+\n+  @parameterized.named_parameters(\n+      {\n+          \"num_bins\": 5,\n+          \"data\": np.array([[1.], [2.], [3.], [4.], [5.]]),\n+          \"expected\": {\n+              \"bins\": np.array([1., 2., 3., 4., np.Inf])\n+          },\n+          \"testcase_name\": \"2d_single_element_all_bins\"\n+      }, {\n+          \"num_bins\": 5,\n+          \"data\": np.array([[1., 6.], [2., 7.], [3., 8.], [4., 9.], [5., 10.]]),\n+          \"expected\": {\n+              \"bins\": np.array([2., 4., 6., 8., np.Inf])\n+          },\n+          \"testcase_name\": \"2d_multi_element_all_bins\",\n+      }, {\n+          \"num_bins\": 3,\n+          \"data\": np.array([[0.], [1.], [2.], [3.], [4.], [5.]]),\n+          \"expected\": {\n+              \"bins\": np.array([1., 3., np.Inf])\n+          },\n+          \"testcase_name\": \"2d_single_element_3_bins\"\n+      })\n+  def test_combiner_computation(self, num_bins, data, expected):\n+    epsilon = 0.01\n+    combiner = discretization.Discretization.DiscretizingCombiner(\n+        epsilon, num_bins)\n+    self.validate_accumulator_extract(combiner, data, expected)\n+\n+\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+class DiscretizationAdaptTest(keras_parameterized.TestCase,\n+                              preprocessing_test_utils.PreprocessingLayerTest):\n \n   @parameterized.named_parameters([\n       {\n@@ -207,8 +233,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n       test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(\n           test_data.shape[0] // 2)\n \n-    cls = get_layer_class()\n-    layer = cls(epsilon=epsilon, num_bins=num_bins)\n+    layer = discretization.Discretization(epsilon=epsilon, num_bins=num_bins)\n     layer.adapt(adapt_data)\n \n     input_data = keras.Input(shape=input_shape)\n@@ -218,34 +243,6 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     output_data = model.predict(test_data)\n     self.assertAllClose(expected, output_data)\n \n-  @parameterized.named_parameters(\n-      {\n-          \"num_bins\": 5,\n-          \"data\": np.array([[1.], [2.], [3.], [4.], [5.]]),\n-          \"expected\": {\n-              \"bins\": np.array([1., 2., 3., 4., np.Inf])\n-          },\n-          \"testcase_name\": \"2d_single_element_all_bins\"\n-      }, {\n-          \"num_bins\": 5,\n-          \"data\": np.array([[1., 6.], [2., 7.], [3., 8.], [4., 9.], [5., 10.]]),\n-          \"expected\": {\n-              \"bins\": np.array([2., 4., 6., 8., np.Inf])\n-          },\n-          \"testcase_name\": \"2d_multi_element_all_bins\",\n-      }, {\n-          \"num_bins\": 3,\n-          \"data\": np.array([[0.], [1.], [2.], [3.], [4.], [5.]]),\n-          \"expected\": {\n-              \"bins\": np.array([1., 3., np.Inf])\n-          },\n-          \"testcase_name\": \"2d_single_element_3_bins\"\n-      })\n-  def test_combiner_computation(self, num_bins, data, expected):\n-    epsilon = 0.01\n-    combiner = discretization.Discretization.DiscretizingCombiner(epsilon,\n-                                                                  num_bins)\n-    self.validate_accumulator_extract(combiner, data, expected)\n \n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -1,31 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tensorflow V1 version of the Discretization preprocessing layer.\"\"\"\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from keras.engine import base_preprocessing_layer\n-from keras.engine.base_preprocessing_layer_v1 import CombinerPreprocessingLayer\n-from keras.layers.preprocessing import discretization\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export(v1=['keras.layers.experimental.preprocessing.Discretization'])\n-class Discretization(discretization.Discretization, CombinerPreprocessingLayer):\n-  base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-      'Discretization_V1').set(True)\n-  pass\n\n@@ -114,7 +114,6 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n       These should be non-zero. Defaults to `None` (in that\n       case, the FarmHash64 hash function is used). It also supports\n       tuple/list of 2 unsigned integer numbers, see reference paper for details.\n-    name: Name to give to the layer.\n     **kwargs: Keyword arguments to construct a layer.\n \n   Input shape: A single or list of string, int32 or int64 `Tensor`,\n@@ -127,10 +126,10 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n \n   \"\"\"\n \n-  def __init__(self, num_bins, mask_value=None, salt=None, name=None, **kwargs):\n+  def __init__(self, num_bins, mask_value=None, salt=None, **kwargs):\n     if num_bins is None or num_bins <= 0:\n       raise ValueError('`num_bins` cannot be `None` or non-positive values.')\n-    super(Hashing, self).__init__(name=name, **kwargs)\n+    super(Hashing, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Hashing').set(True)\n     self.num_bins = num_bins\n     self.mask_value = mask_value\n\n@@ -69,21 +69,19 @@ class Resizing(PreprocessingLayer):\n     interpolation: String, the interpolation method. Defaults to `bilinear`.\n       Supports `bilinear`, `nearest`, `bicubic`, `area`, `lanczos3`, `lanczos5`,\n       `gaussian`, `mitchellcubic`\n-    name: A string, the name of the layer.\n   \"\"\"\n \n   def __init__(self,\n                height,\n                width,\n                interpolation='bilinear',\n-               name=None,\n                **kwargs):\n     self.target_height = height\n     self.target_width = width\n     self.interpolation = interpolation\n     self._interpolation_method = get_interpolation(interpolation)\n     self.input_spec = InputSpec(ndim=4)\n-    super(Resizing, self).__init__(name=name, **kwargs)\n+    super(Resizing, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Resizing').set(True)\n \n   def call(self, inputs):\n@@ -126,14 +124,13 @@ class CenterCrop(PreprocessingLayer):\n   Args:\n     height: Integer, the height of the output shape.\n     width: Integer, the width of the output shape.\n-    name: A string, the name of the layer.\n   \"\"\"\n \n-  def __init__(self, height, width, name=None, **kwargs):\n+  def __init__(self, height, width, **kwargs):\n     self.target_height = height\n     self.target_width = width\n     self.input_spec = InputSpec(ndim=4)\n-    super(CenterCrop, self).__init__(name=name, **kwargs)\n+    super(CenterCrop, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('CenterCrop').set(True)\n \n   def call(self, inputs):\n@@ -199,16 +196,15 @@ class RandomCrop(PreprocessingLayer):\n     height: Integer, the height of the output shape.\n     width: Integer, the width of the output shape.\n     seed: Integer. Used to create a random seed.\n-    name: A string, the name of the layer.\n   \"\"\"\n \n-  def __init__(self, height, width, seed=None, name=None, **kwargs):\n+  def __init__(self, height, width, seed=None, **kwargs):\n     self.height = height\n     self.width = width\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n     self.input_spec = InputSpec(ndim=4)\n-    super(RandomCrop, self).__init__(name=name, **kwargs)\n+    super(RandomCrop, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomCrop').set(True)\n \n   def call(self, inputs, training=True):\n@@ -307,13 +303,12 @@ class Rescaling(PreprocessingLayer):\n   Args:\n     scale: Float, the scale to apply to the inputs.\n     offset: Float, the offset to apply to the inputs.\n-    name: A string, the name of the layer.\n   \"\"\"\n \n-  def __init__(self, scale, offset=0., name=None, **kwargs):\n+  def __init__(self, scale, offset=0., **kwargs):\n     self.scale = scale\n     self.offset = offset\n-    super(Rescaling, self).__init__(name=name, **kwargs)\n+    super(Rescaling, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Rescaling').set(True)\n \n   def call(self, inputs):\n@@ -361,15 +356,13 @@ class RandomFlip(PreprocessingLayer):\n       \"horizontal_and_vertical\". \"horizontal\" is a left-right flip and\n       \"vertical\" is a top-bottom flip.\n     seed: Integer. Used to create a random seed.\n-    name: A string, the name of the layer.\n   \"\"\"\n \n   def __init__(self,\n                mode=HORIZONTAL_AND_VERTICAL,\n                seed=None,\n-               name=None,\n                **kwargs):\n-    super(RandomFlip, self).__init__(name=name, **kwargs)\n+    super(RandomFlip, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomFlip').set(True)\n     self.mode = mode\n     if mode == HORIZONTAL:\n@@ -383,7 +376,7 @@ class RandomFlip(PreprocessingLayer):\n       self.vertical = True\n     else:\n       raise ValueError('RandomFlip layer {name} received an unknown mode '\n-                       'argument {arg}'.format(name=name, arg=mode))\n+                       'argument {arg}'.format(name=self.name, arg=mode))\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n     self.input_spec = InputSpec(ndim=4)\n@@ -453,7 +446,6 @@ class RandomTranslation(PreprocessingLayer):\n         nearest pixel.\n     interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n     seed: Integer. Used to create a random seed.\n-    name: A string, the name of the layer.\n     fill_value: a float represents the value to be filled outside the boundaries\n       when `fill_mode` is \"constant\".\n   Input shape:\n@@ -473,7 +465,6 @@ class RandomTranslation(PreprocessingLayer):\n                fill_mode='reflect',\n                interpolation='bilinear',\n                seed=None,\n-               name=None,\n                fill_value=0.0,\n                **kwargs):\n     self.height_factor = height_factor\n@@ -512,7 +503,7 @@ class RandomTranslation(PreprocessingLayer):\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n     self.input_spec = InputSpec(ndim=4)\n-    super(RandomTranslation, self).__init__(name=name, **kwargs)\n+    super(RandomTranslation, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomTranslation').set(\n         True)\n \n@@ -768,7 +759,6 @@ class RandomRotation(PreprocessingLayer):\n         nearest pixel.\n     interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n     seed: Integer. Used to create a random seed.\n-    name: A string, the name of the layer.\n     fill_value: a float represents the value to be filled outside the boundaries\n       when `fill_mode` is \"constant\".\n   Raise:\n@@ -781,7 +771,6 @@ class RandomRotation(PreprocessingLayer):\n                fill_mode='reflect',\n                interpolation='bilinear',\n                seed=None,\n-               name=None,\n                fill_value=0.0,\n                **kwargs):\n     self.factor = factor\n@@ -801,7 +790,7 @@ class RandomRotation(PreprocessingLayer):\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n     self.input_spec = InputSpec(ndim=4)\n-    super(RandomRotation, self).__init__(name=name, **kwargs)\n+    super(RandomRotation, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomRotation').set(\n         True)\n \n@@ -878,7 +867,6 @@ class RandomZoom(PreprocessingLayer):\n         nearest pixel.\n     interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n     seed: Integer. Used to create a random seed.\n-    name: A string, the name of the layer.\n     fill_value: a float represents the value to be filled outside the boundaries\n       when `fill_mode` is \"constant\".\n   Example:  >>> input_img = np.random.random((32, 224, 224, 3)) >>> layer =\n@@ -901,7 +889,6 @@ class RandomZoom(PreprocessingLayer):\n                fill_mode='reflect',\n                interpolation='bilinear',\n                seed=None,\n-               name=None,\n                fill_value=0.0,\n                **kwargs):\n     self.height_factor = height_factor\n@@ -937,7 +924,7 @@ class RandomZoom(PreprocessingLayer):\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n     self.input_spec = InputSpec(ndim=4)\n-    super(RandomZoom, self).__init__(name=name, **kwargs)\n+    super(RandomZoom, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomZoom').set(True)\n \n   def call(self, inputs, training=True):\n@@ -1058,13 +1045,12 @@ class RandomContrast(PreprocessingLayer):\n       float, lower = upper. The contrast factor will be randomly picked between\n       [1.0 - lower, 1.0 + upper].\n     seed: Integer. Used to create a random seed.\n-    name: A string, the name of the layer.\n   Raise:\n     ValueError: if lower bound is not between [0, 1], or upper bound is\n       negative.\n   \"\"\"\n \n-  def __init__(self, factor, seed=None, name=None, **kwargs):\n+  def __init__(self, factor, seed=None, **kwargs):\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.lower = factor[0]\n@@ -1076,7 +1062,7 @@ class RandomContrast(PreprocessingLayer):\n                        ' got {}'.format(factor))\n     self.seed = seed\n     self.input_spec = InputSpec(ndim=4)\n-    super(RandomContrast, self).__init__(name=name, **kwargs)\n+    super(RandomContrast, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomContrast').set(\n         True)\n \n@@ -1127,7 +1113,6 @@ class RandomHeight(PreprocessingLayer):\n       Supports `bilinear`, `nearest`, `bicubic`, `area`, `lanczos3`, `lanczos5`,\n       `gaussian`, `mitchellcubic`\n     seed: Integer. Used to create a random seed.\n-    name: A string, the name of the layer.\n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`\n       (data_format='channels_last').\n@@ -1139,7 +1124,6 @@ class RandomHeight(PreprocessingLayer):\n                factor,\n                interpolation='bilinear',\n                seed=None,\n-               name=None,\n                **kwargs):\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n@@ -1160,7 +1144,7 @@ class RandomHeight(PreprocessingLayer):\n     self.input_spec = InputSpec(ndim=4)\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n-    super(RandomHeight, self).__init__(name=name, **kwargs)\n+    super(RandomHeight, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomHeight').set(True)\n \n   def call(self, inputs, training=True):\n@@ -1225,7 +1209,6 @@ class RandomWidth(PreprocessingLayer):\n       Supports `bilinear`, `nearest`, `bicubic`, `area`, `lanczos3`, `lanczos5`,\n       `gaussian`, `mitchellcubic`\n     seed: Integer. Used to create a random seed.\n-    name: A string, the name of the layer.\n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`\n       (data_format='channels_last').\n@@ -1237,7 +1220,6 @@ class RandomWidth(PreprocessingLayer):\n                factor,\n                interpolation='bilinear',\n                seed=None,\n-               name=None,\n                **kwargs):\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n@@ -1257,7 +1239,7 @@ class RandomWidth(PreprocessingLayer):\n     self.input_spec = InputSpec(ndim=4)\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n-    super(RandomWidth, self).__init__(name=name, **kwargs)\n+    super(RandomWidth, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomWidth').set(True)\n \n   def call(self, inputs, training=True):\n\n@@ -31,6 +31,7 @@ from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import table_utils\n from keras.utils import layer_utils\n from tensorflow.python.ops import lookup_ops\n+from tensorflow.python.platform import tf_logging as logging\n \n INT = \"int\"\n BINARY = \"binary\"\n@@ -138,9 +139,9 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     # ops do not set shape on their outputs, which means we have to set it\n     # ourselves. We persist the current vocab size as a hidden part of the\n     # config when serializing our model.\n-    if \"vocab_size\" in kwargs:\n-      self._vocab_size = kwargs[\"vocab_size\"]\n-      del kwargs[\"vocab_size\"]\n+    if \"vocabulary_size\" in kwargs:\n+      self._vocab_size = kwargs[\"vocabulary_size\"]\n+      del kwargs[\"vocabulary_size\"]\n \n     if max_tokens is not None:\n       available_vocab_size = max_tokens - self._token_start_index()\n@@ -290,9 +291,18 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     lookup = collections.defaultdict(lambda: self.oov_token, index_to_token)\n     return [lookup[x] for x in range(self._vocab_size)]\n \n-  def vocab_size(self):\n+  def vocabulary_size(self):\n+    \"\"\"Gets the current size of the layer's vocabulary.\n+\n+    Returns:\n+      The integer size of the voculary, including optional mask and oov indices.\n+    \"\"\"\n     return self._vocab_size\n \n+  def vocab_size(self):\n+    logging.warning(\"vocab_size is deprecated, please use vocabulary_size.\")\n+    return self.vocabulary_size()\n+\n   def get_config(self):\n     config = {\n         \"invert\": self.invert,\n@@ -302,7 +312,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         \"mask_token\": self.mask_token,\n         \"output_mode\": self.output_mode,\n         \"pad_to_max_tokens\": self.pad_to_max_tokens,\n-        \"vocab_size\": self._vocab_size\n+        \"vocabulary_size\": self._vocab_size\n     }\n     base_config = super(IndexLookup, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n@@ -314,7 +324,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     # abstraction for ease of saving!) we return 0.\n     return 0\n \n-  def set_vocabulary(self, vocab, idf_weights=None):\n+  def set_vocabulary(self, vocabulary, idf_weights=None):\n     \"\"\"Sets vocabulary (and optionally document frequency) data for this layer.\n \n     This method sets the vocabulary and idf weights for this layer directly,\n@@ -324,7 +334,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     it.\n \n     Args:\n-      vocab: An array of hashable tokens.\n+      vocabulary: An array of hashable tokens.\n       idf_weights: An array of inverse document frequency weights with equal\n         length to vocab. Only necessary if the layer output_mode is TFIDF.\n \n@@ -349,11 +359,11 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     oov_start = self._oov_start_index()\n     token_start = self._token_start_index()\n     should_have_mask = (oov_start > 0)\n-    has_mask = should_have_mask and vocab[0] == self.mask_token\n+    has_mask = should_have_mask and vocabulary[0] == self.mask_token\n \n     should_have_oov = (self.num_oov_indices > 0)\n     expected_oov = [self.oov_token] * self.num_oov_indices\n-    found_oov = vocab[oov_start:token_start]\n+    found_oov = vocabulary[oov_start:token_start]\n     has_oov = should_have_oov and found_oov == expected_oov\n     # If we get a numpy array, then has_oov may end up being a numpy array\n     # instead of a bool. Fix this by collapsing the variable if it's not bool.\n@@ -388,13 +398,13 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n               oov=self.oov_token,\n               start=oov_start,\n               end=token_start,\n-              found=vocab[0]))\n+              found=vocabulary[0]))\n \n     found_special_tokens = has_oov or has_mask\n     if found_special_tokens:\n-      tokens = vocab[token_start:]\n+      tokens = vocabulary[token_start:]\n     else:\n-      tokens = vocab\n+      tokens = vocabulary\n \n     repeated_tokens = table_utils.find_repeated_tokens(tokens)\n     if repeated_tokens:\n@@ -425,10 +435,10 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     if self.output_mode == TFIDF:\n       if idf_weights is None:\n         raise ValueError(\"`idf_weights` must be set if output_mode is TFIDF\")\n-      if len(vocab) != len(idf_weights):\n-        raise ValueError(\"`idf_weights` must be the same length as vocab. \"\n-                         \"len(idf_weights) is {}, len(vocab) is {}\".format(\n-                             len(vocab), len(idf_weights)))\n+      if len(vocabulary) != len(idf_weights):\n+        raise ValueError(\"`idf_weights` must be the same length as vocabulary. \"\n+                         \"len(idf_weights) is {}, len(vocabulary) is {}\".format(\n+                             len(vocabulary), len(idf_weights)))\n       idf_weights = self._convert_to_ndarray(idf_weights)\n       if idf_weights.ndim != 1:\n         raise ValueError(\n\n@@ -26,17 +26,9 @@ import keras\n from keras import keras_parameterized\n from keras.distribute.strategy_combinations import all_strategies\n from keras.layers.preprocessing import index_lookup\n-from keras.layers.preprocessing import index_lookup_v1\n from keras.layers.preprocessing import preprocessing_test_utils\n \n \n-def get_layer_class():\n-  if tf.executing_eagerly():\n-    return index_lookup.IndexLookup\n-  else:\n-    return index_lookup_v1.IndexLookup\n-\n-\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         distribution=all_strategies,\n@@ -61,7 +53,7 @@ class IndexLookupDistributionTest(\n \n     with distribution.scope():\n       input_data = keras.Input(shape=(None,), dtype=tf.string)\n-      layer = get_layer_class()(\n+      layer = index_lookup.IndexLookup(\n           max_tokens=None,\n           num_oov_indices=1,\n           mask_token=\"\",\n\n@@ -32,17 +32,12 @@ import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers.preprocessing import index_lookup\n-from keras.layers.preprocessing import index_lookup_v1\n from keras.layers.preprocessing import preprocessing_test_utils\n-from keras.saving import save\n from keras.utils.generic_utils import CustomObjectScope\n \n \n def get_layer_class():\n-  if tf.executing_eagerly():\n   return index_lookup.IndexLookup\n-  else:\n-    return index_lookup_v1.IndexLookup\n \n \n def _get_end_to_end_test_cases():\n@@ -300,7 +295,7 @@ def _get_end_to_end_test_cases():\n   return crossed_test_cases\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupLayerTest(keras_parameterized.TestCase,\n                            preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -352,7 +347,7 @@ class IndexLookupLayerTest(keras_parameterized.TestCase,\n       self.assertAllClose(expected_output, output_data)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingInputTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -467,7 +462,7 @@ class CategoricalEncodingInputTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingMultiOOVTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -563,7 +558,7 @@ class CategoricalEncodingMultiOOVTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingAdaptTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -684,7 +679,7 @@ class CategoricalEncodingAdaptTest(\n     layer.adapt(batched_ds)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupOutputTest(keras_parameterized.TestCase,\n                             preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -1090,7 +1085,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n                                 preprocessing_test_utils.PreprocessingLayerTest\n                                ):\n@@ -1144,7 +1139,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n     self.assertAllEqual(vocab_data, returned_vocab)\n-    self.assertAllEqual(layer.vocab_size(), 5)\n+    self.assertAllEqual(layer.vocabulary_size(), 5)\n \n   def test_int_vocab_with_max_cap(self):\n     vocab_data = [0, -1, 42, 1276, 1138]\n@@ -1157,7 +1152,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n     self.assertAllEqual(vocab_data, returned_vocab)\n-    self.assertAllEqual(layer.vocab_size(), 5)\n+    self.assertAllEqual(layer.vocabulary_size(), 5)\n \n   def test_vocab_with_multiple_oov_indices(self):\n     vocab_data = [\"\", \"[OOV]\", \"[OOV]\", \"[OOV]\", \"wind\"]\n@@ -1395,7 +1390,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n       layer.set_vocabulary(vocab_data)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupInverseVocabularyTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -1518,49 +1513,7 @@ class IndexLookupInverseVocabularyTest(\n       layer.set_vocabulary(vocab_data)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_eager=True)\n-class IndexLookupSaveableTest(keras_parameterized.TestCase,\n-                              preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_ops_are_not_added_with_multiple_get_set_weights(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n-        max_tokens=10,\n-        num_oov_indices=1,\n-        mask_token=\"\",\n-        oov_token=\"[OOV]\",\n-        dtype=tf.string)\n-    layer.set_vocabulary(vocab_data)\n-    int_data = layer(input_data)\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    weights = model.get_weights()\n-    model.set_weights(weights)\n-    keras.backend.get_session().graph.finalize()\n-    weights = model.get_weights()\n-    model.set_weights(weights)\n-\n-  def test_layer_saving_with_h5(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n-        max_tokens=10,\n-        num_oov_indices=1,\n-        mask_token=\"\",\n-        oov_token=\"[OOV]\",\n-        dtype=tf.string)\n-    layer.set_vocabulary(vocab_data)\n-    int_data = layer(input_data)\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    path = os.path.join(self.get_temp_dir(), \"model\")\n-    with self.assertRaisesRegex(NotImplementedError,\n-                                \"Save or restore weights that is not.*\"):\n-      save.save_model(model, path, save_format=\"h5\")\n-\n-\n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupErrorTest(keras_parameterized.TestCase,\n                            preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -1587,7 +1540,7 @@ class IndexLookupErrorTest(keras_parameterized.TestCase,\n           dtype=tf.string)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupSavingTest(keras_parameterized.TestCase,\n                             preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -1632,7 +1585,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n     self.assertAllEqual(new_output_dataset, expected_output)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupStringCombinerTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -1769,7 +1722,7 @@ class IndexLookupStringCombinerTest(\n     self.validate_accumulator_extract(combiner, data, expected_extract_output)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupIntCombinerTest(keras_parameterized.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n\n@@ -1,65 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tensorflow V1 version of the text vectorization preprocessing layer.\"\"\"\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow.compat.v2 as tf\n-\n-\n-from keras.engine import base_preprocessing_layer_v1\n-from keras.layers.preprocessing import index_lookup\n-\n-\n-class IndexLookup(index_lookup.IndexLookup,\n-                  base_preprocessing_layer_v1.CombinerPreprocessingLayer):\n-  \"\"\"IndexLookup layer.\n-\n-  This layer translates a set of arbitray strings or integers into an integer\n-  output via a table-based lookup, with optional out-of-vocabulary handling.\n-\n-  If desired, the user can call this layer's adapt() method on a data set.\n-  When this layer is adapted, it will analyze the dataset, determine the\n-  frequency of individual string or integer values, and create a vocabulary\n-  from them. This vocabulary can have unlimited size or be capped, depending on\n-  the configuration options for this layer; if there are more unique values in\n-  the input than the maximum vocabulary size, the most frequent terms will be\n-  used to create the vocabulary.\n-\n-  Attributes:\n-    max_vocab_size: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary. Note that the vocabulary\n-      does include OOV buckets, so the effective number of unique values in the\n-      vocabulary is (max_vocab_size - num_oov_buckets) when this value is set.\n-    num_oov_buckets: The number of out-of-vocabulary tokens to use; defaults to\n-      1. If this value is more than 1, OOV inputs are hashed to determine their\n-      OOV value; if this value is 0, passing an OOV input will result in a\n-      runtime error.\n-    reserve_zero: Whether to reserve the index '0', which has a special meaning\n-      in the Keras masking system. If True, the output of this layer will be in\n-      the range [1...max_vocab_size+1); if False, the output will be in the\n-      range [0...max_vocab_size). Defaults to True.\n-    mask_inputs: If True, input values of 0 (for integers) and \"\" (for strings)\n-      will be treated as masked values and assigned an output value of 0. If\n-      this option is set, reserve_zero must also be set. Defaults to False.\n-  \"\"\"\n-\n-  def _use_v1_apis(self):\n-    return True\n-\n-  def _static_table_class(self):\n-    return tf.compat.v1.lookup.StaticHashTable\n\n@@ -326,17 +326,17 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n     return base_config\n \n-  def set_vocabulary(self, vocab, idf_weights=None):\n-    if isinstance(vocab, str):\n+  def set_vocabulary(self, vocabulary, idf_weights=None):\n+    if isinstance(vocabulary, str):\n       if self.output_mode == index_lookup.TFIDF:\n         raise RuntimeError(\n             \"Setting vocabulary directly from a file is not \"\n             \"supported in TF-IDF mode, since this layer cannot \"\n             \"read files containing TF-IDF weight data. Please \"\n-            \"read the file using Python and set the vocab \"\n+            \"read the file using Python and set the vocabulary \"\n             \"and weights by passing lists or arrays to the \"\n-            \"set_vocabulary function's `vocab` and `idf_weights` \"\n+            \"set_vocabulary function's `vocabulary` and `idf_weights` \"\n             \"args.\")\n-      vocab = table_utils.get_vocabulary_from_file(vocab)\n-      vocab = [int(v) for v in vocab]\n-    super().set_vocabulary(vocab, idf_weights=idf_weights)\n+      vocabulary = table_utils.get_vocabulary_from_file(vocabulary)\n+      vocabulary = [int(v) for v in vocabulary]\n+    super().set_vocabulary(vocabulary, idf_weights=idf_weights)\n\n@@ -32,19 +32,10 @@ import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers.preprocessing import integer_lookup\n-from keras.layers.preprocessing import integer_lookup_v1\n from keras.layers.preprocessing import preprocessing_test_utils\n-from keras.saving import save\n from keras.utils.generic_utils import CustomObjectScope\n \n \n-def get_layer_class():\n-  if tf.executing_eagerly():\n-    return integer_lookup.IntegerLookup\n-  else:\n-    return integer_lookup_v1.IntegerLookup\n-\n-\n def _get_end_to_end_test_cases():\n   test_cases = (\n       {\n@@ -82,7 +73,7 @@ def _get_end_to_end_test_cases():\n   return crossed_test_cases\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IntegerLookupLayerTest(keras_parameterized.TestCase,\n                              preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -90,7 +81,7 @@ class IntegerLookupLayerTest(keras_parameterized.TestCase,\n   def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,\n                                        use_dataset, expected_output,\n                                        input_dtype):\n-    cls = get_layer_class()\n+    cls = integer_lookup.IntegerLookup\n     expected_output_dtype = tf.int64\n     input_shape = input_data.shape\n \n@@ -125,7 +116,7 @@ class IntegerLookupLayerTest(keras_parameterized.TestCase,\n     self.assertAllClose(expected_output, output_data)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingInputTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -142,7 +133,7 @@ class CategoricalEncodingInputTest(\n     expected_dense_shape = [3, 4]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n-    layer = get_layer_class()(max_values=None)\n+    layer = integer_lookup.IntegerLookup(max_values=None)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -158,7 +149,7 @@ class CategoricalEncodingInputTest(\n     expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n-    layer = get_layer_class()(max_values=None)\n+    layer = integer_lookup.IntegerLookup(max_values=None)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -166,7 +157,7 @@ class CategoricalEncodingInputTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingMultiOOVTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -183,7 +174,7 @@ class CategoricalEncodingMultiOOVTest(\n     expected_dense_shape = [3, 4]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n-    layer = get_layer_class()(\n+    layer = integer_lookup.IntegerLookup(\n         max_values=None,\n         dtype=tf.int64,\n         num_oov_indices=2,\n@@ -204,7 +195,7 @@ class CategoricalEncodingMultiOOVTest(\n     expected_output = [[3, 4, 6], [6, 5, 3, 2]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n-    layer = get_layer_class()(max_values=None, num_oov_indices=2)\n+    layer = integer_lookup.IntegerLookup(max_values=None, num_oov_indices=2)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -212,7 +203,7 @@ class CategoricalEncodingMultiOOVTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingAdaptTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -224,7 +215,7 @@ class CategoricalEncodingAdaptTest(\n         dense_shape=[3, 4])\n     vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)\n \n-    layer = get_layer_class()()\n+    layer = integer_lookup.IntegerLookup()\n     layer.adapt(vocab_dataset)\n     expected_vocabulary = [0, -1, 203, 1729]\n     self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())\n@@ -233,46 +224,11 @@ class CategoricalEncodingAdaptTest(\n     vocab_data = tf.ragged.constant([[203], [1729, 203]])\n     vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)\n \n-    layer = get_layer_class()()\n+    layer = integer_lookup.IntegerLookup()\n     layer.adapt(vocab_dataset)\n     expected_vocabulary = [0, -1, 203, 1729]\n     self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())\n \n-  def test_sparse_int_input(self):\n-    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n-    input_array = tf.SparseTensor(\n-        indices=[[0, 0], [1, 2]],\n-        values=np.array([13, 32], dtype=np.int64),\n-        dense_shape=[3, 4])\n-\n-    expected_indices = [[0, 0], [1, 2]]\n-    expected_values = [5, 1]\n-    expected_dense_shape = [3, 4]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n-    layer = get_layer_class()(max_values=None)\n-    layer.set_vocabulary(vocab_data)\n-    int_data = layer(input_data)\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_data = model.predict(input_array, steps=1)\n-    self.assertAllEqual(expected_indices, output_data.indices)\n-    self.assertAllEqual(expected_values, output_data.values)\n-    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)\n-\n-  def test_ragged_int_input(self):\n-    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n-    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],\n-                                              dtype=np.int64)\n-    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n-    layer = get_layer_class()(max_values=None)\n-    layer.set_vocabulary(vocab_data)\n-    int_data = layer(input_data)\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(input_array)\n-    self.assertAllEqual(expected_output, output_dataset)\n-\n   def test_single_int_generator_dataset(self):\n \n     def word_gen():\n@@ -283,13 +239,13 @@ class CategoricalEncodingAdaptTest(\n                                             tf.TensorShape([]))\n     batched_ds = ds.take(2)\n     input_t = keras.Input(shape=(), dtype=tf.int64)\n-    layer = get_layer_class()(\n+    layer = integer_lookup.IntegerLookup(\n         max_values=10, num_oov_indices=0, mask_value=None, oov_value=None)\n     _ = layer(input_t)\n     layer.adapt(batched_ds)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IntegerLookupOutputTest(keras_parameterized.TestCase,\n                               preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -299,7 +255,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()()\n+    layer = integer_lookup.IntegerLookup()\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -308,7 +264,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n \n   def test_output_shape(self):\n     input_data = keras.Input(shape=(4,), dtype=tf.int64)\n-    layer = get_layer_class()(max_values=2, num_oov_indices=1)\n+    layer = integer_lookup.IntegerLookup(max_values=2, num_oov_indices=1)\n     int_data = layer(input_data)\n     self.assertAllEqual(int_data.shape[1:], input_data.shape[1:])\n \n@@ -318,7 +274,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(max_values=None, mask_value=None)\n+    layer = integer_lookup.IntegerLookup(max_values=None, mask_value=None)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -331,7 +287,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(\n+    layer = integer_lookup.IntegerLookup(\n         vocabulary=vocab_data,\n         max_values=None,\n     )\n@@ -346,7 +302,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(\n+    layer = integer_lookup.IntegerLookup(\n         vocabulary=vocab_data,\n         max_values=None,\n     )\n@@ -361,7 +317,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(invert=True)\n+    layer = integer_lookup.IntegerLookup(invert=True)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -374,8 +330,9 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(vocabulary=vocab_data)\n-    inverse_layer = get_layer_class()(vocabulary=vocab_data, invert=True)\n+    layer = integer_lookup.IntegerLookup(vocabulary=vocab_data)\n+    inverse_layer = integer_lookup.IntegerLookup(\n+        vocabulary=vocab_data, invert=True)\n     int_data = layer(input_data)\n     inverse_data = inverse_layer(int_data)\n     model = keras.Model(inputs=input_data, outputs=inverse_data)\n@@ -388,9 +345,9 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()()\n+    layer = integer_lookup.IntegerLookup()\n     layer.adapt(adapt_data)\n-    inverse_layer = get_layer_class()(\n+    inverse_layer = integer_lookup.IntegerLookup(\n         vocabulary=layer.get_vocabulary(), invert=True)\n     int_data = layer(input_data)\n     inverse_data = inverse_layer(int_data)\n@@ -399,7 +356,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IntegerLookupVocabularyTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -419,7 +376,7 @@ class IntegerLookupVocabularyTest(\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(vocabulary=vocab_data)\n+    layer = integer_lookup.IntegerLookup(vocabulary=vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -428,7 +385,7 @@ class IntegerLookupVocabularyTest(\n   def test_no_vocab(self):\n     with self.assertRaisesRegex(\n         ValueError, \"You must set the layer's vocabulary\"):\n-      layer = get_layer_class()()\n+      layer = integer_lookup.IntegerLookup()\n       layer([[1]])\n \n   def test_binary_output(self):\n@@ -437,7 +394,8 @@ class IntegerLookupVocabularyTest(\n     expected_output = [[0, 1, 1, 1, 0], [1, 1, 0, 0, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(vocabulary=vocab_data, output_mode=\"binary\")\n+    layer = integer_lookup.IntegerLookup(\n+        vocabulary=vocab_data, output_mode=\"binary\")\n     res = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=res)\n     output_data = model.predict(input_array)\n@@ -449,7 +407,8 @@ class IntegerLookupVocabularyTest(\n     expected_output = [[0, 2, 1, 1, 0], [2, 0, 0, 0, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(vocabulary=vocab_data, output_mode=\"count\")\n+    layer = integer_lookup.IntegerLookup(\n+        vocabulary=vocab_data, output_mode=\"count\")\n     res = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=res)\n     output_data = model.predict(input_array)\n@@ -459,7 +418,7 @@ class IntegerLookupVocabularyTest(\n     vocab_data = [2, 3, 4, 5]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(\n+    layer = integer_lookup.IntegerLookup(\n         vocabulary=vocab_data, output_mode=\"binary\", sparse=True)\n     res = layer(input_data)\n     self.assertTrue(res.__class__.__name__, \"SparseKerasTensor\")\n@@ -467,7 +426,7 @@ class IntegerLookupVocabularyTest(\n   def test_get_vocab_returns_int(self):\n     vocab_data = [42, 1138, 725, 1729]\n     expected_vocab = [0, -1, 42, 1138, 725, 1729]\n-    layer = get_layer_class()(vocabulary=vocab_data)\n+    layer = integer_lookup.IntegerLookup(vocabulary=vocab_data)\n     layer_vocab = layer.get_vocabulary()\n     self.assertAllEqual(expected_vocab, layer_vocab)\n     self.assertIsInstance(layer_vocab[0], np.int64)\n@@ -480,7 +439,7 @@ class IntegerLookupVocabularyTest(\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(vocabulary=vocab_path)\n+    layer = integer_lookup.IntegerLookup(vocabulary=vocab_path)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -494,7 +453,7 @@ class IntegerLookupVocabularyTest(\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()()\n+    layer = integer_lookup.IntegerLookup()\n     layer.set_vocabulary(vocab_path)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -504,66 +463,33 @@ class IntegerLookupVocabularyTest(\n   def test_non_unique_vocab_fails(self):\n     vocab_data = [42, 1138, 725, 1729, 1729]\n     with self.assertRaisesRegex(ValueError, \".*repeated term.*1729.*\"):\n-      _ = get_layer_class()(vocabulary=vocab_data)\n+      _ = integer_lookup.IntegerLookup(vocabulary=vocab_data)\n \n   def test_non_unique_vocab_from_file_fails(self):\n     vocab_list = [42, 1138, 725, 1729, 42]\n     vocab_path = self._write_to_temp_file(\"repeat_vocab_file\", vocab_list)\n     with self.assertRaisesRegex(ValueError, \".*repeated term.*42.*\"):\n-      _ = get_layer_class()(vocabulary=vocab_path)\n+      _ = integer_lookup.IntegerLookup(vocabulary=vocab_path)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_eager=True)\n-class IntegerLookupSaveableTest(keras_parameterized.TestCase,\n-                                preprocessing_test_utils.PreprocessingLayerTest\n-                               ):\n-\n-  def test_ops_are_not_added_with_multiple_get_set_weights(self):\n-    vocab_data = [42, 1138, 725, 1729]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(max_values=10)\n-    layer.set_vocabulary(vocab_data)\n-    int_data = layer(input_data)\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    weights = model.get_weights()\n-    model.set_weights(weights)\n-    keras.backend.get_session().graph.finalize()\n-    weights = model.get_weights()\n-    model.set_weights(weights)\n-\n-  def test_layer_saving_with_h5(self):\n-    vocab_data = [42, 1138, 725, 1729]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(max_values=10)\n-    layer.set_vocabulary(vocab_data)\n-    int_data = layer(input_data)\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    path = os.path.join(self.get_temp_dir(), \"model\")\n-    with self.assertRaisesRegex(NotImplementedError,\n-                                \"Save or restore weights that is not.*\"):\n-      save.save_model(model, path, save_format=\"h5\")\n-\n-\n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IntegerLookupErrorTest(keras_parameterized.TestCase,\n                              preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_too_long_vocab_fails_in_single_setting(self):\n     vocab_data = [42, 1138, 725, 1729]\n \n-    layer = get_layer_class()(max_values=4, num_oov_indices=1)\n+    layer = integer_lookup.IntegerLookup(max_values=4, num_oov_indices=1)\n     with self.assertRaisesRegex(ValueError,\n                                 \"vocabulary larger than the maximum vocab.*\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_zero_max_values_fails(self):\n     with self.assertRaisesRegex(ValueError, \".*max_values.*\"):\n-      _ = get_layer_class()(max_values=0, num_oov_indices=1)\n+      _ = integer_lookup.IntegerLookup(max_values=0, num_oov_indices=1)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IntegerLookupSavingTest(keras_parameterized.TestCase,\n                               preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -579,7 +505,7 @@ class IntegerLookupSavingTest(keras_parameterized.TestCase,\n \n     # Build and validate a golden model.\n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(max_values=None, num_oov_indices=1)\n+    layer = integer_lookup.IntegerLookup(max_values=None, num_oov_indices=1)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -597,7 +523,8 @@ class IntegerLookupSavingTest(keras_parameterized.TestCase,\n       keras.backend.clear_session()\n \n     loaded_model = keras.models.load_model(\n-        output_path, custom_objects={\"IntegerLookup\": get_layer_class()})\n+        output_path,\n+        custom_objects={\"IntegerLookup\": integer_lookup.IntegerLookup})\n \n     # Ensure that the loaded model is unique (so that the save/load is real)\n     self.assertIsNot(model, loaded_model)\n\n@@ -1,41 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Keras string lookup preprocessing layer.\"\"\"\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from keras.engine import base_preprocessing_layer\n-from keras.layers.preprocessing import index_lookup_v1\n-from keras.layers.preprocessing import integer_lookup\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export(v1=[\"keras.layers.experimental.preprocessing.IntegerLookup\"])\n-class IntegerLookup(integer_lookup.IntegerLookup, index_lookup_v1.IndexLookup):\n-  \"\"\"Maps integers from a vocabulary to integer indices.\"\"\"\n-\n-  def __init__(self,\n-               max_values=None,\n-               num_oov_indices=1,\n-               mask_value=0,\n-               oov_value=-1,\n-               vocabulary=None,\n-               invert=False,\n-               **kwargs):\n-    super(IntegerLookup, self).__init__(max_values, num_oov_indices, mask_value,\n-                                        oov_value, vocabulary, invert, **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        \"IntegerLookup_V1\").set(True)\n\n@@ -26,7 +26,7 @@ from keras.engine import base_preprocessing_layer\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export('keras.layers.experimental.preprocessing.Normalization', v1=[])\n+@keras_export('keras.layers.experimental.preprocessing.Normalization')\n class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Feature-wise normalization of the data.\n \n@@ -164,8 +164,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     if (self.mean_val is not None and self.variance_val is not None):\n       mean_val = self.mean_val * np.ones(mean_and_var_shape)\n       variance_val = self.variance_val * np.ones(mean_and_var_shape)\n-      self.mean.assign(mean_val)\n-      self.variance.assign(variance_val)\n+      self.set_weights([mean_val, variance_val])\n \n     self.built = True\n \n\n@@ -26,17 +26,9 @@ import keras\n from keras import keras_parameterized\n from keras.distribute.strategy_combinations import all_strategies\n from keras.layers.preprocessing import normalization\n-from keras.layers.preprocessing import normalization_v1\n from keras.layers.preprocessing import preprocessing_test_utils\n \n \n-def get_layer_class():\n-  if tf.executing_eagerly():\n-    return normalization.Normalization\n-  else:\n-    return normalization_v1.Normalization\n-\n-\n def _get_layer_computation_test_cases():\n   test_cases = ({\n       \"adapt_data\": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),\n@@ -104,9 +96,8 @@ def _get_layer_computation_test_cases():\n \n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.times(\n-        tf.__internal__.test.combinations.combine(\n-            distribution=all_strategies,\n-            mode=[\"eager\", \"graph\"]), _get_layer_computation_test_cases()))\n+        tf.__internal__.test.combinations.combine(distribution=all_strategies, mode=[\"eager\"]),\n+        _get_layer_computation_test_cases()))\n class NormalizationTest(keras_parameterized.TestCase,\n                         preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -122,7 +113,7 @@ class NormalizationTest(keras_parameterized.TestCase,\n \n     with distribution.scope():\n       input_data = keras.Input(shape=input_shape)\n-      layer = get_layer_class()(axis=axis)\n+      layer = normalization.Normalization(axis=axis)\n       layer.adapt(adapt_data)\n       output = layer(input_data)\n       model = keras.Model(input_data, output)\n\n@@ -28,18 +28,10 @@ import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers.preprocessing import normalization\n-from keras.layers.preprocessing import normalization_v1\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.utils.generic_utils import CustomObjectScope\n \n \n-def get_layer_class():\n-  if tf.executing_eagerly():\n-    return normalization.Normalization\n-  else:\n-    return normalization_v1.Normalization\n-\n-\n def _get_layer_computation_test_cases():\n   test_cases = ({\n       \"adapt_data\": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),\n@@ -125,8 +117,59 @@ def _get_layer_computation_test_cases():\n class NormalizationTest(keras_parameterized.TestCase,\n                         preprocessing_test_utils.PreprocessingLayerTest):\n \n+  def test_broadcasting_during_direct_setting(self):\n+    layer = normalization.Normalization(axis=-1, mean=[1.0], variance=[2.0])\n+    layer.build((None, 2))\n+    weights = layer.get_weights()\n+    self.assertAllClose([1.0, 1.0], weights[0])\n+    self.assertAllClose([2.0, 2.0], weights[1])\n+\n+  def test_broadcasting_during_direct_setting_with_tensors(self):\n+    layer = normalization.Normalization(\n+        axis=-1,\n+        mean=tf.constant([1.0]),\n+        variance=tf.constant([2.0]))\n+    layer.build((None, 2))\n+    weights = layer.get_weights()\n+    self.assertAllClose([1.0, 1.0], weights[0])\n+    self.assertAllClose([2.0, 2.0], weights[1])\n+\n+  def test_broadcasting_during_direct_setting_with_variables_fails(self):\n+    with self.assertRaisesRegex(ValueError, \"passing a Variable\"):\n+      _ = normalization.Normalization(\n+          axis=-1,\n+          mean=tf.Variable([1.0]),\n+          variance=tf.Variable([2.0]))\n+\n+  @parameterized.parameters(\n+      {\"axis\": 0},\n+      {\"axis\": (-1, 0)},\n+  )\n+  def test_zeros_fail_init(self, axis):\n+    with self.assertRaisesRegex(ValueError,\n+                                \"The argument 'axis' may not be 0.\"):\n+      normalization.Normalization(axis=axis)\n+\n+  @parameterized.parameters(\n+      # Out of bounds\n+      {\"axis\": 3},\n+      {\"axis\": -3},\n+      # In a tuple\n+      {\"axis\": (1, 3)},\n+      {\"axis\": (1, -3)},\n+  )\n+  def test_bad_axis_fail_build(self, axis):\n+    layer = normalization.Normalization(axis=axis)\n+    with self.assertRaisesRegex(ValueError, r\"in the range\"):\n+      layer.build([None, 2, 3])\n+\n+\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+class NormalizationAdaptTest(keras_parameterized.TestCase,\n+                             preprocessing_test_utils.PreprocessingLayerTest):\n+\n   def test_layer_api_compatibility(self):\n-    cls = get_layer_class()\n+    cls = normalization.Normalization\n     with CustomObjectScope({\"Normalization\": cls}):\n       output_data = testing_utils.layer_test(\n           cls,\n@@ -149,8 +192,7 @@ class NormalizationTest(keras_parameterized.TestCase,\n       test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(\n           test_data.shape[0] // 2)\n \n-    cls = get_layer_class()\n-    layer = cls(axis=axis)\n+    layer = normalization.Normalization(axis=axis)\n     layer.adapt(adapt_data)\n \n     input_data = keras.Input(shape=input_shape)\n@@ -164,7 +206,8 @@ class NormalizationTest(keras_parameterized.TestCase,\n     mean = weights[0]\n     var = weights[1]\n \n-    direct_set_layer = cls(axis=axis, mean=mean, variance=var)\n+    direct_set_layer = normalization.Normalization(\n+        axis=axis, mean=mean, variance=var)\n     input_data = keras.Input(shape=input_shape)\n     output = direct_set_layer(input_data)\n     model = keras.Model(input_data, output)\n@@ -172,37 +215,9 @@ class NormalizationTest(keras_parameterized.TestCase,\n     output_data = model.predict(test_data)\n     self.assertAllClose(expected, output_data)\n \n-  def test_broadcasting_during_direct_setting(self):\n-    cls = get_layer_class()\n-    layer = cls(axis=-1, mean=[1.0], variance=[2.0])\n-    layer.build((None, 2))\n-    weights = layer.get_weights()\n-    self.assertAllClose([1.0, 1.0], weights[0])\n-    self.assertAllClose([2.0, 2.0], weights[1])\n-\n-  def test_broadcasting_during_direct_setting_with_tensors(self):\n-    cls = get_layer_class()\n-    layer = cls(\n-        axis=-1,\n-        mean=tf.constant([1.0]),\n-        variance=tf.constant([2.0]))\n-    layer.build((None, 2))\n-    weights = layer.get_weights()\n-    self.assertAllClose([1.0, 1.0], weights[0])\n-    self.assertAllClose([2.0, 2.0], weights[1])\n-\n-  def test_broadcasting_during_direct_setting_with_variables_fails(self):\n-    cls = get_layer_class()\n-    with self.assertRaisesRegex(ValueError, \"passing a Variable\"):\n-      _ = cls(\n-          axis=-1,\n-          mean=tf.Variable([1.0]),\n-          variance=tf.Variable([2.0]))\n-\n   def test_1d_data(self):\n     data = [0, 2, 0, 2]\n-    cls = get_layer_class()\n-    layer = cls(axis=-1)\n+    layer = normalization.Normalization(axis=-1)\n     layer.adapt(data)\n     output = layer(data)\n     self.assertListEqual(output.shape.as_list(), [4, 1])\n@@ -214,37 +229,12 @@ class NormalizationTest(keras_parameterized.TestCase,\n       self.skipTest(\"Only supported in TF2.\")\n \n     data = [0, 2, 0, 2]\n-    cls = get_layer_class()\n-    layer = cls(axis=-1)\n+    layer = normalization.Normalization(axis=-1)\n     layer.adapt(data)\n     output = layer(0.)\n     self.assertListEqual(output.shape.as_list(), [1, 1])\n     self.assertAllClose(output.numpy(), [[-1]])\n \n-  @parameterized.parameters(\n-      {\"axis\": 0},\n-      {\"axis\": (-1, 0)},\n-  )\n-  def test_zeros_fail_init(self, axis):\n-    cls = get_layer_class()\n-    with self.assertRaisesRegex(ValueError,\n-                                \"The argument 'axis' may not be 0.\"):\n-      cls(axis=axis)\n-\n-  @parameterized.parameters(\n-      # Out of bounds\n-      {\"axis\": 3},\n-      {\"axis\": -3},\n-      # In a tuple\n-      {\"axis\": (1, 3)},\n-      {\"axis\": (1, -3)},\n-  )\n-  def test_bad_axis_fail_build(self, axis):\n-    cls = get_layer_class()\n-    layer = cls(axis=axis)\n-    with self.assertRaisesRegex(ValueError, r\"in the range\"):\n-      layer.build([None, 2, 3])\n-\n   @parameterized.parameters(\n       # Results should be identical no matter how the axes are specified (3d).\n       {\"axis\": (1, 2)},\n@@ -253,8 +243,7 @@ class NormalizationTest(keras_parameterized.TestCase,\n       {\"axis\": (-1, 1)},\n   )\n   def test_axis_permutations(self, axis):\n-    cls = get_layer_class()\n-    layer = cls(axis=axis)\n+    layer = normalization.Normalization(axis=axis)\n     # data.shape = [2, 2, 3]\n     data = np.array([[[0., 1., 2.], [0., 2., 6.]],\n                      [[2., 3., 4.], [3., 6., 10.]]])\n@@ -266,8 +255,7 @@ class NormalizationTest(keras_parameterized.TestCase,\n   def test_model_summary_after_layer_adapt(self):\n     data = np.array([[[0., 1., 2.], [0., 2., 6.]],\n                      [[2., 3., 4.], [3., 6., 10.]]])\n-    cls = get_layer_class()\n-    layer = cls(axis=-1)\n+    layer = normalization.Normalization(axis=-1)\n     layer.adapt(data)\n     model = keras.Sequential(\n         [layer,\n@@ -279,19 +267,18 @@ class NormalizationTest(keras_parameterized.TestCase,\n     if not tf.executing_eagerly():\n       self.skipTest(\"`merge_state` only supported in TF2\")\n \n-    cls = get_layer_class()\n     data = np.random.rand(30, 10, 2)\n     ds = tf.data.Dataset.from_tensor_slices(data).batch(2)\n-    norm = cls(axis=(1, 2))\n+    norm = normalization.Normalization(axis=(1, 2))\n     norm.adapt(ds)\n \n     partial_ds_1 = ds.shard(3, 0)\n     partial_ds_2 = ds.shard(3, 1)\n     partial_ds_3 = ds.shard(3, 2)\n \n-    norm_1 = cls(axis=(1, 2))\n-    norm_2 = cls(axis=(1, 2))\n-    norm_3 = cls(axis=(1, 2))\n+    norm_1 = normalization.Normalization(axis=(1, 2))\n+    norm_2 = normalization.Normalization(axis=(1, 2))\n+    norm_3 = normalization.Normalization(axis=(1, 2))\n \n     norm_1.adapt(partial_ds_1)\n     norm_2.adapt(partial_ds_2)\n\n@@ -1,336 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tensorflow V1 version of the Normalization preprocessing layer.\"\"\"\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow.compat.v2 as tf\n-\n-import json\n-\n-import numpy as np\n-from keras import backend as K\n-from keras.engine import base_preprocessing_layer\n-from keras.engine.base_preprocessing_layer_v1 import CombinerPreprocessingLayer\n-from tensorflow.python.util.tf_export import keras_export\n-\n-_COUNT_NAME = 'count'\n-_MEAN_NAME = 'mean'\n-_VARIANCE_NAME = 'variance'\n-\n-\n-def convert_to_ndarray(values):\n-  if isinstance(values, np.ndarray):\n-    return values\n-  elif isinstance(values, tf.Tensor):\n-    return K.get_value(values)\n-  else:\n-    return np.array(values)\n-\n-\n-@keras_export(v1=['keras.layers.experimental.preprocessing.Normalization'])\n-class Normalization(CombinerPreprocessingLayer):\n-  \"\"\"Feature-wise normalization of the data.\n-\n-  This layer will coerce its inputs into a distribution centered around\n-  0 with standard deviation 1. It accomplishes this by precomputing the mean and\n-  variance of the data, and calling (input-mean)/sqrt(var) at runtime.\n-\n-  What happens in `adapt`: Compute mean and variance of the data and store them\n-    as the layer's weights. `adapt` should be called before `fit`, `evaluate`,\n-    or `predict`.\n-\n-  Attributes:\n-      axis: Integer or tuple of integers, the axis or axes that should be\n-        \"kept\". These axes are not be summed over when calculating the\n-        normalization statistics. By default the last axis, the `features` axis\n-        is kept and any `space` or `time` axes are summed. Each element in the\n-        the axes that are kept is normalized independently. If `axis` is set to\n-        'None', the layer will perform scalar normalization (dividing the input\n-        by a single scalar value). The `batch` axis, 0, is always summed over\n-        (`axis=0` is not allowed).\n-  \"\"\"\n-\n-  def __init__(self, axis=-1, **kwargs):\n-    # Standardize `axis` to a tuple.\n-    if axis is None:\n-      axis = ()\n-    elif isinstance(axis, int):\n-      axis = (axis,)\n-    else:\n-      axis = tuple(axis)\n-\n-    mean = kwargs.pop('mean', None)\n-    variance = kwargs.pop('variance', None)\n-\n-    super(Normalization, self).__init__(\n-        combiner=_NormalizingCombiner(axis), **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell('Normalization').set(True)\n-\n-    if 0 in axis:\n-      raise ValueError('The argument \\'axis\\' may not be 0.')\n-\n-    self.axis = axis\n-\n-    if isinstance(mean, tf.Variable):\n-      raise ValueError('Normalization does not support passing a Variable '\n-                       'for the `mean` init arg.')\n-    if isinstance(variance, tf.Variable):\n-      raise ValueError('Normalization does not support passing a Variable '\n-                       'for the `variance` init arg.')\n-    if mean is not None and variance is not None:\n-      mean = convert_to_ndarray(mean)\n-      variance = convert_to_ndarray(variance)\n-    elif mean is not None or variance is not None:\n-      raise ValueError(\n-          'When setting values directly, both `mean` and `variance` '\n-          'must be set. Got mean: {} and variance: {}'.format(mean, variance))\n-\n-    self.mean_val = mean\n-    self.variance_val = variance\n-\n-  def build(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if len(input_shape) == 1:\n-      input_shape = input_shape + [1]\n-\n-    ndim = len(input_shape)\n-\n-    # Sort `self.axis` to avoid transposing `mean_and_var_shape`.\n-    # Negative axes are not sortable until you know the number of dimensions.\n-    original_axis = self.axis\n-    self.axis = tuple(\n-        sorted(self.axis, key=lambda a: a if a >= 0 else ndim + a))\n-\n-    if any(a < 1 - ndim for a in self.axis) or any(\n-        a >= ndim for a in self.axis):\n-      raise ValueError('All `axis` values must be in '\n-                       'the range [1-ndim, ndim-1].\\n'\n-                       'Got:\\n'\n-                       '    ndim: {}\\n'\n-                       '    axis: {}'.format(ndim, original_axis))\n-\n-    self._broadcast_shape = [1 for _ in range(len(input_shape))]\n-    mean_and_var_shape = []\n-    for i in self.axis:\n-      mean_and_var_shape.append(input_shape[i])\n-      self._broadcast_shape[i] = input_shape[i]\n-\n-    # count is not used in this class's call() method, but is used to re-create\n-    # the accumulator during multiple calls to 'adapt'.\n-    # TODO(omalleyt): should mean and variance be set to self.dtype?\n-    self.mean = self._add_state_variable(\n-        name=_MEAN_NAME,\n-        shape=mean_and_var_shape,\n-        dtype=K.floatx(),\n-        initializer=tf.compat.v1.zeros_initializer)\n-    self.variance = self._add_state_variable(\n-        name=_VARIANCE_NAME,\n-        shape=mean_and_var_shape,\n-        dtype=K.floatx(),\n-        initializer=tf.compat.v1.ones_initializer)\n-    self.count = self._add_state_variable(\n-        name=_COUNT_NAME,\n-        shape=(),\n-        dtype=tf.int64,\n-        initializer=tf.compat.v1.zeros_initializer)\n-\n-    super(Normalization, self).build(input_shape)\n-\n-    if (self.mean_val is not None and self.variance_val is not None):\n-      mean_val = self.mean_val * np.ones(mean_and_var_shape)\n-      variance_val = self.variance_val * np.ones(mean_and_var_shape)\n-      self.set_weights([mean_val, variance_val])\n-\n-  def call(self, inputs):\n-    inputs = tf.convert_to_tensor(inputs)\n-    if inputs.shape.rank == 1:\n-      inputs = tf.compat.v1.expand_dims(inputs, 1)\n-    # If the inputs are not floats, cast them to floats. This avoids issues\n-    # with int-float multiplication and division below.\n-    if inputs.dtype != K.floatx():\n-      inputs = tf.cast(inputs, K.floatx())\n-    # We need to reshape the mean and variance data to ensure that Tensorflow\n-    # broadcasts the data correctly.\n-    mean = tf.reshape(self.mean, self._broadcast_shape)\n-    variance = tf.reshape(self.variance, self._broadcast_shape)\n-    return ((inputs - mean) /\n-            tf.maximum(tf.sqrt(variance), K.epsilon()))\n-\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-  def compute_output_signature(self, input_spec):\n-    return input_spec\n-\n-  def get_config(self):\n-    config = {'axis': self.axis}\n-    base_config = super(Normalization, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  def set_weights(self, weights):\n-    \"\"\"Override for set_weights to ensure we can set just mean/var weights.\"\"\"\n-    if len(weights) == 2:\n-      weights.append(np.array(0))\n-    super(Normalization, self).set_weights(weights)\n-\n-\n-class _NormalizingCombiner(base_preprocessing_layer.Combiner):\n-  \"\"\"Combiner for the Normalization preprocessing layer.\n-\n-  This class encapsulates the computations for finding the mean and variance\n-  of a set of data in a stable and numerically correct way. Its associated\n-  accumulator is a namedtuple('count', 'mean', 'variance').\n-\n-  Attributes:\n-    axis: The axis to compute mean and var over.\n-  \"\"\"\n-  COUNT_IDX = 0\n-  MEAN_IDX = 1\n-  VAR_IDX = 2\n-\n-  def __init__(self, axis):\n-    self.axis = axis\n-\n-  def compute(self, values, accumulator=None):\n-    \"\"\"Compute a step in this computation, returning a new accumulator.\"\"\"\n-    values = np.array(values)\n-    if values.ndim == 1:\n-      values = np.expand_dims(values, 1)\n-\n-    # `np.delete` ignores negative indexes, so use a mask to delete items.\n-    axis_mask = np.ones([values.ndim], dtype=bool)\n-    axis_mask[np.array(self.axis, dtype=np.int32)] = False\n-\n-    # This is the shape of all reduced axes (not specified in 'axis').\n-\n-    reduction_counts = np.array(values.shape)[axis_mask]\n-    # We get the number of elements that will be reduced by multiplying all\n-    # values of 'shape' corresponding to the reduced axes.\n-    count = np.prod(reduction_counts, dtype=np.int64)\n-\n-    # We want to reduce across dimensions except those specified in 'axis'\n-    # when using np.mean or np.variance; create the tuple of axes to reduce\n-    # over here.\n-    reduction_axes = tuple(np.arange(values.ndim)[axis_mask])\n-\n-    mean = np.mean(values, axis=reduction_axes, dtype=np.float64)\n-    variance = np.var(values, axis=reduction_axes, dtype=np.float64)\n-\n-    # Create an accumulator with our new data and either return it or combine\n-    # it with the passed accumulator.\n-    if accumulator is None:\n-      return self._create_accumulator(count, mean, variance)\n-    else:\n-      return self.add_data_to_accumulator(count, mean, variance, accumulator)\n-\n-  def add_data_to_accumulator(self, count, mean, variance, accumulator):\n-    \"\"\"Add new data to the totals in an accumulator.\"\"\"\n-    # Combine accumulators and return the result.\n-    combined_count = count + accumulator[self.COUNT_IDX]\n-\n-    # To combine accumulator means, we weight each accumulator's mean by the\n-    # number of elements that were accumulated, and then divide by the\n-    # total number of elements.\n-    combined_mean = (mean * count + accumulator[self.MEAN_IDX] *\n-                     accumulator[self.COUNT_IDX]) / combined_count\n-\n-    # The variance is computed using the lack-of-fit sum of squares\n-    # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n-    accumulator_var_contribution = accumulator[self.COUNT_IDX] * (\n-        accumulator[self.VAR_IDX] +\n-        np.square(accumulator[self.MEAN_IDX] - combined_mean))\n-    data_var_contribution = count * (variance + np.square(mean - combined_mean))\n-    combined_variance = (accumulator_var_contribution +\n-                         data_var_contribution) / combined_count\n-\n-    accumulator[self.COUNT_IDX] = combined_count\n-    accumulator[self.MEAN_IDX] = np.nan_to_num(combined_mean)\n-    accumulator[self.VAR_IDX] = np.nan_to_num(combined_variance)\n-    return accumulator\n-\n-  def merge(self, accumulators):\n-    \"\"\"Merge several accumulators to a single accumulator.\"\"\"\n-    # Combine accumulators and return the result.\n-    combined_count = np.sum(\n-        [accumulator[self.COUNT_IDX] for accumulator in accumulators])\n-\n-    # To combine accumulator means, we weight each accumulator's mean by the\n-    # number of elements that were accumulated, and then divide by the\n-    # total number of elements.\n-    combined_mean = np.add.reduce([\n-        accumulator[self.MEAN_IDX] * accumulator[self.COUNT_IDX]\n-        for accumulator in accumulators\n-    ]) / combined_count\n-\n-    # The variance is computed using the lack-of-fit sum of squares\n-    # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n-    def variance_contribution(accumulator):\n-      return accumulator[self.COUNT_IDX] * (\n-          accumulator[self.VAR_IDX] +\n-          np.square(accumulator[self.MEAN_IDX] - combined_mean))\n-\n-    combined_variance = np.add.reduce([\n-        variance_contribution(accumulator) for accumulator in accumulators\n-    ]) / combined_count\n-\n-    return self._create_accumulator(combined_count, combined_mean,\n-                                    combined_variance)\n-\n-  def extract(self, accumulator):\n-    \"\"\"Convert an accumulator into a dict of output values.\"\"\"\n-    return {\n-        _COUNT_NAME: accumulator[self.COUNT_IDX],\n-        _MEAN_NAME: accumulator[self.MEAN_IDX],\n-        _VARIANCE_NAME: accumulator[self.VAR_IDX]\n-    }\n-\n-  def restore(self, output):\n-    \"\"\"Create an accumulator based on 'output'.\"\"\"\n-    # There is no special internal state here, so we just return the relevant\n-    # internal value.\n-    count = output[_COUNT_NAME]\n-    mean = output[_MEAN_NAME]\n-    var = output[_VARIANCE_NAME]\n-    if (count == 0 and (mean.any() != 0.0 or var.any() != 0.0)):\n-      raise RuntimeError(\n-          'The mean and/or variance of a Normalization preprocessing layer '\n-          \"were set without also setting 'count'. If 'count' is not also set, \"\n-          \" or was set to 0, 'adapt' cannot be called unless the 'reset_state'\"\n-          'arg is True.')\n-    return self._create_accumulator(output[_COUNT_NAME], output[_MEAN_NAME],\n-                                    output[_VARIANCE_NAME])\n-\n-  def serialize(self, accumulator):\n-    \"\"\"Serialize an accumulator for a remote call.\"\"\"\n-    output_dict = {\n-        _COUNT_NAME: accumulator[self.COUNT_IDX].tolist(),\n-        _MEAN_NAME: accumulator[self.MEAN_IDX].tolist(),\n-        _VARIANCE_NAME: accumulator[self.VAR_IDX].tolist()\n-    }\n-    return tf.compat.as_bytes(json.dumps(output_dict))\n-\n-  def deserialize(self, encoded_accumulator):\n-    \"\"\"Deserialize an accumulator received from 'serialize()'.\"\"\"\n-    value_dict = json.loads(tf.compat.as_text(encoded_accumulator))\n-    return self._create_accumulator(\n-        np.array(value_dict[_COUNT_NAME]), np.array(value_dict[_MEAN_NAME]),\n-        np.array(value_dict[_VARIANCE_NAME]))\n-\n-  def _create_accumulator(self, count, mean, variance):\n-    \"\"\"Convert any 'nan' values in the given accumulator to numeric values.\"\"\"\n-    return [count, mean, variance]\n\n@@ -310,15 +310,16 @@ class StringLookup(index_lookup.IndexLookup):\n     vocab = super(StringLookup, self).get_vocabulary()\n     return [tf.compat.as_text(x, self.encoding) for x in vocab]\n \n-  def set_vocabulary(self, vocab, idf_weights=None):\n-    if isinstance(vocab, str):\n+  def set_vocabulary(self, vocabulary, idf_weights=None):\n+    if isinstance(vocabulary, str):\n       if self.output_mode == index_lookup.TFIDF:\n         raise RuntimeError(\"Setting vocabulary directly from a file is not \"\n                            \"supported in TF-IDF mode, since this layer cannot \"\n                            \"read files containing TF-IDF weight data. Please \"\n-                           \"read the file using Python and set the vocab \"\n+                           \"read the file using Python and set the vocabulary \"\n                            \"and weights by passing lists or arrays to the \"\n-                           \"set_vocabulary function's `vocab` and \"\n+                           \"set_vocabulary function's `vocabulary` and \"\n                            \"`idf_weights` args.\")\n-      vocab = table_utils.get_vocabulary_from_file(vocab, self.encoding)\n-    super().set_vocabulary(vocab, idf_weights=idf_weights)\n+      vocabulary = table_utils.get_vocabulary_from_file(vocabulary,\n+                                                        self.encoding)\n+    super().set_vocabulary(vocabulary, idf_weights=idf_weights)\n\n@@ -31,18 +31,9 @@ from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import string_lookup\n-from keras.layers.preprocessing import string_lookup_v1\n-from keras.saving import save\n from keras.utils.generic_utils import CustomObjectScope\n \n \n-def get_layer_class():\n-  if tf.executing_eagerly():\n-    return string_lookup.StringLookup\n-  else:\n-    return string_lookup_v1.StringLookup\n-\n-\n def _get_end_to_end_test_cases():\n   test_cases = (\n       {\n@@ -78,7 +69,7 @@ def _get_end_to_end_test_cases():\n   return crossed_test_cases\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class StringLookupLayerTest(keras_parameterized.TestCase,\n                             preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -86,7 +77,7 @@ class StringLookupLayerTest(keras_parameterized.TestCase,\n   def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,\n                                        use_dataset, expected_output,\n                                        input_dtype):\n-    cls = get_layer_class()\n+    cls = string_lookup.StringLookup\n     expected_output_dtype = tf.int64\n     input_shape = input_data.shape\n \n@@ -121,7 +112,7 @@ class StringLookupLayerTest(keras_parameterized.TestCase,\n     self.assertAllClose(expected_output, output_data)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class StringLookupVocabularyTest(keras_parameterized.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n@@ -142,7 +133,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(vocabulary=vocab_data)\n+    layer = string_lookup.StringLookup(vocabulary=vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_data = model.predict(input_array)\n@@ -155,7 +146,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(vocabulary=vocab_data)\n+    layer = string_lookup.StringLookup(vocabulary=vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_data = model.predict(input_array)\n@@ -164,7 +155,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n   def test_no_vocab(self):\n     with self.assertRaisesRegex(\n         ValueError, \"You must set the layer's vocabulary\"):\n-      layer = get_layer_class()()\n+      layer = string_lookup.StringLookup()\n       layer([[\"a\"]])\n \n   def test_binary_output(self):\n@@ -174,7 +165,8 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     expected_output = [[0, 1, 1, 1, 1], [1, 1, 0, 1, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(vocabulary=vocab_data, output_mode=\"binary\")\n+    layer = string_lookup.StringLookup(\n+        vocabulary=vocab_data, output_mode=\"binary\")\n     res = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=res)\n     output_data = model.predict(input_array)\n@@ -187,7 +179,8 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     expected_output = [[0, 2, 0, 0, 2], [1, 1, 0, 1, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(vocabulary=vocab_data, output_mode=\"count\")\n+    layer = string_lookup.StringLookup(\n+        vocabulary=vocab_data, output_mode=\"count\")\n     res = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=res)\n     output_data = model.predict(input_array)\n@@ -197,7 +190,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = string_lookup.StringLookup(\n         vocabulary=vocab_data, output_mode=\"binary\", sparse=True)\n     res = layer(input_data)\n     self.assertTrue(res.__class__.__name__, \"SparseKerasTensor\")\n@@ -205,13 +198,13 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n   def test_get_vocab_returns_str(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     expected_vocab = [\"\", \"[UNK]\", \"earth\", \"wind\", \"and\", \"fire\"]\n-    layer = get_layer_class()(vocabulary=vocab_data)\n+    layer = string_lookup.StringLookup(vocabulary=vocab_data)\n     layer_vocab = layer.get_vocabulary()\n     self.assertAllEqual(expected_vocab, layer_vocab)\n     self.assertIsInstance(layer_vocab[0], six.text_type)\n \n-    inverse_layer = get_layer_class()(vocabulary=layer.get_vocabulary(),\n-                                      invert=True)\n+    inverse_layer = string_lookup.StringLookup(\n+        vocabulary=layer.get_vocabulary(), invert=True)\n     layer_vocab = inverse_layer.get_vocabulary()\n     self.assertAllEqual(expected_vocab, layer_vocab)\n     self.assertIsInstance(layer_vocab[0], six.text_type)\n@@ -225,7 +218,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(vocabulary=vocab_path)\n+    layer = string_lookup.StringLookup(vocabulary=vocab_path)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_data = model.predict(input_array)\n@@ -240,7 +233,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()()\n+    layer = string_lookup.StringLookup()\n     layer.set_vocabulary(vocab_path)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -250,13 +243,13 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n   def test_non_unique_vocab_fails(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\", \"fire\"]\n     with self.assertRaisesRegex(ValueError, \".*repeated term.*fire.*\"):\n-      _ = get_layer_class()(vocabulary=vocab_data)\n+      _ = string_lookup.StringLookup(vocabulary=vocab_data)\n \n   def test_non_unique_vocab_from_file_fails(self):\n     vocab_list = [\"earth\", \"wind\", \"and\", \"fire\", \"earth\"]\n     vocab_path = self._write_to_temp_file(\"repeat_vocab_file\", vocab_list)\n     with self.assertRaisesRegex(ValueError, \".*repeated term.*earth.*\"):\n-      _ = get_layer_class()(vocabulary=vocab_path)\n+      _ = string_lookup.StringLookup(vocabulary=vocab_path)\n \n   def test_inverse_layer(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n@@ -265,7 +258,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n                                 [\"fire\", \"and\", \"earth\", \"\"]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = get_layer_class()(vocabulary=vocab_data, invert=True)\n+    layer = string_lookup.StringLookup(vocabulary=vocab_data, invert=True)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_data = model.predict(input_array)\n@@ -279,8 +272,9 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n                                 [\"fire\", \"and\", \"earth\", \"[UNK]\"]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(vocabulary=vocab_data)\n-    invert_layer = get_layer_class()(vocabulary=vocab_data, invert=True)\n+    layer = string_lookup.StringLookup(vocabulary=vocab_data)\n+    invert_layer = string_lookup.StringLookup(\n+        vocabulary=vocab_data, invert=True)\n     int_data = layer(input_data)\n     out_data = invert_layer(int_data)\n     model = keras.Model(inputs=input_data, outputs=out_data)\n@@ -295,9 +289,9 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n                                 [\"fire\", \"and\", \"earth\", \"[UNK]\"]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()()\n+    layer = string_lookup.StringLookup()\n     layer.adapt(adapt_data)\n-    invert_layer = get_layer_class()(\n+    invert_layer = string_lookup.StringLookup(\n         vocabulary=layer.get_vocabulary(), invert=True)\n     int_data = layer(input_data)\n     out_data = invert_layer(int_data)\n@@ -313,7 +307,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     expected_output = [[3, 4, 6], [6, 5, 3, 2]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)\n-    layer = get_layer_class()(num_oov_indices=2)\n+    layer = string_lookup.StringLookup(num_oov_indices=2)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -321,37 +315,5 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_output, output_data)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_eager=True)\n-class StringLookupSaveableTest(keras_parameterized.TestCase,\n-                               preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_ops_are_not_added_with_multiple_get_set_weights(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(max_tokens=10)\n-    layer.set_vocabulary(vocab_data)\n-    int_data = layer(input_data)\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    weights = model.get_weights()\n-    model.set_weights(weights)\n-    keras.backend.get_session().graph.finalize()\n-    weights = model.get_weights()\n-    model.set_weights(weights)\n-\n-  def test_layer_saving_with_h5(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(max_tokens=10)\n-    layer.set_vocabulary(vocab_data)\n-    int_data = layer(input_data)\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    path = os.path.join(self.get_temp_dir(), \"model\")\n-    with self.assertRaisesRegex(NotImplementedError,\n-                                \"Save or restore weights that is not.*\"):\n-      save.save_model(model, path, save_format=\"h5\")\n-\n-\n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -1,48 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Keras string lookup preprocessing layer.\"\"\"\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from keras.engine import base_preprocessing_layer\n-from keras.layers.preprocessing import index_lookup_v1\n-from keras.layers.preprocessing import string_lookup\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export(v1=[\"keras.layers.experimental.preprocessing.StringLookup\"])\n-class StringLookup(string_lookup.StringLookup, index_lookup_v1.IndexLookup):\n-  \"\"\"Maps strings from a vocabulary to integer indices.\"\"\"\n-\n-  def __init__(self,\n-               max_tokens=None,\n-               num_oov_indices=1,\n-               mask_token=\"\",\n-               oov_token=\"[UNK]\",\n-               vocabulary=None,\n-               encoding=None,\n-               invert=False,\n-               **kwargs):\n-    super(StringLookup, self).__init__(\n-        max_tokens=max_tokens,\n-        num_oov_indices=num_oov_indices,\n-        mask_token=mask_token,\n-        oov_token=oov_token,\n-        vocabulary=vocabulary,\n-        invert=invert,\n-        **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        \"StringLookup_V1\").set(True)\n\n@@ -225,21 +225,3 @@ def find_repeated_tokens(vocabulary):\n   else:\n     return []\n \n-\n-def assert_same_type(expected_type, values, value_name):\n-  \"\"\"Assert that 'values' is of type 'expected_type'.\"\"\"\n-  if tf.as_dtype(expected_type) != tf.as_dtype(values.dtype):\n-    raise RuntimeError(\"Expected %s type %s, got %s\" %\n-                       (value_name, expected_type, values.dtype))\n-\n-\n-def convert_to_ndarray(x, dtype=None):\n-  \"\"\"Convert 'x' to a numpy array.\"\"\"\n-  array = np.array(x) if isinstance(x, (list, tuple)) else x\n-  if dtype not in (None, tf.string):\n-    # If the dtype is an integer, we do permissive casting. This allows\n-    # users to examine int32 data if the dtype is int64 without trouble.\n-    np_dtype = tf.as_dtype(dtype).as_numpy_dtype\n-    if np.can_cast(array.dtype, np_dtype):\n-      array = array.astype(np_dtype, casting=\"safe\")\n-  return array\n\n@@ -308,13 +308,13 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n     self._output_mode = output_mode\n     self._output_sequence_length = output_sequence_length\n-    vocab_size = 0\n+    vocabulary_size = 0\n     # IndexLookup needs to keep track the current vocab size outside of its\n     # layer weights. We persist it as a hidden part of the config during\n     # serialization.\n-    if \"vocab_size\" in kwargs:\n-      vocab_size = kwargs[\"vocab_size\"]\n-      del kwargs[\"vocab_size\"]\n+    if \"vocabulary_size\" in kwargs:\n+      vocabulary_size = kwargs[\"vocabulary_size\"]\n+      del kwargs[\"vocabulary_size\"]\n \n     super(TextVectorization, self).__init__(\n         combiner=None,\n@@ -327,7 +327,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n         vocabulary=vocabulary,\n         pad_to_max_tokens=pad_to_max_tokens,\n         output_mode=output_mode if output_mode is not None else INT,\n-        vocab_size=vocab_size)\n+        vocabulary_size=vocabulary_size)\n \n   def _get_index_lookup_class(self):\n     return string_lookup.StringLookup\n@@ -410,6 +410,14 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def get_vocabulary(self):\n     return self._index_lookup_layer.get_vocabulary()\n \n+  def vocabulary_size(self):\n+    \"\"\"Gets the current size of the layer's vocabulary.\n+\n+    Returns:\n+      The integer size of the voculary, including optional mask and oov indices.\n+    \"\"\"\n+    return self._index_lookup_layer.vocabulary_size()\n+\n   def get_config(self):\n     # This does not include the 'vocabulary' arg, since if the vocab was passed\n     # at init time it's now stored in variable state - we don't need to\n@@ -422,7 +430,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n         \"output_mode\": self._output_mode,\n         \"output_sequence_length\": self._output_sequence_length,\n         \"pad_to_max_tokens\": self._index_lookup_layer.pad_to_max_tokens,\n-        \"vocab_size\": self._index_lookup_layer.vocab_size(),\n+        \"vocabulary_size\": self._index_lookup_layer.vocabulary_size(),\n     }\n     base_config = super(TextVectorization, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n@@ -434,7 +442,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     # abstraction for ease of saving!) we return 0.\n     return 0\n \n-  def set_vocabulary(self, vocab, idf_weights=None):\n+  def set_vocabulary(self, vocabulary, idf_weights=None):\n     \"\"\"Sets vocabulary (and optionally document frequency) data for this layer.\n \n     This method sets the vocabulary and idf weights for this layer directly,\n@@ -444,7 +452,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     it.\n \n     Args:\n-      vocab: An array of string tokens, or a path to a file containing one\n+      vocabulary: An array of string tokens, or a path to a file containing one\n         token per line.\n       idf_weights: An array of document frequency data with equal length to\n         vocab. Only necessary if the layer output_mode is TFIDF.\n@@ -457,7 +465,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n         if \"pad_to_max_tokens\" is False and the layer itself has already been\n         called.\n     \"\"\"\n-    self._index_lookup_layer.set_vocabulary(vocab, idf_weights=idf_weights)\n+    self._index_lookup_layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n \n   def build(self, input_shape):\n     # We have to use 'and not ==' here, because input_shape[1] !/== 1 can result\n\n@@ -27,20 +27,10 @@ from keras import keras_parameterized\n from keras.distribute.strategy_combinations import all_strategies\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import text_vectorization\n-from keras.layers.preprocessing import text_vectorization_v1\n-\n-\n-def get_layer_class():\n-  if tf.executing_eagerly():\n-    return text_vectorization.TextVectorization\n-  else:\n-    return text_vectorization_v1.TextVectorization\n \n \n @tf.__internal__.distribute.combinations.generate(\n-    tf.__internal__.test.combinations.combine(\n-        distribution=all_strategies,\n-        mode=[\"eager\", \"graph\"]))\n+    tf.__internal__.test.combinations.combine(distribution=all_strategies, mode=[\"eager\"]))\n class TextVectorizationDistributionTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -58,7 +48,7 @@ class TextVectorizationDistributionTest(\n \n     with distribution.scope():\n       input_data = keras.Input(shape=(None,), dtype=tf.string)\n-      layer = get_layer_class()(\n+      layer = text_vectorization.TextVectorization(\n           max_tokens=None,\n           standardize=None,\n           split=None,\n@@ -87,7 +77,7 @@ class TextVectorizationDistributionTest(\n \n     with distribution.scope():\n       input_data = keras.Input(shape=(None,), dtype=tf.string)\n-      layer = get_layer_class()(\n+      layer = text_vectorization.TextVectorization(\n           max_tokens=None,\n           standardize=None,\n           split=None,\n\n@@ -35,17 +35,9 @@ from keras.layers import core\n from keras.layers import embeddings\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import text_vectorization\n-from keras.layers.preprocessing import text_vectorization_v1\n from keras.utils import generic_utils\n \n \n-def get_layer_class():\n-  if tf.executing_eagerly():\n-    return text_vectorization.TextVectorization\n-  else:\n-    return text_vectorization_v1.TextVectorization\n-\n-\n def _get_end_to_end_test_cases():\n   test_cases = (\n       {\n@@ -272,7 +264,7 @@ def _get_end_to_end_test_cases():\n   return crossed_test_cases\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationLayerTest(keras_parameterized.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n@@ -280,7 +272,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n   @parameterized.named_parameters(*_get_end_to_end_test_cases())\n   def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,\n                                        use_dataset, expected_output):\n-    cls = get_layer_class()\n+    cls = text_vectorization.TextVectorization\n     if kwargs.get(\"output_mode\") == text_vectorization.INT:\n       expected_output_dtype = tf.int64\n     else:\n@@ -319,7 +311,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n   def test_list_inputs_1d(self):\n     vocab_data = [\"two two two\", \"two three three\", \"three four four five\"]\n     input_data = [\"two three\", \"four five\"]\n-    layer = get_layer_class()()\n+    layer = text_vectorization.TextVectorization()\n     layer.adapt(vocab_data)\n     out = layer(input_data)\n     if tf.executing_eagerly():\n@@ -333,7 +325,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     vocab_data = tf.constant(\n         [\"two two two\", \"two three three\", \"three four four five\"])\n     input_data = tf.constant([\"two three\", \"four five\"])\n-    layer = get_layer_class()()\n+    layer = text_vectorization.TextVectorization()\n     layer.adapt(vocab_data)\n     out = layer(input_data)\n     if tf.executing_eagerly():\n@@ -347,7 +339,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     vocab_data = [\n         [\"two two two\"], [\"two three three\"], [\"three four four five\"]]\n     input_data = [[\"two three\"], [\"four five\"]]\n-    layer = get_layer_class()()\n+    layer = text_vectorization.TextVectorization()\n     layer.adapt(vocab_data)\n     out = layer(input_data)\n     if tf.executing_eagerly():\n@@ -361,7 +353,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     vocab_data = [\"two two two\", \"two three three\", \"three four four five\"]\n     input_data = [\"two three\", \"four five\"]\n     vocab_ds = tf.data.Dataset.from_tensor_slices(vocab_data)  # unbatched\n-    layer = get_layer_class()()\n+    layer = text_vectorization.TextVectorization()\n     layer.adapt(vocab_ds)\n     out = layer(input_data)\n     if tf.executing_eagerly():\n@@ -389,7 +381,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n   )\n   def test_layer_dimensionality_handling(self, data, expected):\n     vocab = [\"a\", \"b\", \"c\", \"d\"]\n-    vectorization = get_layer_class()(\n+    vectorization = text_vectorization.TextVectorization(\n         max_tokens=None, standardize=None, split=None, pad_to_max_tokens=False)\n     vectorization.set_vocabulary(vocab)\n     output = vectorization(tf.ragged.constant(data))\n@@ -411,7 +403,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n   )\n   def test_layer_dimensionality_handling_with_split(self, data, expected):\n     vocab = [\"a\", \"b\", \"c\", \"d\"]\n-    vectorization = get_layer_class()(\n+    vectorization = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n@@ -421,7 +413,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected, output)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationPreprocessingTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -437,7 +429,7 @@ class TextVectorizationPreprocessingTest(\n \n   def test_summary_before_adapt(self):\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=10,\n         standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,\n         split=None,\n@@ -456,7 +448,7 @@ class TextVectorizationPreprocessingTest(\n                                 [b\"fire\", b\"and\", b\"earth\", b\"michigan\"]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,\n         split=None,\n@@ -474,7 +466,7 @@ class TextVectorizationPreprocessingTest(\n                        [b\"fire\", b\"and\", b\"earth\"]]\n \n     input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,\n         split=None,\n@@ -494,7 +486,7 @@ class TextVectorizationPreprocessingTest(\n \n     custom_standardization = tf.strings.lower\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=custom_standardization,\n         split=None,\n@@ -512,7 +504,7 @@ class TextVectorizationPreprocessingTest(\n                        [b\"fire\", b\"and\", b\"earth\", b\"michigan\"]]\n \n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n@@ -531,7 +523,7 @@ class TextVectorizationPreprocessingTest(\n \n     custom_split = lambda x: tf.strings.split(x, sep=\">\")\n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=custom_split,\n@@ -555,7 +547,7 @@ class TextVectorizationPreprocessingTest(\n     # pyformat: enable\n \n     input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -579,7 +571,7 @@ class TextVectorizationPreprocessingTest(\n     # pyformat: enable\n \n     input_data = keras.Input(shape=(4,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -601,7 +593,7 @@ class TextVectorizationPreprocessingTest(\n     # pyformat: enable\n \n     input_data = keras.Input(shape=(4,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -635,7 +627,7 @@ class TextVectorizationPreprocessingTest(\n                        ]]\n \n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n@@ -648,7 +640,7 @@ class TextVectorizationPreprocessingTest(\n \n   def test_string_splitting_with_non_1d_array_fails(self):\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n@@ -659,7 +651,7 @@ class TextVectorizationPreprocessingTest(\n \n   def test_string_splitting_with_non_1d_raggedarray_fails(self):\n     input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         vocabulary=[\"a\"],\n         max_tokens=None,\n         standardize=None,\n@@ -671,7 +663,7 @@ class TextVectorizationPreprocessingTest(\n \n   def test_standardization_with_invalid_standardize_arg(self):\n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(vocabulary=[\"a\"])\n+    layer = text_vectorization.TextVectorization(vocabulary=[\"a\"])\n     layer._standardize = \"unsupported\"\n     with self.assertRaisesRegex(ValueError,\n                                 \".*is not a supported standardization.*\"):\n@@ -679,7 +671,7 @@ class TextVectorizationPreprocessingTest(\n \n   def test_splitting_with_invalid_split_arg(self):\n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(vocabulary=[\"a\"])\n+    layer = text_vectorization.TextVectorization(vocabulary=[\"a\"])\n     layer._split = \"unsupported\"\n     with self.assertRaisesRegex(ValueError, \".*is not a supported splitting.*\"):\n       _ = layer(input_data)\n@@ -691,7 +683,7 @@ class TextVectorizationPreprocessingTest(\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -711,7 +703,7 @@ class TextVectorizationPreprocessingTest(\n \n     vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_data)\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -731,7 +723,7 @@ class TextVectorizationPreprocessingTest(\n \n     vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_data)\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -751,7 +743,7 @@ class TextVectorizationPreprocessingTest(\n \n     vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_data)\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -764,7 +756,7 @@ class TextVectorizationPreprocessingTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationDistributionTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -778,7 +770,7 @@ class TextVectorizationDistributionTest(\n     strategy = tf.distribute.OneDeviceStrategy(\"/cpu:0\")\n     with strategy.scope():\n       input_data = keras.Input(shape=(None,), dtype=tf.string)\n-      layer = get_layer_class()(\n+      layer = text_vectorization.TextVectorization(\n           max_tokens=None,\n           standardize=None,\n           split=None,\n@@ -791,7 +783,7 @@ class TextVectorizationDistributionTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationOutputTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -803,7 +795,7 @@ class TextVectorizationOutputTest(\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -829,7 +821,7 @@ class TextVectorizationOutputTest(\n \n     # The input shape here is explicitly 1 because we're tokenizing.\n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n@@ -856,7 +848,7 @@ class TextVectorizationOutputTest(\n \n     # The input shape here is explicitly 1 because we're tokenizing.\n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n@@ -883,7 +875,7 @@ class TextVectorizationOutputTest(\n \n     # The input shape here is explicitly 1 because we're tokenizing.\n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n@@ -910,7 +902,7 @@ class TextVectorizationOutputTest(\n \n     # The input shape here is explicitly 1 because we're tokenizing.\n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n@@ -945,7 +937,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=max_tokens,\n         standardize=None,\n         split=None,\n@@ -972,7 +964,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n@@ -999,7 +991,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=max_tokens,\n         standardize=None,\n         split=None,\n@@ -1029,7 +1021,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=max_tokens,\n         standardize=None,\n         split=None,\n@@ -1058,7 +1050,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=max_tokens,\n         standardize=None,\n         split=None,\n@@ -1090,7 +1082,7 @@ class TextVectorizationOutputTest(\n     ]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=5,\n         standardize=None,\n         split=None,\n@@ -1121,7 +1113,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n@@ -1140,7 +1132,7 @@ class TextVectorizationOutputTest(\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -1157,7 +1149,7 @@ class TextVectorizationOutputTest(\n     }\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -1181,7 +1173,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=6,\n         standardize=None,\n         split=None,\n@@ -1208,7 +1200,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n@@ -1239,7 +1231,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=6,\n         standardize=None,\n         split=None,\n@@ -1270,7 +1262,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n@@ -1300,7 +1292,7 @@ class TextVectorizationOutputTest(\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n@@ -1317,15 +1309,13 @@ class TextVectorizationOutputTest(\n   def test_accept_1D_input(self):\n     input_array = np.array([\"earth wind and fire\",\n                             \"fire and earth michigan\"])\n-    layer = get_layer_class()(\n-        standardize=None,\n-        split=None,\n-        output_mode=\"int\")\n+    layer = text_vectorization.TextVectorization(\n+        standardize=None, split=None, output_mode=\"int\")\n     layer.adapt(input_array)\n     _ = layer(input_array)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationModelBuildingTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -1363,7 +1353,7 @@ class TextVectorizationModelBuildingTest(\n                             [\"ohio\", \"and\", \"earth\", \"michigan\"]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n@@ -1389,7 +1379,7 @@ class TextVectorizationModelBuildingTest(\n \n     # The input shape here is explicitly 1 because we're tokenizing.\n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n@@ -1408,32 +1398,7 @@ class TextVectorizationModelBuildingTest(\n     _ = model.predict(input_array)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_eager=True)\n-class TextVectorizationSaveableTest(\n-    keras_parameterized.TestCase,\n-    preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_ops_are_not_added_with_multiple_saves(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n-        max_tokens=10,\n-        standardize=None,\n-        split=None,\n-        output_mode=text_vectorization.COUNT,\n-        pad_to_max_tokens=False)\n-    layer.set_vocabulary(vocab_data)\n-    int_data = layer(input_data)\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    weights = model.get_weights()\n-    model.set_weights(weights)\n-    keras.backend.get_session().graph.finalize()\n-    weights = model.get_weights()\n-    model.set_weights(weights)\n-\n-\n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationErrorTest(keras_parameterized.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n@@ -1441,7 +1406,7 @@ class TextVectorizationErrorTest(keras_parameterized.TestCase,\n   def test_too_long_vocab_fails_in_single_setting(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n \n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=4,\n         standardize=None,\n         split=None,\n@@ -1453,7 +1418,7 @@ class TextVectorizationErrorTest(keras_parameterized.TestCase,\n   def test_setting_vocab_without_idf_weights_fails_in_tfidf_mode(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n \n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=5,\n         standardize=None,\n         split=None,\n@@ -1465,7 +1430,7 @@ class TextVectorizationErrorTest(keras_parameterized.TestCase,\n   def test_idf_weights_length_mismatch_fails(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     idf_weights = [1, 2, 3]\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=5,\n         standardize=None,\n         split=None,\n@@ -1477,7 +1442,7 @@ class TextVectorizationErrorTest(keras_parameterized.TestCase,\n   def test_set_tfidf_in_non_tfidf_fails(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     idf_weights = [1, 2, 3, 4]\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=5,\n         standardize=None,\n         split=None,\n@@ -1488,46 +1453,48 @@ class TextVectorizationErrorTest(keras_parameterized.TestCase,\n \n   def test_zero_max_tokens_fails(self):\n     with self.assertRaisesRegex(ValueError, \"max_tokens.*\"):\n-      _ = get_layer_class()(max_tokens=0)\n+      _ = text_vectorization.TextVectorization(max_tokens=0)\n \n   def test_non_string_dtype_fails(self):\n     with self.assertRaisesRegex(ValueError, \"dtype of string.*\"):\n-      _ = get_layer_class()(dtype=tf.int64)\n+      _ = text_vectorization.TextVectorization(dtype=tf.int64)\n \n   def test_unknown_standardize_arg_fails(self):\n     with self.assertRaisesRegex(ValueError,\n                                 \"standardize arg.*unsupported_value\"):\n-      _ = get_layer_class()(standardize=\"unsupported_value\")\n+      _ = text_vectorization.TextVectorization(standardize=\"unsupported_value\")\n \n   def test_unknown_split_arg_fails(self):\n     with self.assertRaisesRegex(ValueError, \"split arg.*unsupported_value\"):\n-      _ = get_layer_class()(split=\"unsupported_value\")\n+      _ = text_vectorization.TextVectorization(split=\"unsupported_value\")\n \n   def test_unknown_output_mode_arg_fails(self):\n     with self.assertRaisesRegex(ValueError,\n                                 \"output_mode arg.*unsupported_value\"):\n-      _ = get_layer_class()(output_mode=\"unsupported_value\")\n+      _ = text_vectorization.TextVectorization(output_mode=\"unsupported_value\")\n \n   def test_unknown_ngrams_arg_fails(self):\n     with self.assertRaisesRegex(ValueError, \"ngrams.*unsupported_value\"):\n-      _ = get_layer_class()(ngrams=\"unsupported_value\")\n+      _ = text_vectorization.TextVectorization(ngrams=\"unsupported_value\")\n \n   def test_float_ngrams_arg_fails(self):\n     with self.assertRaisesRegex(ValueError, \"ngrams.*2.9\"):\n-      _ = get_layer_class()(ngrams=2.9)\n+      _ = text_vectorization.TextVectorization(ngrams=2.9)\n \n   def test_float_tuple_ngrams_arg_fails(self):\n     with self.assertRaisesRegex(ValueError, \"ngrams.*(1.3, 2.9)\"):\n-      _ = get_layer_class()(ngrams=(1.3, 2.9))\n+      _ = text_vectorization.TextVectorization(ngrams=(1.3, 2.9))\n \n   def test_non_int_output_sequence_length_dtype_fails(self):\n     with self.assertRaisesRegex(ValueError, \"output_sequence_length.*2.0\"):\n-      _ = get_layer_class()(output_mode=\"int\", output_sequence_length=2.0)\n+      _ = text_vectorization.TextVectorization(\n+          output_mode=\"int\", output_sequence_length=2.0)\n \n   def test_non_none_output_sequence_length_fails_if_output_type_not_int(self):\n     with self.assertRaisesRegex(ValueError,\n                                 \"`output_sequence_length` must not be set\"):\n-      _ = get_layer_class()(output_mode=\"count\", output_sequence_length=2)\n+      _ = text_vectorization.TextVectorization(\n+          output_mode=\"count\", output_sequence_length=2)\n \n \n # Custom functions for the custom callable serialization test. Declared here\n@@ -1542,7 +1509,7 @@ def custom_split_fn(x):\n   return tf.strings.split(x, sep=\">\")\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationSavingTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -1560,7 +1527,7 @@ class TextVectorizationSavingTest(\n \n     # Build and validate a golden model.\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -1591,7 +1558,7 @@ class TextVectorizationSavingTest(\n \n     # Build and validate a golden model.\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n@@ -1634,7 +1601,7 @@ class TextVectorizationSavingTest(\n \n     # Build and validate a golden model.\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=5,\n         standardize=None,\n         split=None,\n@@ -1665,7 +1632,7 @@ class TextVectorizationSavingTest(\n                        [b\"\\tfire\", b\"and\\nearth\", b\"michigan\"]]\n \n     input_data = keras.Input(shape=(1,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=None,\n         standardize=custom_standardize_fn,\n         split=custom_split_fn,\n@@ -1682,7 +1649,7 @@ class TextVectorizationSavingTest(\n     self.assertAllEqual(expected_output, new_output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationE2ETest(keras_parameterized.TestCase,\n                                preprocessing_test_utils.PreprocessingLayerTest):\n \n@@ -1702,7 +1669,7 @@ class TextVectorizationE2ETest(keras_parameterized.TestCase,\n     expected_output_shape = [None, max_tokens]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = get_layer_class()(\n+    layer = text_vectorization.TextVectorization(\n         max_tokens=max_tokens,\n         standardize=None,\n         split=None,\n\n@@ -1,95 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tensorflow V1 version of the text vectorization preprocessing layer.\"\"\"\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from keras.engine import base_preprocessing_layer\n-from keras.engine import base_preprocessing_layer_v1\n-from keras.layers.preprocessing import string_lookup_v1\n-from keras.layers.preprocessing import text_vectorization\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export(v1=[\"keras.layers.experimental.preprocessing.TextVectorization\"])\n-class TextVectorization(text_vectorization.TextVectorization,\n-                        base_preprocessing_layer_v1.CombinerPreprocessingLayer):\n-  \"\"\"Text vectorization layer.\n-\n-  This layer has basic options for managing text in a Keras model. It\n-  transforms a batch of strings (one sample = one string) into either a list of\n-  token indices (one sample = 1D tensor of integer token indices) or a dense\n-  representation (one sample = 1D tensor of float values representing data about\n-  the sample's tokens).\n-\n-  The processing of each sample contains the following steps:\n-    1) standardize each sample (usually lowercasing + punctuation stripping)\n-    2) split each sample into substrings (usually words)\n-    3) recombine substrings into tokens (usually ngrams)\n-    4) index tokens (associate a unique int value with each token)\n-    5) transform each sample using this index, either into a vector of ints or\n-       a dense float vector.\n-\n-  Attributes:\n-    max_tokens: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary.\n-    standardize: Optional specification for standardization to apply to the\n-      input text. Values can be None (no standardization),\n-      LOWER_AND_STRIP_PUNCTUATION (lowercase and remove punctuation) or a\n-      Callable.\n-    split: Optional specification for splitting the input text. Values can be\n-      None (no splitting), SPLIT_ON_WHITESPACE (split on ASCII whitespace), or a\n-      Callable.\n-    ngrams: Optional specification for ngrams to create from the possibly-split\n-      input text. Values can be None, an integer or tuple of integers; passing\n-      an integer will create ngrams up to that integer, and passing a tuple of\n-      integers will create ngrams for the specified values in the tuple. Passing\n-      None means that no ngrams will be created.\n-    output_mode: Optional specification for the output of the layer. Values can\n-      be INT, BINARY, COUNT or TFIDF, which control the outputs as follows:\n-        INT: Outputs integer indices, one integer index per split string token.\n-        BINARY: Outputs a single int array per batch, of either vocab_size or\n-          max_tokens size, containing 1s in all elements where the token mapped\n-          to that index exists at least once in the batch item.\n-        COUNT: As BINARY, but the int array contains a count of the number of\n-          times the token at that index appeared in the batch item.\n-        TFIDF: As BINARY, but the TF-IDF algorithm is applied to find the value\n-          in each token slot.\n-    output_sequence_length: Optional length for the output tensor. If set, the\n-      output will be padded or truncated to this value in INT mode.\n-    pad_to_max_tokens: If True, BINARY, COUNT, and TFIDF modes will have their\n-      outputs padded to max_tokens, even if the number of unique tokens in the\n-      vocabulary is less than max_tokens.\n-  \"\"\"\n-\n-  def __init__(self,\n-               max_tokens=None,\n-               standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,\n-               split=text_vectorization.SPLIT_ON_WHITESPACE,\n-               ngrams=None,\n-               output_mode=text_vectorization.INT,\n-               output_sequence_length=None,\n-               pad_to_max_tokens=False,\n-               **kwargs):\n-    super(TextVectorization,\n-          self).__init__(max_tokens, standardize, split, ngrams, output_mode,\n-                         output_sequence_length, pad_to_max_tokens, **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        \"TextVectorization_V1\").set(True)\n-\n-  def _get_index_lookup_class(self):\n-    return string_lookup_v1.StringLookup\n\n@@ -51,30 +51,22 @@ from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import discretization\n from keras.layers.preprocessing import hashing\n from keras.layers.preprocessing import image_preprocessing\n-from keras.layers.preprocessing import integer_lookup as preprocessing_integer_lookup\n-from keras.layers.preprocessing import integer_lookup_v1 as preprocessing_integer_lookup_v1\n+from keras.layers.preprocessing import integer_lookup\n from keras.layers.preprocessing import normalization as preprocessing_normalization\n-from keras.layers.preprocessing import normalization_v1 as preprocessing_normalization_v1\n-from keras.layers.preprocessing import string_lookup as preprocessing_string_lookup\n-from keras.layers.preprocessing import string_lookup_v1 as preprocessing_string_lookup_v1\n-from keras.layers.preprocessing import text_vectorization as preprocessing_text_vectorization\n-from keras.layers.preprocessing import text_vectorization_v1 as preprocessing_text_vectorization_v1\n+from keras.layers.preprocessing import string_lookup\n+from keras.layers.preprocessing import text_vectorization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect as inspect\n from tensorflow.python.util.tf_export import keras_export\n \n-\n ALL_MODULES = (base_layer, input_layer, advanced_activations, convolutional,\n                convolutional_recurrent, core, cudnn_recurrent, dense_attention,\n                embeddings, einsum_dense, local, merge, noise, normalization,\n-               pooling, image_preprocessing, preprocessing_integer_lookup_v1,\n-               preprocessing_normalization_v1, preprocessing_string_lookup_v1,\n-               preprocessing_text_vectorization_v1, recurrent, wrappers,\n-               hashing, category_crossing, category_encoding, discretization,\n-               multi_head_attention)\n-ALL_V2_MODULES = (rnn_cell_wrapper_v2, normalization_v2, recurrent_v2,\n-                  preprocessing_integer_lookup, preprocessing_normalization,\n-                  preprocessing_string_lookup, preprocessing_text_vectorization)\n+               pooling, image_preprocessing, recurrent, wrappers, hashing,\n+               category_crossing, category_encoding, discretization,\n+               multi_head_attention, integer_lookup,\n+               preprocessing_normalization, string_lookup, text_vectorization)\n+ALL_V2_MODULES = (rnn_cell_wrapper_v2, normalization_v2, recurrent_v2)\n # ALL_OBJECTS is meant to be a global mutable. Hence we need to make it\n # thread-local to avoid concurrent mutations.\n LOCAL = threading.local()\n\n@@ -571,7 +571,6 @@ class BinaryCrossentropy(LossFunctionWrapper):\n       from_logits: Whether to interpret `y_pred` as a tensor of\n         [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n           assume that `y_pred` contains probabilities (i.e., values in [0, 1]).\n-          **Note - Using from_logits=True may be more numerically stable.\n       label_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When > 0,\n         we compute the loss between the predicted labels and a smoothed version\n         of the true labels, where the smoothing squeezes the labels towards 0.5.\n@@ -651,7 +650,6 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n     Args:\n       from_logits: Whether `y_pred` is expected to be a logits tensor. By\n         default, we assume that `y_pred` encodes a probability distribution.\n-        **Note - Using from_logits=True is more numerically stable.**\n       label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n         meaning the confidence on label values are relaxed. For example, if\n         `0.1`, use `0.1 / num_classes` for non-target labels and \n@@ -732,7 +730,6 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     Args:\n       from_logits: Whether `y_pred` is expected to be a logits tensor. By\n         default, we assume that `y_pred` encodes a probability distribution.\n-        **Note - Using from_logits=True may be more numerically stable.\n       reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n         loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n         option will be determined by the usage context. For almost all cases\n@@ -1228,7 +1225,7 @@ def _ragged_tensor_apply_loss(loss_fn, y_true, y_pred):\n        without loss of information.\n \n     Args:\n-      rt: RaggedTensor\n+      rt: RaggedTensor.\n     \"\"\"\n     return tf.reduce_all([\n         tf.equal(\n@@ -2026,7 +2023,7 @@ def get(identifier):\n   Args:\n     identifier: A loss identifier. One of None or string name of a loss\n       function/class or loss configuration dictionary or a loss function or a\n-      loss class instance\n+      loss class instance.\n \n   Returns:\n     A Keras loss as a `function`/ `Loss` class instance.\n\n@@ -20,9 +20,8 @@ from __future__ import print_function\n import tensorflow.compat.v2 as tf\n \n import threading\n-\n-from tensorflow.python.distribute import distribute_utils\n from tensorflow.python.distribute import ps_values as ps_distribute_values\n+from keras.distribute import distributed_training_utils\n \n \n # _autocast_dtype.dtype is the dtype AutoCastVariables should be cast to, or\n@@ -509,7 +508,7 @@ def create_autocast_variable(variable):\n   Returns:\n     An AutoCastVariable that wraps the variable.\n   \"\"\"\n-  if (not distribute_utils.is_distributed_variable(variable) and\n+  if ((not distributed_training_utils.is_distributed_variable(variable)) and\n       not isinstance(variable, ps_distribute_values.AggregatingVariable)):\n     return AutoCastVariable(variable)\n \n\n@@ -28,7 +28,6 @@ from keras.saving.saved_model import load_context\n from keras.saving.saved_model import save as saved_model_save\n from keras.utils import generic_utils\n from keras.utils.io_utils import path_to_string\n-from tensorflow.python.saved_model import loader_impl\n from tensorflow.python.util import keras_deps\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -210,7 +209,6 @@ def load_model(filepath, custom_objects=None, compile=True, options=None):  # py\n \n         filepath = path_to_string(filepath)\n         if isinstance(filepath, six.string_types):\n-          loader_impl.parse_saved_model(filepath)\n           return saved_model_load.load(filepath, compile, options)\n \n   raise IOError(\n\n@@ -33,7 +33,6 @@ from keras.utils import mode_keys\n from keras.utils.generic_utils import LazyLoader\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.saved_model import builder as saved_model_builder\n-from tensorflow.python.saved_model import constants\n from tensorflow.python.training.tracking import graph_view\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -53,6 +52,10 @@ sequential = LazyLoader(\n # pylint:enable=g-inconsistent-quotes\n \n \n+# File name for json format of SavedModel.\n+SAVED_MODEL_FILENAME_JSON = 'saved_model.json'\n+\n+\n @keras_export(v1=['keras.experimental.export_saved_model'])\n def export_saved_model(model,\n                        saved_model_path,\n@@ -143,7 +146,7 @@ def _export_model_json(model, saved_model_path):\n   model_json = model.to_json()\n   model_json_filepath = os.path.join(\n       _get_or_create_assets_dir(saved_model_path),\n-      tf.compat.as_text(constants.SAVED_MODEL_FILENAME_JSON))\n+      tf.compat.as_text(SAVED_MODEL_FILENAME_JSON))\n   with tf.io.gfile.GFile(model_json_filepath, 'w') as f:\n     f.write(model_json)\n \n@@ -408,7 +411,7 @@ def load_from_saved_model(saved_model_path, custom_objects=None):\n   model_json_filepath = os.path.join(\n       tf.compat.as_bytes(saved_model_path),\n       tf.compat.as_bytes(tf.saved_model.ASSETS_DIRECTORY),\n-      tf.compat.as_bytes(constants.SAVED_MODEL_FILENAME_JSON))\n+      tf.compat.as_bytes(SAVED_MODEL_FILENAME_JSON))\n   with tf.io.gfile.GFile(model_json_filepath, 'r') as f:\n     model_json = f.read()\n   model = model_config.model_from_json(\n\n@@ -304,6 +304,18 @@ def _makedirs_exist_ok(datadir):\n     os.makedirs(datadir, exist_ok=True)  # pylint: disable=unexpected-keyword-arg\n \n \n+def _resolve_hasher(algorithm, file_hash=None):\n+  \"\"\"Returns hash algorithm as hashlib function.\"\"\"\n+  if algorithm == 'sha256':\n+    return hashlib.sha256()\n+\n+  if algorithm == 'auto' and file_hash is not None and len(file_hash) == 64:\n+    return hashlib.sha256()\n+\n+  # This is used only for legacy purposes.\n+  return hashlib.md5()\n+\n+\n def _hash_file(fpath, algorithm='sha256', chunk_size=65535):\n   \"\"\"Calculates a file sha256 or md5 hash.\n \n@@ -323,10 +335,10 @@ def _hash_file(fpath, algorithm='sha256', chunk_size=65535):\n   Returns:\n       The file hash\n   \"\"\"\n-  if (algorithm == 'sha256') or (algorithm == 'auto' and len(hash) == 64):\n-    hasher = hashlib.sha256()\n+  if isinstance(algorithm, str):\n+    hasher = _resolve_hasher(algorithm)\n   else:\n-    hasher = hashlib.md5()\n+    hasher = algorithm\n \n   with open(fpath, 'rb') as fpath_file:\n     for chunk in iter(lambda: fpath_file.read(chunk_size), b''):\n@@ -349,10 +361,7 @@ def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n   Returns:\n       Whether the file is valid\n   \"\"\"\n-  if (algorithm == 'sha256') or (algorithm == 'auto' and len(file_hash) == 64):\n-    hasher = 'sha256'\n-  else:\n-    hasher = 'md5'\n+  hasher = _resolve_hasher(algorithm, file_hash)\n \n   if str(_hash_file(fpath, hasher, chunk_size)) == str(file_hash):\n     return True\n\n@@ -47,6 +47,31 @@ class DatasetCreatorTest(tf.test.TestCase):\n         next(iter(got)),\n         next(iter(tf.data.Dataset.from_tensor_slices([1, 1]))))\n \n+  def _get_dataset_fn(self):\n+\n+    def dataset_fn(input_context):\n+      global_batch_size = 64\n+      batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n+      dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat()\n+      dataset = dataset.shard(input_context.num_input_pipelines,\n+                              input_context.input_pipeline_id)\n+      dataset = dataset.batch(batch_size)\n+      dataset = dataset.prefetch(2)\n+      return dataset\n+\n+    return dataset_fn\n+\n+  def test_dataset_creator_model_fit_without_strategy(self):\n+    model = sequential.Sequential([core_layers.Dense(10)])\n+    model.compile(gradient_descent.SGD(), loss=\"mse\")\n+\n+    history = model.fit(\n+        dataset_creator.DatasetCreator(self._get_dataset_fn()),\n+        epochs=10,\n+        steps_per_epoch=10,\n+        verbose=0)\n+    self.assertLen(history.history[\"loss\"], 10)\n+\n   def test_dataset_creator_usage_in_parameter_server_model_fit(self):\n     cluster_def = multi_worker_test_base.create_in_process_cluster(\n         num_workers=2, num_ps=1, rpc_layer=\"grpc\")\n@@ -59,18 +84,8 @@ class DatasetCreatorTest(tf.test.TestCase):\n       model = sequential.Sequential([core_layers.Dense(10)])\n     model.compile(gradient_descent.SGD(), loss=\"mse\")\n \n-    def dataset_fn(input_context):\n-      global_batch_size = 64\n-      batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n-      dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat()\n-      dataset = dataset.shard(input_context.num_input_pipelines,\n-                              input_context.input_pipeline_id)\n-      dataset = dataset.batch(batch_size)\n-      dataset = dataset.prefetch(2)\n-      return dataset\n-\n     history = model.fit(\n-        dataset_creator.DatasetCreator(dataset_fn),\n+        dataset_creator.DatasetCreator(self._get_dataset_fn()),\n         epochs=10,\n         steps_per_epoch=10,\n         verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
