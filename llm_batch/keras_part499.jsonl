{"custom_id": "keras#11d15ba35f78a3075626e60fe344fe00d62759e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 8317 | Contributors (this commit): 22 | Commits (past 90d): 5 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -337,7 +337,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     @property\n     def _map_fn(self):\n         if self.auto_vectorize:\n-            return tf.vectorized_map\n+            return lambda fn, x: tf.vectorized_map(fn, x, warn=False)\n         else:\n             return tf.map_fn\n \n\n@@ -457,10 +457,10 @@ def _update_confusion_matrix_variables_optimized(\n             )\n \n         tp_bucket_v = tf.vectorized_map(\n-            gather_bucket, (true_labels, bucket_indices)\n+            gather_bucket, (true_labels, bucket_indices), warn=False\n         )\n         fp_bucket_v = tf.vectorized_map(\n-            gather_bucket, (false_labels, bucket_indices)\n+            gather_bucket, (false_labels, bucket_indices), warn=False\n         )\n         tp = tf.transpose(tf.cumsum(tp_bucket_v, reverse=True, axis=1))\n         fp = tf.transpose(tf.cumsum(fp_bucket_v, reverse=True, axis=1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#727f1f3106312ca53b5c09f77087dafaaa25da0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 573 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -71,13 +71,6 @@ class AdamW(optimizer.Optimizer):\n \n     Notes:\n \n-    The default value of 1e-7 for epsilon might not be a good default in\n-    general. For example, when training an Inception network on ImageNet a\n-    current good choice is 1.0 or 0.1. Note that since Adam uses the\n-    formulation just before Section 2.1 of the Kingma and Ba paper rather than\n-    the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n-    hat\" in the paper.\n-\n     The sparse implementation of this algorithm (used when the gradient is an\n     IndexedSlices object, typically because of `tf.gather` or an embedding\n     lookup in the forward pass) does apply momentum to variable slices even if\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#968c301dfb783dc1dfb6f0679a7782f7a9254a4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 608 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -19,8 +19,18 @@ import inspect as _inspect\n \n import tensorflow.compat.v2 as tf\n \n+if hasattr(_inspect, \"ArgSpec\"):\n     ArgSpec = _inspect.ArgSpec\n-\n+else:\n+    ArgSpec = collections.namedtuple(\n+        \"ArgSpec\",\n+        [\n+            \"args\",\n+            \"varargs\",\n+            \"keywords\",\n+            \"defaults\",\n+        ],\n+    )\n \n if hasattr(_inspect, \"FullArgSpec\"):\n     FullArgSpec = _inspect.FullArgSpec\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9a7a60802a7fc8c2dbb1e9ce69f62822dbd6055", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 29 | Churn Cumulative: 45 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -384,7 +384,7 @@ def tracing_scope():\n     finally:\n         # Run traces from the queue.\n         while _thread_local_data.trace_queue:\n-            fn, args, kwargs, training = _thread_local_data.trace_queue.pop()\n+            fn, args, kwargs, training = _thread_local_data.trace_queue.pop(0)\n             if training is not None:\n                 with backend.deprecated_internal_learning_phase_scope(training):\n                     fn.get_concrete_function(*args, **kwargs)\n\n@@ -1257,6 +1257,33 @@ class TestSavedModelFormat(tf.test.TestCase):\n         output = loaded(tf.random.uniform([1, 3]), training=True)\n         self.assertAllEqual([1, 3], output.shape)\n \n+    def test_random_generator_with_tracing(self):\n+        # This test is to ensure we trace the training = True function first,\n+        # otherwise tf.function will raise error about creating variables in the\n+        # non-first call.\n+        class LayerWithDropout(keras.layers.Layer):\n+            def __init__(self, dropout_rate):\n+                super().__init__()\n+                self.dropout_rate = dropout_rate\n+                self.dropout_layer = keras.layers.Dropout(self.dropout_rate)\n+\n+            def call(self, inputs, training=None):\n+                if not training:\n+                    return inputs\n+                else:\n+                    return self.dropout_layer(inputs, training=training)\n+\n+        root = keras.models.Sequential(\n+            [keras.layers.Input(shape=(3,)), LayerWithDropout(0.1)]\n+        )\n+        saved_model_dir = self._save_model_dir()\n+        root.save(saved_model_dir, save_format=\"tf\")\n+\n+        loaded = keras_load.load(saved_model_dir)\n+\n+        output = loaded(tf.random.uniform([1, 3]), training=True)\n+        self.assertAllEqual([1, 3], output.shape)\n+\n \n class TestLayerCallTracing(tf.test.TestCase, parameterized.TestCase):\n     def test_functions_have_same_trace(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a08ac801b7bc5dfcdb6599259ddaa1f33c1d7187", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 4243 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -235,17 +235,17 @@ class Layer(base_layer.Layer):\n         # Manage initial weight values if passed.\n         self._initial_weights = kwargs.get(\"weights\", None)\n \n-        # Whether the layer will track any layers that is set as attribute on\n+        # Whether the layer will track any layers that are set as attribute on\n         # itself as sub-layers, the weights from the sub-layers will be included\n         # in the parent layer's variables() as well.  Default to True, which\n         # means auto tracking is turned on. Certain subclass might want to turn\n-        # it off, like Sequential model.\n+        # it off, like the Sequential model.\n         self._auto_track_sub_layers = True\n \n         # Mark this layer as having been originally built as a tf1 layer/model\n         self._originally_built_as_v1 = True\n \n-        # For backwards compat reasons, most built-in layers do not guarantee\n+        # For backward compat reasons, most built-in layers do not guarantee\n         # That they will 100% preserve the structure of input args when saving\n         # / loading configs. E.g. they may un-nest an arg that is\n         # a list with one element.\n@@ -342,7 +342,7 @@ class Layer(base_layer.Layer):\n           constraint: Constraint instance (callable).\n           partitioner: Partitioner to be passed to the `Trackable` API.\n           use_resource: Whether to use `ResourceVariable`.\n-          synchronization: Indicates when a distributed a variable will be\n+          synchronization: Indicates when a distributed variable will be\n             aggregated. Accepted values are constants defined in the class\n             `tf.VariableSynchronization`. By default the synchronization is set\n             to `AUTO` and the current `DistributionStrategy` chooses when to\n@@ -407,7 +407,7 @@ class Layer(base_layer.Layer):\n                     \"synchronization=VariableSynchronization.ON_READ.\"\n                 )\n             else:\n-                # Set trainable to be false when variable is to be synced on\n+                # Set trainable to be false when the variable is to be synced on\n                 # read.\n                 trainable = False\n         elif trainable is None:\n@@ -739,7 +739,7 @@ class Layer(base_layer.Layer):\n             inputs = tf.nest.map_structure(_convert_non_tensor, inputs)\n             input_list = tf.nest.flatten(inputs)\n \n-        # Handle `mask` propagation from previous layer to current layer. Masks\n+        # Handle `mask` propagation from the previous layer to the current layer. Masks\n         # can be propagated explicitly via the `mask` argument, or implicitly\n         # via setting the `_keras_mask` attribute on the inputs to a Layer.\n         # Masks passed explicitly take priority.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a105aadbdc6fde2c2529280c4789864adbb81c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 170 | Lines Deleted: 217 | Files Changed: 31 | Hunks: 63 | Methods Changed: 22 | Complexity Δ (Sum/Max): 225/67 | Churn Δ: 387 | Churn Cumulative: 56121 | Contributors (this commit): 230 | Commits (past 90d): 145 | Contributors (cumulative): 383 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,7 +31,7 @@ import tensorflow.compat.v2 as tf\n from keras import backend\n from keras.distribute import distributed_file_utils\n from keras.distribute import worker_training_state\n-from keras.optimizers import optimizer_experimental\n+from keras.optimizers import optimizer\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.utils import generic_utils\n from keras.utils import io_utils\n@@ -2828,7 +2828,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         self._is_tracing = False\n \n     def _collect_learning_rate(self, logs):\n-        if isinstance(self.model.optimizer, optimizer_experimental.Optimizer):\n+        if isinstance(self.model.optimizer, optimizer.Optimizer):\n             lr_schedule = getattr(self.model.optimizer, \"_learning_rate\", None)\n         else:\n             lr_schedule = getattr(self.model.optimizer, \"lr\", None)\n\n@@ -37,9 +37,7 @@ from keras.distribute.strategy_combinations import strategies_minus_tpu\n from keras.distribute.strategy_combinations import tpu_strategies\n from keras.engine import base_layer_utils\n from keras.mixed_precision import policy\n-from keras.optimizers.optimizer_experimental import (\n-    optimizer as optimizer_experimental,\n-)\n+from keras.optimizers import optimizer as optimizer_base\n from keras.optimizers.optimizer_v2 import (\n     gradient_descent as gradient_descent_keras,\n )\n@@ -3041,7 +3039,7 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n         with distribution.scope():\n             model = create_model()\n             model.load_weights(temp_dir)\n-            if isinstance(model.optimizer, optimizer_experimental.Optimizer):\n+            if isinstance(model.optimizer, optimizer_base.Optimizer):\n                 model.optimizer.build(model.trainable_variables)\n             self.assertNotEmpty(model.optimizer.variables())\n             self.assertTrue(\n@@ -3054,7 +3052,7 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n             model = create_model()\n         # create/restore slot variables outside of scope is fine.\n         model.load_weights(temp_dir)\n-        if isinstance(model.optimizer, optimizer_experimental.Optimizer):\n+        if isinstance(model.optimizer, optimizer_base.Optimizer):\n             # Experimental optimizer has to restore variables in scope.\n             return\n         self.assertNotEmpty(model.optimizer.variables())\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import adam as adam_experimental\n+from keras.optimizers import adam as adam_experimental\n from keras.optimizers.optimizer_v2 import adadelta as adadelta_keras_v2\n from keras.optimizers.optimizer_v2 import adagrad as adagrad_keras_v2\n from keras.optimizers.optimizer_v2 import adam as adam_keras_v2\n\n@@ -14,16 +14,17 @@\n # ==============================================================================\n \"\"\"DTensor specific Keras optimizers.\"\"\"\n \n+\n import tensorflow.compat.v2 as tf\n \n from keras.dtensor import dtensor_api as dtensor\n-from keras.optimizers.optimizer_experimental import adadelta\n-from keras.optimizers.optimizer_experimental import adagrad\n-from keras.optimizers.optimizer_experimental import adam\n-from keras.optimizers.optimizer_experimental import adamw\n-from keras.optimizers.optimizer_experimental import optimizer as optimizer_lib\n-from keras.optimizers.optimizer_experimental import rmsprop\n-from keras.optimizers.optimizer_experimental import sgd\n+from keras.optimizers import adadelta\n+from keras.optimizers import adagrad\n+from keras.optimizers import adam\n+from keras.optimizers import adamw\n+from keras.optimizers import optimizer as optimizer_lib\n+from keras.optimizers import rmsprop\n+from keras.optimizers import sgd\n from keras.optimizers.schedules import learning_rate_schedule\n \n # isort: off\n@@ -36,7 +37,6 @@ class Optimizer(optimizer_lib._BaseOptimizer):\n \n     The major changes for this class is that all the variable init logic will be\n     mesh/layout aware.\n-\n     \"\"\"\n \n     # Note that we didn't subclass optimizer_lib.Optimizer since it contains the\n@@ -49,8 +49,8 @@ class Optimizer(optimizer_lib._BaseOptimizer):\n         Args:\n           name: String. The name of the optimizer, which will appear in all the\n             state variables created by this optimizer.\n-          mesh: dtensor.Mesh. The optional Mesh which will be used to create\n-            the states. Note that usually the state variable will use the layout\n+          mesh: dtensor.Mesh. The optional Mesh which will be used to create the\n+            states. Note that usually the state variable will use the layout\n             from the corresponding model variables. This mesh only used for\n             global variables like globle steps, learning rate, etc.\n         \"\"\"\n\n@@ -34,10 +34,8 @@ from keras.engine import data_adapter\n from keras.engine import input_layer as input_layer_module\n from keras.engine import training_utils\n from keras.mixed_precision import loss_scale_optimizer as lso\n+from keras.optimizers import optimizer\n from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_experimental import (\n-    optimizer as optimizer_experimental,\n-)\n from keras.saving import pickle_utils\n from keras.saving import saving_api\n from keras.saving.experimental import saving_lib\n@@ -1746,10 +1744,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 if self.stop_training:\n                     break\n \n-            if (\n-                isinstance(self.optimizer, optimizer_experimental.Optimizer)\n-                and epochs > 0\n-            ):\n+            if isinstance(self.optimizer, optimizer.Optimizer) and epochs > 0:\n                 self.optimizer.finalize_variable_values(\n                     self.trainable_variables\n                 )\n\n@@ -37,8 +37,8 @@ from keras.engine import training_utils_v1\n from keras.layers.preprocessing import string_lookup\n from keras.mixed_precision import policy\n from keras.optimizers import optimizer_v2\n-from keras.optimizers.optimizer_experimental import rmsprop\n-from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n+from keras.optimizers import rmsprop\n+from keras.optimizers import sgd as sgd_experimental\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import data_utils\n\n@@ -18,9 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n from keras import optimizers\n-from keras.optimizers.optimizer_experimental import (\n-    optimizer as optimizer_experimental,\n-)\n+from keras.optimizers import optimizer\n from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n from keras.saving.legacy import serialization\n@@ -332,7 +330,7 @@ class LossScaleOptimizerMetaclass(type):\n             )\n         if isinstance(inner_optimizer, optimizer_v2.OptimizerV2):\n             return LossScaleOptimizer(inner_optimizer, *args, **kwargs)\n-        elif isinstance(inner_optimizer, optimizer_experimental.Optimizer):\n+        elif isinstance(inner_optimizer, optimizer.Optimizer):\n             return LossScaleOptimizerV3(inner_optimizer, *args, **kwargs)\n \n         # Raise TypeError because inner_optimizer is not an optimizer\n@@ -593,12 +591,12 @@ class LossScaleOptimizer(\n         dynamic_growth_steps=None,\n     ):\n         if not isinstance(inner_optimizer, optimizer_v2.OptimizerV2):\n-            if isinstance(inner_optimizer, optimizer_experimental.Optimizer):\n+            if isinstance(inner_optimizer, optimizer.Optimizer):\n                 # Give better error message if the new experimental optimizer is\n                 # passed.\n                 raise TypeError(\n                     \"You passed an instance of the new experimental \"\n-                    \"optimizer, `optimizer_experimental.Optimizer`, \"\n+                    \"optimizer, `optimizer.Optimizer`, \"\n                     \"to LossScaleOptimizer, but \"\n                     \"only the classic optimizers subclassing from \"\n                     \"`tf.keras.optimizers.Optimizer` can be passed. Please \"\n@@ -1076,7 +1074,7 @@ class LossScaleOptimizer(\n \n class LossScaleOptimizerV3(\n     tf.__internal__.tracking.DelegatingTrackableMixin,\n-    optimizer_experimental.Optimizer,\n+    optimizer.Optimizer,\n     BaseLossScaleOptimizer,\n ):\n     \"\"\"An optimizer that applies loss scaling to prevent numeric underflow.\n@@ -1103,7 +1101,7 @@ class LossScaleOptimizerV3(\n         initial_scale=None,\n         dynamic_growth_steps=None,\n     ):\n-        if not isinstance(inner_optimizer, optimizer_experimental.Optimizer):\n+        if not isinstance(inner_optimizer, optimizer.Optimizer):\n             if isinstance(inner_optimizer, optimizer_v2.OptimizerV2):\n                 # Give better error message if the OptimizerV2 class is passed\n                 # instead of the new experimental optimizer.\n\n@@ -24,11 +24,9 @@ from absl.testing import parameterized\n from keras import optimizers\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import test_util as mp_test_util\n-from keras.optimizers.optimizer_experimental import adam as adam_experimental\n-from keras.optimizers.optimizer_experimental import (\n-    optimizer as optimizer_experimental,\n-)\n-from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n+from keras.optimizers import adam as adam_experimental\n+from keras.optimizers import optimizer as optimizer_experimental\n+from keras.optimizers import sgd as sgd_experimental\n from keras.optimizers.optimizer_v2 import adam\n from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.optimizers.optimizer_v2 import optimizer_v2\n\n@@ -7,7 +7,7 @@ from absl.testing import parameterized\n \n import keras\n from keras.models import sharpness_aware_minimization\n-from keras.optimizers.optimizer_experimental import adam\n+from keras.optimizers import adam\n from keras.testing_infra import test_utils\n \n ds_combinations = tf.__internal__.distribute.combinations\n\n@@ -12,45 +12,34 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\n-\n \"\"\"Built-in optimizer classes.\n \n For more examples see the base class `tf.keras.optimizers.Optimizer`.\n \"\"\"\n \n+# Imports needed for deserialization.\n+\n import tensorflow.compat.v2 as tf\n \n-# Imports needed for deserialization.\n from keras import backend\n-from keras.optimizers.optimizer_experimental import (\n-    adadelta as adadelta_experimental,\n-)\n-from keras.optimizers.optimizer_experimental import adafactor\n-from keras.optimizers.optimizer_experimental import (\n-    adagrad as adagrad_experimental,\n-)\n-from keras.optimizers.optimizer_experimental import adam as adam_experimental\n-from keras.optimizers.optimizer_experimental import (\n-    adamax as adamax_experimental,\n-)\n-from keras.optimizers.optimizer_experimental import adamw as adamw_experimental\n-from keras.optimizers.optimizer_experimental import ftrl as ftrl_experimental\n-from keras.optimizers.optimizer_experimental import nadam as nadam_experimental\n-from keras.optimizers.optimizer_experimental import (\n-    optimizer as optimizer_experimental,\n-)\n-from keras.optimizers.optimizer_experimental import (\n-    rmsprop as rmsprop_experimental,\n-)\n-from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n+from keras.optimizers import adadelta as adadelta_experimental\n+from keras.optimizers import adafactor\n+from keras.optimizers import adagrad as adagrad_experimental\n+from keras.optimizers import adam as adam_experimental\n+from keras.optimizers import adamax as adamax_experimental\n+from keras.optimizers import adamw as adamw_experimental\n+from keras.optimizers import ftrl as ftrl_experimental\n+from keras.optimizers import nadam as nadam_experimental\n+from keras.optimizers import optimizer as base_optimizer\n+from keras.optimizers import rmsprop as rmsprop_experimental\n+from keras.optimizers import sgd as sgd_experimental\n from keras.optimizers.optimizer_v1 import Optimizer\n from keras.optimizers.optimizer_v1 import TFOptimizer\n from keras.optimizers.optimizer_v2 import adadelta as adadelta_v2\n from keras.optimizers.optimizer_v2 import adagrad as adagrad_v2\n from keras.optimizers.optimizer_v2 import adam as adam_v2\n from keras.optimizers.optimizer_v2 import adamax as adamax_v2\n-from keras.optimizers.optimizer_v2 import ftrl\n+from keras.optimizers.optimizer_v2 import ftrl as ftrl_v2\n from keras.optimizers.optimizer_v2 import (\n     gradient_descent as gradient_descent_v2,\n )\n@@ -160,7 +149,7 @@ def deserialize(config, custom_objects=None, **kwargs):\n             \"nadam\": nadam_v2.Nadam,\n             \"rmsprop\": rmsprop_v2.RMSprop,\n             \"sgd\": gradient_descent_v2.SGD,\n-            \"ftrl\": ftrl.Ftrl,\n+            \"ftrl\": ftrl_v2.Ftrl,\n             \"lossscaleoptimizer\": loss_scale_optimizer.LossScaleOptimizer,\n             \"lossscaleoptimizerv3\": loss_scale_optimizer.LossScaleOptimizerV3,\n             # LossScaleOptimizerV1 was an old version of LSO that was removed.\n@@ -194,7 +183,7 @@ def convert_to_legacy_optimizer(optimizer):\n     Args:\n         optimizer: An instance of `tf.keras.optimizers.experimental.Optimizer`.\n     \"\"\"\n-    if not isinstance(optimizer, optimizer_experimental.Optimizer):\n+    if not isinstance(optimizer, base_optimizer.Optimizer):\n         raise ValueError(\n             \"`convert_to_legacy_optimizer` should only be called \"\n             \"on instances of `tf.keras.optimizers.Optimizer`, but \"\n@@ -231,12 +220,10 @@ def get(identifier, **kwargs):\n     \"\"\"Retrieves a Keras Optimizer instance.\n \n     Args:\n-        identifier: Optimizer identifier, one of\n-            - String: name of an optimizer\n-            - Dictionary: configuration dictionary.\n-            - Keras Optimizer instance (it will be returned unchanged).\n-            - TensorFlow Optimizer instance (it will be wrapped as a Keras\n-              Optimizer).\n+        identifier: Optimizer identifier, one of - String: name of an optimizer\n+          - Dictionary: configuration dictionary. - Keras Optimizer instance (it\n+          will be returned unchanged). - TensorFlow Optimizer instance (it will\n+          be wrapped as a Keras Optimizer).\n \n     Returns:\n         A Keras Optimizer instance.\n@@ -253,7 +240,7 @@ def get(identifier, **kwargs):\n         ),\n     ):\n         return identifier\n-    elif isinstance(identifier, optimizer_experimental.Optimizer):\n+    elif isinstance(identifier, base_optimizer.Optimizer):\n         if tf.__internal__.tf2.enabled():\n             return identifier\n         else:\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n@@ -44,13 +44,11 @@ class Adadelta(optimizer.Optimizer):\n     learning rate can be set, as in most other Keras optimizers.\n \n     Args:\n-      learning_rate: Initial value for the learning rate:\n-        either a floating point value,\n-        or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n-        Defaults to 0.001.\n-        Note that `Adadelta` tends to benefit from higher initial learning rate\n-        values compared to other optimizers.\n-        To match the exact form in the original paper, use 1.0.\n+      learning_rate: Initial value for the learning rate: either a floating\n+        point value, or a `tf.keras.optimizers.schedules.LearningRateSchedule`\n+        instance. Defaults to 0.001. Note that `Adadelta` tends to benefit from\n+        higher initial learning rate values compared to other optimizers. To\n+        match the exact form in the original paper, use 1.0.\n       rho: A `Tensor` or a floating point value. The decay rate. Defaults to\n         0.95.\n       epsilon: Small floating point value used to maintain numerical stability.\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.saving.object_registration import register_keras_serializable\n \n\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras import initializers\n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n\n@@ -17,7 +17,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n\n\n@@ -1,15 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Experimental optimizer package.\"\"\"\n\n@@ -4,15 +4,15 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.optimizers.optimizer_experimental import adadelta\n-from keras.optimizers.optimizer_experimental import adagrad\n-from keras.optimizers.optimizer_experimental import adam\n-from keras.optimizers.optimizer_experimental import adamax\n-from keras.optimizers.optimizer_experimental import adamw\n-from keras.optimizers.optimizer_experimental import ftrl\n-from keras.optimizers.optimizer_experimental import nadam\n-from keras.optimizers.optimizer_experimental import rmsprop\n-from keras.optimizers.optimizer_experimental import sgd\n+from keras.optimizers import adadelta\n+from keras.optimizers import adagrad\n+from keras.optimizers import adam\n+from keras.optimizers import adamax\n+from keras.optimizers import adamw\n+from keras.optimizers import ftrl\n+from keras.optimizers import nadam\n+from keras.optimizers import rmsprop\n+from keras.optimizers import sgd\n from keras.utils import dataset_creator\n from keras.utils import losses_utils\n \n\n@@ -10,16 +10,16 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.optimizers.optimizer_experimental import adadelta as adadelta_new\n-from keras.optimizers.optimizer_experimental import adafactor as adafactor_new\n-from keras.optimizers.optimizer_experimental import adagrad as adagrad_new\n-from keras.optimizers.optimizer_experimental import adam as adam_new\n-from keras.optimizers.optimizer_experimental import adamax as adamax_new\n-from keras.optimizers.optimizer_experimental import adamw as adamw_new\n-from keras.optimizers.optimizer_experimental import ftrl as ftrl_new\n-from keras.optimizers.optimizer_experimental import nadam as nadam_new\n-from keras.optimizers.optimizer_experimental import rmsprop as rmsprop_new\n-from keras.optimizers.optimizer_experimental import sgd as sgd_new\n+from keras.optimizers import adadelta as adadelta_new\n+from keras.optimizers import adafactor as adafactor_new\n+from keras.optimizers import adagrad as adagrad_new\n+from keras.optimizers import adam as adam_new\n+from keras.optimizers import adamax as adamax_new\n+from keras.optimizers import adamw as adamw_new\n+from keras.optimizers import ftrl as ftrl_new\n+from keras.optimizers import nadam as nadam_new\n+from keras.optimizers import rmsprop as rmsprop_new\n+from keras.optimizers import sgd as sgd_new\n from keras.optimizers.optimizer_v2 import adadelta as adadelta_old\n from keras.optimizers.optimizer_v2 import adagrad as adagrad_old\n from keras.optimizers.optimizer_v2 import adam as adam_old\n@@ -27,6 +27,7 @@ from keras.optimizers.optimizer_v2 import ftrl as ftrl_old\n from keras.optimizers.optimizer_v2 import gradient_descent as sgd_old\n from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_old\n from keras.optimizers.schedules import learning_rate_schedule\n+from keras.testing_infra import test_utils\n from keras.utils import losses_utils\n \n ds_combinations = tf.__internal__.distribute.combinations\n@@ -532,6 +533,76 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n             optimizer_2.apply_gradients(zip([sparse_grads], [x2]))\n             self.assertAllClose(x1, x2)\n \n+    @test_utils.run_v2_only\n+    def test_convert_to_legacy_optimizer(self):\n+        if not tf.executing_eagerly():\n+            # The conversion could only happen in eager mode.\n+            return\n+        optimizer_list = [\n+            \"adadelta\",\n+            \"adagrad\",\n+            \"adam\",\n+            \"adamax\",\n+            \"nadam\",\n+            \"rmsprop\",\n+            \"sgd\",\n+            \"ftrl\",\n+        ]\n+        # Test conversion does not throw errors.\n+        for name in optimizer_list:\n+            experimental_optimizer = keras.optimizers.get(\n+                name, use_legacy_optimizer=False\n+            )\n+            reference_legacy_optimizer = keras.optimizers.get(\n+                name, use_legacy_optimizer=True\n+            )\n+            converted_legacy_optimizer = (\n+                keras.optimizers.convert_to_legacy_optimizer(\n+                    experimental_optimizer\n+                )\n+            )\n+            self.assertEqual(\n+                type(reference_legacy_optimizer),\n+                type(converted_legacy_optimizer),\n+            )\n+            self.assertDictEqual(\n+                reference_legacy_optimizer.get_config(),\n+                converted_legacy_optimizer.get_config(),\n+            )\n+\n+        lr_schedule = learning_rate_schedule.ExponentialDecay(\n+            initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.9\n+        )\n+        optimizer = adam_new.Adam(learning_rate=lr_schedule)\n+        legacy_optimizer = keras.optimizers.convert_to_legacy_optimizer(\n+            optimizer\n+        )\n+        self.assertDictEqual(\n+            optimizer.get_config()[\"learning_rate\"],\n+            legacy_optimizer.get_config()[\"learning_rate\"],\n+        )\n+\n+        class CustomLRSchedule(learning_rate_schedule.LearningRateSchedule):\n+            def __init__(self, initial_learning_rate):\n+                self.initial_learning_rate = initial_learning_rate\n+\n+            def __call__(self, step):\n+                step = tf.cast(step, tf.float32)\n+                return self.initial_learning_rate / (step + 1)\n+\n+            def get_config(self):\n+                return {\"initial_learning_rate\": self.initial_learning_rate}\n+\n+        lr_schedule = CustomLRSchedule(0.001)\n+        optimizer = adam_new.Adam(learning_rate=lr_schedule)\n+        legacy_optimizer = keras.optimizers.convert_to_legacy_optimizer(\n+            optimizer\n+        )\n+        self.assertDictEqual(\n+            optimizer.get_config()[\"learning_rate\"],\n+            legacy_optimizer.get_config()[\"learning_rate\"],\n+        )\n+\n \n class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n     \"\"\"Test optimizer outputs the same numerical results as optimizer_v2.\"\"\"\n\n@@ -22,8 +22,6 @@ import tensorflow.compat.v2 as tf\n \n import keras\n from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_experimental import adam as adam_experimental\n-from keras.optimizers.schedules import learning_rate_schedule\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.utils import np_utils\n@@ -301,76 +299,6 @@ class KerasOptimizersTest(test_combinations.TestCase):\n         ):\n             keras.optimizers.get(0)\n \n-    @test_utils.run_v2_only\n-    def test_convert_to_legacy_optimizer(self):\n-        if not tf.executing_eagerly():\n-            # The conversion could only happen in eager mode.\n-            return\n-        optimizer_list = [\n-            \"adadelta\",\n-            \"adagrad\",\n-            \"adam\",\n-            \"adamax\",\n-            \"nadam\",\n-            \"rmsprop\",\n-            \"sgd\",\n-            \"ftrl\",\n-        ]\n-        # Test conversion does not throw errors.\n-        for name in optimizer_list:\n-            experimental_optimizer = keras.optimizers.get(\n-                name, use_legacy_optimizer=False\n-            )\n-            reference_legacy_optimizer = keras.optimizers.get(\n-                name, use_legacy_optimizer=True\n-            )\n-            converted_legacy_optimizer = (\n-                keras.optimizers.convert_to_legacy_optimizer(\n-                    experimental_optimizer\n-                )\n-            )\n-            self.assertEqual(\n-                type(reference_legacy_optimizer),\n-                type(converted_legacy_optimizer),\n-            )\n-            self.assertDictEqual(\n-                reference_legacy_optimizer.get_config(),\n-                converted_legacy_optimizer.get_config(),\n-            )\n-\n-        lr_schedule = learning_rate_schedule.ExponentialDecay(\n-            initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.9\n-        )\n-        optimizer = adam_experimental.Adam(learning_rate=lr_schedule)\n-        legacy_optimizer = keras.optimizers.convert_to_legacy_optimizer(\n-            optimizer\n-        )\n-        self.assertDictEqual(\n-            optimizer.get_config()[\"learning_rate\"],\n-            legacy_optimizer.get_config()[\"learning_rate\"],\n-        )\n-\n-        class CustomLRSchedule(learning_rate_schedule.LearningRateSchedule):\n-            def __init__(self, initial_learning_rate):\n-                self.initial_learning_rate = initial_learning_rate\n-\n-            def __call__(self, step):\n-                step = tf.cast(step, tf.float32)\n-                return self.initial_learning_rate / (step + 1)\n-\n-            def get_config(self):\n-                return {\"initial_learning_rate\": self.initial_learning_rate}\n-\n-        lr_schedule = CustomLRSchedule(0.001)\n-        optimizer = adam_experimental.Adam(learning_rate=lr_schedule)\n-        legacy_optimizer = keras.optimizers.convert_to_legacy_optimizer(\n-            optimizer\n-        )\n-        self.assertDictEqual(\n-            optimizer.get_config()[\"learning_rate\"],\n-            legacy_optimizer.get_config()[\"learning_rate\"],\n-        )\n-\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n@@ -70,7 +70,6 @@ class RMSprop(optimizer.Optimizer):\n     Reference:\n       - [Hinton, 2012](\n         http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n-\n     \"\"\"\n \n     def __init__(\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n\n@@ -30,7 +30,7 @@ import tensorflow.compat.v2 as tf\n import keras\n from keras import losses\n from keras.engine import base_layer\n-from keras.optimizers.optimizer_experimental import optimizer\n+from keras.optimizers import optimizer\n from keras.saving.experimental.serialization_lib import deserialize_keras_object\n from keras.saving.experimental.serialization_lib import serialize_keras_object\n from keras.utils import generic_utils\n\n@@ -26,7 +26,7 @@ from tensorflow.python.platform import tf_logging as logging\n \n import keras\n from keras import backend\n-from keras.optimizers.optimizer_experimental import adam\n+from keras.optimizers import adam\n from keras.saving import object_registration\n from keras.saving.experimental import saving_lib\n from keras.saving.legacy.saved_model import json_utils\n@@ -152,7 +152,9 @@ class CompileOverridingSequential(keras.Sequential):\n @keras.utils.register_keras_serializable(package=\"my_custom_package\")\n def my_mean_squared_error(y_true, y_pred):\n     \"\"\"Identical to built-in `mean_squared_error`, added here as a custom\n-    func.\"\"\"\n+\n+    func.\n+    \"\"\"\n     return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n \n \n\n@@ -22,10 +22,8 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n+from keras.optimizers import optimizer as optimizer_base\n from keras.optimizers import optimizer_v1\n-from keras.optimizers.optimizer_experimental import (\n-    optimizer as optimizer_experimental,\n-)\n from keras.saving.legacy import model_config as model_config_lib\n from keras.saving.legacy import saving_utils\n from keras.saving.legacy.saved_model import json_utils\n@@ -223,9 +221,7 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):\n             # Set optimizer weights.\n             if \"optimizer_weights\" in f:\n                 try:\n-                    if isinstance(\n-                        model.optimizer, optimizer_experimental.Optimizer\n-                    ):\n+                    if isinstance(model.optimizer, optimizer_base.Optimizer):\n                         model.optimizer.build(model.trainable_variables)\n                     else:\n                         model.optimizer._create_all_weights(\n@@ -668,7 +664,7 @@ def save_optimizer_weights_to_hdf5_group(hdf5_group, optimizer):\n         hdf5_group: HDF5 group.\n         optimizer: optimizer instance.\n     \"\"\"\n-    if isinstance(optimizer, optimizer_experimental.Optimizer):\n+    if isinstance(optimizer, optimizer_base.Optimizer):\n         symbolic_weights = optimizer.variables()\n     else:\n         symbolic_weights = getattr(optimizer, \"weights\")\n\n@@ -483,7 +483,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n                 return\n             if isinstance(\n                 loaded_model.optimizer,\n-                keras.optimizers.optimizer_experimental.Optimizer,\n+                keras.optimizers.optimizer.Optimizer,\n             ):\n                 loaded_model.optimizer.build(loaded_model.trainable_variables)\n                 self.assertAllClose(\n\n@@ -19,9 +19,7 @@ import tensorflow.compat.v2 as tf\n # isort: off\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util import deprecation\n-from keras.optimizers.optimizer_experimental import (\n-    optimizer as optimizer_experimental,\n-)\n+from keras.optimizers import optimizer\n from tensorflow.python.util.tf_export import keras_export\n \n _PRINT_EVAL_STEP_EVERY_SEC = 60.0\n@@ -205,7 +203,7 @@ class SidecarEvaluator:\n     def start(self):\n         \"\"\"Starts the evaluation loop.\"\"\"\n         if self.model.optimizer and isinstance(\n-            self.model.optimizer, optimizer_experimental.Optimizer\n+            self.model.optimizer, optimizer.Optimizer\n         ):\n             checkpoint = tf.train.Checkpoint(\n                 model=self.model, optimizer=self.model.optimizer\n@@ -241,7 +239,7 @@ class SidecarEvaluator:\n                 # iterations property to be used in callbacks.\n                 if self.model.optimizer and not isinstance(\n                     self.model.optimizer,\n-                    optimizer_experimental.Optimizer,\n+                    optimizer.Optimizer,\n                 ):\n                     # experimental optimizer automatically restores the\n                     # iteration value.\n@@ -265,7 +263,7 @@ class SidecarEvaluator:\n                 self._iterations.numpy() == _ITERATIONS_UNINITIALIZED\n                 and not isinstance(\n                     self.model.optimizer,\n-                    optimizer_experimental.Optimizer,\n+                    optimizer.Optimizer,\n                 )\n             ):\n                 raise RuntimeError(\n\n@@ -24,7 +24,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.optimizers.optimizer_experimental import sgd\n+from keras.optimizers import sgd\n from keras.testing_infra import test_utils\n from keras.utils import sidecar_evaluator as sidecar_evaluator_lib\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93245cb096b94bc57a71f517a1a08a3dde5da9e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 938 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -377,6 +377,17 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n         obj_map[self] = obj_map[self._variable]\n         return obj_map, resource_map\n \n+    def _export_to_saved_model_graph(\n+        self, object_map, tensor_map, options, **kwargs\n+    ):\n+        # By delegating this method to the wrapped variable, SavedModel with\n+        # AutoCastVariables are identical to SavedModel with normal variables.\n+        resource_list = self._variable._export_to_saved_model_graph(\n+            object_map, tensor_map, options, **kwargs\n+        )\n+        object_map[self] = object_map[self._variable]\n+        return resource_list\n+\n     # TODO(reedwm): Maybe encode the fact the variable is an AutoCastVariable in\n     # to_proto().\n     def to_proto(self, export_scope=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#925346cfa5d692095aed77a28189cbdf2a30e1ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 9517 | Contributors (this commit): 115 | Commits (past 90d): 16 | Contributors (cumulative): 115 | DMM Complexity: None\n\nDIFF:\n@@ -2359,6 +2359,10 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n           your training, especially when used with `tf.distribute.Strategy` as\n           it will incur additional synchronization overhead.\n           Use with `ParameterServerStrategy` is not supported.\n+          Batch-level summary writing is also available via `train_step`\n+          override. Please see\n+          [TensorBoard Scalars tutorial](https://www.tensorflow.org/tensorboard/scalars_and_keras#batch-level_logging)  # noqa: E501\n+          for more details.\n         profile_batch: Profile the batch(es) to sample compute characteristics.\n           profile_batch must be a non-negative integer or a tuple of integers.\n           A pair of positive integers signify a range of batches to profile.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3aa346492630e5d93f0bcbb1024e93db37cc5429", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 159/159 | Churn Δ: 6 | Churn Cumulative: 4896 | Contributors (this commit): 21 | Commits (past 90d): 19 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -19,8 +19,8 @@ import tensorflow.compat.v2 as tf\n from keras import backend\n from keras import optimizers\n from keras.optimizers import optimizer\n+from keras.optimizers import utils as optimizer_utils\n from keras.optimizers.optimizer_v2 import optimizer_v2\n-from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n from keras.saving.legacy import serialization\n \n # isort: off\n\n@@ -22,7 +22,7 @@ from absl import logging\n \n from keras import backend\n from keras import initializers\n-from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n+from keras.optimizers import utils as optimizer_utils\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.utils import tf_utils\n \n\n@@ -25,7 +25,7 @@ import tensorflow.compat.v2 as tf\n from keras import backend\n from keras import initializers\n from keras.engine import base_layer_utils\n-from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n+from keras.optimizers import utils as optimizer_utils\n from keras.optimizers.schedules import learning_rate_schedule\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#caf1797a4410e450f00c16a1d6e0e9adc42eb85b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 10897 | Contributors (this commit): 23 | Commits (past 90d): 12 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -1817,7 +1817,7 @@ def identity(x, name=None):\n # tf.random.Generator to generate random numbers.\n # The legacy behavior is to use TF's legacy stateful RNG ops like\n # tf.random.uniform.\n-_USE_GENERATOR_FOR_RNG = False\n+_USE_GENERATOR_FOR_RNG = True\n \n # The global generator to create the seed when initializing the\n # tf.random.Genrator used by RandomGenerator. When tf.random.Generator becomes\n\n@@ -44,7 +44,7 @@ class Dropout(base_layer.BaseRandomLayer):\n     `trainable` does not affect the layer's behavior, as Dropout does\n     not have any variables/weights that can be frozen during training.)\n \n-    >>> tf.random.set_seed(0)\n+    >>> tf.keras.utils.set_random_seed(0)\n     >>> layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n     >>> data = np.arange(10).reshape(5, 2).astype(np.float32)\n     >>> print(data)\n@@ -57,7 +57,7 @@ class Dropout(base_layer.BaseRandomLayer):\n     >>> print(outputs)\n     tf.Tensor(\n     [[ 0.    1.25]\n-     [ 2.5   3.75]\n+     [ 0.    3.75]\n      [ 5.    6.25]\n      [ 7.5   8.75]\n      [10.    0.  ]], shape=(5, 2), dtype=float32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
