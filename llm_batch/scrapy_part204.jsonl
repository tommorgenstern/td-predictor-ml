{"custom_id": "scrapy#d66efb13badcaf7939b1779d02b28db3f5ab65a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 99 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,19 @@ class XmliterTestCase(unittest.TestCase):\n         self.assertEqual(attrs,\n                          [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n \n+    def test_xmliter_unusual_node(self):\n+        body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+            <root>\n+                <matchme...></matchme...>\n+                <matchmenot></matchmenot>\n+            </root>\n+        \"\"\"\n+        response = XmlResponse(url=\"http://example.com\", body=body)\n+        nodenames = [e.xpath('name()').extract()\n+                 for e in self.xmliter(response, 'matchme...')]\n+        self.assertEqual(nodenames, [['matchme...']])\n+\n+\n     def test_xmliter_text(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f56062d04549e9906b9f824a7dbfa063df2b8abf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 279 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,8 +25,10 @@ def xmliter(obj, nodename):\n     - a unicode string\n     - a string encoded as utf-8\n     \"\"\"\n-    HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename, re.S)\n-    HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename, re.S)\n+    nodename_patt = re.escape(nodename)\n+\n+    HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename_patt, re.S)\n+    HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename_patt, re.S)\n     text = _body_or_str(obj)\n \n     header_start = re.search(HEADER_START_RE, text)\n@@ -34,7 +36,7 @@ def xmliter(obj, nodename):\n     header_end = re_rsearch(HEADER_END_RE, text)\n     header_end = text[header_end[1]:].strip() if header_end else ''\n \n-    r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n+    r = re.compile(r\"<{0}[\\s>].*?</{0}>\".format(nodename_patt), re.DOTALL)\n     for match in r.finditer(text):\n         nodetext = header_start + match.group() + header_end\n         yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b6d60c2514989402cd1c94d5c0ea8594f5aef16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 6 | Churn Cumulative: 6 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -10,12 +10,6 @@ Failed pages are collected on the scraping process and rescheduled at the end,\n once the spider has finished crawling all regular (non failed) pages. Once\n there is no more failed pages to retry this middleware sends a signal\n (retry_complete), so other extensions could connect to that signal.\n-\n-About HTTP errors to consider:\n-\n-- You may want to remove 400 from RETRY_HTTP_CODES, if you stick to the HTTP\n-  protocol. It's included by default because it's a common code used to\n-  indicate server overload, which would be something we want to retry\n \"\"\"\n import logging\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#215905bdb65cb0b97938c4d2acd13c8337a586da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 32 | Files Changed: 4 | Hunks: 35 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 71 | Churn Cumulative: 915 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 24 | DMM Complexity: 0.5\n\nDIFF:\n@@ -11,6 +11,7 @@ from scrapy.commands import ScrapyCommand\n from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.exceptions import UsageError\n \n+\n def sanitize_module_name(module_name):\n     \"\"\"Sanitize the given module name, by replacing dashes and points\n     with underscores and prefixing it with a letter if it doesn't start\n@@ -21,6 +22,7 @@ def sanitize_module_name(module_name):\n         module_name = \"a\" + module_name\n     return module_name\n \n+\n class Command(ScrapyCommand):\n \n     requires_project = True\n@@ -52,7 +54,8 @@ class Command(ScrapyCommand):\n         if opts.dump:\n             template_file = self._find_template(opts.dump)\n             if template_file:\n-                print(open(template_file, 'r').read())\n+                with open(template_file, \"r\") as f:\n+                    print(f.read())\n             return\n         if len(args) != 2:\n             raise UsageError()\n\n@@ -32,33 +32,34 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None, \\\n+        parser.add_option(\"--spider\", dest=\"spider\", default=None,\n             help=\"use this spider without looking for one\")\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n+        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n             help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"--pipelines\", action=\"store_true\", \\\n+        parser.add_option(\"--pipelines\", action=\"store_true\",\n             help=\"process items through pipelines\")\n-        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", \\\n+        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\",\n             help=\"don't show links to follow (extracted requests)\")\n-        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", \\\n+        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\",\n             help=\"don't show scraped items\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", \\\n+        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\",\n             help=\"avoid using pygments to colorize the output\")\n-        parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\", \\\n+        parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\",\n             help=\"use CrawlSpider rules to discover the callback\")\n-        parser.add_option(\"-c\", \"--callback\", dest=\"callback\", \\\n+        parser.add_option(\"-c\", \"--callback\", dest=\"callback\",\n             help=\"use this callback for parsing, instead looking for a callback\")\n-        parser.add_option(\"-d\", \"--depth\", dest=\"depth\", type=\"int\", default=1, \\\n+        parser.add_option(\"-d\", \"--depth\", dest=\"depth\", type=\"int\", default=1,\n             help=\"maximum depth for parsing requests [default: %default]\")\n-        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\", \\\n+        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n             help=\"print each depth level one by one\")\n \n \n     @property\n     def max_level(self):\n-        levels = self.items.keys() + self.requests.keys()\n-        if levels: return max(levels)\n-        else: return 0\n+        levels = list(self.items.keys()) + list(self.requests.keys())\n+        if not levels:\n+            return 0\n+        return max(levels)\n \n     def add_items(self, lvl, new_items):\n         old_items = self.items.get(lvl, [])\n@@ -79,7 +80,7 @@ class Command(ScrapyCommand):\n \n     def print_requests(self, lvl=None, colour=True):\n         if lvl is None:\n-            levels = self.requests.keys()\n+            levels = list(self.requests.keys())\n             if levels:\n                 requests = self.requests[max(levels)]\n             else:\n@@ -94,7 +95,7 @@ class Command(ScrapyCommand):\n         colour = not opts.nocolour\n \n         if opts.verbose:\n-            for level in xrange(1, self.max_level+1):\n+            for level in range(1, self.max_level+1):\n                 print('\\n>>> DEPTH LEVEL: %s <<<' % level)\n                 if not opts.noitems:\n                     self.print_items(level, colour)\n@@ -107,7 +108,6 @@ class Command(ScrapyCommand):\n             if not opts.nolinks:\n                 self.print_requests(colour=colour)\n \n-\n     def run_callback(self, response, cb):\n         items, requests = [], []\n \n@@ -146,7 +146,6 @@ class Command(ScrapyCommand):\n         _start_requests = lambda s: [self.prepare_request(s, request, opts)]\n         self.spidercls.start_requests = _start_requests\n \n-\n     def start_parsing(self, url, opts):\n         self.crawler_process.crawl(self.spidercls, **opts.spargs)\n         self.pcrawler = list(self.crawler_process.crawlers)[0]\n\n@@ -4,18 +4,20 @@ import os\n import re\n import string\n \n+\n def render_templatefile(path, **kwargs):\n-    with open(path, 'rb') as file:\n-        raw = file.read()\n+    with open(path, 'rb') as fp:\n+        raw = fp.read().decode('utf8')\n \n     content = string.Template(raw).substitute(**kwargs)\n \n     render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path\n-    with open(render_path, 'wb') as file:\n-        file.write(content)\n+    with open(render_path, 'wb') as fp:\n+        fp.write(content.encode('utf8'))\n     if path.endswith('.tmpl'):\n         os.remove(path)\n \n+\n CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z\\d]')\n def string_camelcase(string):\n     \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n\n@@ -6,10 +6,12 @@ from time import sleep\n from os.path import exists, join, abspath\n from shutil import rmtree\n from tempfile import mkdtemp\n+import six\n \n from twisted.trial import unittest\n from twisted.internet import defer\n \n+from scrapy.utils.python import to_native_str\n from scrapy.utils.python import retry_on_eintr\n from scrapy.utils.test import get_testenv\n from scrapy.utils.testsite import SiteTest\n@@ -94,11 +96,11 @@ class GenspiderCommandTest(CommandTest):\n         args = ['--template=%s' % tplname] if tplname else []\n         spname = 'test_spider'\n         p = self.proc('genspider', spname, 'test.com', *args)\n-        out = retry_on_eintr(p.stdout.read)\n+        out = to_native_str(retry_on_eintr(p.stdout.read))\n         self.assertIn(\"Created spider %r using template %r in module\" % (spname, tplname), out)\n         self.assertTrue(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n         p = self.proc('genspider', spname, 'test.com', *args)\n-        out = retry_on_eintr(p.stdout.read)\n+        out = to_native_str(retry_on_eintr(p.stdout.read))\n         self.assertIn(\"Spider %r already exists in module\" % spname, out)\n \n     def test_template_basic(self):\n@@ -146,7 +148,7 @@ class MySpider(scrapy.Spider):\n         return []\n \"\"\")\n         p = self.proc('runspider', fname)\n-        log = p.stderr.read()\n+        log = to_native_str(p.stderr.read())\n         self.assertIn(\"DEBUG: It Works!\", log)\n         self.assertIn(\"INFO: Spider opened\", log)\n         self.assertIn(\"INFO: Closing spider (finished)\", log)\n@@ -161,12 +163,12 @@ class MySpider(scrapy.Spider):\n from scrapy.spiders import Spider\n \"\"\")\n         p = self.proc('runspider', fname)\n-        log = p.stderr.read()\n+        log = to_native_str(p.stderr.read())\n         self.assertIn(\"No spider found in file\", log)\n \n     def test_runspider_file_not_found(self):\n         p = self.proc('runspider', 'some_non_existent_file')\n-        log = p.stderr.read()\n+        log = to_native_str(p.stderr.read())\n         self.assertIn(\"File not found: some_non_existent_file\", log)\n \n     def test_runspider_unable_to_load(self):\n@@ -176,11 +178,12 @@ from scrapy.spiders import Spider\n         with open(fname, 'w') as f:\n             f.write(\"\")\n         p = self.proc('runspider', fname)\n-        log = p.stderr.read()\n+        log = to_native_str(p.stderr.read())\n         self.assertIn(\"Unable to load\", log)\n \n \n class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n+    skip = not six.PY2\n \n     command = 'parse'\n \n@@ -226,7 +229,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            '-a', 'test_arg=1',\n                                            '-c', 'parse',\n                                            self.url('/html')])\n-        self.assertIn(\"DEBUG: It Works!\", stderr)\n+        self.assertIn(\"DEBUG: It Works!\", to_native_str(stderr))\n \n     @defer.inlineCallbacks\n     def test_pipelines(self):\n@@ -234,14 +237,14 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            '--pipelines',\n                                            '-c', 'parse',\n                                            self.url('/html')])\n-        self.assertIn(\"INFO: It Works!\", stderr)\n+        self.assertIn(\"INFO: It Works!\", to_native_str(stderr))\n \n     @defer.inlineCallbacks\n     def test_parse_items(self):\n         status, out, stderr = yield self.execute(\n             ['--spider', self.spider_name, '-c', 'parse', self.url('/html')]\n         )\n-        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", out)\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", to_native_str(out))\n \n \n \n@@ -250,5 +253,5 @@ class BenchCommandTest(CommandTest):\n     def test_run(self):\n         p = self.proc('bench', '-s', 'LOGSTATS_INTERVAL=0.001',\n                 '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n-        log = p.stderr.read()\n+        log = to_native_str(p.stderr.read())\n         self.assertIn('INFO: Crawled', log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26586ef5a605c6f6a23143b22812779857cd4e3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 162 | Lines Deleted: 84 | Files Changed: 15 | Hunks: 50 | Methods Changed: 31 | Complexity Δ (Sum/Max): 24/6 | Churn Δ: 246 | Churn Cumulative: 2580 | Contributors (this commit): 33 | Commits (past 90d): 60 | Contributors (cumulative): 92 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,10 +58,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         # load contracts\n-        contracts = build_component_list(\n-            self.settings['SPIDER_CONTRACTS_BASE'],\n-            self.settings['SPIDER_CONTRACTS'],\n-        )\n+        contracts = build_component_list(self.settings._getcomposite('SPIDER_CONTRACTS'))\n         conman = ContractsManager(load_object(c) for c in contracts)\n         runner = TextTestRunner(verbosity=2 if opts.verbose else 1)\n         result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n\n@@ -1,6 +1,6 @@\n import os\n from scrapy.commands import ScrapyCommand\n-from scrapy.utils.conf import arglist_to_dict\n+from scrapy.utils.conf import arglist_to_dict, remove_none_values\n from scrapy.exceptions import UsageError\n \n \n@@ -34,10 +34,8 @@ class Command(ScrapyCommand):\n                 self.settings.set('FEED_URI', 'stdout:', priority='cmdline')\n             else:\n                 self.settings.set('FEED_URI', opts.output, priority='cmdline')\n-            valid_output_formats = (\n-                list(self.settings.getdict('FEED_EXPORTERS').keys()) +\n-                list(self.settings.getdict('FEED_EXPORTERS_BASE').keys())\n-            )\n+            feed_exporters = remove_none_values(self.settings._getcomposite('FEED_EXPORTERS'))\n+            valid_output_formats = feed_exporters.keys()\n             if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n             if opts.output_format not in valid_output_formats:\n\n@@ -5,7 +5,7 @@ from importlib import import_module\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n-from scrapy.utils.conf import arglist_to_dict\n+from scrapy.utils.conf import arglist_to_dict, remove_none_values\n \n \n def _import_file(filepath):\n@@ -57,10 +57,8 @@ class Command(ScrapyCommand):\n                 self.settings.set('FEED_URI', 'stdout:', priority='cmdline')\n             else:\n                 self.settings.set('FEED_URI', opts.output, priority='cmdline')\n-            valid_output_formats = (\n-                list(self.settings.getdict('FEED_EXPORTERS').keys()) +\n-                list(self.settings.getdict('FEED_EXPORTERS_BASE').keys())\n-            )\n+            feed_exporters = remove_none_values(self.settings._getcomposite('FEED_EXPORTERS'))\n+            valid_output_formats = feed_exporters.keys()\n             if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n             if opts.output_format not in valid_output_formats:\n\n@@ -4,6 +4,7 @@ import logging\n from twisted.internet import defer\n import six\n from scrapy.exceptions import NotSupported, NotConfigured\n+from scrapy.utils.conf import remove_none_values\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n from scrapy import signals\n@@ -19,13 +20,8 @@ class DownloadHandlers(object):\n         self._schemes = {}  # stores acceptable schemes on instancing\n         self._handlers = {}  # stores instanced handlers for schemes\n         self._notconfigured = {}  # remembers failed handlers\n-        handlers = crawler.settings.get('DOWNLOAD_HANDLERS_BASE')\n-        handlers.update(crawler.settings.get('DOWNLOAD_HANDLERS', {}))\n+        handlers = remove_none_values(crawler.settings._getcomposite('DOWNLOAD_HANDLERS'))\n         for scheme, clspath in six.iteritems(handlers):\n-            # Allow to disable a handler just like any other\n-            # component (extension, middleware, etc).\n-            if clspath is None:\n-                continue\n             self._schemes[scheme] = clspath\n \n         crawler.signals.connect(self._close, signals.engine_stopped)\n\n@@ -19,8 +19,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings['DOWNLOADER_MIDDLEWARES_BASE'], \\\n-            settings['DOWNLOADER_MIDDLEWARES'])\n+        return build_component_list(settings._getcomposite('DOWNLOADER_MIDDLEWARES'))\n \n     def _add_middleware(self, mw):\n         if hasattr(mw, 'process_request'):\n\n@@ -18,8 +18,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings['SPIDER_MIDDLEWARES_BASE'], \\\n-            settings['SPIDER_MIDDLEWARES'])\n+        return build_component_list(settings._getcomposite('SPIDER_MIDDLEWARES'))\n \n     def _add_middleware(self, mw):\n         super(SpiderMiddlewareManager, self)._add_middleware(mw)\n\n@@ -4,6 +4,8 @@ DefaultHeaders downloader middleware\n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n+from scrapy.utils.conf import remove_none_values\n+\n \n class DefaultHeadersMiddleware(object):\n \n@@ -12,7 +14,8 @@ class DefaultHeadersMiddleware(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler.settings.get('DEFAULT_REQUEST_HEADERS').items())\n+        headers = remove_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])\n+        return cls(headers.items())\n \n     def process_request(self, request, spider):\n         for k, v in self._headers:\n\n@@ -12,5 +12,4 @@ class ExtensionManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return build_component_list(settings['EXTENSIONS_BASE'], \\\n-            settings['EXTENSIONS'])\n+        return build_component_list(settings._getcomposite('EXTENSIONS'))\n\n@@ -18,6 +18,7 @@ from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n \n from scrapy import signals\n+from scrapy.utils.conf import remove_none_values\n from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n@@ -195,8 +196,7 @@ class FeedExporter(object):\n         return item\n \n     def _load_components(self, setting_prefix):\n-        conf = dict(self.settings['%s_BASE' % setting_prefix])\n-        conf.update(self.settings[setting_prefix])\n+        conf = remove_none_values(self.settings._getcomposite(setting_prefix))\n         d = {}\n         for k, v in conf.items():\n             try:\n\n@@ -13,15 +13,7 @@ class ItemPipelineManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        item_pipelines = settings['ITEM_PIPELINES']\n-        if isinstance(item_pipelines, (tuple, list, set, frozenset)):\n-            from scrapy.exceptions import ScrapyDeprecationWarning\n-            import warnings\n-            warnings.warn('ITEM_PIPELINES defined as a list or a set is deprecated, switch to a dict',\n-                category=ScrapyDeprecationWarning, stacklevel=1)\n-            # convert old ITEM_PIPELINE list to a dict with order 500\n-            item_pipelines = dict(zip(item_pipelines, range(500, 500+len(item_pipelines))))\n-        return build_component_list(settings['ITEM_PIPELINES_BASE'], item_pipelines)\n+        return build_component_list(settings._getcomposite('ITEM_PIPELINES'))\n \n     def _add_middleware(self, pipe):\n         super(ItemPipelineManager, self)._add_middleware(pipe)\n\n@@ -36,10 +36,18 @@ class SettingsAttribute(object):\n \n     def __init__(self, value, priority):\n         self.value = value\n+        if isinstance(self.value, BaseSettings):\n+            self.priority = max(self.value.maxpriority(), priority)\n+        else:\n             self.priority = priority\n \n     def set(self, value, priority):\n         \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n+        if isinstance(self.value, BaseSettings):\n+            # Ignore self.priority if self.value has per-key priorities\n+            self.value.update(value, priority)\n+            self.priority = max(self.value.maxpriority(), priority)\n+        else:\n             if priority >= self.priority:\n                 self.value = value\n                 self.priority = priority\n@@ -95,6 +103,20 @@ class BaseSettings(MutableMapping):\n             value = json.loads(value)\n         return dict(value)\n \n+    def _getcomposite(self, name):\n+        # DO NOT USE THIS FUNCTION IN YOUR CUSTOM PROJECTS\n+        # It's for internal use in the transition away from the _BASE settings and\n+        # will be removed along with _BASE support in a future release\n+        basename = name + \"_BASE\"\n+        if basename in self:\n+            warnings.warn('_BASE settings are deprecated.',\n+                          category=ScrapyDeprecationWarning)\n+            compsett = BaseSettings(self[name + \"_BASE\"], priority='default')\n+            compsett.update(self[name])\n+            return compsett\n+        else:\n+            return self[name]\n+\n     def getpriority(self, name):\n         prio = None\n         if name in self:\n@@ -232,16 +254,25 @@ class _DictProxy(MutableMapping):\n class Settings(BaseSettings):\n \n     def __init__(self, values=None, priority='project'):\n+        # Do not pass kwarg values here. We don't want to promote user-defined\n+        # dicts, and we want to update, not replace, default dicts with the\n+        # values given by the user\n         super(Settings, self).__init__()\n         self.setmodule(default_settings, 'default')\n+        # Promote default dictionaries to BaseSettings instances for per-key\n+        # priorities\n+        for name in self:\n+            val = self[name]\n+            if isinstance(val, dict):\n+                self.set(name, BaseSettings(val, 'default'), 'default')\n         self.update(values, priority)\n \n \n class CrawlerSettings(Settings):\n \n     def __init__(self, settings_module=None, **kw):\n-        Settings.__init__(self, **kw)\n         self.settings_module = settings_module\n+        Settings.__init__(self, **kw)\n \n     def __getitem__(self, opt_name):\n         if opt_name in self.overrides:\n\n@@ -63,8 +63,7 @@ DNS_TIMEOUT = 60\n \n DOWNLOAD_DELAY = 0\n \n-DOWNLOAD_HANDLERS = {}\n-DOWNLOAD_HANDLERS_BASE = {\n+DOWNLOAD_HANDLERS = {\n     'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n     'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n     'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n@@ -82,9 +81,7 @@ DOWNLOADER = 'scrapy.core.downloader.Downloader'\n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'\n \n-DOWNLOADER_MIDDLEWARES = {}\n-\n-DOWNLOADER_MIDDLEWARES_BASE = {\n+DOWNLOADER_MIDDLEWARES = {\n     # Engine side\n     'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\n     'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\n@@ -116,9 +113,7 @@ except KeyError:\n     else:\n         EDITOR = 'vi'\n \n-EXTENSIONS = {}\n-\n-EXTENSIONS_BASE = {\n+EXTENSIONS = {\n     'scrapy.extensions.corestats.CoreStats': 0,\n     'scrapy.telnet.TelnetConsole': 0,\n     'scrapy.extensions.memusage.MemoryUsage': 0,\n@@ -135,16 +130,14 @@ FEED_URI_PARAMS = None  # a function to extend uri arguments\n FEED_FORMAT = 'jsonlines'\n FEED_STORE_EMPTY = False\n FEED_EXPORT_FIELDS = None\n-FEED_STORAGES = {}\n-FEED_STORAGES_BASE = {\n+FEED_STORAGES = {\n     '': 'scrapy.extensions.feedexport.FileFeedStorage',\n     'file': 'scrapy.extensions.feedexport.FileFeedStorage',\n     'stdout': 'scrapy.extensions.feedexport.StdoutFeedStorage',\n     's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n     'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n }\n-FEED_EXPORTERS = {}\n-FEED_EXPORTERS_BASE = {\n+FEED_EXPORTERS = {\n     'json': 'scrapy.exporters.JsonItemExporter',\n     'jsonlines': 'scrapy.exporters.JsonLinesItemExporter',\n     'jl': 'scrapy.exporters.JsonLinesItemExporter',\n@@ -170,7 +163,6 @@ HTTPCACHE_GZIP = False\n ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n \n ITEM_PIPELINES = {}\n-ITEM_PIPELINES_BASE = {}\n \n LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n@@ -229,9 +221,7 @@ SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'\n \n SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader'\n \n-SPIDER_MIDDLEWARES = {}\n-\n-SPIDER_MIDDLEWARES_BASE = {\n+SPIDER_MIDDLEWARES = {\n     # Engine side\n     'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': 50,\n     'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': 500,\n@@ -258,8 +248,7 @@ TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n TELNETCONSOLE_HOST = '127.0.0.1'\n \n-SPIDER_CONTRACTS = {}\n-SPIDER_CONTRACTS_BASE = {\n+SPIDER_CONTRACTS = {\n     'scrapy.contracts.default.UrlContract': 1,\n     'scrapy.contracts.default.ReturnsContract': 2,\n     'scrapy.contracts.default.ScrapesContract': 3,\n\n@@ -1,36 +1,49 @@\n import os\n import sys\n+import warnings\n from operator import itemgetter\n \n import six\n from six.moves.configparser import SafeConfigParser\n \n+from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n \n \n-def build_component_list(base, custom, convert=update_classpath):\n-    \"\"\"Compose a component list based on a custom and base dict of components\n-    (typically middlewares or extensions), unless custom is already a list, in\n-    which case it's returned.\n-    \"\"\"\n+def build_component_list(compdict, convert=update_classpath):\n+    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n \n     def _check_components(complist):\n         if len({convert(c) for c in complist}) != len(complist):\n             raise ValueError('Some paths in {!r} convert to the same object, '\n                              'please update your settings'.format(complist))\n \n-    if isinstance(custom, (list, tuple)):\n-        _check_components(custom)\n-        return type(custom)(convert(c) for c in custom)\n-\n     def _map_keys(compdict):\n+        if isinstance(compdict, BaseSettings):\n+            compbs = BaseSettings()\n+            for k, v in six.iteritems(compdict):\n+                prio = compdict.getpriority(k)\n+                if compbs.getpriority(convert(k)) == prio:\n+                    raise ValueError('Some paths in {!r} convert to the same '\n+                                     'object, please update your settings'\n+                                     ''.format(list(compdict.keys())))\n+                else:\n+                    compbs.set(convert(k), v, priority=prio)\n+            return compbs\n+        else:\n             _check_components(compdict)\n             return {convert(k): v for k, v in six.iteritems(compdict)}\n \n-    compdict = _map_keys(base)\n-    compdict.update(_map_keys(custom))\n-    items = (x for x in six.iteritems(compdict) if x[1] is not None)\n-    return [x[0] for x in sorted(items, key=itemgetter(1))]\n+    if isinstance(compdict, (list, tuple)):\n+        _check_components(compdict)\n+        return type(compdict)(convert(c) for c in compdict)\n+    compdict = remove_none_values(_map_keys(compdict))\n+    return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]\n+\n+\n+def remove_none_values(compdict):\n+    \"\"\"Return dict with all pairs that have value 'None' removed\"\"\"\n+    return {k: v for k, v in six.iteritems(compdict) if v is not None}\n \n \n def arglist_to_dict(arglist):\n\n@@ -28,11 +28,22 @@ class SettingsAttributeTest(unittest.TestCase):\n         self.assertEqual(self.attribute.value, 'value')\n         self.assertEqual(self.attribute.priority, 10)\n \n+    def test_set_per_key_priorities(self):\n+        attribute = SettingsAttribute(\n+                        BaseSettings({'one': 10, 'two': 20}, 0),\n+                        0)\n \n-class SettingsTest(unittest.TestCase):\n+        new_dict = {'one': 11, 'two': 21}\n+        attribute.set(new_dict, 10)\n+        self.assertEqual(attribute.value['one'], 11)\n+        self.assertEqual(attribute.value['two'], 21)\n \n-    if six.PY3:\n-        assertItemsEqual = unittest.TestCase.assertCountEqual\n+        new_settings = BaseSettings()\n+        new_settings.set('one', 12, 20)\n+        new_settings.set('two', 12, 0)\n+        attribute.set(new_settings, 0)\n+        self.assertEqual(attribute.value['one'], 12)\n+        self.assertEqual(attribute.value['two'], 21)\n \n \n class BaseSettingsTest(unittest.TestCase):\n@@ -239,6 +250,20 @@ class BaseSettingsTest(unittest.TestCase):\n         self.assertEqual(settings.getpriority('key'), 99)\n         self.assertEqual(settings.getpriority('nonexistentkey'), None)\n \n+    def test_getcomposite(self):\n+        s = BaseSettings({'TEST_BASE': {1: 1, 2: 2},\n+                          'TEST': BaseSettings({1: 10}),\n+                          'HASNOBASE': BaseSettings({1: 1})})\n+        cs = s._getcomposite('TEST')\n+        self.assertEqual(len(cs), 2)\n+        self.assertEqual(cs[1], 10)\n+        self.assertEqual(cs[2], 2)\n+        cs = s._getcomposite('HASNOBASE')\n+        self.assertEqual(len(cs), 1)\n+        self.assertEqual(cs[1], 1)\n+        cs = s._getcomposite('NONEXISTENT')\n+        self.assertIsNone(cs)\n+\n     def test_maxpriority(self):\n         # Empty settings should return 'default'\n         self.assertEqual(self.settings.maxpriority(), 0)\n@@ -342,6 +367,24 @@ class SettingsTest(unittest.TestCase):\n         self.assertEqual(attr.value, 'value')\n         self.assertEqual(attr.priority, 10)\n \n+    @mock.patch('scrapy.settings.default_settings', default_settings)\n+    def test_autopromote_dicts(self):\n+        settings = Settings()\n+        mydict = settings.get('TEST_DICT')\n+        self.assertIsInstance(mydict, BaseSettings)\n+        self.assertIn('key', mydict)\n+        self.assertEqual(mydict['key'], 'val')\n+        self.assertEqual(mydict.getpriority('key'), 0)\n+\n+    @mock.patch('scrapy.settings.default_settings', default_settings)\n+    def test_getdict_autodegrade_basesettings(self):\n+        settings = Settings()\n+        mydict = settings.getdict('TEST_DICT')\n+        self.assertIsInstance(mydict, dict)\n+        self.assertEqual(len(mydict), 1)\n+        self.assertIn('key', mydict)\n+        self.assertEqual(mydict['key'], 'val')\n+\n \n class CrawlerSettingsTest(unittest.TestCase):\n \n\n@@ -1,43 +1,64 @@\n import unittest\n \n-from scrapy.utils.conf import build_component_list, arglist_to_dict\n+from scrapy.settings import BaseSettings\n+from scrapy.utils.conf import (build_component_list, arglist_to_dict,\n+                               remove_none_values)\n \n \n class BuildComponentListTest(unittest.TestCase):\n \n     def test_build_dict(self):\n-        base = {'one': 1, 'two': 2, 'three': 3, 'five': 5, 'six': None}\n-        custom = {'two': None, 'three': 8, 'four': 4}\n-        self.assertEqual(build_component_list(base, custom, lambda x: x),\n-                         ['one', 'four', 'five', 'three'])\n+        d = {'one': 1, 'two': None, 'three': 8, 'four': 4}\n+        self.assertEqual(build_component_list(d, lambda x: x),\n+                         ['one', 'four', 'three'])\n \n     def test_return_list(self):\n         custom = ['a', 'b', 'c']\n-        self.assertEqual(build_component_list(None, custom, lambda x: x),\n-                         custom)\n+        self.assertEqual(build_component_list(custom, lambda x: x), custom)\n \n     def test_map_dict(self):\n         custom = {'one': 1, 'two': 2, 'three': 3}\n-        self.assertEqual(build_component_list({}, custom, lambda x: x.upper()),\n+        self.assertEqual(build_component_list(custom, lambda x: x.upper()),\n                          ['ONE', 'TWO', 'THREE'])\n \n     def test_map_list(self):\n         custom = ['a', 'b', 'c']\n-        self.assertEqual(build_component_list(None, custom, lambda x: x.upper()),\n+        self.assertEqual(build_component_list(custom, lambda x: x.upper()),\n                          ['A', 'B', 'C'])\n \n     def test_duplicate_components_in_dict(self):\n         duplicate_dict = {'one': 1, 'two': 2, 'ONE': 4}\n         self.assertRaises(ValueError,\n-                          build_component_list, {}, duplicate_dict, lambda x: x.lower())\n+                          build_component_list, duplicate_dict, lambda x: x.lower())\n \n     def test_duplicate_components_in_list(self):\n         duplicate_list = ['a', 'b', 'a']\n         self.assertRaises(ValueError,\n-                          build_component_list, None, duplicate_list, lambda x: x)\n+                          build_component_list, duplicate_list, lambda x: x)\n+\n+    def test_duplicate_components_in_basesettings(self):\n+        # Higher priority takes precedence\n+        duplicate_bs = BaseSettings({'one': 1, 'two': 2}, priority=0)\n+        duplicate_bs.set('ONE', 4, priority=10)\n+        self.assertEqual(build_component_list(duplicate_bs, convert=lambda x: x.lower()),\n+                         ['two', 'one'])\n+        duplicate_bs.set('one', duplicate_bs['one'], priority=20)\n+        self.assertEqual(build_component_list(duplicate_bs, convert=lambda x: x.lower()),\n+                         ['one', 'two'])\n+        # Same priority raises ValueError\n+        duplicate_bs.set('ONE', duplicate_bs['ONE'], priority=20)\n+        self.assertRaises(ValueError,\n+                          build_component_list, duplicate_bs, convert=lambda x: x.lower())\n+\n \n class UtilsConfTestCase(unittest.TestCase):\n \n+    def test_remove_none_values(self):\n+        comps = {'one': 1, 'none': None, 'three': 3, 'four': 4}\n+        compscopy = dict(comps)\n+        del compscopy['none']\n+        self.assertEqual(remove_none_values(comps), compscopy)\n+\n     def test_arglist_to_dict(self):\n         self.assertEqual(arglist_to_dict(['arg1=val1', 'arg2=val2']),\n             {'arg1': 'val1', 'arg2': 'val2'})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb6dee611ca72cda61d0daa12880c5d4c5aba9c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 189 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 18 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 193 | Churn Cumulative: 658 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,6 +20,11 @@ SETTINGS_PRIORITIES = {\n }\n \n def get_settings_priority(priority):\n+    \"\"\"\n+    Small helper function that looks up a given string priority in the\n+    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n+    numerical value, or directly returns a given numerical priority.\n+    \"\"\"\n     if isinstance(priority, six.string_types):\n         return SETTINGS_PRIORITIES[priority]\n     else:\n@@ -60,6 +65,26 @@ class SettingsAttribute(object):\n \n \n class BaseSettings(MutableMapping):\n+    \"\"\"\n+    Instances of this class behave like dictionaries, but store priorities\n+    along with their ``(key, value)`` pairs, and can be frozen (i.e. marked\n+    immutable).\n+\n+    Key-value entries can be passed on initialization with the ``values``\n+    argument, and they would take the ``priority`` level (unless ``values`` is\n+    already an instance of :class:`~scrapy.settings.BaseSettings`, in which\n+    case the existing priority levels will be kept).  If the ``priority``\n+    argument is a string, the priority name will be looked up in\n+    :attr:`~scrapy.settings.SETTINGS_PRIORITIES`. Otherwise, a specific integer\n+    should be provided.\n+\n+    Once the object is created, new settings can be loaded or updated with the\n+    :meth:`~scrapy.settings.BaseSettings.set` method, and can be accessed with\n+    the square bracket notation of dictionaries, or with the\n+    :meth:`~scrapy.settings.BaseSettings.get` method of the instance and its\n+    value conversion variants. When requesting a stored key, the value with the\n+    highest priority will be retrieved.\n+    \"\"\"\n \n     def __init__(self, values=None, priority='project'):\n         self.frozen = False\n@@ -76,28 +101,94 @@ class BaseSettings(MutableMapping):\n         return name in self.attributes\n \n     def get(self, name, default=None):\n+        \"\"\"\n+        Get a setting value without affecting its original type.\n+\n+        :param name: the setting name\n+        :type name: string\n+\n+        :param default: the value to return if no setting is found\n+        :type default: any\n+        \"\"\"\n         return self[name] if self[name] is not None else default\n \n     def getbool(self, name, default=False):\n         \"\"\"\n-        True is: 1, '1', True\n-        False is: 0, '0', False, None\n+        Get a setting value as a boolean.\n+        \n+        ``1``, ``'1'``, and ``True`` return ``True``, while ``0``, ``'0'``,\n+        ``False`` and ``None`` return ``False``. \n+\n+        For example, settings populated through environment variables set to\n+        ``'0'`` will return ``False`` when using this method.\n+\n+        :param name: the setting name\n+        :type name: string\n+\n+        :param default: the value to return if no setting is found\n+        :type default: any\n         \"\"\"\n         return bool(int(self.get(name, default)))\n \n     def getint(self, name, default=0):\n+        \"\"\"\n+        Get a setting value as an int.\n+\n+        :param name: the setting name\n+        :type name: string\n+\n+        :param default: the value to return if no setting is found\n+        :type default: any\n+        \"\"\"\n         return int(self.get(name, default))\n \n     def getfloat(self, name, default=0.0):\n+        \"\"\"\n+        Get a setting value as a float.\n+\n+        :param name: the setting name\n+        :type name: string\n+\n+        :param default: the value to return if no setting is found\n+        :type default: any\n+        \"\"\"\n         return float(self.get(name, default))\n \n     def getlist(self, name, default=None):\n+        \"\"\"\n+        Get a setting value as a list. If the setting original type is a list, a\n+        copy of it will be returned. If it's a string it will be split by \",\".\n+\n+        For example, settings populated through environment variables set to\n+        ``'one,two'`` will return a list ['one', 'two'] when using this method.\n+\n+        :param name: the setting name\n+        :type name: string\n+\n+        :param default: the value to return if no setting is found\n+        :type default: any\n+        \"\"\"\n         value = self.get(name, default or [])\n         if isinstance(value, six.string_types):\n             value = value.split(',')\n         return list(value)\n \n     def getdict(self, name, default=None):\n+        \"\"\"\n+        Get a setting value as a dictionary. If the setting original type is a\n+        dictionary, a copy of it will be returned. If it is a string it will be\n+        evaluated as a JSON dictionary. In the case that it is a\n+        :class:`~scrapy.settings.BaseSettings` instance itself, it will be\n+        converted to a dictionary, containing all its current settings values\n+        as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,\n+        and losing all information about priority and mutability.\n+\n+        :param name: the setting name\n+        :type name: string\n+\n+        :param default: the value to return if no setting is found\n+        :type default: any\n+        \"\"\"\n         value = self.get(name, default or {})\n         if isinstance(value, six.string_types):\n             value = json.loads(value)\n@@ -118,12 +209,25 @@ class BaseSettings(MutableMapping):\n             return self[name]\n \n     def getpriority(self, name):\n+        \"\"\"\n+        Return the current numerical priority value of a setting, or ``None`` if\n+        the given ``name`` does not exist.\n+\n+        :param name: the setting name\n+        :type name: string\n+        \"\"\"\n         prio = None\n         if name in self:\n             prio = self.attributes[name].priority\n         return prio\n \n     def maxpriority(self):\n+        \"\"\"\n+        Return the numerical value of the highest priority present throughout\n+        all settings, or the numerical value for ``default`` from\n+        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings\n+        stored.\n+        \"\"\"\n         if len(self) > 0:\n             return max(self.getpriority(name) for name in self)\n         else:\n@@ -133,6 +237,23 @@ class BaseSettings(MutableMapping):\n         self.set(name, value)\n \n     def set(self, name, value, priority='project'):\n+        \"\"\"\n+        Store a key/value attribute with a given priority.\n+\n+        Settings should be populated *before* configuring the Crawler object\n+        (through the :meth:`~scrapy.crawler.Crawler.configure` method),\n+        otherwise they won't have any effect.\n+\n+        :param name: the setting name\n+        :type name: string\n+\n+        :param value: the value to associate with the setting\n+        :type value: any\n+\n+        :param priority: the priority of the setting. Should be a key of\n+            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n+        :type priority: string or int\n+        \"\"\"\n         self._assert_mutability()\n         priority = get_settings_priority(priority)\n         if name not in self:\n@@ -147,6 +268,20 @@ class BaseSettings(MutableMapping):\n         self.update(values, priority)\n \n     def setmodule(self, module, priority='project'):\n+        \"\"\"\n+        Store settings from a module with a given priority.\n+\n+        This is a helper function that calls\n+        :meth:`~scrapy.settings.BaseSettings.set` for every globally declared\n+        uppercase variable of ``module`` with the provided ``priority``.\n+\n+        :param module: the module or the path of the module\n+        :type module: module object or string\n+\n+        :param priority: the priority of the settings. Should be a key of\n+            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n+        :type priority: string or int\n+        \"\"\"\n         self._assert_mutability()\n         if isinstance(module, six.string_types):\n             module = import_module(module)\n@@ -155,6 +290,27 @@ class BaseSettings(MutableMapping):\n                 self.set(key, getattr(module, key), priority)\n \n     def update(self, values, priority='project'):\n+        \"\"\"\n+        Store key/value pairs with a given priority.\n+\n+        This is a helper function that calls\n+        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``\n+        with the provided ``priority``.\n+\n+        If ``values`` is a string, it is assumed to be JSON-encoded and parsed\n+        into a dict with ``json.loads()`` first. If it is a\n+        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities\n+        will be used and the ``priority`` parameter ignored. This allows\n+        inserting/updating settings with different priorities with a single\n+        command.\n+\n+        :param values: the settings names and values\n+        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`\n+\n+        :param priority: the priority of the settings. Should be a key of\n+            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n+        :type priority: string or int\n+        \"\"\"\n         self._assert_mutability()\n         if isinstance(values, six.string_types):\n             values = json.loads(values)\n@@ -181,12 +337,33 @@ class BaseSettings(MutableMapping):\n             raise TypeError(\"Trying to modify an immutable Settings object\")\n \n     def copy(self):\n+        \"\"\"\n+        Make a deep copy of current settings.\n+\n+        This method returns a new instance of the :class:`Settings` class,\n+        populated with the same values and their priorities.\n+\n+        Modifications to the new object won't be reflected on the original\n+        settings.\n+        \"\"\"\n         return copy.deepcopy(self)\n \n     def freeze(self):\n+        \"\"\"\n+        Disable further changes to the current settings.\n+\n+        After calling this method, the present state of the settings will become\n+        immutable. Trying to change values through the :meth:`~set` method and\n+        its variants won't be possible and will be alerted.\n+        \"\"\"\n         self.frozen = True\n \n     def frozencopy(self):\n+        \"\"\"\n+        Return an immutable copy of the current settings.\n+\n+        Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.\n+        \"\"\"\n         copy = self.copy()\n         copy.freeze()\n         return copy\n@@ -252,6 +429,15 @@ class _DictProxy(MutableMapping):\n \n \n class Settings(BaseSettings):\n+    \"\"\"\n+    This object stores Scrapy settings for the configuration of internal\n+    components, and can be used for any further customization.\n+\n+    It is a direct subclass and supports all methods of\n+    :class:`~scrapy.settings.BaseSettings`. Additionally, after instantiation\n+    of this class, the new object will have the global default settings\n+    described on :ref:`topics-settings-ref` already populated.\n+    \"\"\"\n \n     def __init__(self, values=None, priority='project'):\n         # Do not pass kwarg values here. We don't want to promote user-defined\n@@ -261,8 +447,7 @@ class Settings(BaseSettings):\n         self.setmodule(default_settings, 'default')\n         # Promote default dictionaries to BaseSettings instances for per-key\n         # priorities\n-        for name in self:\n-            val = self[name]\n+        for name, val in six.iteritems(self):\n             if isinstance(val, dict):\n                 self.set(name, BaseSettings(val, 'default'), 'default')\n         self.update(values, priority)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
