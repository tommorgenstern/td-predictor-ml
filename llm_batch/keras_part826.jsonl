{"custom_id": "keras#42798665067c1449d3e76b681f06343340098672", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 39 | Files Changed: 12 | Hunks: 26 | Methods Changed: 23 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 121 | Churn Cumulative: 2659 | Contributors (this commit): 7 | Commits (past 90d): 87 | Contributors (cumulative): 34 | DMM Complexity: 0.5\n\nDIFF:\n@@ -62,7 +62,7 @@ class DTypePolicy:\n             name = dtype_policy().name\n         self._name = name\n         self._compute_dtype, self._variable_dtype = self._parse_name(name)\n-        self._is_quantized = False\n+        self._quantization_mode = None\n \n     def _parse_name(self, name):\n         \"\"\"Parses a `DTypePolicy` name into a compute and variable dtype.\n@@ -137,9 +137,14 @@ class DTypePolicy:\n         return self._name\n \n     @property\n-    def is_quantized(self):\n-        \"\"\"Whether a quantized dtype policy.\"\"\"\n-        return self._is_quantized\n+    def quantization_mode(self):\n+        \"\"\"The quantization mode of this policy.\n+\n+        Returns:\n+            The quantization mode of this policy, as a string. If this policy is\n+            not quantized, it will return `None`.\n+        \"\"\"\n+        return self._quantization_mode\n \n     def convert_input(self, x, autocast, dtype):\n         \"\"\"Converts the input dtype based on `autocast` and `dtype`.\n@@ -218,16 +223,6 @@ class QuantizedDTypePolicy(DTypePolicy):\n         self._name = name\n         self._source_name = source_name\n         self._quantization_mode = mode\n-        self._is_quantized = True\n-\n-    @property\n-    def quantization_mode(self):\n-        \"\"\"The quantization mode of this policy.\n-\n-        Returns:\n-            The quantization mode of this policy, as a string.\n-        \"\"\"\n-        return self._quantization_mode\n \n     def __eq__(self, other):\n         if super().__eq__(other) is False:\n\n@@ -23,7 +23,7 @@ class DTypePolicyMap(DTypePolicy, MutableMapping):\n             config = super().get_config()\n             dtype_policy_map = dtype_policies.DTypePolicyMap()\n             for layer in self._flatten_layers():\n-                if layer.dtype_policy.is_quantized:\n+                if layer.dtype_policy.quantization_mode is not None:\n                     dtype_policy_map[layer.path] = layer.dtype_policy\n             if len(dtype_policy_map) > 0:\n                 config.update({\"dtype\": dtype_policy_map})\n@@ -85,10 +85,6 @@ class DTypePolicyMap(DTypePolicy, MutableMapping):\n         \"\"\"\n         return dtype_policies.get(self._default_policy)\n \n-    @property\n-    def is_quantized(self):\n-        return self.default_policy._is_quantized\n-\n     @property\n     def variable_dtype(self):\n         return self.default_policy.variable_dtype\n@@ -97,6 +93,10 @@ class DTypePolicyMap(DTypePolicy, MutableMapping):\n     def compute_dtype(self):\n         return self.default_policy.compute_dtype\n \n+    @property\n+    def quantization_mode(self):\n+        return self.default_policy.quantization_mode\n+\n     def __getitem__(self, key):\n         \"\"\"Retrieves the corresponding `DTypePolicy` by the string key.\n \n\n@@ -113,13 +113,13 @@ class FloatDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n         self.assertEqual(policy.variable_dtype, \"float32\")\n         self.assertEqual(policy.compute_dtype, \"float16\")\n         self.assertEqual(policy.name, \"mixed_float16\")\n-        self.assertFalse(policy.is_quantized)\n+        self.assertIsNone(policy.quantization_mode)\n \n         policy = FloatDTypePolicy(\"mixed_float16\")\n         self.assertEqual(policy.variable_dtype, \"float32\")\n         self.assertEqual(policy.compute_dtype, \"float16\")\n         self.assertEqual(policy.name, \"mixed_float16\")\n-        self.assertFalse(policy.is_quantized)\n+        self.assertIsNone(policy.quantization_mode)\n \n     def test_properties_uint8(self):\n         \"\"\"Test properties for 'uint8'.\"\"\"\n@@ -365,7 +365,7 @@ class QuantizedDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n         self.assertEqual(policy.variable_dtype, \"float32\")\n         self.assertEqual(policy.compute_dtype, \"bfloat16\")\n         self.assertEqual(policy.name, \"int8_from_mixed_bfloat16\")\n-        self.assertTrue(policy.is_quantized)\n+        self.assertEqual(policy.quantization_mode, \"int8\")\n \n         # Test float8\n         policy = QuantizedFloat8DTypePolicy(\n@@ -374,7 +374,7 @@ class QuantizedDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n         self.assertEqual(policy.variable_dtype, \"float32\")\n         self.assertEqual(policy.compute_dtype, \"bfloat16\")\n         self.assertEqual(policy.name, \"float8_from_mixed_bfloat16\")\n-        self.assertTrue(policy.is_quantized)\n+        self.assertEqual(policy.quantization_mode, \"float8\")\n         self.assertEqual(policy.amax_history_length, 1024)\n \n         # Test float8 with amax_history_length\n\n@@ -559,7 +559,7 @@ class Dense(Layer):\n         self._tracker.lock()\n \n         # Set new dtype policy\n-        if not self.dtype_policy.is_quantized:\n+        if self.dtype_policy.quantization_mode is None:\n             policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n             self.dtype_policy = policy\n \n@@ -567,7 +567,7 @@ class Dense(Layer):\n         gc.collect()\n \n     def _get_kernel_with_merged_lora(self):\n-        if self.dtype_policy.is_quantized:\n+        if self.dtype_policy.quantization_mode is not None:\n             kernel_value = self._kernel\n             kernel_scale = self.kernel_scale\n             if self.lora_enabled:\n\n@@ -475,7 +475,7 @@ class DenseTest(testing.TestCase, parameterized.TestCase):\n             NotImplementedError, \"Invalid quantization mode.\"\n         ):\n             # Explicitly set quantization_mode\n-            layer._dtype_policy.quantization_mode = mode\n+            layer._dtype_policy._quantization_mode = mode\n             layer.quantized_call(x)\n         self.assertEqual(layer.dtype_policy, original_dtype_policy)\n \n\n@@ -697,7 +697,7 @@ class EinsumDense(Layer):\n         self._tracker.lock()\n \n         # Set new dtype policy\n-        if not self.dtype_policy.is_quantized:\n+        if self.dtype_policy.quantization_mode is None:\n             policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n             self.dtype_policy = policy\n \n@@ -705,7 +705,7 @@ class EinsumDense(Layer):\n         gc.collect()\n \n     def _get_kernel_with_merged_lora(self):\n-        if self.dtype_policy.is_quantized:\n+        if self.dtype_policy.quantization_mode is not None:\n             kernel_value = self._kernel\n             kernel_scale = self.kernel_scale\n             if self.lora_enabled:\n\n@@ -599,7 +599,7 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             NotImplementedError, \"Invalid quantization mode.\"\n         ):\n             # Explicitly set quantization_mode\n-            layer._dtype_policy.quantization_mode = mode\n+            layer._dtype_policy._quantization_mode = mode\n             layer.quantized_call(x)\n         self.assertEqual(layer.dtype_policy, original_dtype_policy)\n \n\n@@ -376,7 +376,7 @@ class Embedding(Layer):\n         self._tracker.lock()\n \n         # Set new dtype policy\n-        if not self.dtype_policy.is_quantized:\n+        if self.dtype_policy.quantization_mode is None:\n             policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n             self.dtype_policy = policy\n \n@@ -384,7 +384,7 @@ class Embedding(Layer):\n         gc.collect()\n \n     def _get_embeddings_with_merged_lora(self):\n-        if self.dtype_policy.is_quantized:\n+        if self.dtype_policy.quantization_mode is not None:\n             embeddings_value = self._embeddings\n             embeddings_scale = self.embeddings_scale\n             if self.lora_enabled:\n\n@@ -367,7 +367,7 @@ class EmbeddingTest(test_case.TestCase, parameterized.TestCase):\n             NotImplementedError, \"Invalid quantization mode.\"\n         ):\n             # Explicitly set quantization_mode\n-            layer._dtype_policy.quantization_mode = mode\n+            layer._dtype_policy._quantization_mode = mode\n             layer.quantized_call(x)\n         self.assertEqual(layer.dtype_policy, original_dtype_policy)\n \n\n@@ -701,8 +701,8 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n             self._dtype_policy[self.path] = policy\n         else:\n             self._dtype_policy = policy\n-        if policy.is_quantized and not getattr(self, \"_is_quantized\", False):\n-            if self.built:\n+        if policy.quantization_mode is not None:\n+            if self.built and not getattr(self, \"_is_quantized\", False):\n                 self.quantize(policy.quantization_mode)\n \n     @property\n@@ -730,12 +730,12 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n \n     @property\n     def quantization_mode(self):\n-        \"\"\"The quantization_mode, or None if the layer is not quantized.\"\"\"\n+        \"\"\"The quantization mode of this layer, `None` if not quantized.\"\"\"\n         if isinstance(self._dtype_policy, DTypePolicyMap) and self.path:\n             policy = self._dtype_policy[self.path]\n         else:\n             policy = self._dtype_policy\n-        return policy.quantization_mode if policy.is_quantized else None\n+        return policy.quantization_mode\n \n     @property\n     def input_dtype(self):\n@@ -1025,7 +1025,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         with backend.StatelessScope(\n             state_mapping=mapping, collect_losses=return_losses\n         ) as scope:\n-            if self.dtype_policy.is_quantized:\n+            if self.dtype_policy.quantization_mode is not None:\n                 outputs = self.quantized_call(*args, **kwargs)\n             else:\n                 outputs = self.call(*args, **kwargs)\n\n@@ -994,6 +994,46 @@ class LayerTest(testing.TestCase):\n         reloaded = pickle.loads(pickle.dumps(layer))\n         self.assertEqual(layer.get_config(), reloaded.get_config())\n \n+    def test_serialize_dtype(self):\n+        assertIsNone = self.assertIsNone\n+        assertIsNotNone = self.assertIsNotNone\n+\n+        class AssertionDense(layers.Dense):\n+            def __init__(self, *args, **kwargs):\n+                dtype = kwargs[\"dtype\"]\n+                if isinstance(dtype, str):\n+                    # `dtype` is a plain string, it should be the `name` from a\n+                    # `FloatDTypePolicy`\n+                    dtype = dtype_policies.get(dtype)\n+                    assertIsNone(dtype.quantization_mode)\n+                else:\n+                    # `dtype` is a DTypePolicy instance, it should be an\n+                    # instance of `QuantizedDTypePolicy`\n+                    assertIsNotNone(dtype.quantization_mode)\n+                super().__init__(*args, **kwargs)\n+\n+        # Test floating dtype serialization\n+        layer = layers.Dense(2, dtype=\"bfloat16\")\n+        config = layer.get_config()\n+        self.assertIn(\"dtype\", config)\n+        self.assertEqual(\n+            config[\"dtype\"],\n+            dtype_policies.serialize(\n+                dtype_policies.FloatDTypePolicy(\"bfloat16\")\n+            ),\n+        )\n+        AssertionDense.from_config(config)  # Assertion inside\n+\n+        # Test quantized dtype serialization\n+        layer = layers.Dense(2, dtype=\"int8_from_bfloat16\")\n+        config = layer.get_config()\n+        self.assertIn(\"dtype\", config)\n+        self.assertEqual(\n+            config[\"dtype\"],\n+            dtype_policies.serialize(dtype_policies.get(\"int8_from_bfloat16\")),\n+        )\n+        AssertionDense.from_config(config)  # Assertion inside\n+\n     def test_serialize_activity_regularizer(self):\n         layer = layers.Dense(2, activity_regularizer=\"l2\")\n         config = layer.get_config()\n\n@@ -107,7 +107,7 @@ class Operation:\n         if dtype is not None and isinstance(dtype, dtype_policies.DTypePolicy):\n             # For backward compatibility, we use a str (`name`) for\n             # `FloatDTypePolicy`\n-            if not dtype.is_quantized:\n+            if dtype.quantization_mode is None:\n                 kwargs[\"dtype\"] = dtype.name\n             # Otherwise, use `dtype_policies.serialize`\n             else:\n@@ -221,7 +221,15 @@ class Operation:\n         # directly interact with the instance of `DTypePolicy`.\n         if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n             config = config.copy()\n-            config[\"dtype\"] = dtype_policies.deserialize(config[\"dtype\"])\n+            policy = dtype_policies.deserialize(config[\"dtype\"])\n+            if (\n+                not isinstance(policy, dtype_policies.DTypePolicyMap)\n+                and policy.quantization_mode is None\n+            ):\n+                # For backward compatibility, we use a str (`name`) for\n+                # `FloatDTypePolicy`\n+                policy = policy.name\n+            config[\"dtype\"] = policy\n         try:\n             return cls(**config)\n         except Exception as e:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fbe03aa0cb71c44f47c9be04772d08684846cf9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 26 | Churn Cumulative: 669 | Contributors (this commit): 8 | Commits (past 90d): 29 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,6 +13,7 @@ from keras.src.ops.core import cond\n from keras.src.ops.core import convert_to_numpy\n from keras.src.ops.core import convert_to_tensor\n from keras.src.ops.core import custom_gradient\n+from keras.src.ops.core import dtype\n from keras.src.ops.core import fori_loop\n from keras.src.ops.core import is_tensor\n from keras.src.ops.core import scan\n\n@@ -13,6 +13,7 @@ from keras.src.ops.core import cond\n from keras.src.ops.core import convert_to_numpy\n from keras.src.ops.core import convert_to_tensor\n from keras.src.ops.core import custom_gradient\n+from keras.src.ops.core import dtype\n from keras.src.ops.core import fori_loop\n from keras.src.ops.core import is_tensor\n from keras.src.ops.core import scan\n\n@@ -588,7 +588,7 @@ def shape(x):\n \n     Example:\n \n-    >>> x = keras.zeros((8, 12))\n+    >>> x = keras.ops.zeros((8, 12))\n     >>> keras.ops.shape(x)\n     (8, 12)\n     \"\"\"\n@@ -597,6 +597,28 @@ def shape(x):\n     return backend.core.shape(x)\n \n \n+@keras_export(\"keras.ops.dtype\")\n+def dtype(x):\n+    \"\"\"Gets the dtype of the tensor input as a standardized string.\n+    Note that due to the standardization, the dtype will not compare equal\n+    to the backend-specific version of the dtype.\n+\n+    Args:\n+        x: A tensor. This function will try to access the `dtype` attribute of\n+            the input tensor.\n+\n+    Returns: A string indicating the dtype of the input tensor.\n+\n+    Example:\n+\n+    >>> x = keras.ops.zeros((8, 12))\n+    >>> keras.ops.dtype(x)\n+    'float32'\n+\n+    \"\"\"\n+    return backend.standardize_dtype(x.dtype)\n+\n+\n class Cast(Operation):\n     def __init__(self, dtype):\n         super().__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5783bc71ed2a76779b28482301d09ba75b8f2449", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 207 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -599,7 +599,8 @@ def shape(x):\n \n @keras_export(\"keras.ops.dtype\")\n def dtype(x):\n-    \"\"\"Gets the dtype of the tensor input as a standardized string.\n+    \"\"\"Return the dtype of the tensor input as a standardized string.\n+\n     Note that due to the standardization, the dtype will not compare equal\n     to the backend-specific version of the dtype.\n \n@@ -607,7 +608,8 @@ def dtype(x):\n         x: A tensor. This function will try to access the `dtype` attribute of\n             the input tensor.\n \n-    Returns: A string indicating the dtype of the input tensor.\n+    Returns:\n+        A string indicating the dtype of the input tensor, e.g. `\"float32\"`.\n \n     Example:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f3fa0e06ef67f9955d3d6b65f33eb0f4c11dbb6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 57 | Churn Cumulative: 227 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -338,8 +338,15 @@ class ExportArchive:\n                 # 5. Wrap in a `tf.function`\n                 def stateless_fn(variables, *args, **kwargs):\n                     state_mapping = zip(self._backend_variables, variables)\n-                    with StatelessScope(state_mapping=state_mapping):\n-                        return fn(*args, **kwargs)\n+                    with StatelessScope(state_mapping=state_mapping) as scope:\n+                        output = fn(*args, **kwargs)\n+\n+                    # Gather updated non-trainable variables\n+                    non_trainable_variables = []\n+                    for var in self._backend_non_trainable_variables:\n+                        new_value = scope.get_current_value(var)\n+                        non_trainable_variables.append(new_value)\n+                    return output, non_trainable_variables\n \n                 jax2tf_stateless_fn = self._convert_jax2tf_function(\n                     stateless_fn,\n@@ -348,12 +355,18 @@ class ExportArchive:\n                 )\n \n                 def stateful_fn(*args, **kwargs):\n-                    return jax2tf_stateless_fn(\n+                    output, non_trainable_variables = jax2tf_stateless_fn(\n                         # Change the trackable `ListWrapper` to a plain `list`\n                         list(self._tf_trackable.variables),\n                         *args,\n                         **kwargs,\n                     )\n+                    for var, new_value in zip(\n+                        self._tf_trackable.non_trainable_variables,\n+                        non_trainable_variables,\n+                    ):\n+                        var.assign(new_value)\n+                    return output\n \n                 # Note: we truncate the number of parameters to what is\n                 # specified by `input_signature`.\n\n@@ -93,7 +93,43 @@ class ExportArchiveTest(testing.TestCase, parameterized.TestCase):\n         revived_model = tf.saved_model.load(temp_filepath)\n         self.assertEqual(ref_output.shape, revived_model.serve(ref_input).shape)\n         # Test with a different batch size\n-        revived_model.serve(tf.random.normal((6, 10)))\n+        input = tf.random.normal((6, 10))\n+        output1 = revived_model.serve(input)\n+        output2 = revived_model.serve(input)\n+        # Verify RNG seeding works and produces random outputs\n+        self.assertNotAllClose(output1, output2)\n+\n+    @parameterized.named_parameters(\n+        named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n+    )\n+    def test_model_with_non_trainable_state_export(self, model_type):\n+\n+        class StateLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.counter = self.add_variable(\n+                    (), \"zeros\", \"int32\", trainable=False\n+                )\n+\n+            def call(self, inputs):\n+                self.counter.assign_add(1)\n+                return ops.array(inputs), ops.array(self.counter.value)\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        model = get_model(model_type, layer_list=[StateLayer()])\n+        model(tf.random.normal((3, 10)))\n+\n+        export_lib.export_model(model, temp_filepath)\n+        revived_model = tf.saved_model.load(temp_filepath)\n+\n+        # The non-trainable counter is expected to increment\n+        input = tf.random.normal((6, 10))\n+        output1, counter1 = revived_model.serve(input)\n+        self.assertAllClose(output1, input)\n+        self.assertAllClose(counter1, 2)\n+        output2, counter2 = revived_model.serve(input)\n+        self.assertAllClose(output2, input)\n+        self.assertAllClose(counter2, 3)\n \n     @parameterized.named_parameters(\n         named_product(struct_type=[\"tuple\", \"array\", \"dict\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#596d5ba420dd2865d576db2c5f860d9d77db8054", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 26 | Churn Cumulative: 183 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 0.15384615384615385\n\nDIFF:\n@@ -131,6 +131,21 @@ class ExportArchiveTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(output2, input)\n         self.assertAllClose(counter2, 3)\n \n+    @parameterized.named_parameters(\n+        named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n+    )\n+    def test_model_with_tf_data_layer(self, model_type):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        model = get_model(model_type, layer_list=[layers.Rescaling(scale=2.0)])\n+        ref_input = tf.random.normal((3, 10))\n+        ref_output = model(ref_input)\n+\n+        export_lib.export_model(model, temp_filepath)\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertAllClose(ref_output, revived_model.serve(ref_input))\n+        # Test with a different batch size\n+        revived_model.serve(tf.random.normal((6, 10)))\n+\n     @parameterized.named_parameters(\n         named_product(struct_type=[\"tuple\", \"array\", \"dict\"])\n     )\n\n@@ -3,6 +3,7 @@ from keras.src import tree\n from keras.src.layers.layer import Layer\n from keras.src.random.seed_generator import SeedGenerator\n from keras.src.utils import backend_utils\n+from keras.src.utils import jax_utils\n from keras.src.utils import tracking\n \n \n@@ -20,8 +21,11 @@ class TFDataLayer(Layer):\n         self._allow_non_tensor_positional_args = True\n \n     def __call__(self, inputs, **kwargs):\n-        if backend_utils.in_tf_graph() and not isinstance(\n-            inputs, keras.KerasTensor\n+        sample_input = tree.flatten(inputs)[0]\n+        if (\n+            not isinstance(sample_input, keras.KerasTensor)\n+            and backend_utils.in_tf_graph()\n+            and not jax_utils.is_in_jax_tracing_scope(sample_input)\n         ):\n             # We're in a TF graph, e.g. a tf.data pipeline.\n             self.backend.set_backend(\"tensorflow\")\n\n@@ -5,6 +5,7 @@ def is_in_jax_tracing_scope(x=None):\n     if backend.backend() == \"jax\":\n         if x is None:\n             x = backend.numpy.ones(())\n-        if x.__class__.__name__ == \"DynamicJaxprTracer\":\n+        for c in x.__class__.__mro__:\n+            if c.__name__ == \"Tracer\" and c.__module__.startswith(\"jax\"):\n                 return True\n     return False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8a7f28cc80800ccd55123eda63a3a13082a0595", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 148 | Lines Deleted: 2 | Files Changed: 8 | Hunks: 14 | Methods Changed: 19 | Complexity Δ (Sum/Max): 21/8 | Churn Δ: 150 | Churn Cumulative: 1452 | Contributors (this commit): 10 | Commits (past 90d): 65 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,7 @@ from keras.src.ops.core import custom_gradient\n from keras.src.ops.core import dtype\n from keras.src.ops.core import fori_loop\n from keras.src.ops.core import is_tensor\n+from keras.src.ops.core import map\n from keras.src.ops.core import scan\n from keras.src.ops.core import scatter\n from keras.src.ops.core import scatter_update\n\n@@ -16,6 +16,7 @@ from keras.src.ops.core import custom_gradient\n from keras.src.ops.core import dtype\n from keras.src.ops.core import fori_loop\n from keras.src.ops.core import is_tensor\n+from keras.src.ops.core import map\n from keras.src.ops.core import scan\n from keras.src.ops.core import scatter\n from keras.src.ops.core import scatter_update\n\n@@ -253,6 +253,10 @@ def vectorized_map(function, elements):\n     return jax.vmap(function)(elements)\n \n \n+def map(f, xs):\n+    return jax.lax.map(f, xs)\n+\n+\n def scan(f, init, xs=None, length=None, reverse=False, unroll=1):\n     if not isinstance(unroll, bool):\n         if not isinstance(unroll, int) or unroll < 1:\n\n@@ -1,3 +1,4 @@\n+import builtins\n import warnings\n \n import numpy as np\n@@ -91,7 +92,9 @@ def compute_output_spec(fn, *args, **kwargs):\n                 return None in x.shape\n             return False\n \n-        none_in_shape = any(map(has_none_shape, tree.flatten((args, kwargs))))\n+        none_in_shape = any(\n+            builtins.map(has_none_shape, tree.flatten((args, kwargs)))\n+        )\n \n         def convert_keras_tensor_to_numpy(x, fill_value=None):\n             if isinstance(x, KerasTensor):\n@@ -142,6 +145,14 @@ def compute_output_spec(fn, *args, **kwargs):\n     return output_spec\n \n \n+def map(f, xs):\n+    def g(_, x):\n+        return (), f(x)\n+\n+    _, ys = scan(g, (), xs)\n+    return ys\n+\n+\n def scan(f, init, xs=None, length=None, reverse=False, unroll=1):\n     # Ref: jax.lax.scan\n     if not callable(f):\n\n@@ -218,6 +218,17 @@ def vectorized_map(function, elements):\n     return tf.vectorized_map(function, elements)\n \n \n+def map(f, xs):\n+    xs = tree.map_structure(convert_to_tensor, xs)\n+\n+    def get_fn_output_signature(x):\n+        out = f(x)\n+        return tree.map_structure(tf.TensorSpec.from_tensor, out)\n+\n+    fn_output_signature = get_fn_output_signature(xs[0])\n+    return tf.map_fn(f, xs, fn_output_signature=fn_output_signature)\n+\n+\n def scan(f, init, xs=None, length=None, reverse=False, unroll=1):\n     # We have reimplemented `scan` to match the behavior of `jax.lax.scan`\n     # Ref: tf.scan, jax.lax.scan\n\n@@ -1,3 +1,4 @@\n+import builtins\n import contextlib\n \n import ml_dtypes\n@@ -305,7 +306,9 @@ def compute_output_spec(fn, *args, **kwargs):\n     with StatelessScope(), torch.no_grad():\n         outputs = symbolic_call(fn, args, kwargs, fill_value=83)\n \n-        none_in_shape = any(map(has_none_shape, tree.flatten((args, kwargs))))\n+        none_in_shape = any(\n+            builtins.map(has_none_shape, tree.flatten((args, kwargs)))\n+        )\n         if none_in_shape:\n             outputs_1 = outputs\n             outputs_2 = symbolic_call(fn, args, kwargs, fill_value=89)\n@@ -340,6 +343,14 @@ def vectorized_map(function, elements):\n     return torch.vmap(function)(elements)\n \n \n+def map(f, xs):\n+    def g(_, x):\n+        return (), f(x)\n+\n+    _, ys = scan(g, (), xs)\n+    return ys\n+\n+\n def scan(f, init, xs=None, length=None, reverse=False, unroll=1):\n     # Ref: jax.lax.scan\n     if not callable(f):\n\n@@ -26,6 +26,71 @@ from keras.src.ops.operation import Operation\n from keras.src.utils import traceback_utils\n \n \n+class Map(Operation):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, f, xs):\n+        return backend.core.map(f, xs)\n+\n+    def compute_output_spec(self, f, xs):\n+        x = xs[0]\n+        n = xs.shape[0]\n+        y = backend.compute_output_spec(f, x)\n+\n+        def append_batch_axis(x):\n+            x.shape = (n,) + x.shape\n+            return x\n+\n+        y = tree.map_structure(append_batch_axis, y)\n+        return y\n+\n+\n+@keras_export(\"keras.ops.map\")\n+def map(f, xs):\n+    \"\"\"Map a function over leading array axes.\n+\n+    Like Python’s builtin map, except inputs and outputs are in the form of\n+    stacked arrays. Consider using the `vectorized_map()` transform instead,\n+    unless you need to apply a function element by element for reduced memory\n+    usage or heterogeneous computation with other control flow primitives.\n+\n+    When `xs` is an array type, the semantics of `map()` are given by this\n+    Python implementation:\n+\n+    ```python\n+    def map(f, xs):\n+        return np.stack([f(x) for x in xs])\n+    ```\n+\n+    Args:\n+        f: Callable defines the function to apply element-wise over the first\n+            axis or axes of `xs`.\n+        xs: Values over which to map along the leading axis.\n+\n+    Returns:\n+        Mapped values.\n+\n+    Examples:\n+\n+    >>> f = lambda x: x**2\n+    >>> xs = keras.ops.arange(10)\n+    >>> ys = keras.ops.map(f, xs)\n+    >>> ys\n+    [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n+\n+    >>> f = lambda x: {\"y1\": x**2, \"y2\": x * 10}  # Can have nested outputs\n+    >>> ys = keras.ops.map(f, xs)\n+    >>> ys[\"y1\"]\n+    [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n+    >>> ys[\"y2\"]\n+    [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n+    \"\"\"\n+    if any_symbolic_tensors((xs,)):\n+        return Map().symbolic_call(f, xs)\n+    return backend.core.map(f, xs)\n+\n+\n class Scan(Operation):\n     def __init__(self, reverse=False, unroll=1):\n         super().__init__()\n\n@@ -19,6 +19,23 @@ from keras.src.ops import core\n \n \n class CoreOpsStaticShapeTest(testing.TestCase):\n+    def test_map(self):\n+        def f(x):\n+            return x**2\n+\n+        xs = KerasTensor((6,))\n+        ys = core.map(f, xs)\n+        self.assertEqual(ys.shape, (6,))\n+\n+        # Test nested output\n+        def f2(x):\n+            return {\"a\": x**2, \"b\": x * 10}\n+\n+        xs = KerasTensor((6,))\n+        ys = core.map(f2, xs)\n+        self.assertEqual(ys[\"a\"].shape, (6,))\n+        self.assertEqual(ys[\"b\"].shape, (6,))\n+\n     def test_scan(self):\n         def f(carry, xs):\n             xs = xs + carry\n@@ -113,6 +130,22 @@ class CoreOpsStaticShapeTest(testing.TestCase):\n \n \n class CoreOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    def test_map(self):\n+        def f(x):\n+            return x**2\n+\n+        xs = np.arange(10)\n+        self.assertAllClose(ops.map(f, xs), xs**2)\n+\n+        # Test nested output\n+        def f2(x):\n+            return {\"a\": x**2, \"b\": x * 10}\n+\n+        xs = np.random.rand(2, 3, 4).astype(\"float32\")\n+        outputs = ops.map(f2, xs)\n+        self.assertAllClose(outputs[\"a\"], xs**2)\n+        self.assertAllClose(outputs[\"b\"], xs * 10)\n+\n     def test_scan(self):\n         # Test cumsum\n         def cumsum(carry, xs):\n@@ -756,6 +789,15 @@ class CoreOpsDtypeTest(testing.TestCase, parameterized.TestCase):\n \n \n class CoreOpsCallsTests(testing.TestCase):\n+    def test_map_basic_call(self):\n+        def f(x):\n+            return x**2\n+\n+        xs = np.arange(10)\n+        map_op = core.Map()\n+        ys = map_op.call(f, xs)\n+        self.assertAllClose(ys, xs**2)\n+\n     def test_scan_basic_call(self):\n         def cumsum(carry, xs):\n             carry = carry + xs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#75c3f8e6c2f36ac197f81a906655ad3e9bdd650c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 959 | Contributors (this commit): 8 | Commits (past 90d): 37 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3886,7 +3886,9 @@ class Nonzero(Operation):\n         return backend.numpy.nonzero(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor([None] * len(x.shape), dtype=\"int32\")\n+        return tuple(\n+            [KerasTensor((None,), dtype=\"int32\") for _ in range(len(x.shape))]\n+        )\n \n \n @keras_export([\"keras.ops.nonzero\", \"keras.ops.numpy.nonzero\"])\n\n@@ -1317,7 +1317,11 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n \n     def test_nonzero(self):\n         x = KerasTensor((None, 5, 6))\n-        self.assertEqual(knp.nonzero(x).shape, (None, None, None))\n+        result = knp.nonzero(x)\n+        self.assertLen(result, 3)\n+        self.assertEqual(result[0].shape, (None,))\n+        self.assertEqual(result[1].shape, (None,))\n+        self.assertEqual(result[2].shape, (None,))\n \n     def test_ones_like(self):\n         x = KerasTensor((None, 3))\n@@ -3790,7 +3794,7 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(knp.Ndim()(x), np.ndim(x))\n \n     def test_nonzero(self):\n-        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        x = np.array([[0, 0, 3], [3, 0, 0]])\n         self.assertAllClose(knp.nonzero(x), np.nonzero(x))\n         self.assertAllClose(knp.Nonzero()(x), np.nonzero(x))\n \n@@ -7119,8 +7123,8 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n     def test_nonzero(self, dtype):\n         import jax.numpy as jnp\n \n-        x = knp.ones((1,), dtype=dtype)\n-        x_jax = jnp.ones((1,), dtype=dtype)\n+        x = knp.zeros((1,), dtype=dtype)\n+        x_jax = jnp.zeros((1,), dtype=dtype)\n         expected_dtype = standardize_dtype(jnp.nonzero(x_jax)[0].dtype)\n \n         self.assertEqual(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d78ec26c9bdc6e23c9fa217936d15fa16c5b8f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 298 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -164,7 +164,10 @@ class DTypePolicy:\n                 x.dtype = dtype\n             return x\n         elif hasattr(x, \"__array__\"):\n+            try:\n                 x = backend.convert_to_tensor(x)\n+            except TypeError:\n+                x = backend.convert_to_tensor(x, dtype=dtype)\n             if self._should_cast(x, autocast, dtype):\n                 x = backend.cast(x, dtype=dtype)\n             return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5d308752bc98017c47e6eb4cd29f1e61bc43796", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 107 | Lines Deleted: 356 | Files Changed: 2 | Hunks: 31 | Methods Changed: 13 | Complexity Δ (Sum/Max): -22/0 | Churn Δ: 463 | Churn Cumulative: 479 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.13846153846153847\n\nDIFF:\n@@ -6,6 +6,7 @@ from multiprocessing.pool import ThreadPool\n \n import numpy as np\n \n+from keras.src import tree\n from keras.src.api_export import keras_export\n from keras.src.utils import io_utils\n from keras.src.utils.module_utils import tensorflow as tf\n@@ -137,15 +138,6 @@ def _convert_dataset_to_list(\n         data_size_warning_flag,\n         start_time,\n     ):\n-        if dataset_type_spec in [tuple, list]:\n-            # The try-except here is for NumPy 1.24 compatibility, see:\n-            # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n-            try:\n-                arr = np.array(sample)\n-            except ValueError:\n-                arr = np.array(sample, dtype=object)\n-            dataset_as_list.append(arr)\n-        else:\n         dataset_as_list.append(sample)\n \n     return dataset_as_list\n@@ -169,22 +161,22 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                 \"Please provide a non-empty list of arrays.\"\n             )\n \n-        if _get_type_spec(dataset[0]) is np.ndarray:\n-            expected_shape = dataset[0].shape\n+        expected_shape = None\n         for i, element in enumerate(dataset):\n-                if np.array(element).shape[0] != expected_shape[0]:\n-                    raise ValueError(\n-                        \"Received a list of NumPy arrays with different \"\n-                        f\"lengths. Mismatch found at index {i}, \"\n-                        f\"Expected shape={expected_shape} \"\n-                        f\"Received shape={np.array(element).shape}.\"\n-                        \"Please provide a list of NumPy arrays with \"\n-                        \"the same length.\"\n-                    )\n-        else:\n+            if not isinstance(element, np.ndarray):\n                 raise ValueError(\n                     \"Expected a list of `numpy.ndarray` objects,\"\n-                f\"Received: {type(dataset[0])}\"\n+                    f\"Received: {type(element)} at index {i}.\"\n+                )\n+            if expected_shape is None:\n+                expected_shape = element.shape\n+            elif element.shape[0] != expected_shape[0]:\n+                raise ValueError(\n+                    \"Received a list of NumPy arrays with different lengths.\"\n+                    f\"Mismatch found at index {i}, \"\n+                    f\"Expected shape={expected_shape} \"\n+                    f\"Received shape={np.array(element).shape}.\"\n+                    \"Please provide a list of NumPy arrays of the same length.\"\n                 )\n \n         return iter(zip(*dataset))\n@@ -195,22 +187,22 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                 \"Please provide a non-empty tuple of arrays.\"\n             )\n \n-        if _get_type_spec(dataset[0]) is np.ndarray:\n-            expected_shape = dataset[0].shape\n+        expected_shape = None\n         for i, element in enumerate(dataset):\n-                if np.array(element).shape[0] != expected_shape[0]:\n-                    raise ValueError(\n-                        \"Received a tuple of NumPy arrays with different \"\n-                        f\"lengths. Mismatch found at index {i}, \"\n-                        f\"Expected shape={expected_shape} \"\n-                        f\"Received shape={np.array(element).shape}.\"\n-                        \"Please provide a tuple of NumPy arrays with \"\n-                        \"the same length.\"\n-                    )\n-        else:\n+            if not isinstance(element, np.ndarray):\n                 raise ValueError(\n                     \"Expected a tuple of `numpy.ndarray` objects,\"\n-                f\"Received: {type(dataset[0])}\"\n+                    f\"Received: {type(element)} at index {i}.\"\n+                )\n+            if expected_shape is None:\n+                expected_shape = element.shape\n+            elif element.shape[0] != expected_shape[0]:\n+                raise ValueError(\n+                    \"Received a tuple of NumPy arrays with different lengths.\"\n+                    f\"Mismatch found at index {i}, \"\n+                    f\"Expected shape={expected_shape} \"\n+                    f\"Received shape={np.array(element).shape}.\"\n+                    \"Please provide a tuple of NumPy arrays of the same length.\"\n                 )\n \n         return iter(zip(*dataset))\n@@ -436,23 +428,24 @@ def _restore_dataset_from_list(\n     dataset_as_list, dataset_type_spec, original_dataset\n ):\n     \"\"\"Restore the dataset from the list of arrays.\"\"\"\n-    if dataset_type_spec in [tuple, list]:\n-        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n-    elif dataset_type_spec == tf.data.Dataset:\n-        if isinstance(original_dataset.element_spec, dict):\n-            restored_dataset = {}\n-            for d in dataset_as_list:\n-                for k, v in d.items():\n-                    if k not in restored_dataset:\n-                        restored_dataset[k] = [v]\n-                    else:\n-                        restored_dataset[k].append(v)\n-            return restored_dataset\n-        else:\n-            return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n+    if dataset_type_spec in [tuple, list, tf.data.Dataset] or is_torch_dataset(\n+        original_dataset\n+    ):\n+        # Save structure by taking the first element.\n+        element_spec = dataset_as_list[0]\n+        # Flatten each element.\n+        dataset_as_list = [tree.flatten(sample) for sample in dataset_as_list]\n+        # Combine respective elements at all indices.\n+        dataset_as_list = [np.array(sample) for sample in zip(*dataset_as_list)]\n+        # Recreate the original structure of elements.\n+        dataset_as_list = tree.pack_sequence_as(element_spec, dataset_as_list)\n+        # Turn lists to tuples as tf.data will fail on lists.\n+        return tree.traverse(\n+            lambda x: tuple(x) if isinstance(x, list) else x,\n+            dataset_as_list,\n+            top_down=False,\n+        )\n \n-    elif is_torch_dataset(original_dataset):\n-        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n     return dataset_as_list\n \n \n@@ -477,14 +470,12 @@ def _get_type_spec(dataset):\n         return list\n     elif isinstance(dataset, np.ndarray):\n         return np.ndarray\n-    elif isinstance(dataset, dict):\n-        return dict\n     elif isinstance(dataset, tf.data.Dataset):\n         return tf.data.Dataset\n     elif is_torch_dataset(dataset):\n-        from torch.utils.data import Dataset as torchDataset\n+        from torch.utils.data import Dataset as TorchDataset\n \n-        return torchDataset\n+        return TorchDataset\n     else:\n         return None\n \n\n@@ -1,334 +1,94 @@\n+import itertools\n+\n import numpy as np\n+from absl.testing import parameterized\n+from torch.utils.data import Dataset as TorchDataset\n \n from keras.src.testing import test_case\n+from keras.src.testing.test_utils import named_product\n from keras.src.utils.dataset_utils import split_dataset\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n-class DatasetUtilsTest(test_case.TestCase):\n-    def test_split_dataset_list(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = [\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        ]\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-\n-    def test_split_dataset_tuple(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        dataset = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        dataset_left, dataset_right = split_dataset(\n-            dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-\n-    def test_split_dataset_tensorflow(self):\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n-        dataset_left, dataset_right = split_dataset(\n-            tf_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            int(dataset_left.cardinality()), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            int(dataset_right.cardinality()), int(n_sample * right_size)\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n-        )\n-\n-    def test_split_dataset_torch(self):\n-        # sample torch dataset class\n-        from torch.utils.data import Dataset as torchDataset\n-\n-        class Dataset(torchDataset):\n-            \"Characterizes a dataset for PyTorch\"\n+class MyTorchDataset(TorchDataset):\n \n     def __init__(self, x, y):\n-                \"Initialization\"\n         self.x = x\n         self.y = y\n \n     def __len__(self):\n-                \"Denotes the total number of samples\"\n         return len(self.x)\n \n     def __getitem__(self, index):\n-                \"Generates one sample of data\"\n         return self.x[index], self.y[index]\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n+\n+class DatasetUtilsTest(test_case.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        named_product(\n+            dataset_type=[\"list\", \"tuple\", \"tensorflow\", \"torch\"],\n+            features_shape=[(2,), (100, 2), (10, 10, 2)],\n         )\n-        torch_dataset = Dataset(features, labels)\n+    )\n+    def test_split_dataset(self, dataset_type, features_shape):\n+        n_sample, left_size, right_size = 100, 0.2, 0.8\n+        features = np.random.sample((n_sample,) + features_shape)\n+        labels = np.random.sample((n_sample, 1))\n+\n+        if dataset_type == \"list\":\n+            dataset = [features, labels]\n+        elif dataset_type == \"tuple\":\n+            dataset = (features, labels)\n+        elif dataset_type == \"tensorflow\":\n+            dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n+        elif dataset_type == \"torch\":\n+            dataset = MyTorchDataset(features, labels)\n+\n         dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n+            dataset, left_size=left_size, right_size=right_size\n         )\n         self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n         )\n         self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n         )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (n_cols,)\n+        for sample in itertools.chain(dataset_left, dataset_right):\n+            self.assertEqual(sample[0].shape, features_shape)\n+            self.assertEqual(sample[1].shape, (1,))\n+\n+    @parameterized.named_parameters(\n+        named_product(structure_type=[\"dict\", \"tuple\"])\n+    )\n+    def test_split_dataset_nested_structures(self, structure_type):\n+        n_sample, left_size, right_size = 100, 0.2, 0.8\n+        features1 = np.random.sample((n_sample, 2))\n+        features2 = np.random.sample((n_sample, 10, 2))\n+        labels = np.random.sample((n_sample, 1))\n+\n+        if structure_type == \"dict\":\n+            dataset = tf.data.Dataset.from_tensor_slices(\n+                {\"x1\": features1, \"x2\": features2, \"labels\": labels}\n+            )\n+        elif structure_type == \"tuple\":\n+            dataset = tf.data.Dataset.from_tensor_slices(\n+                ((features1, features2), labels)\n             )\n \n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n         dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n+            dataset, left_size=left_size, right_size=right_size\n         )\n         self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n         )\n         self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 10, 10, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n-        )\n-\n-        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n-        features, labels = (\n-            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n-            np.random.sample((n_sample, n_pred)),\n-        )\n-        torch_dataset = Dataset(features, labels)\n-        dataset_left, dataset_right = split_dataset(\n-            torch_dataset, left_size=left_size, right_size=right_size\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_left]), int(n_sample * left_size)\n-        )\n-        self.assertEqual(\n-            len([sample for sample in dataset_right]),\n-            int(n_sample * right_size),\n-        )\n-        self.assertEqual(\n-            [sample for sample in dataset_right][0][0].shape,\n-            (100, 10, 30, n_cols),\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n         )\n+        for sample in itertools.chain(dataset_left, dataset_right):\n+            if structure_type == \"dict\":\n+                x1, x2, labels = sample[\"x1\"], sample[\"x2\"], sample[\"labels\"]\n+            elif structure_type == \"tuple\":\n+                (x1, x2), labels = sample\n+            self.assertEqual(x1.shape, (2,))\n+            self.assertEqual(x2.shape, (10, 2))\n+            self.assertEqual(labels.shape, (1,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
