{"custom_id": "keras#ba2caeddd716bfa42e52152b25812255044dd3a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 35 | Churn Cumulative: 4539 | Contributors (this commit): 14 | Commits (past 90d): 15 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,7 +19,9 @@ import collections\n import io\n import sys\n import tempfile\n+\n from absl.testing import parameterized\n+import keras\n from keras import backend\n from keras import combinations\n from keras import keras_parameterized\n@@ -34,11 +36,13 @@ from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training as training_module\n from keras.engine import training_utils_v1\n+from keras.layers.preprocessing import string_lookup\n from keras.utils import data_utils\n from keras.utils import io_utils\n from keras.utils import np_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.framework import test_util as tf_test_util\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.rmsprop import RMSPropOptimizer\n@@ -120,6 +124,37 @@ class TrainingTest(keras_parameterized.TestCase):\n       model_input[i, i:, i:] = 0\n     model.fit(model_input, np.random.random((10, 1)), epochs=1, batch_size=10)\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_compile_fit_with_mirrored_strategy(self):\n+    # Test with jit_compile = True\n+    strategy = tf.distribute.MirroredStrategy()\n+    with strategy.scope():\n+      model = sequential.Sequential([layers_module.Dense(1)])\n+    model.compile('sgd', loss='mse', run_eagerly=False, jit_compile=True)\n+    x, y = np.ones((10, 1)), np.ones((10, 1))\n+    model.fit(x, y, epochs=2)\n+\n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_verify_xla_compile_with_jit_compile(self):\n+    vocab_data = ['earth', 'wind', 'and', 'fire']\n+    input_array = np.array([['earth', 'wind', 'and', 'fire'],\n+                            ['fire', 'and', 'earth', 'michigan']])\n+    expected_output = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])\n+    strategy = tf.distribute.MirroredStrategy()\n+    with strategy.scope():\n+      input_data = keras.Input(shape=(None,), dtype=tf.string)\n+      layer = string_lookup.StringLookup(vocabulary=vocab_data)\n+      int_data = layer(input_data)\n+      model = keras.Model(inputs=input_data, outputs=int_data)\n+      model.compile('sgd', loss='mse', run_eagerly=False, jit_compile=True)\n+      # Added a string op unsupported by XLA compiler to make sure that an\n+      # error is thrown, This ensures that the graph is indeed being compiled\n+      # using XLA\n+      with self.assertRaisesRegex(tf.errors.InvalidArgumentError,\n+                                  'Graph execution error'):\n+        model.fit(input_array, expected_output, epochs=1)\n+      model.predict(input_array)\n+\n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n   def test_fit_without_loss_at_compile(self):\n     model = sequential.Sequential([layers_module.Dense(1)])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89879e2c76e86c685e44c47a6cdb82f7e645c142", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 37 | Files Changed: 10 | Hunks: 27 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 70 | Churn Cumulative: 22869 | Contributors (this commit): 47 | Commits (past 90d): 50 | Contributors (cumulative): 90 | DMM Complexity: None\n\nDIFF:\n@@ -150,7 +150,7 @@ class RandomNormal(tf.compat.v1.random_normal_initializer):\n   >>> a = initializer(shape=(2, 2))\n   >>> b = initializer(shape=(2, 2))\n   >>> tf.reduce_sum(a - b) == 0\n-  <tf.Tensor: shape=(), dtype=bool, numpy=True>\n+  <tf.Tensor: shape=(), dtype=bool, numpy=False>\n \n   @end_compatibility\n   \"\"\"\n@@ -267,7 +267,7 @@ class RandomUniform(tf.compat.v1.random_uniform_initializer):\n   >>> a = initializer(shape=(2, 2))\n   >>> b = initializer(shape=(2, 2))\n   >>> tf.reduce_sum(a - b) == 0\n-  <tf.Tensor: shape=(), dtype=bool, numpy=True>\n+  <tf.Tensor: shape=(), dtype=bool, numpy=False>\n \n   @end_compatibility\n   \"\"\"\n@@ -386,7 +386,7 @@ class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):\n   >>> a = initializer(shape=(2, 2))\n   >>> b = initializer(shape=(2, 2))\n   >>> tf.reduce_sum(a - b) == 0\n-  <tf.Tensor: shape=(), dtype=bool, numpy=True>\n+  <tf.Tensor: shape=(), dtype=bool, numpy=False>\n \n   @end_compatibility\n   \"\"\"\n\n@@ -80,7 +80,7 @@ class CategoryEncoding(base_layer.Layer):\n     array([[0.1, 0.2, 0. , 0. ],\n            [0.2, 0. , 0. , 0. ],\n            [0. , 0.2, 0.3, 0. ],\n-           [0. , 0.2, 0. , 0.4]])>\n+           [0. , 0.2, 0. , 0.4]], dtype=float32)>\n \n   Args:\n     num_tokens: The total number of tokens the layer should support. All inputs\n\n@@ -184,7 +184,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n   >>> layer(input)\n   <tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n   array([[0, 2, 3, 1],\n-         [1, 3, 2, 1]], dtype=int64)>\n+         [1, 3, 2, 1]])>\n \n   Bucketize float values based on a number of buckets to compute.\n   >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n@@ -193,7 +193,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n   >>> layer(input)\n   <tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n   array([[0, 2, 3, 2],\n-         [1, 3, 3, 1]], dtype=int64)>\n+         [1, 3, 3, 1]])>\n   \"\"\"\n \n   def __init__(self,\n\n@@ -60,24 +60,26 @@ class HashedCrossing(base_layer.Layer):\n \n   **Crossing two scalar features.**\n \n-  >>> layer = tf.keras.layers.HashedCrossing(num_bins=5)\n+  >>> layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(\n+  ...     num_bins=5)\n   >>> feat1 = tf.constant(['A', 'B', 'A', 'B', 'A'])\n   >>> feat2 = tf.constant([101, 101, 101, 102, 102])\n   >>> layer((feat1, feat2))\n-  <tf.Tensor: shape=(5, 1), dtype=int64, numpy=array([1, 4, 1, 6, 3])>\n+  <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 4, 1, 1, 3])>\n \n   **Crossing and one-hotting two scalar features.**\n \n-  >>> layer = tf.keras.layers.HashedCrossing(num_bins=5, output_mode='one_hot')\n+  >>> layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(\n+  ...     num_bins=5, output_mode='one_hot')\n   >>> feat1 = tf.constant(['A', 'B', 'A', 'B', 'A'])\n   >>> feat2 = tf.constant([101, 101, 101, 102, 102])\n   >>> layer((feat1, feat2))\n-  <tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n-  array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n-         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n-         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n-         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n-         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)>\n+  <tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n+    array([[0., 1., 0., 0., 0.],\n+           [0., 0., 0., 0., 1.],\n+           [0., 1., 0., 0., 0.],\n+           [0., 1., 0., 0., 0.],\n+           [0., 0., 0., 1., 0.]], dtype=float32)>\n   \"\"\"\n \n   def __init__(self,\n\n@@ -243,7 +243,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n   >>> layer = tf.keras.layers.IntegerLookup(\n-  ...     output_mode='tf_idf', vocab, idf_weights=idf_weights)\n+  ...     output_mode='tf_idf', vocabulary=vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n@@ -256,7 +256,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n   >>> layer = tf.keras.layers.IntegerLookup(\n-  ...     output_mode='tf_idf', vocab, idf_weights=idf_weights)\n+  ...     output_mode='tf_idf', vocabulary=vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n\n@@ -85,7 +85,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   >>> layer.adapt(adapt_data)\n   >>> layer(input_data)\n   <tf.Tensor: shape=(1, 3), dtype=float32, numpy=\n-  array([0., 0., 0.], dtype=float32)>\n+  array([-1., -1., -1.], dtype=float32)>\n \n   Pass the mean and variance directly.\n \n\n@@ -1720,7 +1720,7 @@ def log_cosh(y_true, y_pred):\n   >>> x = y_pred - y_true\n   >>> assert np.allclose(\n   ...     loss.numpy(),\n-  ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1),\n+  ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.), axis=-1),\n   ...     atol=1e-5)\n \n   Args:\n\n@@ -3139,7 +3139,7 @@ class IoU(_IoUBase):\n   >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n   >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n   >>> # iou = [0.33, 0.33]\n-  >>> m = tf.keras.metrics.IoU(num_classes=2, target_class_id=[0])\n+  >>> m = tf.keras.metrics.IoU(num_classes=2, target_class_ids=[0])\n   >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n   >>> m.result().numpy()\n   0.33333334\n@@ -3152,7 +3152,7 @@ class IoU(_IoUBase):\n   >>> # sum_row = [0.6, 0.4], sum_col = [0.6, 0.4], true_positives = [0.3, 0.1]\n   >>> # iou = [0.33, 0.14]\n   >>> m.result().numpy()\n-  0.33\n+  0.33333334\n \n   Usage with `compile()` API:\n \n@@ -3160,7 +3160,7 @@ class IoU(_IoUBase):\n   model.compile(\n     optimizer='sgd',\n     loss='mse',\n-    metrics=[tf.keras.metrics.IoU(num_classes=2, target_class_id=[0])])\n+    metrics=[tf.keras.metrics.IoU(num_classes=2, target_class_ids=[0])])\n   ```\n   \"\"\"\n \n@@ -3261,7 +3261,7 @@ class BinaryIoU(IoU):\n \n   Standalone usage:\n \n-  >>> m = tf.keras.metrics.BinaryIoU(target_class_id=[0, 1], threshold=0.3)\n+  >>> m = tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n   >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7])\n   >>> m.result().numpy()\n   0.33333334\n@@ -3274,7 +3274,7 @@ class BinaryIoU(IoU):\n   >>> # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n   >>> # iou = [0.222, 0.125]\n   >>> m.result().numpy()\n-  0.17\n+  0.17361112\n \n   Usage with `compile()` API:\n \n@@ -3282,7 +3282,7 @@ class BinaryIoU(IoU):\n   model.compile(\n     optimizer='sgd',\n     loss='mse',\n-    metrics=[tf.keras.metrics.BinaryIoU(target_class_id=[0], threshold=0.5)])\n+    metrics=[tf.keras.metrics.BinaryIoU(target_class_ids=[0], threshold=0.5)])\n   ```\n   \"\"\"\n \n@@ -3458,9 +3458,9 @@ class OneHotIoU(IoU):\n \n   >>> y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n   >>> y_pred = tf.constant([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],\n-  >>>                       [0.1, 0.4, 0.5]])\n+  ...                       [0.1, 0.4, 0.5]])\n   >>> sample_weight = [0.1, 0.2, 0.3, 0.4]\n-  >>> m = metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n+  >>> m = tf.keras.metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n   >>> m.update_state(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n   >>> # cm = [[0, 0, 0.2+0.4],\n   >>> #       [0.3, 0, 0],\n@@ -3562,9 +3562,9 @@ class OneHotMeanIoU(MeanIoU):\n \n   >>> y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n   >>> y_pred = tf.constant([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],\n-  >>>                       [0.1, 0.4, 0.5]])\n+  ...                       [0.1, 0.4, 0.5]])\n   >>> sample_weight = [0.1, 0.2, 0.3, 0.4]\n-  >>> m = metrics.OneHotMeanIoU(num_classes=3)\n+  >>> m = tf.keras.metrics.OneHotMeanIoU(num_classes=3)\n   >>> m.update_state(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n   >>> # cm = [[0, 0, 0.2+0.4],\n   >>> #       [0.3, 0, 0],\n\n@@ -614,7 +614,6 @@ class Optimizer(_BaseOptimizer):\n   >>> opt = tf.keras.optimizers.experimental.SGD(learning_rate=learning_rate)\n   >>> loss = lambda: 3 * var\n   >>> opt.minimize(loss, var_list=[var])\n-  <tf.Variable...\n \n   ### Gradients clipping\n \n@@ -630,7 +629,7 @@ class Optimizer(_BaseOptimizer):\n   ...   loss = 2 * var1 + 2 * var2\n   >>> grads = tape.gradient(loss, [var1, var2])\n   >>> print([grads[0].numpy(), grads[1].numpy()])\n-  [2.0., 2.0]\n+  [2.0, 2.0]\n   >>> opt.apply_gradients(zip(grads, [var1, var2]))\n   >>> # Without clipping, we should get [0, 0], but as gradients are clipped to\n   >>> # have max value 1, we get [1.0, 1.0].\n\n@@ -1044,12 +1044,7 @@ def serialize(learning_rate_schedule):\n   >>> lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n   ...   0.1, decay_steps=100000, decay_rate=0.96, staircase=True)\n   >>> tf.keras.optimizers.schedules.serialize(lr_schedule)\n-  {'class_name': 'ExponentialDecay',\n-   'config': {'decay_rate': 0.96,\n-      'decay_steps': 100000,\n-      'initial_learning_rate': 0.1,\n-      'name': None,\n-      'staircase': True}}\n+  {'class_name': 'ExponentialDecay', 'config': {...}}\n   \"\"\"\n   return generic_utils.serialize_keras_object(learning_rate_schedule)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e57d0af272f6d6ee2e63c0eed5fdefae869436ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 848 | Lines Deleted: 444 | Files Changed: 25 | Hunks: 51 | Methods Changed: 69 | Complexity Δ (Sum/Max): 37/13 | Churn Δ: 1292 | Churn Cumulative: 7226 | Contributors (this commit): 36 | Commits (past 90d): 48 | Contributors (cumulative): 88 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,6 +79,16 @@ from keras.layers.convolutional.conv3d_transpose import Convolution3DTranspose\n from keras.layers.convolutional.separable_conv1d import SeparableConvolution1D\n from keras.layers.convolutional.separable_conv2d import SeparableConvolution2D\n \n+# Regularization layers.\n+from keras.layers.regularization.dropout import Dropout\n+from keras.layers.regularization.spatial_dropout1d import SpatialDropout1D\n+from keras.layers.regularization.spatial_dropout2d import SpatialDropout2D\n+from keras.layers.regularization.spatial_dropout3d import SpatialDropout3D\n+from keras.layers.regularization.gaussian_dropout import GaussianDropout\n+from keras.layers.regularization.gaussian_noise import GaussianNoise\n+from keras.layers.regularization.activity_regularization import ActivityRegularization\n+from keras.layers.regularization.alpha_dropout import AlphaDropout\n+\n # Reshaping layers.\n from keras.layers.reshaping.cropping1d import Cropping1D\n from keras.layers.reshaping.cropping2d import Cropping2D\n@@ -96,14 +106,9 @@ from keras.layers.reshaping.zero_padding3d import ZeroPadding3D\n \n # Core layers.\n from keras.layers.core.activation import Activation\n-from keras.layers.core.activity_regularization import ActivityRegularization\n from keras.layers.core.dense import Dense\n-from keras.layers.core.dropout import Dropout\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n-from keras.layers.core.spatial_dropout import SpatialDropout1D\n-from keras.layers.core.spatial_dropout import SpatialDropout2D\n-from keras.layers.core.spatial_dropout import SpatialDropout3D\n from keras.layers.core.tf_op_layer import ClassMethod\n from keras.layers.core.tf_op_layer import InstanceMethod\n from keras.layers.core.tf_op_layer import InstanceProperty\n@@ -147,11 +152,6 @@ from keras.layers.merging.minimum import minimum\n from keras.layers.merging.concatenate import concatenate\n from keras.layers.merging.dot import dot\n \n-# Noise layers.\n-from keras.layers.noise import AlphaDropout\n-from keras.layers.noise import GaussianNoise\n-from keras.layers.noise import GaussianDropout\n-\n # Normalization layers.\n from keras.layers.normalization.layer_normalization import LayerNormalization\n from keras.layers.normalization.batch_normalization import SyncBatchNormalization\n\n@@ -15,14 +15,9 @@\n \"\"\"Core Keras layers.\"\"\"\n \n from keras.layers.core.activation import Activation\n-from keras.layers.core.activity_regularization import ActivityRegularization\n from keras.layers.core.dense import Dense\n-from keras.layers.core.dropout import Dropout\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n-from keras.layers.core.spatial_dropout import SpatialDropout1D\n-from keras.layers.core.spatial_dropout import SpatialDropout2D\n-from keras.layers.core.spatial_dropout import SpatialDropout3D\n # Required by third_party/py/tensorflow_gnn/graph/keras/keras_tensors.py\n from keras.layers.core.tf_op_layer import _delegate_method\n from keras.layers.core.tf_op_layer import _delegate_property\n@@ -33,6 +28,13 @@ from keras.layers.core.tf_op_layer import InstanceProperty\n from keras.layers.core.tf_op_layer import SlicingOpLambda\n from keras.layers.core.tf_op_layer import TFOpLambda\n \n+# Regularization layers imported for backwards namespace compatibility\n+from keras.layers.regularization.activity_regularization import ActivityRegularization\n+from keras.layers.regularization.dropout import Dropout\n+from keras.layers.regularization.spatial_dropout1d import SpatialDropout1D\n+from keras.layers.regularization.spatial_dropout2d import SpatialDropout2D\n+from keras.layers.regularization.spatial_dropout3d import SpatialDropout3D\n+\n # Reshaping layers imported for backwards namespace compatibility\n from keras.layers.reshaping.flatten import Flatten\n from keras.layers.reshaping.permute import Permute\n\n@@ -624,13 +624,6 @@ class CoreLayersTest(keras_parameterized.TestCase):\n         r'innermost dimension is non-ragged. Received: inputs.shape=.*'):\n       layer.call(tf.ragged.constant([[1., 2], [3, 4, 5]]))\n \n-  def test_activity_regularization(self):\n-    layer = keras.layers.ActivityRegularization(l1=0.1)\n-    layer(keras.backend.variable(np.ones((2, 4))))\n-    self.assertEqual(1, len(layer.losses))\n-    config = layer.get_config()\n-    self.assertEqual(config.pop('l1'), 0.1)\n-\n \n @keras_parameterized.run_all_keras_modes\n class TFOpLambdaTest(keras_parameterized.TestCase):\n\n@@ -1,164 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Contains the spatial dropout layers.\"\"\"\n-\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import,g-bad-import-order\n-import tensorflow.compat.v2 as tf\n-# pylint: enable=g-bad-import-order\n-\n-import keras.backend as K\n-from keras.engine.input_spec import InputSpec\n-from keras.layers.core.dropout import Dropout\n-\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export('keras.layers.SpatialDropout1D')\n-class SpatialDropout1D(Dropout):\n-  \"\"\"Spatial 1D version of Dropout.\n-\n-  This version performs the same function as Dropout, however, it drops\n-  entire 1D feature maps instead of individual elements. If adjacent frames\n-  within feature maps are strongly correlated (as is normally the case in\n-  early convolution layers) then regular dropout will not regularize the\n-  activations and will otherwise just result in an effective learning rate\n-  decrease. In this case, SpatialDropout1D will help promote independence\n-  between feature maps and should be used instead.\n-\n-  Args:\n-    rate: Float between 0 and 1. Fraction of the input units to drop.\n-  Call arguments:\n-    inputs: A 3D tensor.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding dropout) or in inference mode (doing nothing).\n-  Input shape:\n-    3D tensor with shape: `(samples, timesteps, channels)`\n-  Output shape: Same as input.\n-  References: - [Efficient Object Localization Using Convolutional\n-      Networks](https://arxiv.org/abs/1411.4280)\n-  \"\"\"\n-\n-  def __init__(self, rate, **kwargs):\n-    super(SpatialDropout1D, self).__init__(rate, **kwargs)\n-    self.input_spec = InputSpec(ndim=3)\n-\n-  def _get_noise_shape(self, inputs):\n-    input_shape = tf.shape(inputs)\n-    noise_shape = (input_shape[0], 1, input_shape[2])\n-    return noise_shape\n-\n-\n-@keras_export('keras.layers.SpatialDropout2D')\n-class SpatialDropout2D(Dropout):\n-  \"\"\"Spatial 2D version of Dropout.\n-\n-  This version performs the same function as Dropout, however, it drops\n-  entire 2D feature maps instead of individual elements. If adjacent pixels\n-  within feature maps are strongly correlated (as is normally the case in\n-  early convolution layers) then regular dropout will not regularize the\n-  activations and will otherwise just result in an effective learning rate\n-  decrease. In this case, SpatialDropout2D will help promote independence\n-  between feature maps and should be used instead.\n-\n-  Args:\n-    rate: Float between 0 and 1. Fraction of the input units to drop.\n-    data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n-      the channels dimension (the depth) is at index 1, in 'channels_last' mode\n-      is it at index 3. It defaults to the `image_data_format` value found in\n-      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-      it will be \"channels_last\".\n-  Call arguments:\n-    inputs: A 4D tensor.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding dropout) or in inference mode (doing nothing).\n-  Input shape:\n-    4D tensor with shape: `(samples, channels, rows, cols)` if\n-      data_format='channels_first'\n-    or 4D tensor with shape: `(samples, rows, cols, channels)` if\n-      data_format='channels_last'.\n-  Output shape: Same as input.\n-  References: - [Efficient Object Localization Using Convolutional\n-      Networks](https://arxiv.org/abs/1411.4280)\n-  \"\"\"\n-\n-  def __init__(self, rate, data_format=None, **kwargs):\n-    super(SpatialDropout2D, self).__init__(rate, **kwargs)\n-    if data_format is None:\n-      data_format = K.image_data_format()\n-    if data_format not in {'channels_last', 'channels_first'}:\n-      raise ValueError(\n-          f'`data_format` must be \"channels_last\" or \"channels_first\". '\n-          f'Received: data_format={data_format}.')\n-    self.data_format = data_format\n-    self.input_spec = InputSpec(ndim=4)\n-\n-  def _get_noise_shape(self, inputs):\n-    input_shape = tf.shape(inputs)\n-    if self.data_format == 'channels_first':\n-      return (input_shape[0], input_shape[1], 1, 1)\n-    elif self.data_format == 'channels_last':\n-      return (input_shape[0], 1, 1, input_shape[3])\n-\n-\n-@keras_export('keras.layers.SpatialDropout3D')\n-class SpatialDropout3D(Dropout):\n-  \"\"\"Spatial 3D version of Dropout.\n-\n-  This version performs the same function as Dropout, however, it drops\n-  entire 3D feature maps instead of individual elements. If adjacent voxels\n-  within feature maps are strongly correlated (as is normally the case in\n-  early convolution layers) then regular dropout will not regularize the\n-  activations and will otherwise just result in an effective learning rate\n-  decrease. In this case, SpatialDropout3D will help promote independence\n-  between feature maps and should be used instead.\n-\n-  Args:\n-    rate: Float between 0 and 1. Fraction of the input units to drop.\n-    data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n-      the channels dimension (the depth) is at index 1, in 'channels_last' mode\n-      is it at index 4. It defaults to the `image_data_format` value found in\n-      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-      it will be \"channels_last\".\n-  Call arguments:\n-    inputs: A 5D tensor.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding dropout) or in inference mode (doing nothing).\n-  Input shape:\n-    5D tensor with shape: `(samples, channels, dim1, dim2, dim3)` if\n-      data_format='channels_first'\n-    or 5D tensor with shape: `(samples, dim1, dim2, dim3, channels)` if\n-      data_format='channels_last'.\n-  Output shape: Same as input.\n-  References: - [Efficient Object Localization Using Convolutional\n-      Networks](https://arxiv.org/abs/1411.4280)\n-  \"\"\"\n-\n-  def __init__(self, rate, data_format=None, **kwargs):\n-    super(SpatialDropout3D, self).__init__(rate, **kwargs)\n-    if data_format is None:\n-      data_format = K.image_data_format()\n-    if data_format not in {'channels_last', 'channels_first'}:\n-      raise ValueError(\n-          f'`data_format` must be \"channels_last\" or \"channels_first\". '\n-          f'Received: data_format={data_format}.')\n-    self.data_format = data_format\n-    self.input_spec = InputSpec(ndim=5)\n-\n-  def _get_noise_shape(self, inputs):\n-    input_shape = tf.shape(inputs)\n-    if self.data_format == 'channels_first':\n-      return (input_shape[0], input_shape[1], 1, 1, 1)\n-    elif self.data_format == 'channels_last':\n-      return (input_shape[0], 1, 1, 1, input_shape[4])\n\n@@ -28,8 +28,8 @@ from keras import initializers\n from keras import regularizers\n from keras.engine.base_layer import Layer\n from keras.layers import advanced_activations\n-from keras.layers import core\n from keras.layers import einsum_dense\n+from keras.layers import regularization\n from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n@@ -423,7 +423,7 @@ class MultiHeadAttention(Layer):\n     norm_axes = tuple(\n         range(attn_scores_rank - len(self._attention_axes), attn_scores_rank))\n     self._softmax = advanced_activations.Softmax(axis=norm_axes)\n-    self._dropout_layer = core.Dropout(rate=self._dropout)\n+    self._dropout_layer = regularization.Dropout(rate=self._dropout)\n \n   def _masked_softmax(self, attention_scores, attention_mask=None):\n     # Normalize the attention scores to probabilities.\n\n@@ -13,201 +13,9 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Layers that operate regularization via the addition of noise.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+# pylint: disable=g-bad-import-order,unused-import\n \n-from keras import backend\n-from keras.engine import base_layer\n-from keras.utils import tf_utils\n-\n-import numpy as np\n-import tensorflow.compat.v2 as tf\n-\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export('keras.layers.GaussianNoise')\n-class GaussianNoise(base_layer.BaseRandomLayer):\n-  \"\"\"Apply additive zero-centered Gaussian noise.\n-\n-  This is useful to mitigate overfitting\n-  (you could see it as a form of random data augmentation).\n-  Gaussian Noise (GS) is a natural choice as corruption process\n-  for real valued inputs.\n-\n-  As it is a regularization layer, it is only active at training time.\n-\n-  Args:\n-    stddev: Float, standard deviation of the noise distribution.\n-    seed: Integer, optional random seed to enable deterministic behavior.\n-\n-  Call arguments:\n-    inputs: Input tensor (of any rank).\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding noise) or in inference mode (doing nothing).\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as input.\n-  \"\"\"\n-\n-  def __init__(self, stddev, seed=None, **kwargs):\n-    super(GaussianNoise, self).__init__(seed=seed, **kwargs)\n-    self.supports_masking = True\n-    self.stddev = stddev\n-    self.seed = seed\n-\n-  def call(self, inputs, training=None):\n-\n-    def noised():\n-      return inputs + self._random_generator.random_normal(\n-          shape=tf.shape(inputs),\n-          mean=0.,\n-          stddev=self.stddev,\n-          dtype=inputs.dtype)\n-\n-    return backend.in_train_phase(noised, inputs, training=training)\n-\n-  def get_config(self):\n-    config = {'stddev': self.stddev, 'seed': self.seed}\n-    base_config = super(GaussianNoise, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-\n-@keras_export('keras.layers.GaussianDropout')\n-class GaussianDropout(base_layer.BaseRandomLayer):\n-  \"\"\"Apply multiplicative 1-centered Gaussian noise.\n-\n-  As it is a regularization layer, it is only active at training time.\n-\n-  Args:\n-    rate: Float, drop probability (as with `Dropout`).\n-      The multiplicative noise will have\n-      standard deviation `sqrt(rate / (1 - rate))`.\n-    seed: Integer, optional random seed to enable deterministic behavior.\n-\n-  Call arguments:\n-    inputs: Input tensor (of any rank).\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding dropout) or in inference mode (doing nothing).\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as input.\n-  \"\"\"\n-\n-  def __init__(self, rate, seed=None, **kwargs):\n-    super(GaussianDropout, self).__init__(seed=seed, **kwargs)\n-    self.supports_masking = True\n-    self.rate = rate\n-    self.seed = seed\n-\n-  def call(self, inputs, training=None):\n-    if 0 < self.rate < 1:\n-\n-      def noised():\n-        stddev = np.sqrt(self.rate / (1.0 - self.rate))\n-        return inputs * self._random_generator.random_normal(\n-            shape=tf.shape(inputs),\n-            mean=1.0,\n-            stddev=stddev,\n-            dtype=inputs.dtype)\n-\n-      return backend.in_train_phase(noised, inputs, training=training)\n-    return inputs\n-\n-  def get_config(self):\n-    config = {'rate': self.rate, 'seed': self.seed}\n-    base_config = super(GaussianDropout, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-\n-@keras_export('keras.layers.AlphaDropout')\n-class AlphaDropout(base_layer.BaseRandomLayer):\n-  \"\"\"Applies Alpha Dropout to the input.\n-\n-  Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n-  to their original values, in order to ensure the self-normalizing property\n-  even after this dropout.\n-  Alpha Dropout fits well to Scaled Exponential Linear Units\n-  by randomly setting activations to the negative saturation value.\n-\n-  Args:\n-    rate: float, drop probability (as with `Dropout`).\n-      The multiplicative noise will have\n-      standard deviation `sqrt(rate / (1 - rate))`.\n-    seed: Integer, optional random seed to enable deterministic behavior.\n-\n-  Call arguments:\n-    inputs: Input tensor (of any rank).\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode (adding dropout) or in inference mode (doing nothing).\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as input.\n-  \"\"\"\n-\n-  def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n-    super(AlphaDropout, self).__init__(seed=seed, **kwargs)\n-    self.rate = rate\n-    self.noise_shape = noise_shape\n-    self.seed = seed\n-    self.supports_masking = True\n-\n-  def _get_noise_shape(self, inputs):\n-    return self.noise_shape if self.noise_shape else tf.shape(inputs)\n-\n-  def call(self, inputs, training=None):\n-    if 0. < self.rate < 1.:\n-      noise_shape = self._get_noise_shape(inputs)\n-\n-      def dropped_inputs(inputs=inputs, rate=self.rate):  # pylint: disable=missing-docstring\n-        alpha = 1.6732632423543772848170429916717\n-        scale = 1.0507009873554804934193349852946\n-        alpha_p = -alpha * scale\n-\n-        kept_idx = tf.greater_equal(\n-            self._random_generator.random_uniform(noise_shape), rate)\n-        kept_idx = tf.cast(kept_idx, inputs.dtype)\n-\n-        # Get affine transformation params\n-        a = ((1 - rate) * (1 + rate * alpha_p**2))**-0.5\n-        b = -a * alpha_p * rate\n-\n-        # Apply mask\n-        x = inputs * kept_idx + alpha_p * (1 - kept_idx)\n-\n-        # Do affine transformation\n-        return a * x + b\n-\n-      return backend.in_train_phase(dropped_inputs, inputs, training=training)\n-    return inputs\n-\n-  def get_config(self):\n-    config = {'rate': self.rate, 'seed': self.seed}\n-    base_config = super(AlphaDropout, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n+# Regularization layers imported for backwards namespace compatibility\n+from keras.layers.regularization.gaussian_dropout import GaussianDropout\n+from keras.layers.regularization.gaussian_noise import GaussianNoise\n+from keras.layers.regularization.alpha_dropout import AlphaDropout\n\n@@ -0,0 +1,26 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras regularization layers.\"\"\"\n+# pylint: disable=g-bad-import-order\n+\n+from keras.layers.regularization.dropout import Dropout\n+from keras.layers.regularization.spatial_dropout1d import SpatialDropout1D\n+from keras.layers.regularization.spatial_dropout2d import SpatialDropout2D\n+from keras.layers.regularization.spatial_dropout3d import SpatialDropout3D\n+from keras.layers.regularization.gaussian_dropout import GaussianDropout\n+from keras.layers.regularization.gaussian_noise import GaussianNoise\n+from keras.layers.regularization.activity_regularization import ActivityRegularization\n+from keras.layers.regularization.alpha_dropout import AlphaDropout\n+\n\n\n@@ -0,0 +1,34 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for activity regularization layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class ActivityRegularizationTest(keras_parameterized.TestCase):\n+\n+  def test_activity_regularization(self):\n+    layer = keras.layers.ActivityRegularization(l1=0.1)\n+    layer(keras.backend.variable(np.ones((2, 4))))\n+    self.assertEqual(1, len(layer.losses))\n+    config = layer.get_config()\n+    self.assertEqual(config.pop('l1'), 0.1)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,100 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the AlphaDropout layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine import base_layer\n+from keras.utils import tf_utils\n+\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.AlphaDropout')\n+class AlphaDropout(base_layer.BaseRandomLayer):\n+  \"\"\"Applies Alpha Dropout to the input.\n+\n+  Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n+  to their original values, in order to ensure the self-normalizing property\n+  even after this dropout.\n+  Alpha Dropout fits well to Scaled Exponential Linear Units\n+  by randomly setting activations to the negative saturation value.\n+\n+  Args:\n+    rate: float, drop probability (as with `Dropout`).\n+      The multiplicative noise will have\n+      standard deviation `sqrt(rate / (1 - rate))`.\n+    seed: Integer, optional random seed to enable deterministic behavior.\n+\n+  Call arguments:\n+    inputs: Input tensor (of any rank).\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding dropout) or in inference mode (doing nothing).\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as input.\n+  \"\"\"\n+\n+  def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n+    super(AlphaDropout, self).__init__(seed=seed, **kwargs)\n+    self.rate = rate\n+    self.noise_shape = noise_shape\n+    self.seed = seed\n+    self.supports_masking = True\n+\n+  def _get_noise_shape(self, inputs):\n+    return self.noise_shape if self.noise_shape else tf.shape(inputs)\n+\n+  def call(self, inputs, training=None):\n+    if 0. < self.rate < 1.:\n+      noise_shape = self._get_noise_shape(inputs)\n+\n+      def dropped_inputs(inputs=inputs, rate=self.rate):  # pylint: disable=missing-docstring\n+        alpha = 1.6732632423543772848170429916717\n+        scale = 1.0507009873554804934193349852946\n+        alpha_p = -alpha * scale\n+\n+        kept_idx = tf.greater_equal(\n+            self._random_generator.random_uniform(noise_shape), rate)\n+        kept_idx = tf.cast(kept_idx, inputs.dtype)\n+\n+        # Get affine transformation params\n+        a = ((1 - rate) * (1 + rate * alpha_p**2))**-0.5\n+        b = -a * alpha_p * rate\n+\n+        # Apply mask\n+        x = inputs * kept_idx + alpha_p * (1 - kept_idx)\n+\n+        # Do affine transformation\n+        return a * x + b\n+\n+      return backend.in_train_phase(dropped_inputs, inputs, training=training)\n+    return inputs\n+\n+  def get_config(self):\n+    config = {'rate': self.rate, 'seed': self.seed}\n+    base_config = super(AlphaDropout, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n\n@@ -12,77 +12,43 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for noise layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n+\"\"\"Tests for alpha dropout layer.\"\"\"\n \n import keras\n from keras import keras_parameterized\n from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @keras_parameterized.run_all_keras_modes\n-class NoiseLayersTest(keras_parameterized.TestCase):\n-\n-  def test_GaussianNoise(self):\n-    testing_utils.layer_test(\n-        keras.layers.GaussianNoise,\n-        kwargs={'stddev': 1.},\n-        input_shape=(3, 2, 3))\n-\n-  def test_GaussianDropout(self):\n-    testing_utils.layer_test(\n-        keras.layers.GaussianDropout,\n-        kwargs={'rate': 0.5},\n-        input_shape=(3, 2, 3))\n+class AlphaDropoutTest(keras_parameterized.TestCase):\n \n   def test_AlphaDropout(self):\n     testing_utils.layer_test(\n         keras.layers.AlphaDropout, kwargs={'rate': 0.2}, input_shape=(3, 2, 3))\n \n-  @staticmethod\n-  def _make_model(dtype, class_type):\n+  def _make_model(self, dtype):\n     assert dtype in (tf.float32, tf.float64)\n-    assert class_type in ('gaussian_noise', 'gaussian_dropout', 'alpha_noise')\n     model = keras.Sequential()\n     model.add(keras.layers.Dense(8, input_shape=(32,), dtype=dtype))\n-    if class_type == 'gaussian_noise':\n-      layer = keras.layers.GaussianNoise(0.0003, dtype=dtype)\n-    elif class_type == 'gaussian_dropout':\n-      layer = keras.layers.GaussianDropout(0.1, dtype=dtype)\n-    else:\n     layer = keras.layers.AlphaDropout(0.5, dtype=dtype)\n     model.add(layer)\n     return model\n \n-  def _train_model(self, dtype, gtype):\n-    model = self._make_model(dtype, gtype)\n+  def _train_model(self, dtype):\n+    model = self._make_model(dtype)\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n         run_eagerly=testing_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((8, 32)), np.zeros((8, 8)))\n \n-  def test_noise_float32(self):\n-    self._train_model(tf.float32, 'gaussian_noise')\n-\n-  def test_noise_float64(self):\n-    self._train_model(tf.float64, 'gaussian_noise')\n-\n-  def test_dropout_float32(self):\n-    self._train_model(tf.float32, 'gaussian_dropout')\n-\n-  def test_dropout_float64(self):\n-    self._train_model(tf.float64, 'gaussian_dropout')\n-\n   def test_alpha_dropout_float32(self):\n-    self._train_model(tf.float32, 'alpha_noise')\n+    self._train_model(tf.float32)\n \n   def test_alpha_dropout_float64(self):\n-    self._train_model(tf.float64, 'alpha_noise')\n-\n+    self._train_model(tf.float64)\n \n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Contains the dropout layer.\"\"\"\n+\"\"\"Contains the Dropout layer.\"\"\"\n # pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n from keras import backend\n\n@@ -0,0 +1,94 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for dropout layer.\"\"\"\n+\n+import os\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class DropoutTest(keras_parameterized.TestCase):\n+\n+  def test_dropout(self):\n+    testing_utils.layer_test(\n+        keras.layers.Dropout, kwargs={'rate': 0.5}, input_shape=(3, 2))\n+\n+    testing_utils.layer_test(\n+        keras.layers.Dropout,\n+        kwargs={\n+            'rate': 0.5,\n+            'noise_shape': [3, 1]\n+        },\n+        input_shape=(3, 2))\n+\n+  def test_dropout_supports_masking(self):\n+    dropout = keras.layers.Dropout(0.5)\n+    self.assertEqual(True, dropout.supports_masking)\n+\n+  def test_dropout_partial_noise_shape(self):\n+    inputs = keras.Input(shape=(5, 10))\n+    layer = keras.layers.Dropout(0.5, noise_shape=(None, 1, None))\n+    outputs = layer(inputs)\n+    model = keras.Model(inputs, outputs)\n+    out = model(np.ones((20, 5, 10)), training=True)\n+    out_np = keras.backend.get_value(out)\n+    # Test that dropout mask is shared across second dim.\n+    self.assertAllClose(out_np[:, 0, :], out_np[:, 1, :])\n+\n+  def test_dropout_with_savemodel(self):\n+    inputs = keras.Input(shape=(5, 10))\n+    layer = keras.layers.Dropout(0.5)\n+    layer._random_generator._force_generator = True\n+    outputs = layer(inputs)\n+    model = keras.Model(inputs, outputs)\n+    train = model(np.ones((20, 5, 10)), training=True)\n+    predict = model(np.ones((20, 5, 10)))\n+    # Make sure the weights from tf.random.Generator is not present in the model\n+    # which will cause weight loading issue for existing application models if\n+    # it contains dropout layer.\n+    self.assertEmpty(layer.get_weights())\n+    self.assertEmpty(model.get_weights())\n+\n+    # Make sure the layer does dropout value when training\n+    self.assertNotAllClose(train, predict)\n+\n+    model.save(os.path.join(self.get_temp_dir(), 'savedmodel'),\n+               save_format='tf')\n+    loaded_model = keras.models.load_model(\n+        os.path.join(self.get_temp_dir(), 'savedmodel'))\n+    predict2 = loaded_model(np.ones((20, 5, 10)))\n+\n+    self.assertAllClose(predict, predict2)\n+    # Make sure the model dropout different value after loading\n+    train2 = loaded_model(np.ones((20, 5, 10)), training=True)\n+    self.assertNotAllClose(train, train2)\n+    self.assertIsNotNone(loaded_model.layers[1]._random_generator)\n+\n+    # Also make sure the checkpoint doesn't contain any variable from the\n+    # dropout layer, to keep the backward compatibility.\n+    checkpoint = tf.train.Checkpoint(model)\n+    save_path = checkpoint.save(os.path.join(self.get_temp_dir(), 'checkpoint'))\n+    checkpoint_var_names = [name_value_tuple[0] for name_value_tuple in\n+                            tf.train.list_variables(save_path)]\n+    for name in checkpoint_var_names:\n+      self.assertNotIn('dropout', name)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,81 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the GaussianDropout layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine import base_layer\n+from keras.utils import tf_utils\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.GaussianDropout')\n+class GaussianDropout(base_layer.BaseRandomLayer):\n+  \"\"\"Apply multiplicative 1-centered Gaussian noise.\n+\n+  As it is a regularization layer, it is only active at training time.\n+\n+  Args:\n+    rate: Float, drop probability (as with `Dropout`).\n+      The multiplicative noise will have\n+      standard deviation `sqrt(rate / (1 - rate))`.\n+    seed: Integer, optional random seed to enable deterministic behavior.\n+\n+  Call arguments:\n+    inputs: Input tensor (of any rank).\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding dropout) or in inference mode (doing nothing).\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as input.\n+  \"\"\"\n+\n+  def __init__(self, rate, seed=None, **kwargs):\n+    super(GaussianDropout, self).__init__(seed=seed, **kwargs)\n+    self.supports_masking = True\n+    self.rate = rate\n+    self.seed = seed\n+\n+  def call(self, inputs, training=None):\n+    if 0 < self.rate < 1:\n+\n+      def noised():\n+        stddev = np.sqrt(self.rate / (1.0 - self.rate))\n+        return inputs * self._random_generator.random_normal(\n+            shape=tf.shape(inputs),\n+            mean=1.0,\n+            stddev=stddev,\n+            dtype=inputs.dtype)\n+\n+      return backend.in_train_phase(noised, inputs, training=training)\n+    return inputs\n+\n+  def get_config(self):\n+    config = {'rate': self.rate, 'seed': self.seed}\n+    base_config = super(GaussianDropout, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n\n@@ -0,0 +1,56 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for gaussian dropout layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class NoiseLayersTest(keras_parameterized.TestCase):\n+\n+  def test_GaussianDropout(self):\n+    testing_utils.layer_test(\n+        keras.layers.GaussianDropout,\n+        kwargs={'rate': 0.5},\n+        input_shape=(3, 2, 3))\n+\n+  def _make_model(self, dtype):\n+    assert dtype in (tf.float32, tf.float64)\n+    model = keras.Sequential()\n+    model.add(keras.layers.Dense(8, input_shape=(32,), dtype=dtype))\n+    layer = keras.layers.GaussianDropout(0.1, dtype=dtype)\n+    model.add(layer)\n+    return model\n+\n+  def _train_model(self, dtype):\n+    model = self._make_model(dtype)\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        run_eagerly=testing_utils.should_run_eagerly())\n+    model.train_on_batch(np.zeros((8, 32)), np.zeros((8, 8)))\n+\n+  def test_gaussian_dropout_float32(self):\n+    self._train_model(tf.float32)\n+\n+  def test_gaussian_dropout_float64(self):\n+    self._train_model(tf.float64)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,80 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the GaussianNoise layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine import base_layer\n+from keras.utils import tf_utils\n+\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.GaussianNoise')\n+class GaussianNoise(base_layer.BaseRandomLayer):\n+  \"\"\"Apply additive zero-centered Gaussian noise.\n+\n+  This is useful to mitigate overfitting\n+  (you could see it as a form of random data augmentation).\n+  Gaussian Noise (GS) is a natural choice as corruption process\n+  for real valued inputs.\n+\n+  As it is a regularization layer, it is only active at training time.\n+\n+  Args:\n+    stddev: Float, standard deviation of the noise distribution.\n+    seed: Integer, optional random seed to enable deterministic behavior.\n+\n+  Call arguments:\n+    inputs: Input tensor (of any rank).\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding noise) or in inference mode (doing nothing).\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as input.\n+  \"\"\"\n+\n+  def __init__(self, stddev, seed=None, **kwargs):\n+    super(GaussianNoise, self).__init__(seed=seed, **kwargs)\n+    self.supports_masking = True\n+    self.stddev = stddev\n+    self.seed = seed\n+\n+  def call(self, inputs, training=None):\n+\n+    def noised():\n+      return inputs + self._random_generator.random_normal(\n+          shape=tf.shape(inputs),\n+          mean=0.,\n+          stddev=self.stddev,\n+          dtype=inputs.dtype)\n+\n+    return backend.in_train_phase(noised, inputs, training=training)\n+\n+  def get_config(self):\n+    config = {'stddev': self.stddev, 'seed': self.seed}\n+    base_config = super(GaussianNoise, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n\n@@ -0,0 +1,56 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for gaussian noise layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class NoiseLayersTest(keras_parameterized.TestCase):\n+\n+  def test_GaussianNoise(self):\n+    testing_utils.layer_test(\n+        keras.layers.GaussianNoise,\n+        kwargs={'stddev': 1.},\n+        input_shape=(3, 2, 3))\n+\n+  def _make_model(self, dtype):\n+    assert dtype in (tf.float32, tf.float64)\n+    model = keras.Sequential()\n+    model.add(keras.layers.Dense(8, input_shape=(32,), dtype=dtype))\n+    layer = keras.layers.GaussianNoise(0.0003, dtype=dtype)\n+    model.add(layer)\n+    return model\n+\n+  def _train_model(self, dtype):\n+    model = self._make_model(dtype)\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        run_eagerly=testing_utils.should_run_eagerly())\n+    model.train_on_batch(np.zeros((8, 32)), np.zeros((8, 8)))\n+\n+  def test_gaussian_noise_float32(self):\n+    self._train_model(tf.float32)\n+\n+  def test_gaussian_noise_float64(self):\n+    self._train_model(tf.float64)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,57 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the SpatialDropout1D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.regularization.dropout import Dropout\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.SpatialDropout1D')\n+class SpatialDropout1D(Dropout):\n+  \"\"\"Spatial 1D version of Dropout.\n+\n+  This version performs the same function as Dropout, however, it drops\n+  entire 1D feature maps instead of individual elements. If adjacent frames\n+  within feature maps are strongly correlated (as is normally the case in\n+  early convolution layers) then regular dropout will not regularize the\n+  activations and will otherwise just result in an effective learning rate\n+  decrease. In this case, SpatialDropout1D will help promote independence\n+  between feature maps and should be used instead.\n+\n+  Args:\n+    rate: Float between 0 and 1. Fraction of the input units to drop.\n+  Call arguments:\n+    inputs: A 3D tensor.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding dropout) or in inference mode (doing nothing).\n+  Input shape:\n+    3D tensor with shape: `(samples, timesteps, channels)`\n+  Output shape: Same as input.\n+  References: - [Efficient Object Localization Using Convolutional\n+      Networks](https://arxiv.org/abs/1411.4280)\n+  \"\"\"\n+\n+  def __init__(self, rate, **kwargs):\n+    super(SpatialDropout1D, self).__init__(rate, **kwargs)\n+    self.input_spec = InputSpec(ndim=3)\n+\n+  def _get_noise_shape(self, inputs):\n+    input_shape = tf.shape(inputs)\n+    noise_shape = (input_shape[0], 1, input_shape[2])\n+    return noise_shape\n\n@@ -0,0 +1,75 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the SpatialDropout2D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.regularization.dropout import Dropout\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.SpatialDropout2D')\n+class SpatialDropout2D(Dropout):\n+  \"\"\"Spatial 2D version of Dropout.\n+\n+  This version performs the same function as Dropout, however, it drops\n+  entire 2D feature maps instead of individual elements. If adjacent pixels\n+  within feature maps are strongly correlated (as is normally the case in\n+  early convolution layers) then regular dropout will not regularize the\n+  activations and will otherwise just result in an effective learning rate\n+  decrease. In this case, SpatialDropout2D will help promote independence\n+  between feature maps and should be used instead.\n+\n+  Args:\n+    rate: Float between 0 and 1. Fraction of the input units to drop.\n+    data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n+      the channels dimension (the depth) is at index 1, in 'channels_last' mode\n+      is it at index 3. It defaults to the `image_data_format` value found in\n+      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n+      it will be \"channels_last\".\n+  Call arguments:\n+    inputs: A 4D tensor.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding dropout) or in inference mode (doing nothing).\n+  Input shape:\n+    4D tensor with shape: `(samples, channels, rows, cols)` if\n+      data_format='channels_first'\n+    or 4D tensor with shape: `(samples, rows, cols, channels)` if\n+      data_format='channels_last'.\n+  Output shape: Same as input.\n+  References: - [Efficient Object Localization Using Convolutional\n+      Networks](https://arxiv.org/abs/1411.4280)\n+  \"\"\"\n+\n+  def __init__(self, rate, data_format=None, **kwargs):\n+    super(SpatialDropout2D, self).__init__(rate, **kwargs)\n+    if data_format is None:\n+      data_format = backend.image_data_format()\n+    if data_format not in {'channels_last', 'channels_first'}:\n+      raise ValueError(\n+          f'`data_format` must be \"channels_last\" or \"channels_first\". '\n+          f'Received: data_format={data_format}.')\n+    self.data_format = data_format\n+    self.input_spec = InputSpec(ndim=4)\n+\n+  def _get_noise_shape(self, inputs):\n+    input_shape = tf.shape(inputs)\n+    if self.data_format == 'channels_first':\n+      return (input_shape[0], input_shape[1], 1, 1)\n+    elif self.data_format == 'channels_last':\n+      return (input_shape[0], 1, 1, input_shape[3])\n\n@@ -0,0 +1,75 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Contains the SpatialDropout3D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.regularization.dropout import Dropout\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.SpatialDropout3D')\n+class SpatialDropout3D(Dropout):\n+  \"\"\"Spatial 3D version of Dropout.\n+\n+  This version performs the same function as Dropout, however, it drops\n+  entire 3D feature maps instead of individual elements. If adjacent voxels\n+  within feature maps are strongly correlated (as is normally the case in\n+  early convolution layers) then regular dropout will not regularize the\n+  activations and will otherwise just result in an effective learning rate\n+  decrease. In this case, SpatialDropout3D will help promote independence\n+  between feature maps and should be used instead.\n+\n+  Args:\n+    rate: Float between 0 and 1. Fraction of the input units to drop.\n+    data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n+      the channels dimension (the depth) is at index 1, in 'channels_last' mode\n+      is it at index 4. It defaults to the `image_data_format` value found in\n+      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n+      it will be \"channels_last\".\n+  Call arguments:\n+    inputs: A 5D tensor.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode (adding dropout) or in inference mode (doing nothing).\n+  Input shape:\n+    5D tensor with shape: `(samples, channels, dim1, dim2, dim3)` if\n+      data_format='channels_first'\n+    or 5D tensor with shape: `(samples, dim1, dim2, dim3, channels)` if\n+      data_format='channels_last'.\n+  Output shape: Same as input.\n+  References: - [Efficient Object Localization Using Convolutional\n+      Networks](https://arxiv.org/abs/1411.4280)\n+  \"\"\"\n+\n+  def __init__(self, rate, data_format=None, **kwargs):\n+    super(SpatialDropout3D, self).__init__(rate, **kwargs)\n+    if data_format is None:\n+      data_format = backend.image_data_format()\n+    if data_format not in {'channels_last', 'channels_first'}:\n+      raise ValueError(\n+          f'`data_format` must be \"channels_last\" or \"channels_first\". '\n+          f'Received: data_format={data_format}.')\n+    self.data_format = data_format\n+    self.input_spec = InputSpec(ndim=5)\n+\n+  def _get_noise_shape(self, inputs):\n+    input_shape = tf.shape(inputs)\n+    if self.data_format == 'channels_first':\n+      return (input_shape[0], input_shape[1], 1, 1, 1)\n+    elif self.data_format == 'channels_last':\n+      return (input_shape[0], 1, 1, 1, input_shape[4])\n\n@@ -0,0 +1,61 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for spatial dropout layers.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class SpacialDropoutTest(keras_parameterized.TestCase):\n+\n+  def test_spatial_dropout_1d(self):\n+    testing_utils.layer_test(\n+        keras.layers.SpatialDropout1D,\n+        kwargs={'rate': 0.5},\n+        input_shape=(2, 3, 4))\n+\n+  def test_spatial_dropout_2d(self):\n+    testing_utils.layer_test(\n+        keras.layers.SpatialDropout2D,\n+        kwargs={'rate': 0.5},\n+        input_shape=(2, 3, 4, 5))\n+\n+    testing_utils.layer_test(\n+        keras.layers.SpatialDropout2D,\n+        kwargs={\n+            'rate': 0.5,\n+            'data_format': 'channels_first'\n+        },\n+        input_shape=(2, 3, 4, 5))\n+\n+  def test_spatial_dropout_3d(self):\n+    testing_utils.layer_test(\n+        keras.layers.SpatialDropout3D,\n+        kwargs={'rate': 0.5},\n+        input_shape=(2, 3, 4, 4, 5))\n+\n+    testing_utils.layer_test(\n+        keras.layers.SpatialDropout3D,\n+        kwargs={\n+            'rate': 0.5,\n+            'data_format': 'channels_first'\n+        },\n+        input_shape=(2, 3, 4, 4, 5))\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -36,6 +36,7 @@ from keras.layers import noise\n from keras.layers import pooling\n from keras.layers import recurrent\n from keras.layers import recurrent_v2\n+from keras.layers import regularization\n from keras.layers import reshaping\n from keras.layers import rnn_cell_wrapper_v2\n from keras.layers import wrappers\n@@ -59,11 +60,11 @@ from tensorflow.python.util.tf_export import keras_export\n \n ALL_MODULES = (base_layer, input_layer, advanced_activations, convolutional,\n                convolutional_recurrent, core, cudnn_recurrent, dense_attention,\n-               embeddings, einsum_dense, local, merging, noise,\n-               batch_normalization_v1, layer_normalization, unit_normalization,\n-               pooling, image_preprocessing, recurrent, reshaping, wrappers,\n-               hashing, hashed_crossing, category_encoding, discretization,\n-               multi_head_attention, integer_lookup,\n+               embeddings, einsum_dense, local, merging, batch_normalization_v1,\n+               layer_normalization, unit_normalization, pooling,\n+               image_preprocessing, recurrent, regularization, reshaping,\n+               wrappers, hashing, hashed_crossing, category_encoding,\n+               discretization, multi_head_attention, integer_lookup,\n                preprocessing_normalization, string_lookup, text_vectorization)\n ALL_V2_MODULES = (rnn_cell_wrapper_v2, batch_normalization, layer_normalization,\n                   recurrent_v2)\n\n@@ -29,10 +29,10 @@ from keras.layers import dense_attention\n from keras.layers import embeddings\n from keras.layers import local\n from keras.layers import merging\n-from keras.layers import noise\n from keras.layers import pooling\n from keras.layers import recurrent\n from keras.layers import recurrent_v2\n+from keras.layers import regularization\n from keras.layers import reshaping\n from keras.layers import wrappers\n from keras.layers.normalization import batch_normalization\n@@ -104,8 +104,9 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n        lambda: convolutional_recurrent.ConvLSTM2D(4, kernel_size=(2, 2)),\n        (4, 4, 4, 4, 4)),\n       ('Dense', lambda: core.Dense(2), (2, 2)),\n-      ('Dropout', lambda: core.Dropout(0.5), (2, 2)),\n-      ('SpatialDropout2D', lambda: core.SpatialDropout2D(0.5), (2, 2, 2, 2)),\n+      ('Dropout', lambda: regularization.Dropout(0.5), (2, 2)),\n+      ('SpatialDropout2D',\n+       lambda: regularization.SpatialDropout2D(0.5), (2, 2, 2, 2)),\n       ('Activation', lambda: core.Activation('sigmoid'), (2, 2)),\n       ('Reshape', lambda: reshaping.Reshape((1, 4, 1)), (2, 2, 2)),\n       ('Permute', lambda: reshaping.Permute((2, 1)), (2, 2, 2)),\n@@ -127,9 +128,9 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n       ('Minimum', merging.Minimum, [(2, 2), (2, 2)]),\n       ('Concatenate', merging.Concatenate, [(2, 2), (2, 2)]),\n       ('Dot', lambda: merging.Dot(1), [(2, 2), (2, 2)]),\n-      ('GaussianNoise', lambda: noise.GaussianNoise(0.5), (2, 2)),\n-      ('GaussianDropout', lambda: noise.GaussianDropout(0.5), (2, 2)),\n-      ('AlphaDropout', lambda: noise.AlphaDropout(0.5), (2, 2)),\n+      ('GaussianNoise', lambda: regularization.GaussianNoise(0.5), (2, 2)),\n+      ('GaussianDropout', lambda: regularization.GaussianDropout(0.5), (2, 2)),\n+      ('AlphaDropout', lambda: regularization.AlphaDropout(0.5), (2, 2)),\n       ('BatchNormalization', batch_normalization.BatchNormalization,\n        (2, 2), 1e-2, 1e-2),\n       ('LayerNormalization', layer_normalization.LayerNormalization, (2, 2)),\n\n@@ -34,6 +34,7 @@ from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n+from keras.layers import regularization\n from keras.optimizer_v2 import adadelta\n from keras.optimizer_v2 import adagrad\n from keras.optimizer_v2 import adam\n@@ -488,7 +489,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     dense = core.Dense(4, name='dense')\n     c = dense(a)\n     d = dense(b)\n-    e = core.Dropout(0.5, name='dropout')(c)\n+    e = regularization.Dropout(0.5, name='dropout')(c)\n \n     model = training.Model([a, b], [d, e])\n \n@@ -514,7 +515,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     a = input_layer.Input(shape=(3,), name='input_a')\n     model = sequential.Sequential()\n     model.add(core.Dense(4, kernel_initializer='zeros', name='dense'))\n-    model.add(core.Dropout(0.5, name='dropout'))\n+    model.add(regularization.Dropout(0.5, name='dropout'))\n     model(a)\n     optimizer = gradient_descent.SGD(learning_rate=0.1)\n     model.compile(optimizer, loss='mse', metrics=['mae'])\n\n@@ -26,6 +26,7 @@ from keras.engine import training\n from keras.layers import convolutional as conv_layer_lib\n from keras.layers import core as layer_lib\n from keras.layers import pooling as pool_layer_lib\n+from keras.layers import regularization as regularization_layer_lib\n from keras.layers import reshaping as reshaping_layer_lib\n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -140,10 +141,10 @@ def mnist_model(input_shape, enable_histograms=True):\n           32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n   model.add(conv_layer_lib.Conv2D(64, (3, 3), activation='relu'))\n   model.add(pool_layer_lib.MaxPooling2D(pool_size=(2, 2)))\n-  model.add(layer_lib.Dropout(0.25))\n+  model.add(regularization_layer_lib.Dropout(0.25))\n   model.add(reshaping_layer_lib.Flatten())\n   model.add(layer_lib.Dense(128, activation='relu'))\n-  model.add(layer_lib.Dropout(0.5))\n+  model.add(regularization_layer_lib.Dropout(0.5))\n   model.add(layer_lib.Dense(NUM_CLASSES, activation='softmax'))\n \n   # Adding custom pass-through layer for summary recording.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
