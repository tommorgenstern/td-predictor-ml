{"custom_id": "keras#d5a242e9f6c17a60bbbc7bd114b34ba59dfcca29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 22 | Churn Cumulative: 1220 | Contributors (this commit): 13 | Commits (past 90d): 15 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,6 +14,7 @@ import time\n import traceback\n import zipfile\n from abc import abstractmethod\n+from contextlib import closing\n from multiprocessing.pool import ThreadPool\n \n import numpy as np\n@@ -479,7 +480,7 @@ class OrderedEnqueuer(SequenceEnqueuer):\n         self.use_multiprocessing = use_multiprocessing\n         self.shuffle = shuffle\n         self.workers = 0\n-        self.executor = None\n+        self.executor_fn = None\n         self.queue = None\n         self.run_thread = None\n         self.stop_signal = None\n@@ -496,9 +497,9 @@ class OrderedEnqueuer(SequenceEnqueuer):\n                 (when full, workers could block on `put()`)\n         \"\"\"\n         if self.use_multiprocessing:\n-            self.executor = multiprocessing.Pool(workers)\n+            self.executor_fn = lambda: multiprocessing.Pool(workers)\n         else:\n-            self.executor = ThreadPool(workers)\n+            self.executor_fn = lambda: ThreadPool(workers)\n         self.workers = workers\n         self.queue = queue.Queue(max_queue_size)\n         self.stop_signal = threading.Event()\n@@ -520,11 +521,13 @@ class OrderedEnqueuer(SequenceEnqueuer):\n         while True:\n             if self.shuffle:\n                 random.shuffle(sequence)\n+\n+            with closing(self.executor_fn()) as executor:\n                 for i in sequence:\n                     if self.stop_signal.is_set():\n                         return\n                     self.queue.put(\n-                    self.executor.apply_async(get_index, (self.uid, i)), block=True)\n+                        executor.apply_async(get_index, (self.uid, i)), block=True)\n \n                 # Done with the current epoch, waiting for the final batches\n                 self._wait_queue()\n@@ -561,12 +564,6 @@ class OrderedEnqueuer(SequenceEnqueuer):\n         global _SHARED_SEQUENCES\n         _SHARED_SEQUENCES[self.uid] = self.sequence  # For new processes that may spawn\n \n-        self._close_pool()\n-        if self.use_multiprocessing:\n-            self.executor = multiprocessing.Pool(self.workers)\n-        else:\n-            self.executor = ThreadPool(self.workers)\n-\n     def stop(self, timeout=None):\n         \"\"\"Stops running threads and wait for them to exit, if necessary.\n \n@@ -581,14 +578,9 @@ class OrderedEnqueuer(SequenceEnqueuer):\n             self.queue.queue.clear()\n             self.queue.unfinished_tasks = 0\n             self.queue.not_full.notify()\n-        self._close_pool()\n         self.run_thread.join(timeout)\n         _SHARED_SEQUENCES[self.uid] = None\n \n-    def _close_pool(self):\n-        self.executor.close()\n-        self.executor.join()\n-\n \n class GeneratorEnqueuer(SequenceEnqueuer):\n     \"\"\"Builds a queue out of a data generator.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#409b0005b5d3bec469009a95302e95fa8bdf2452", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 9850 | Contributors (this commit): 80 | Commits (past 90d): 14 | Contributors (cumulative): 80 | DMM Complexity: None\n\nDIFF:\n@@ -959,7 +959,7 @@ class Sequential(Model):\n                               steps_per_epoch=steps_per_epoch,\n                               validation_steps=validation_steps)\n \n-    def evaluate(self, x, y, batch_size=32, verbose=1,\n+    def evaluate(self, x, y, batch_size=None, verbose=1,\n                  sample_weight=None):\n         \"\"\"Computes the loss on some input data, batch by batch.\n \n@@ -967,7 +967,7 @@ class Sequential(Model):\n             x: input data, as a Numpy array or list of Numpy arrays\n                 (if the model has multiple inputs).\n             y: labels, as a Numpy array.\n-            batch_size: integer. Number of samples per gradient update.\n+            batch_size: Integer. If unspecified, it will default to 32.\n             verbose: verbosity mode, 0 or 1.\n             sample_weight: sample weights, as a Numpy array.\n \n@@ -988,14 +988,14 @@ class Sequential(Model):\n                                    verbose=verbose,\n                                    sample_weight=sample_weight)\n \n-    def predict(self, x, batch_size=32, verbose=0):\n+    def predict(self, x, batch_size=None, verbose=0):\n         \"\"\"Generates output predictions for the input samples.\n \n         The input samples are processed batch by batch.\n \n         # Arguments\n             x: the input data, as a Numpy array.\n-            batch_size: integer.\n+            batch_size: Integer. If unspecified, it will default to 32.\n             verbose: verbosity mode, 0 or 1.\n \n         # Returns\n@@ -1072,7 +1072,7 @@ class Sequential(Model):\n         return self.model.test_on_batch(x, y,\n                                         sample_weight=sample_weight)\n \n-    def predict_proba(self, x, batch_size=32, verbose=0):\n+    def predict_proba(self, x, batch_size=None, verbose=0):\n         \"\"\"Generates class probability predictions for the input samples.\n \n         The input samples are processed batch by batch.\n@@ -1080,7 +1080,7 @@ class Sequential(Model):\n         # Arguments\n             x: input data, as a Numpy array or list of Numpy arrays\n                 (if the model has multiple inputs).\n-            batch_size: integer.\n+            batch_size: Integer. If unspecified, it will default to 32.\n             verbose: verbosity mode, 0 or 1.\n \n         # Returns\n@@ -1094,7 +1094,7 @@ class Sequential(Model):\n                           '(like softmax or sigmoid would).')\n         return preds\n \n-    def predict_classes(self, x, batch_size=32, verbose=0):\n+    def predict_classes(self, x, batch_size=None, verbose=0):\n         \"\"\"Generate class predictions for the input samples.\n \n         The input samples are processed batch by batch.\n@@ -1102,7 +1102,7 @@ class Sequential(Model):\n         # Arguments\n             x: input data, as a Numpy array or list of Numpy arrays\n                 (if the model has multiple inputs).\n-            batch_size: integer.\n+            batch_size: Integer. If unspecified, it will default to 32.\n             verbose: verbosity mode, 0 or 1.\n \n         # Returns\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fefa846e573fa160da033d1c90060bc0dcff4a56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 4 | Churn Cumulative: 3190 | Contributors (this commit): 16 | Commits (past 90d): 15 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@ from .. import backend as K\n CLASS_INDEX = None\n CLASS_INDEX_PATH = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n \n-# Global Numpy arrays of imagenet mean for preprocessing symbolic inputs\n+# Global tensor of imagenet mean for preprocessing symbolic inputs\n _IMAGENET_MEAN = None\n \n \n\n@@ -21,7 +21,7 @@ if dev.type() == 0:\n \n # A learning phase is a bool tensor used to run Keras models in\n # either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n-_LEARNING_PHASE = C.constant(shape=(), dtype=np.float32, value=1.0, name=\"_keras_learning_phase\")\n+_LEARNING_PHASE = C.constant(shape=(), dtype=np.float32, value=1.0, name='_keras_learning_phase')\n _UID_PREFIXES = defaultdict(int)\n \n # cntk doesn't support gradient as symbolic op, to hook up with keras model,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3502dc599e87e2dcdaf7a8922c1c1fb882eb98a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 487 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@ import numpy as np\n \n # the type of float to use throughout the session.\n _FLOATX = 'float32'\n-_EPSILON = 10e-8\n+_EPSILON = 1e-7\n _IMAGE_DATA_FORMAT = 'channels_last'\n \n \n@@ -15,7 +15,7 @@ def epsilon():\n     # Example\n     ```python\n         >>> keras.backend.epsilon()\n-        1e-08\n+        1e-07\n     ```\n     \"\"\"\n     return _EPSILON\n@@ -31,7 +31,7 @@ def set_epsilon(e):\n     ```python\n         >>> from keras import backend as K\n         >>> K.epsilon()\n-        1e-08\n+        1e-07\n         >>> K.set_epsilon(1e-05)\n         >>> K.epsilon()\n         1e-05\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d045e10195126981b79743adc9ad67e0ca5a4b62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2666 | Contributors (this commit): 45 | Commits (past 90d): 10 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -529,7 +529,7 @@ class ImageDataGenerator(object):\n         if self.samplewise_center:\n             x -= np.mean(x, keepdims=True)\n         if self.samplewise_std_normalization:\n-            x /= np.std(x, keepdims=True) + 1e-7\n+            x /= (np.std(x, keepdims=True) + K.epsilon())\n \n         if self.featurewise_center:\n             if self.mean is not None:\n@@ -541,7 +541,7 @@ class ImageDataGenerator(object):\n                               'first by calling `.fit(numpy_data)`.')\n         if self.featurewise_std_normalization:\n             if self.std is not None:\n-                x /= (self.std + 1e-7)\n+                x /= (self.std + K.epsilon())\n             else:\n                 warnings.warn('This ImageDataGenerator specifies '\n                               '`featurewise_std_normalization`, but it hasn\\'t '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35bc1d7900fc4e1da3e4fa4ac8cb778e13c4a8e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2125 | Contributors (this commit): 23 | Commits (past 90d): 1 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -135,7 +135,8 @@ class SGD(Optimizer):\n \n     # Arguments\n         lr: float >= 0. Learning rate.\n-        momentum: float >= 0. Parameter updates momentum.\n+        momentum: float >= 0. Parameter that accelerates SGD\n+            in the relevant direction and dampens oscillations.\n         decay: float >= 0. Learning rate decay over each update.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#017c93c07dff11a55691dc1a0ddf0f18c91e5267", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 2794 | Contributors (this commit): 13 | Commits (past 90d): 12 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -1052,7 +1052,7 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n     elif ndim(beta) == ndim(x) and shape(beta)[0] == 1:\n         beta = _reshape_dummy_dim(beta, [0])\n \n-    return gamma * ((x - mean) / C.sqrt(var + epsilon)) + beta\n+    return (x - mean) / (C.sqrt(var) + epsilon) * gamma + beta\n \n \n def concatenate(tensors, axis=-1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd71399ab5be2b86d5b9aebfb601713cfa660487", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 99 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 10/5 | Churn Δ: 100 | Churn Cumulative: 21491 | Contributors (this commit): 120 | Commits (past 90d): 27 | Contributors (cumulative): 157 | DMM Complexity: 0.6142857142857143\n\nDIFF:\n@@ -272,6 +272,7 @@ class Layer(object):\n                           'dtype',\n                           'name',\n                           'trainable',\n+                          'updatable',\n                           'weights',\n                           'input_dtype',  # legacy\n                           }\n@@ -285,6 +286,7 @@ class Layer(object):\n         self.name = name\n \n         self.trainable = kwargs.get('trainable', True)\n+        self.updatable = kwargs.get('updatable', True)\n         if 'input_shape' in kwargs or 'batch_input_shape' in kwargs:\n             # In this case we will later create an input layer\n             # to insert before the current layer\n@@ -1495,6 +1497,7 @@ class Container(Layer):\n \n         self.supports_masking = False\n         self.trainable = True\n+        self.updatable = True\n         self._per_input_losses = {}\n         self._per_input_updates = {}\n \n@@ -1882,9 +1885,13 @@ class Container(Layer):\n         # Returns\n             A list of update ops.\n         \"\"\"\n+        if not self.updatable:\n+            return []\n         updates = []\n         for layer in self.layers:\n             if hasattr(layer, 'updates'):\n+                if hasattr(layer, 'updatable') and not layer.updatable:\n+                    continue\n                 # Collect updates that are dependent on inputs\n                 # that are part of the model.\n                 for node_index, node in enumerate(layer.inbound_nodes):\n@@ -1955,6 +1962,8 @@ class Container(Layer):\n         for layer in self.layers:\n             if getattr(layer, 'stateful', False):\n                 if hasattr(layer, 'updates'):\n+                    if hasattr(layer, 'updatable') and not layer.updatable:\n+                        continue\n                     state_updates += layer.updates\n         return state_updates\n \n\n@@ -389,6 +389,7 @@ class Sequential(Model):\n         self.inputs = []  # List of input tensors\n         self.outputs = []  # List of length 1: the output tensor (unique).\n         self._trainable = True\n+        self._updatable = True\n         self._initial_weights = None\n \n         # Model attributes.\n@@ -553,6 +554,7 @@ class Sequential(Model):\n         self.model = Model(self.inputs, self.outputs[0],\n                            name=self.name + '_model')\n         self.model.trainable = self.trainable\n+        self.model.updatable = self.updatable\n \n         # mirror model attributes\n         self.supports_masking = self.model.supports_masking\n@@ -622,10 +624,20 @@ class Sequential(Model):\n \n     @trainable.setter\n     def trainable(self, value):\n-        if self.model:\n+        if self.built:\n             self.model.trainable = value\n         self._trainable = value\n \n+    @property\n+    def updatable(self):\n+        return self._updatable\n+\n+    @updatable.setter\n+    def updatable(self, value):\n+        if self.built:\n+            self.model.updatable = value\n+        self._updatable = value\n+\n     @property\n     def trainable_weights(self):\n         if not self.trainable:\n\n@@ -1,6 +1,7 @@\n import pytest\n import json\n import numpy as np\n+from numpy.testing import assert_allclose\n \n from keras.layers import Dense, Dropout, Conv2D, InputLayer\n from keras import layers\n@@ -767,5 +768,47 @@ def test_multi_output_mask():\n     assert K.int_shape(z)[1:] == (16, 16, 3)\n \n \n+@keras_test\n+def test_model_updatable_attribute():\n+    val_a = np.random.random((10, 4))\n+    val_out = np.random.random((10, 4))\n+\n+    a = Input(shape=(4,))\n+    layer = layers.BatchNormalization(input_shape=(4,))\n+    b = layer(a)\n+    model = Model(a, b)\n+\n+    model.trainable = False\n+    model.updatable = False\n+\n+    assert not model.updatable\n+    assert not model.updates\n+\n+    model.compile('sgd', 'mse')\n+    assert not model.updates\n+\n+    x1 = model.predict(val_a)\n+    model.train_on_batch(val_a, val_out)\n+    x2 = model.predict(val_a)\n+    assert_allclose(x1, x2, atol=1e-7)\n+\n+    model.updatable = True\n+    model.compile('sgd', 'mse')\n+    assert model.updates\n+\n+    model.train_on_batch(val_a, val_out)\n+    x2 = model.predict(val_a)\n+    assert np.abs(np.sum(x1 - x2)) > 1e-5\n+\n+    layer.updatable = False\n+    model.compile('sgd', 'mse')\n+    assert not model.updates\n+\n+    x1 = model.predict(val_a)\n+    model.train_on_batch(val_a, val_out)\n+    x2 = model.predict(val_a)\n+    assert_allclose(x1, x2, atol=1e-7)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -3,6 +3,7 @@ from __future__ import print_function\n import pytest\n import os\n import numpy as np\n+from numpy.testing import assert_allclose\n \n from keras import backend as K\n import keras\n@@ -366,5 +367,38 @@ def test_clone_sequential_model():\n     new_model.train_on_batch(None, val_out)\n \n \n+@keras_test\n+def test_sequential_updatable_attribute():\n+    val_a = np.random.random((10, 4))\n+    val_out = np.random.random((10, 4))\n+\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.BatchNormalization(input_shape=(4,)))\n+\n+    model.trainable = False\n+    model.updatable = False\n+\n+    assert not model.updatable\n+    assert not model.updates\n+\n+    model.compile('sgd', 'mse')\n+    assert not model.updates\n+    assert not model.model.updates\n+\n+    x1 = model.predict(val_a)\n+    model.train_on_batch(val_a, val_out)\n+    x2 = model.predict(val_a)\n+    assert_allclose(x1, x2, atol=1e-7)\n+\n+    model.updatable = True\n+    model.compile('sgd', 'mse')\n+    assert model.updates\n+    assert model.model.updates\n+\n+    model.train_on_batch(val_a, val_out)\n+    x2 = model.predict(val_a)\n+    assert np.abs(np.sum(x1 - x2)) > 1e-5\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e6825209c8f141f7cf44604cb69e16528a55b3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9052 | Contributors (this commit): 98 | Commits (past 90d): 25 | Contributors (cumulative): 98 | DMM Complexity: None\n\nDIFF:\n@@ -1090,7 +1090,7 @@ def batch_dot(x, y, axes=None):\n \n     # Examples\n         Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n-        `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n+        `batch_dot(x, y, axes=1) = [[17], [53]]` which is the main diagonal\n         of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n         elements.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a04c41a7050f36178962b40354da5094eaea73ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 249 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -19,9 +19,10 @@ from keras.utils.data_utils import get_file\n import numpy as np\n import random\n import sys\n+import io\n \n path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n-text = open(path).read().lower()\n+text = io.open(path, encoding='utf-8').read().lower()\n print('corpus length:', len(text))\n \n chars = sorted(list(set(text)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1bb34d70d69bd77d3b5559166e5753b10226e5ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 5854 | Contributors (this commit): 75 | Commits (past 90d): 28 | Contributors (cumulative): 75 | DMM Complexity: None\n\nDIFF:\n@@ -1698,8 +1698,7 @@ class Model(Container):\n             steps: Integer or `None`.\n                 Total number of steps (batches of samples)\n                 before declaring the evaluation round finished.\n-                The default `None` is equal to the number of samples in\n-                your dataset divided by the batch size.\n+                Ignored with the default value of `None`.\n \n \n         # Returns\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00a691dd5c55f31f672e2ce0d82e048040301c51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 9882 | Contributors (this commit): 80 | Commits (past 90d): 15 | Contributors (cumulative): 80 | DMM Complexity: 1.0\n\nDIFF:\n@@ -971,17 +971,28 @@ class Sequential(Model):\n                               steps_per_epoch=steps_per_epoch,\n                               validation_steps=validation_steps)\n \n-    def evaluate(self, x, y, batch_size=None, verbose=1,\n-                 sample_weight=None):\n+    def evaluate(self, x=None, y=None,\n+                 batch_size=None,\n+                 verbose=1,\n+                 sample_weight=None,\n+                 steps=None):\n         \"\"\"Computes the loss on some input data, batch by batch.\n \n         # Arguments\n             x: input data, as a Numpy array or list of Numpy arrays\n                 (if the model has multiple inputs).\n+                `x` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n             y: labels, as a Numpy array.\n+                `y` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n             batch_size: Integer. If unspecified, it will default to 32.\n             verbose: verbosity mode, 0 or 1.\n             sample_weight: sample weights, as a Numpy array.\n+            steps: Integer or `None`.\n+                Total number of steps (batches of samples)\n+                before declaring the evaluation round finished.\n+                Ignored with the default value of `None`.\n \n         # Returns\n             Scalar test loss (if the model has no metrics)\n@@ -998,7 +1009,8 @@ class Sequential(Model):\n         return self.model.evaluate(x, y,\n                                    batch_size=batch_size,\n                                    verbose=verbose,\n-                                   sample_weight=sample_weight)\n+                                   sample_weight=sample_weight,\n+                                   steps=steps)\n \n     def predict(self, x, batch_size=None, verbose=0):\n         \"\"\"Generates output predictions for the input samples.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b2ca432cd237d708eff74ec78bea5240aae24fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 29 | Churn Cumulative: 2338 | Contributors (this commit): 26 | Commits (past 90d): 4 | Contributors (cumulative): 28 | DMM Complexity: 0.35714285714285715\n\nDIFF:\n@@ -394,13 +394,17 @@ class Adam(Optimizer):\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor.\n         decay: float >= 0. Learning rate decay over each update.\n+        amsgrad: boolean. Weather to apply the AMSGrad variant of this\n+            algorithm from the paper \"On the Convergence of Adam and\n+            Beyond\".\n \n     # References\n         - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n+        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n     \"\"\"\n \n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n-                 epsilon=1e-8, decay=0., **kwargs):\n+                 epsilon=1e-8, decay=0., amsgrad=False, **kwargs):\n         super(Adam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n@@ -410,6 +414,7 @@ class Adam(Optimizer):\n             self.decay = K.variable(decay, name='decay')\n         self.epsilon = epsilon\n         self.initial_decay = decay\n+        self.amsgrad = amsgrad\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -427,11 +432,20 @@ class Adam(Optimizer):\n \n         ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n         vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n-        self.weights = [self.iterations] + ms + vs\n+        if self.amsgrad:\n+            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n+        else:\n+            vhats = [None for p in params]\n+        self.weights = [self.iterations] + ms + vs + vhats\n \n-        for p, g, m, v in zip(params, grads, ms, vs):\n+        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n             m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n             v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n+            if self.amsgrad:\n+                vhat_t = K.maximum(vhat, v_t)\n+                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n+                self.updates.append(K.update(vhat, vhat_t))\n+            else:\n                 p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n \n             self.updates.append(K.update(m, m_t))\n@@ -450,7 +464,8 @@ class Adam(Optimizer):\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n                   'decay': float(K.get_value(self.decay)),\n-                  'epsilon': self.epsilon}\n+                  'epsilon': self.epsilon,\n+                  'amsgrad': self.amsgrad}\n         base_config = super(Adam, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -105,6 +105,12 @@ def test_nadam():\n     _test_optimizer(optimizers.Nadam())\n \n \n+@keras_test\n+def test_adam_amsgrad():\n+    _test_optimizer(optimizers.Adam(amsgrad=True))\n+    _test_optimizer(optimizers.Adam(amsgrad=True, decay=1e-3))\n+\n+\n @keras_test\n def test_clipnorm():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=0.5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d210dde3e701bfa542cd4384dc28035078049687", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 26 | Churn Cumulative: 2692 | Contributors (this commit): 45 | Commits (past 90d): 10 | Contributors (cumulative): 45 | DMM Complexity: 0.0\n\nDIFF:\n@@ -479,6 +479,32 @@ class ImageDataGenerator(object):\n             raise ValueError('`zoom_range` should be a float or '\n                              'a tuple or list of two floats. '\n                              'Received arg: ', zoom_range)\n+        if zca_whitening:\n+            if not featurewise_center:\n+                self.featurewise_center = True\n+                warnings.warn('This ImageDataGenerator specifies '\n+                              '`zca_whitening`, which overrides '\n+                              'setting of `featurewise_center`.')\n+            if featurewise_std_normalization:\n+                self.featurewise_std_normalization = False\n+                warnings.warn('This ImageDataGenerator specifies '\n+                              '`zca_whitening` '\n+                              'which overrides setting of'\n+                              '`featurewise_std_normalization`.')\n+        if featurewise_std_normalization:\n+            if not featurewise_center:\n+                self.featurewise_center = True\n+                warnings.warn('This ImageDataGenerator specifies '\n+                              '`featurewise_std_normalization`, '\n+                              'which overrides setting of '\n+                              '`featurewise_center`.')\n+        if samplewise_std_normalization:\n+            if not samplewise_center:\n+                self.samplewise_center = True\n+                warnings.warn('This ImageDataGenerator specifies '\n+                              '`samplewise_std_normalization`, '\n+                              'which overrides setting of '\n+                              '`samplewise_center`.')\n \n     def flow(self, x, y=None, batch_size=32, shuffle=True, seed=None,\n              save_to_dir=None, save_prefix='', save_format='png'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb3ee31438d64ed7c2e4dd0df425e51367a8aaec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 1310 | Contributors (this commit): 18 | Commits (past 90d): 2 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -57,7 +57,7 @@ class BaseWrapper(object):\n         self.check_params(sk_params)\n \n     def check_params(self, params):\n-        \"\"\"Checks for user typos in \"params\".\n+        \"\"\"Checks for user typos in `params`.\n \n         # Arguments\n             params: dictionary; the parameters to be checked\n@@ -115,7 +115,7 @@ class BaseWrapper(object):\n \n         # Arguments\n             x : array-like, shape `(n_samples, n_features)`\n-                Training samples where n_samples in the number of samples\n+                Training samples where n_samples is the number of samples\n                 and n_features is the number of features.\n             y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n                 True labels for X.\n@@ -149,14 +149,14 @@ class BaseWrapper(object):\n         return history\n \n     def filter_sk_params(self, fn, override=None):\n-        \"\"\"Filters `sk_params` and return those in `fn`'s arguments.\n+        \"\"\"Filters `sk_params` and returns those in `fn`'s arguments.\n \n         # Arguments\n             fn : arbitrary function\n             override: dictionary, values to override sk_params\n \n         # Returns\n-            res : dictionary dictionary containing variables\n+            res : dictionary containing variables\n                 in both sk_params and fn's arguments.\n         \"\"\"\n         override = override or {}\n@@ -177,7 +177,7 @@ class KerasClassifier(BaseWrapper):\n \n         # Arguments\n             x : array-like, shape `(n_samples, n_features)`\n-                Training samples where n_samples in the number of samples\n+                Training samples where n_samples is the number of samples\n                 and n_features is the number of features.\n             y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n                 True labels for X.\n@@ -207,7 +207,7 @@ class KerasClassifier(BaseWrapper):\n \n         # Arguments\n             x: array-like, shape `(n_samples, n_features)`\n-                Test samples where n_samples in the number of samples\n+                Test samples where n_samples is the number of samples\n                 and n_features is the number of features.\n             **kwargs: dictionary arguments\n                 Legal arguments are the arguments\n@@ -226,7 +226,7 @@ class KerasClassifier(BaseWrapper):\n \n         # Arguments\n             x: array-like, shape `(n_samples, n_features)`\n-                Test samples where n_samples in the number of samples\n+                Test samples where n_samples is the number of samples\n                 and n_features is the number of features.\n             **kwargs: dictionary arguments\n                 Legal arguments are the arguments\n@@ -236,8 +236,8 @@ class KerasClassifier(BaseWrapper):\n             proba: array-like, shape `(n_samples, n_outputs)`\n                 Class probability estimates.\n                 In the case of binary classification,\n-                tp match the scikit-learn API,\n-                will return an array of shape '(n_samples, 2)'\n+                to match the scikit-learn API,\n+                will return an array of shape `(n_samples, 2)`\n                 (instead of `(n_sample, 1)` as in Keras).\n         \"\"\"\n         kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n@@ -254,7 +254,7 @@ class KerasClassifier(BaseWrapper):\n \n         # Arguments\n             x: array-like, shape `(n_samples, n_features)`\n-                Test samples where n_samples in the number of samples\n+                Test samples where n_samples is the number of samples\n                 and n_features is the number of features.\n             y: array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n                 True labels for x.\n@@ -299,7 +299,7 @@ class KerasRegressor(BaseWrapper):\n \n         # Arguments\n             x: array-like, shape `(n_samples, n_features)`\n-                Test samples where n_samples in the number of samples\n+                Test samples where n_samples is the number of samples\n                 and n_features is the number of features.\n             **kwargs: dictionary arguments\n                 Legal arguments are the arguments of `Sequential.predict`.\n@@ -316,7 +316,7 @@ class KerasRegressor(BaseWrapper):\n \n         # Arguments\n             x: array-like, shape `(n_samples, n_features)`\n-                Test samples where n_samples in the number of samples\n+                Test samples where n_samples is the number of samples\n                 and n_features is the number of features.\n             y: array-like, shape `(n_samples,)`\n                 True labels for X.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#321d838ba880c3c3529997354617963bb501eaec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 23 | Methods Changed: 16 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 46 | Churn Cumulative: 12354 | Contributors (this commit): 120 | Commits (past 90d): 33 | Contributors (cumulative): 133 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ layer_name = 'block5_conv1'\n def deprocess_image(x):\n     # normalize tensor: center on 0., ensure std is 0.1\n     x -= x.mean()\n-    x /= (x.std() + 1e-5)\n+    x /= (x.std() + K.epsilon())\n     x *= 0.1\n \n     # clip to [0, 1]\n@@ -55,7 +55,7 @@ layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n \n def normalize(x):\n     # utility function to normalize a tensor by its L2 norm\n-    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n+    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n \n \n kept_filters = []\n\n@@ -96,7 +96,7 @@ for layer_name in settings['features']:\n # Compute the gradients of the dream wrt the loss.\n grads = K.gradients(loss, dream)[0]\n # Normalize gradients.\n-grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n+grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon())\n \n # Set up function to retrieve the value\n # of the loss and gradients given an input image.\n\n@@ -3756,7 +3756,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     input_length = tf.to_int32(tf.squeeze(input_length))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n \n-    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + 1e-8)\n+    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n \n     return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\n                                        labels=sparse_labels,\n@@ -3792,7 +3792,7 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n             Tensor `(top_paths, )` that contains\n                 the log probability of each decoded sequence.\n     \"\"\"\n-    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + 1e-8)\n+    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n     input_length = tf.to_int32(input_length)\n \n     if greedy:\n\n@@ -203,14 +203,14 @@ class RMSprop(Optimizer):\n     # Arguments\n         lr: float >= 0. Learning rate.\n         rho: float >= 0.\n-        epsilon: float >= 0. Fuzz factor.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n         decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [rmsprop: Divide the gradient by a running average of its recent magnitude](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     \"\"\"\n \n-    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-8, decay=0.,\n+    def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.,\n                  **kwargs):\n         super(RMSprop, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n@@ -218,6 +218,8 @@ class RMSprop(Optimizer):\n             self.rho = K.variable(rho, name='rho')\n             self.decay = K.variable(decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n         self.epsilon = epsilon\n         self.initial_decay = decay\n \n@@ -263,19 +265,21 @@ class Adagrad(Optimizer):\n \n     # Arguments\n         lr: float >= 0. Learning rate.\n-        epsilon: float >= 0.\n+        epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n         decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     \"\"\"\n \n-    def __init__(self, lr=0.01, epsilon=1e-8, decay=0., **kwargs):\n+    def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.lr = K.variable(lr, name='lr')\n             self.decay = K.variable(decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n         self.epsilon = epsilon\n         self.initial_decay = decay\n \n@@ -322,20 +326,22 @@ class Adadelta(Optimizer):\n         lr: float >= 0. Learning rate.\n             It is recommended to leave it at the default value.\n         rho: float >= 0.\n-        epsilon: float >= 0. Fuzz factor.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n         decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n     \"\"\"\n \n-    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-8, decay=0.,\n+    def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.,\n                  **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.lr = K.variable(lr, name='lr')\n             self.decay = K.variable(decay, name='decay')\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n         self.rho = rho\n         self.epsilon = epsilon\n         self.initial_decay = decay\n@@ -392,7 +398,7 @@ class Adam(Optimizer):\n         lr: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n-        epsilon: float >= 0. Fuzz factor.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n         decay: float >= 0. Learning rate decay over each update.\n         amsgrad: boolean. Weather to apply the AMSGrad variant of this\n             algorithm from the paper \"On the Convergence of Adam and\n@@ -404,7 +410,7 @@ class Adam(Optimizer):\n     \"\"\"\n \n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n-                 epsilon=1e-8, decay=0., amsgrad=False, **kwargs):\n+                 epsilon=None, decay=0., amsgrad=False, **kwargs):\n         super(Adam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n@@ -412,6 +418,8 @@ class Adam(Optimizer):\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n             self.decay = K.variable(decay, name='decay')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n         self.epsilon = epsilon\n         self.initial_decay = decay\n         self.amsgrad = amsgrad\n@@ -479,7 +487,7 @@ class Adamax(Optimizer):\n     # Arguments\n         lr: float >= 0. Learning rate.\n         beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n-        epsilon: float >= 0. Fuzz factor.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n         decay: float >= 0. Learning rate decay over each update.\n \n     # References\n@@ -487,7 +495,7 @@ class Adamax(Optimizer):\n     \"\"\"\n \n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n-                 epsilon=1e-8, decay=0., **kwargs):\n+                 epsilon=None, decay=0., **kwargs):\n         super(Adamax, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n@@ -495,6 +503,8 @@ class Adamax(Optimizer):\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n             self.decay = K.variable(decay, name='decay')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n         self.epsilon = epsilon\n         self.initial_decay = decay\n \n@@ -558,7 +568,7 @@ class Nadam(Optimizer):\n     # Arguments\n         lr: float >= 0. Learning rate.\n         beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n-        epsilon: float >= 0. Fuzz factor.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n \n     # References\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n@@ -566,7 +576,7 @@ class Nadam(Optimizer):\n     \"\"\"\n \n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n-                 epsilon=1e-8, schedule_decay=0.004, **kwargs):\n+                 epsilon=None, schedule_decay=0.004, **kwargs):\n         super(Nadam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n@@ -574,6 +584,8 @@ class Nadam(Optimizer):\n             self.lr = K.variable(lr, name='lr')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n         self.epsilon = epsilon\n         self.schedule_decay = schedule_decay\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89ac314d3727c33492791a140501244465f7abfa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 20 | Churn Cumulative: 1135 | Contributors (this commit): 27 | Commits (past 90d): 20 | Contributors (cumulative): 33 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -1,6 +1,7 @@\n \"\"\"Python utilities required by Keras.\"\"\"\n from __future__ import absolute_import\n \n+import binascii\n import numpy as np\n \n import time\n@@ -221,7 +222,11 @@ def func_load(code, defaults=None, closure=None, globs=None):\n \n     if closure is not None:\n         closure = tuple(ensure_value_to_cell(_) for _ in closure)\n+    try:\n         raw_code = codecs.decode(code.encode('ascii'), 'base64')\n+    except (UnicodeEncodeError, binascii.Error):\n+        # backwards compatibility for models serialized prior to 2.1.2\n+        raw_code = code.encode('raw_unicode_escape')\n     code = marshal.loads(raw_code)\n     if globs is None:\n         globs = globals()\n\n@@ -1,6 +1,7 @@\n import sys\n import pytest\n import numpy as np\n+import marshal\n from keras.utils.generic_utils import custom_object_scope\n from keras.utils.generic_utils import has_arg\n from keras.utils.generic_utils import Progbar\n@@ -105,5 +106,19 @@ def test_func_dump_and_load(test_function_type):\n     assert deserialized.__closure__ == test_func.__closure__\n \n \n+@pytest.mark.parametrize(\n+    'test_func', [activations.softmax, np.argmax, lambda x: x**2])\n+def test_func_dump_and_load_backwards_compat(test_func):\n+    # this test ensures that models serialized prior to version 2.1.2 can still be\n+    # deserialized\n+\n+    # see https://github.com/evhub/keras/blob/2.1.1/keras/utils/generic_utils.py#L166\n+    serialized = marshal.dumps(test_func.__code__).decode('raw_unicode_escape')\n+\n+    deserialized = func_load(serialized, defaults=test_func.__defaults__)\n+    assert deserialized.__code__ == test_func.__code__\n+    assert deserialized.__defaults__ == test_func.__defaults__\n+    assert deserialized.__closure__ == test_func.__closure__\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#29ee89a0663c96f6476aa7145b4f27684442b779", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 648 | Lines Deleted: 97 | Files Changed: 2 | Hunks: 79 | Methods Changed: 17 | Complexity Δ (Sum/Max): 22/15 | Churn Δ: 745 | Churn Cumulative: 2406 | Contributors (this commit): 18 | Commits (past 90d): 17 | Contributors (cumulative): 23 | DMM Complexity: 0.9571045576407506\n\nDIFF:\n@@ -604,6 +604,15 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n                  seed=None):\n         self.wait_time = wait_time\n         self._generator = generator\n+        if os.name is 'nt' and use_multiprocessing is True:\n+            # On Windows, avoid **SYSTEMATIC** error in `multiprocessing`:\n+            # `TypeError: can't pickle generator objects`\n+            # => Suggest multithreading instead of multiprocessing on Windows\n+            raise ValueError('Using a generator with `use_multiprocessing=True`'\n+                             ' is not supported on Windows (no marshalling of'\n+                             ' generators across process boundaries). Instead,'\n+                             ' use single thread/process or multithreading.')\n+        else:\n             self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n@@ -611,6 +620,48 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n         self.queue = None\n         self.seed = seed\n \n+    def _data_generator_task(self):\n+        if self._use_multiprocessing is False:\n+            while not self._stop_event.is_set():\n+                with self.genlock:\n+                    try:\n+                        if self.queue is not None and self.queue.qsize() < self.max_queue_size:\n+                            # On all OSes, avoid **SYSTEMATIC** error in multithreading mode:\n+                            # `ValueError: generator already executing`\n+                            # => Serialize calls to infinite iterator/generator's next() function\n+                            generator_output = next(self._generator)\n+                            self.queue.put((True, generator_output))\n+                        else:\n+                            time.sleep(self.wait_time)\n+                    except StopIteration:\n+                        break\n+                    except Exception as e:\n+                        # Can't pickle tracebacks.\n+                        # As a compromise, print the traceback and pickle None instead.\n+                        if not hasattr(e, '__traceback__'):\n+                            setattr(e, '__traceback__', sys.exc_info()[2])\n+                        self.queue.put((False, e))\n+                        self._stop_event.set()\n+                        break\n+        else:\n+            while not self._stop_event.is_set():\n+                try:\n+                    if self.queue is not None and self.queue.qsize() < self.max_queue_size:\n+                        generator_output = next(self._generator)\n+                        self.queue.put((True, generator_output))\n+                    else:\n+                        time.sleep(self.wait_time)\n+                except StopIteration:\n+                    break\n+                except Exception as e:\n+                    # Can't pickle tracebacks.\n+                    # As a compromise, print the traceback and pickle None instead.\n+                    traceback.print_exc()\n+                    setattr(e, '__traceback__', None)\n+                    self.queue.put((False, e))\n+                    self._stop_event.set()\n+                    break\n+\n     def start(self, workers=1, max_queue_size=10):\n         \"\"\"Kicks off threads which add data from the generator into the queue.\n \n@@ -619,36 +670,18 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n             max_queue_size: queue size\n                 (when full, threads could block on `put()`)\n         \"\"\"\n-\n-        def data_generator_task():\n-            while not self._stop_event.is_set():\n-                try:\n-                    if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n-                        generator_output = next(self._generator)\n-                        self.queue.put((True, generator_output))\n-                    else:\n-                        time.sleep(self.wait_time)\n-                except StopIteration:\n-                    break\n-                except Exception as e:\n-                    # Can't pick tracebacks.\n-                    # As a compromise, print the traceback and pickle None instead.\n-                    if self._use_multiprocessing:\n-                        traceback.print_exc()\n-                        setattr(e, '__traceback__', None)\n-                    elif not hasattr(e, '__traceback__'):\n-                        setattr(e, '__traceback__', sys.exc_info()[2])\n-                    self.queue.put((False, e))\n-                    self._stop_event.set()\n-                    break\n-\n         try:\n+            self.max_queue_size = max_queue_size\n             if self._use_multiprocessing:\n                 self._manager = multiprocessing.Manager()\n                 self.queue = self._manager.Queue(maxsize=max_queue_size)\n                 self._stop_event = multiprocessing.Event()\n             else:\n-                self.queue = queue.Queue()\n+                # On all OSes, avoid **SYSTEMATIC** error in multithreading mode:\n+                # `ValueError: generator already executing`\n+                # => Serialize calls to infinite iterator/generator's next() function\n+                self.genlock = threading.Lock()\n+                self.queue = queue.Queue(maxsize=max_queue_size)\n                 self._stop_event = threading.Event()\n \n             for _ in range(workers):\n@@ -656,12 +689,12 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n                     # Reset random seed else all children processes\n                     # share the same seed\n                     np.random.seed(self.seed)\n-                    thread = multiprocessing.Process(target=data_generator_task)\n+                    thread = multiprocessing.Process(target=self._data_generator_task)\n                     thread.daemon = True\n                     if self.seed is not None:\n                         self.seed += 1\n                 else:\n-                    thread = threading.Thread(target=data_generator_task)\n+                    thread = threading.Thread(target=self._data_generator_task)\n                 self._threads.append(thread)\n                 thread.start()\n         except:\n@@ -683,10 +716,14 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n             self._stop_event.set()\n \n         for thread in self._threads:\n-            if thread.is_alive():\n             if self._use_multiprocessing:\n+                if thread.is_alive():\n                     thread.terminate()\n             else:\n+                # The thread.is_alive() test is subject to a race condition:\n+                # the thread could terminate right after the test and before the\n+                # join, rendering this test meaningless -> Call thread.join()\n+                # always, which is ok no matter what the status of the thread.\n                 thread.join(timeout)\n \n         if self._manager:\n\n@@ -1,11 +1,16 @@\n from __future__ import print_function\n import os\n+import threading\n import pytest\n import numpy as np\n from keras.models import Sequential\n from keras.layers.core import Dense\n from keras.utils.test_utils import keras_test\n \n+STEPS_PER_EPOCH = 100\n+STEPS = 100\n+WORKERS = 4\n+\n \n @pytest.fixture\n def in_tmpdir(tmpdir):\n@@ -45,38 +50,130 @@ def test_multiprocessing_training():\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n+    # - Produce data on 4 worker processes, consume on main process:\n+    #   - Each worker process runs OWN copy of generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `fit_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n             model.fit_generator(custom_generator(),\n-                        steps_per_epoch=5,\n+                                steps_per_epoch=STEPS_PER_EPOCH,\n                                 epochs=1,\n                                 verbose=1,\n+                                validation_steps=None,\n                                 max_queue_size=10,\n-                        workers=4,\n+                                workers=WORKERS,\n+                                use_multiprocessing=True)\n+    else:\n+        model.fit_generator(custom_generator(),\n+                            steps_per_epoch=STEPS_PER_EPOCH,\n+                            epochs=1,\n+                            verbose=1,\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=WORKERS,\n                             use_multiprocessing=True)\n \n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n     model.fit_generator(custom_generator(),\n-                        steps_per_epoch=5,\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n                         epochs=1,\n                         verbose=1,\n+                        validation_steps=None,\n                         max_queue_size=10,\n+                        workers=WORKERS,\n                         use_multiprocessing=False)\n \n+    # - Produce data on 1 worker process, consume on main process:\n+    #   - Worker process runs generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `fit_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n             model.fit_generator(custom_generator(True),\n-                        steps_per_epoch=5,\n+                                steps_per_epoch=STEPS_PER_EPOCH,\n                                 validation_data=(arr_data[:10],\n                                                  arr_labels[:10],\n                                                  arr_weights[:10]),\n-                        validation_steps=1)\n-\n+                                validation_steps=1,\n+                                max_queue_size=10,\n+                                workers=1,\n+                                use_multiprocessing=True)\n+    else:\n         model.fit_generator(custom_generator(True),\n-                        steps_per_epoch=5,\n-                        validation_data=custom_generator(True),\n-                        validation_steps=1)\n+                            steps_per_epoch=STEPS_PER_EPOCH,\n+                            validation_data=(arr_data[:10],\n+                                             arr_labels[:10],\n+                                             arr_weights[:10]),\n+                            validation_steps=1,\n+                            max_queue_size=10,\n+                            workers=1,\n+                            use_multiprocessing=True)\n \n+    # - Produce data on 1 worker thread, consume on main thread:\n+    #   - Worker thread is the only thread running the generator\n     model.fit_generator(custom_generator(True),\n-                        steps_per_epoch=5,\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        validation_data=(arr_data[:10],\n+                                         arr_labels[:10],\n+                                         arr_weights[:10]),\n+                        validation_steps=1,\n+                        max_queue_size=10,\n+                        workers=1,\n+                        use_multiprocessing=False)\n+\n+    # - Produce data on 1 worker process, consume on main process:\n+    #   - Worker process runs generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `fit_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.fit_generator(custom_generator(True),\n+                                steps_per_epoch=STEPS_PER_EPOCH,\n                                 validation_data=custom_generator(True),\n                                 validation_steps=1,\n-                        workers=0)\n+                                max_queue_size=10,\n+                                workers=1,\n+                                use_multiprocessing=True)\n+    else:\n+        model.fit_generator(custom_generator(True),\n+                            steps_per_epoch=STEPS_PER_EPOCH,\n+                            validation_data=custom_generator(True),\n+                            validation_steps=1,\n+                            max_queue_size=10,\n+                            workers=1,\n+                            use_multiprocessing=True)\n+\n+    # - Produce data on 1 worker thread AT A TIME, consume on main thread:\n+    #   - Worker threads for training and validation run generator SEQUENTIALLY\n+    model.fit_generator(custom_generator(True),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        validation_data=custom_generator(True),\n+                        validation_steps=1,\n+                        max_queue_size=10,\n+                        workers=1,\n+                        use_multiprocessing=False)\n+\n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    model.fit_generator(custom_generator(True),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        validation_data=custom_generator(True),\n+                        validation_steps=1,\n+                        max_queue_size=10,\n+                        workers=0,\n+                        use_multiprocessing=True)\n+    model.fit_generator(custom_generator(True),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        validation_data=custom_generator(True),\n+                        validation_steps=1,\n+                        max_queue_size=10,\n+                        workers=0,\n+                        use_multiprocessing=False)\n \n     # Test invalid use cases\n     def invalid_generator():\n@@ -86,29 +183,39 @@ def test_multiprocessing_training():\n     # not specified `validation_steps`\n     with pytest.raises(ValueError):\n         model.fit_generator(custom_generator(),\n-                            steps_per_epoch=5,\n-                            validation_data=custom_generator())\n+                            steps_per_epoch=STEPS_PER_EPOCH,\n+                            validation_data=custom_generator(),\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=1,\n+                            use_multiprocessing=False)\n \n     # validation data is neither a tuple nor a triple.\n     with pytest.raises(ValueError):\n         model.fit_generator(custom_generator(),\n-                            steps_per_epoch=5,\n+                            steps_per_epoch=STEPS_PER_EPOCH,\n                             validation_data=(arr_data[:10],\n                                              arr_data[:10],\n                                              arr_labels[:10],\n                                              arr_weights[:10]),\n-                            validation_steps=1)\n+                            validation_steps=1,\n+                            max_queue_size=10,\n+                            workers=1,\n+                            use_multiprocessing=False)\n \n     # validation generator is neither a tuple nor a triple.\n     with pytest.raises(ValueError):\n         model.fit_generator(custom_generator(),\n-                            steps_per_epoch=5,\n+                            steps_per_epoch=STEPS_PER_EPOCH,\n                             validation_data=invalid_generator(),\n-                            validation_steps=1)\n+                            validation_steps=1,\n+                            max_queue_size=10,\n+                            workers=1,\n+                            use_multiprocessing=False)\n \n \n @keras_test\n-def test_multiprocessing_training_fromfile(in_tmpdir):\n+def test_multiprocessing_training_from_file(in_tmpdir):\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n     np.savez('data.npz', **{'data': arr_data, 'labels': arr_labels})\n@@ -133,19 +240,95 @@ def test_multiprocessing_training_fromfile(in_tmpdir):\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n+    # - Produce data on 4 worker processes, consume on main process:\n+    #   - Each worker process runs OWN copy of generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `fit_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n             model.fit_generator(custom_generator(),\n-                        steps_per_epoch=5,\n+                                steps_per_epoch=STEPS_PER_EPOCH,\n                                 epochs=1,\n                                 verbose=1,\n+                                validation_steps=None,\n                                 max_queue_size=10,\n-                        workers=2,\n+                                workers=WORKERS,\n+                                use_multiprocessing=True)\n+    else:\n+        model.fit_generator(custom_generator(),\n+                            steps_per_epoch=STEPS_PER_EPOCH,\n+                            epochs=1,\n+                            verbose=1,\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=WORKERS,\n                             use_multiprocessing=True)\n \n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n     model.fit_generator(custom_generator(),\n-                        steps_per_epoch=5,\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n                         epochs=1,\n                         verbose=1,\n+                        validation_steps=None,\n                         max_queue_size=10,\n+                        workers=WORKERS,\n+                        use_multiprocessing=False)\n+\n+    # - Produce data on 1 worker process, consume on main process:\n+    #   - Worker process runs generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `fit_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.fit_generator(custom_generator(),\n+                                steps_per_epoch=STEPS_PER_EPOCH,\n+                                epochs=1,\n+                                verbose=1,\n+                                validation_steps=None,\n+                                max_queue_size=10,\n+                                workers=1,\n+                                use_multiprocessing=True)\n+    else:\n+        model.fit_generator(custom_generator(),\n+                            steps_per_epoch=STEPS_PER_EPOCH,\n+                            epochs=1,\n+                            verbose=1,\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=1,\n+                            use_multiprocessing=True)\n+\n+    # - Produce data on 1 worker thread, consume on main thread:\n+    #   - Worker thread is the only thread running the generator\n+    model.fit_generator(custom_generator(),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        epochs=1,\n+                        verbose=1,\n+                        validation_steps=None,\n+                        max_queue_size=10,\n+                        workers=1,\n+                        use_multiprocessing=False)\n+\n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    model.fit_generator(custom_generator(),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        epochs=1,\n+                        verbose=1,\n+                        validation_steps=None,\n+                        max_queue_size=10,\n+                        workers=0,\n+                        use_multiprocessing=True)\n+    model.fit_generator(custom_generator(),\n+                        steps_per_epoch=STEPS_PER_EPOCH,\n+                        epochs=1,\n+                        verbose=1,\n+                        validation_steps=None,\n+                        max_queue_size=10,\n+                        workers=0,\n                         use_multiprocessing=False)\n \n     os.remove('data.npz')\n@@ -170,19 +353,73 @@ def test_multiprocessing_predicting():\n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n+\n+    # - Produce data on 4 worker processes, consume on main process:\n+    #   - Each worker process runs OWN copy of generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `predict_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n             model.predict_generator(custom_generator(),\n-                            steps=5,\n+                                    steps=STEPS,\n                                     max_queue_size=10,\n-                            workers=2,\n+                                    workers=WORKERS,\n+                                    use_multiprocessing=True)\n+    else:\n+        model.predict_generator(custom_generator(),\n+                                steps=STEPS,\n+                                max_queue_size=10,\n+                                workers=WORKERS,\n+                                use_multiprocessing=True)\n+\n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    model.predict_generator(custom_generator(),\n+                            steps=STEPS,\n+                            max_queue_size=10,\n+                            workers=WORKERS,\n+                            use_multiprocessing=False)\n+\n+    # - Produce data on 1 worker process, consume on main process:\n+    #   - Worker process runs generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `predict_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.predict_generator(custom_generator(),\n+                                    steps=STEPS,\n+                                    max_queue_size=10,\n+                                    workers=1,\n+                                    use_multiprocessing=True)\n+    else:\n+        model.predict_generator(custom_generator(),\n+                                steps=STEPS,\n+                                max_queue_size=10,\n+                                workers=1,\n+                                use_multiprocessing=True)\n+\n+    # - Produce data on 1 worker thread, consume on main thread:\n+    #   - Worker thread is the only thread running the generator\n+    model.predict_generator(custom_generator(),\n+                            steps=STEPS,\n+                            max_queue_size=10,\n+                            workers=1,\n+                            use_multiprocessing=False)\n+\n+    # - Main thread runs the generator without a queue\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    model.predict_generator(custom_generator(),\n+                            steps=STEPS,\n+                            max_queue_size=10,\n+                            workers=0,\n                             use_multiprocessing=True)\n     model.predict_generator(custom_generator(),\n-                            steps=5,\n+                            steps=STEPS,\n                             max_queue_size=10,\n+                            workers=0,\n                             use_multiprocessing=False)\n-    model.predict_generator(custom_generator(),\n-                            steps=5,\n-                            max_queue_size=10,\n-                            workers=0)\n \n \n @keras_test\n@@ -207,32 +444,92 @@ def test_multiprocessing_evaluating():\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n+    # - Produce data on 4 worker processes, consume on main process:\n+    #   - Each worker process runs OWN copy of generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries\n+    #       -> make sure `evaluate_generator()` raises raises ValueError\n+    #          exception and does not attempt to run the generator.\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n             model.evaluate_generator(custom_generator(),\n-                             steps=5,\n+                                     steps=STEPS,\n                                      max_queue_size=10,\n-                             workers=2,\n+                                     workers=WORKERS,\n+                                     use_multiprocessing=True)\n+    else:\n+        model.evaluate_generator(custom_generator(),\n+                                 steps=STEPS,\n+                                 max_queue_size=10,\n+                                 workers=WORKERS,\n+                                 use_multiprocessing=True)\n+\n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    model.evaluate_generator(custom_generator(),\n+                             steps=STEPS,\n+                             max_queue_size=10,\n+                             workers=WORKERS,\n+                             use_multiprocessing=False)\n+\n+    # - Produce data on 1 worker process, consume on main process:\n+    #   - Worker process runs generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `evaluate_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.evaluate_generator(custom_generator(),\n+                                     steps=STEPS,\n+                                     max_queue_size=10,\n+                                     workers=1,\n+                                     use_multiprocessing=True)\n+    else:\n+        model.evaluate_generator(custom_generator(),\n+                                 steps=STEPS,\n+                                 max_queue_size=10,\n+                                 workers=1,\n+                                 use_multiprocessing=True)\n+\n+    # - Produce data on 1 worker thread, consume on main thread:\n+    #   - Worker thread is the only thread running the generator\n+    model.evaluate_generator(custom_generator(),\n+                             steps=STEPS,\n+                             max_queue_size=10,\n+                             workers=1,\n+                             use_multiprocessing=False)\n+\n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    model.evaluate_generator(custom_generator(),\n+                             steps=STEPS,\n+                             max_queue_size=10,\n+                             workers=0,\n                              use_multiprocessing=True)\n     model.evaluate_generator(custom_generator(),\n-                             steps=5,\n+                             steps=STEPS,\n                              max_queue_size=10,\n+                             workers=0,\n                              use_multiprocessing=False)\n-    model.evaluate_generator(custom_generator(),\n-                             steps=5,\n-                             max_queue_size=10,\n-                             use_multiprocessing=False,\n-                             workers=0)\n \n \n @keras_test\n def test_multiprocessing_fit_error():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    arr_labels = np.random.randint(0, 2, 50)\n     batch_size = 10\n+    n_samples = 50\n     good_batches = 3\n \n-    def custom_generator():\n+    def custom_generator(use_weights=False):\n         \"\"\"Raises an exception after a few good batches\"\"\"\n         for i in range(good_batches):\n-            yield (np.random.randint(batch_size, 256, (50, 2)),\n-                   np.random.randint(batch_size, 12, 50))\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            y = arr_labels[start: end]\n+            yield X, y\n         raise RuntimeError\n \n     model = Sequential()\n@@ -241,77 +538,294 @@ def test_multiprocessing_fit_error():\n \n     samples = batch_size * (good_batches + 1)\n \n+    # - Produce data on 4 worker processes, consume on main process:\n+    #   - Each worker process runs OWN copy of generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `fit_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.fit_generator(custom_generator(),\n+                                steps_per_epoch=samples,\n+                                validation_steps=None,\n+                                max_queue_size=10,\n+                                workers=WORKERS,\n+                                use_multiprocessing=True)\n+    else:\n         with pytest.raises(RuntimeError):\n-        model.fit_generator(\n-            custom_generator(), samples, 1,\n-            workers=4, use_multiprocessing=True,\n-        )\n+            model.fit_generator(custom_generator(),\n+                                steps_per_epoch=samples,\n+                                validation_steps=None,\n+                                max_queue_size=10,\n+                                workers=WORKERS,\n+                                use_multiprocessing=True)\n \n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    #   - Make sure `RuntimeError` exception bubbles up\n     with pytest.raises(RuntimeError):\n-        model.fit_generator(\n-            custom_generator(), samples, 1,\n-            use_multiprocessing=False,\n-        )\n+        model.fit_generator(custom_generator(),\n+                            steps_per_epoch=samples,\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=WORKERS,\n+                            use_multiprocessing=False)\n+\n+    # - Produce data on 1 worker process, consume on main process:\n+    #   - Worker process runs generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `fit_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.fit_generator(custom_generator(),\n+                                steps_per_epoch=samples,\n+                                validation_steps=None,\n+                                max_queue_size=10,\n+                                workers=1,\n+                                use_multiprocessing=True)\n+    else:\n+        with pytest.raises(RuntimeError):\n+            model.fit_generator(custom_generator(),\n+                                steps_per_epoch=samples,\n+                                validation_steps=None,\n+                                max_queue_size=10,\n+                                workers=1,\n+                                use_multiprocessing=True)\n+\n+    # - Produce data on 1 worker thread, consume on main thread:\n+    #   - Worker thread is the only thread running the generator\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.fit_generator(custom_generator(),\n+                            steps_per_epoch=samples,\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=1,\n+                            use_multiprocessing=False)\n+\n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.fit_generator(custom_generator(),\n+                            steps_per_epoch=samples,\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=0,\n+                            use_multiprocessing=True)\n+    with pytest.raises(RuntimeError):\n+        model.fit_generator(custom_generator(),\n+                            steps_per_epoch=samples,\n+                            validation_steps=None,\n+                            max_queue_size=10,\n+                            workers=0,\n+                            use_multiprocessing=False)\n \n \n @keras_test\n def test_multiprocessing_evaluate_error():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    arr_labels = np.random.randint(0, 2, 50)\n     batch_size = 10\n+    n_samples = 50\n     good_batches = 3\n-    workers = 4\n \n     def custom_generator():\n         \"\"\"Raises an exception after a few good batches\"\"\"\n         for i in range(good_batches):\n-            yield (np.random.randint(batch_size, 256, (50, 2)),\n-                   np.random.randint(batch_size, 12, 50))\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            y = arr_labels[start: end]\n+            yield X, y\n         raise RuntimeError\n \n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n+    # - Produce data on 4 worker processes, consume on main process:\n+    #   - Each worker process runs OWN copy of generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `evaluate_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.evaluate_generator(custom_generator(),\n+                                     steps=good_batches * WORKERS + 1,\n+                                     max_queue_size=10,\n+                                     workers=WORKERS,\n+                                     use_multiprocessing=True)\n+    else:\n         with pytest.raises(RuntimeError):\n-        model.evaluate_generator(\n-            custom_generator(), good_batches * workers + 1, 1,\n-            workers=workers, use_multiprocessing=True,\n-        )\n+            model.evaluate_generator(custom_generator(),\n+                                     steps=good_batches * WORKERS + 1,\n+                                     max_queue_size=10,\n+                                     workers=WORKERS,\n+                                     use_multiprocessing=True)\n \n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    #   - Make sure `RuntimeError` exception bubbles up\n     with pytest.raises(RuntimeError):\n-        model.evaluate_generator(\n-            custom_generator(), good_batches + 1, 1,\n-            use_multiprocessing=False,\n-        )\n+        model.evaluate_generator(custom_generator(),\n+                                 steps=good_batches * WORKERS + 1,\n+                                 max_queue_size=10,\n+                                 workers=WORKERS,\n+                                 use_multiprocessing=False)\n+\n+    # - Produce data on 1 worker process, consume on main process:\n+    #   - Worker process runs generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `evaluate_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.evaluate_generator(custom_generator(),\n+                                     steps=good_batches + 1,\n+                                     max_queue_size=10,\n+                                     workers=1,\n+                                     use_multiprocessing=True)\n+    else:\n+        with pytest.raises(RuntimeError):\n+            model.evaluate_generator(custom_generator(),\n+                                     steps=good_batches + 1,\n+                                     max_queue_size=10,\n+                                     workers=1,\n+                                     use_multiprocessing=True)\n+\n+    # - Produce data on 1 worker thread, consume on main thread:\n+    #   - Worker thread is the only thread running the generator\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.evaluate_generator(custom_generator(),\n+                                 steps=good_batches + 1,\n+                                 max_queue_size=10,\n+                                 workers=1,\n+                                 use_multiprocessing=False)\n+\n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.evaluate_generator(custom_generator(),\n+                                 steps=good_batches + 1,\n+                                 max_queue_size=10,\n+                                 workers=0,\n+                                 use_multiprocessing=True)\n+    with pytest.raises(RuntimeError):\n+        model.evaluate_generator(custom_generator(),\n+                                 steps=good_batches + 1,\n+                                 max_queue_size=10,\n+                                 workers=0,\n+                                 use_multiprocessing=False)\n \n \n @keras_test\n def test_multiprocessing_predict_error():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n     good_batches = 3\n-    workers = 4\n \n     def custom_generator():\n         \"\"\"Raises an exception after a few good batches\"\"\"\n+        batch_size = 10\n+        n_samples = 50\n+\n         for i in range(good_batches):\n-            yield (np.random.randint(1, 256, size=(2, 5)),\n-                   np.random.randint(1, 256, size=(2, 5)))\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            yield X\n         raise RuntimeError\n \n     model = Sequential()\n-    model.add(Dense(1, input_shape=(5,)))\n+    model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n \n+    # - Produce data on 4 worker processes, consume on main process:\n+    #   - Each worker process runs OWN copy of generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `predict_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.predict_generator(custom_generator(),\n+                                    steps=good_batches * WORKERS + 1,\n+                                    max_queue_size=10,\n+                                    workers=WORKERS,\n+                                    use_multiprocessing=True)\n+    else:\n         with pytest.raises(RuntimeError):\n-        model.predict_generator(\n-            custom_generator(), good_batches * workers + 1, 1,\n-            workers=workers, use_multiprocessing=True,\n-        )\n+            model.predict_generator(custom_generator(),\n+                                    steps=good_batches * WORKERS + 1,\n+                                    max_queue_size=10,\n+                                    workers=WORKERS,\n+                                    use_multiprocessing=True)\n \n+    # - Produce data on 4 worker threads, consume on main thread:\n+    #   - All worker threads share the SAME generator\n+    #   - Make sure `RuntimeError` exception bubbles up\n     with pytest.raises(RuntimeError):\n-        model.predict_generator(\n-            custom_generator(), good_batches + 1, 1,\n-            use_multiprocessing=False,\n-        )\n+        model.predict_generator(custom_generator(),\n+                                steps=good_batches * WORKERS + 1,\n+                                max_queue_size=10,\n+                                workers=WORKERS,\n+                                use_multiprocessing=False)\n \n+    # - Produce data on 1 worker process, consume on main process:\n+    #   - Worker process runs generator\n+    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n+    #     process boundaries -> make sure `predict_generator()` raises ValueError\n+    #     exception and does not attempt to run the generator.\n+    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n+    if os.name is 'nt':\n+        with pytest.raises(ValueError):\n+            model.predict_generator(custom_generator(),\n+                                    steps=good_batches + 1,\n+                                    max_queue_size=10,\n+                                    workers=1,\n+                                    use_multiprocessing=True)\n+    else:\n+        with pytest.raises(RuntimeError):\n+            model.predict_generator(custom_generator(),\n+                                    steps=good_batches + 1,\n+                                    max_queue_size=10,\n+                                    workers=1,\n+                                    use_multiprocessing=True)\n+\n+    # - Produce data on 1 worker thread, consume on main thread:\n+    #   - Worker thread is the only thread running the generator\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.predict_generator(custom_generator(),\n+                                steps=good_batches + 1,\n+                                max_queue_size=10,\n+                                workers=1,\n+                                use_multiprocessing=False)\n+\n+    # - Produce and consume data without a queue on main thread\n+    #   - Make sure the value of `use_multiprocessing` is ignored\n+    #   - Make sure `RuntimeError` exception bubbles up\n+    with pytest.raises(RuntimeError):\n+        model.predict_generator(custom_generator(),\n+                                steps=good_batches + 1,\n+                                max_queue_size=10,\n+                                workers=0,\n+                                use_multiprocessing=True)\n+    with pytest.raises(RuntimeError):\n+        model.predict_generator(custom_generator(),\n+                                steps=good_batches + 1,\n+                                max_queue_size=10,\n+                                workers=0,\n+                                use_multiprocessing=False)\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
