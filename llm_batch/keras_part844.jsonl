{"custom_id": "keras#d3b032c1484b419468bf5fc9da465497d476c7d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 6 | Churn Cumulative: 87 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -49,12 +49,6 @@ class Pipeline(Layer):\n     def layers(self):\n         return self._pipeline_layers\n \n-    def build(self, input_shape):\n-        for layer in self._pipeline_layers:\n-            layer.build(input_shape)\n-            input_shape = layer.compute_output_shape(input_shape)\n-        self.built = True\n-\n     def call(self, inputs, training=True, mask=None):\n         for layer in self._pipeline_layers:\n             kwargs = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#444713e58838bcfe775deadec9d27bd2f3c1f4e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 7 | Churn Cumulative: 198 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import numpy as np\n+import pytest\n from tensorflow import data as tf_data\n \n from keras.src import backend\n@@ -27,6 +28,7 @@ class CanaryLayer(layers.Layer):\n \n class PipelineTest(testing.TestCase):\n     def test_basics(self):\n+        run_training_check = False if backend.backend() == \"numpy\" else True\n         self.run_layer_test(\n             layers.Pipeline,\n             init_kwargs={\n@@ -36,8 +38,12 @@ class PipelineTest(testing.TestCase):\n             supports_masking=False,\n             expected_output_shape=(8, 3, 4, 3),\n             run_mixed_precision_check=False,\n+            run_training_check=run_training_check,\n         )\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\", reason=\"masking not working in numpy\"\n+    )\n     def test_correctness(self):\n         pipeline = layers.Pipeline([CanaryLayer(), CanaryLayer()])\n         x = np.array([0])\n\n@@ -59,6 +59,7 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n         )\n \n     def assertAlmostEqual(self, x1, x2, decimal=3, msg=None):\n+        msg = msg or \"\"\n         if not isinstance(x1, np.ndarray):\n             x1 = backend.convert_to_numpy(x1)\n         if not isinstance(x2, np.ndarray):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b1fca14fcd5083704306cc68da28766a43fc772", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 46 | Files Changed: 11 | Hunks: 29 | Methods Changed: 15 | Complexity Δ (Sum/Max): -2/1 | Churn Δ: 102 | Churn Cumulative: 1686 | Contributors (this commit): 12 | Commits (past 90d): 36 | Contributors (cumulative): 49 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,8 @@ from keras.src.backend.common.dtypes import result_type\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.keras_tensor import any_symbolic_tensors\n from keras.src.backend.common.keras_tensor import is_keras_tensor\n+from keras.src.backend.common.masking import get_keras_mask\n+from keras.src.backend.common.masking import set_keras_mask\n from keras.src.backend.common.name_scope import name_scope\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.common.stateless_scope import get_stateless_scope\n\n@@ -59,11 +59,7 @@ class Masking(Layer):\n         # Set masked outputs to 0\n         outputs = inputs * backend.cast(boolean_mask, dtype=inputs.dtype)\n         # Compute the mask and outputs simultaneously.\n-        try:\n-            outputs._keras_mask = ops.squeeze(boolean_mask, axis=-1)\n-        except AttributeError:\n-            # tensor is a C type.\n-            pass\n+        backend.set_keras_mask(outputs, mask=ops.squeeze(boolean_mask, axis=-1))\n         return outputs\n \n     def compute_output_shape(self, input_shape):\n\n@@ -860,7 +860,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n                 arg_name = list(call_spec.tensor_arguments_dict.keys())[0]\n                 only_tensor_arg = call_spec.tensor_arguments_dict[arg_name]\n                 mask = tree.map_structure(\n-                    lambda x: getattr(x, \"_keras_mask\", None),\n+                    backend.get_keras_mask,\n                     only_tensor_arg,\n                 )\n                 kwargs[\"mask\"] = mask\n@@ -869,9 +869,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n                 expected_mask_arg_name = f\"{k}_mask\"\n                 if expected_mask_arg_name in call_spec.argument_names:\n                     if call_spec.arguments_dict[expected_mask_arg_name] is None:\n-                        mask = tree.map_structure(\n-                            lambda x: getattr(x, \"_keras_mask\", None), v\n-                        )\n+                        mask = tree.map_structure(backend.get_keras_mask, v)\n                         kwargs[expected_mask_arg_name] = mask\n \n         ####################\n@@ -924,7 +922,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n             # provided only the first positional input arg and its mask.\n             # TODO: consider extending this to all args and kwargs.\n             previous_mask = tree.map_structure(\n-                lambda x: getattr(x, \"_keras_mask\", None), call_spec.first_arg\n+                backend.get_keras_mask, call_spec.first_arg\n             )\n             if self.supports_masking:\n                 self._set_mask_metadata(\n@@ -1510,7 +1508,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         flat_outputs = tree.flatten(outputs)\n \n         mask_already_computed = all(\n-            getattr(x, \"_keras_mask\", None) is not None for x in flat_outputs\n+            backend.get_keras_mask(x) is not None for x in flat_outputs\n         )\n         if mask_already_computed:\n             return\n@@ -1521,18 +1519,14 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n \n         flat_masks = tree.flatten(output_masks)\n         for tensor, mask in zip(flat_outputs, flat_masks):\n-            if getattr(tensor, \"_keras_mask\", None) is None:\n-                try:\n-                    # Numpy backend does not support masking.\n+            if backend.get_keras_mask(tensor) is None and mask is not None:\n                 if backend.backend() == \"numpy\":\n                     warnings.warn(\n                         \"The NumPy backend does not support masking at this\"\n                         \"time. Masks will be ignored.\"\n                     )\n-                    tensor._keras_mask = mask\n-                except AttributeError:\n-                    # It's a C type.\n-                    pass\n+                else:\n+                    backend.set_keras_mask(tensor, mask)\n \n     @python_utils.default\n     def get_config(self):\n\n@@ -572,8 +572,24 @@ class LayerTest(testing.TestCase):\n         CustomLayer()(x)\n \n     @pytest.mark.skipif(\n-        backend.backend() == \"numpy\",\n-        reason=\"Numpy backend does not support masking.\",\n+        backend.backend() == \"numpy\", reason=\"masking not supported with numpy\"\n+    )\n+    def test_end_to_end_masking(self):\n+        # Check that masking survives compilation\n+        model = models.Sequential(\n+            [\n+                layers.Embedding(\n+                    2, 2, mask_zero=True, embeddings_initializer=\"ones\"\n+                ),\n+            ]\n+        )\n+        model.compile(loss=\"mse\")\n+        targets = np.array([[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0], [1.0, 1.0]]])\n+        loss = model.evaluate(np.array([[1, 0, 0, 1]]), targets)\n+        self.assertAllClose(loss, 0.0)\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\", reason=\"masking not supported with numpy\"\n     )\n     def test_masking(self):\n         class BasicMaskedLayer(layers.Layer):\n@@ -587,7 +603,8 @@ class LayerTest(testing.TestCase):\n \n         layer = BasicMaskedLayer()\n         x = backend.numpy.ones((4, 4))\n-        x._keras_mask = backend.numpy.ones((4,))\n+        mask = backend.numpy.ones((4,))\n+        backend.set_keras_mask(x, mask)\n         layer(x)\n \n         layer(backend.numpy.ones((4, 4)), mask=backend.numpy.ones((4,)))\n@@ -606,9 +623,11 @@ class LayerTest(testing.TestCase):\n \n         layer = NestedInputMaskedLayer()\n         x1 = backend.numpy.ones((4, 4))\n-        x1._keras_mask = backend.numpy.ones((4,))\n+        mask1 = backend.numpy.ones((4,))\n+        backend.set_keras_mask(x1, mask1)\n         x2 = backend.numpy.ones((4, 4))\n-        x2._keras_mask = backend.numpy.ones((4,))\n+        mask2 = backend.numpy.ones((4,))\n+        backend.set_keras_mask(x2, mask2)\n         layer([x1, x2])\n \n         layer(\n@@ -644,11 +663,14 @@ class LayerTest(testing.TestCase):\n \n         layer = PositionalNestedInputsMaskedLayer()\n         x1_1 = backend.numpy.ones((4, 4))\n-        x1_1._keras_mask = backend.numpy.ones((4,))\n+        mask1 = backend.numpy.ones((4,))\n+        backend.set_keras_mask(x1_1, mask1)\n         x1_2 = backend.numpy.ones((4, 4))\n-        x1_2._keras_mask = backend.numpy.ones((4,))\n+        mask2 = backend.numpy.ones((4,))\n+        backend.set_keras_mask(x1_2, mask2)\n         x2 = backend.numpy.ones((4, 4))\n-        x2._keras_mask = backend.numpy.ones((4,))\n+        mask2 = backend.numpy.ones((4,))\n+        backend.set_keras_mask(x2, mask2)\n         layer((x1_1, x1_2), x2)\n         layer(x1=(x1_1, x1_2), x2=x2)\n \n\n@@ -35,7 +35,7 @@ class Merge(Layer):\n         output_mask = None\n \n         for x in inputs:\n-            mask = getattr(x, \"_keras_mask\", None)\n+            mask = backend.get_keras_mask(x)\n             if mask is not None:\n                 mask = ops.broadcast_to(ops.expand_dims(mask, -1), ops.shape(x))\n             if output is None:\n@@ -54,7 +54,7 @@ class Merge(Layer):\n \n         if output_mask is not None:\n             output_mask = ops.any(output_mask, axis=-1, keepdims=False)\n-            output._keras_mask = output_mask\n+            backend.set_keras_mask(output, output_mask)\n         return output\n \n     def _compute_elemwise_op_output_shape(self, shape1, shape2):\n\n@@ -1,3 +1,4 @@\n+from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n from keras.src.layers.merging.base_merge import Merge\n@@ -31,7 +32,7 @@ class Multiply(Merge):\n     \"\"\"\n \n     def _merge_function(self, inputs):\n-        masks = [getattr(x, \"_keras_mask\", None) for x in inputs]\n+        masks = [backend.get_keras_mask(x) for x in inputs]\n         has_output_mask = all(mask is not None for mask in masks)\n         output = None\n         output_mask = None\n@@ -53,7 +54,7 @@ class Multiply(Merge):\n             # Replace 1s with 0s outside of mask per standard masking rules.\n             output = ops.where(output_mask, output, ops.cast(0, output.dtype))\n             output_mask = ops.any(output_mask, axis=-1, keepdims=False)\n-            output._keras_mask = output_mask\n+            backend.set_keras_mask(output, output_mask)\n         return output\n \n \n\n@@ -47,7 +47,7 @@ class Loss(KerasSaveable):\n         return self._dtype\n \n     def __call__(self, y_true, y_pred, sample_weight=None):\n-        in_mask = getattr(y_pred, \"_keras_mask\", None)\n+        in_mask = backend.get_keras_mask(y_pred)\n \n         with ops.name_scope(self.name):\n             y_pred = tree.map_structure(\n@@ -58,7 +58,7 @@ class Loss(KerasSaveable):\n             )\n \n             losses = self.call(y_true, y_pred)\n-            out_mask = getattr(losses, \"_keras_mask\", None)\n+            out_mask = backend.get_keras_mask(losses)\n \n             if in_mask is not None and out_mask is not None:\n                 mask = in_mask & out_mask\n\n@@ -1877,11 +1877,7 @@ def sparse_categorical_crossentropy(\n     if ignore_class is not None:\n         valid_mask = ops.reshape(valid_mask, res_shape)\n         res = ops.where(valid_mask, res, 0.0)\n-\n-        try:\n-            res._keras_mask = valid_mask\n-        except AttributeError:\n-            pass\n+        backend.set_keras_mask(res, mask=valid_mask)\n \n     return res\n \n\n@@ -9,7 +9,7 @@ from keras.src.saving import serialization_lib\n \n def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n     dtype = dtype or backend.floatx()\n-    mask = getattr(values, \"_keras_mask\", None)\n+    mask = backend.get_keras_mask(values)\n     values = ops.cast(values, dtype=dtype)\n     if sample_weight is not None:\n         sample_weight = ops.convert_to_tensor(sample_weight, dtype=dtype)\n@@ -200,7 +200,7 @@ class MeanMetricWrapper(Mean):\n             self._direction = \"down\"\n \n     def update_state(self, y_true, y_pred, sample_weight=None):\n-        mask = getattr(y_pred, \"_keras_mask\", None)\n+        mask = backend.get_keras_mask(y_pred)\n         values = self._fn(y_true, y_pred, **self._fn_kwargs)\n         if sample_weight is not None and mask is not None:\n             sample_weight = losses.loss.apply_mask(\n\n@@ -171,7 +171,7 @@ class Functional(Function, Model):\n             masks = tree.flatten(mask)\n             for x, mask in zip(inputs, masks):\n                 if mask is not None:\n-                    x._keras_mask = mask\n+                    backend.set_keras_mask(x, mask)\n         outputs = self._run_through_graph(\n             inputs, operation_fn=lambda op: operation_fn(op, training=training)\n         )\n@@ -262,8 +262,9 @@ class Functional(Function, Model):\n         for i in range(len(flat_inputs)):\n             if hasattr(flat_inputs[i], \"_keras_history\"):\n                 adjusted[i]._keras_history = flat_inputs[i]._keras_history\n-            if hasattr(flat_inputs[i], \"_keras_mask\"):\n-                adjusted[i]._keras_mask = flat_inputs[i]._keras_mask\n+            mask = backend.get_keras_mask(flat_inputs[i])\n+            if mask is not None:\n+                backend.set_keras_mask(adjusted[i], mask)\n         return adjusted\n \n     def _standardize_inputs(self, inputs):\n\n@@ -2,6 +2,7 @@ import copy\n import inspect\n import typing\n \n+from keras.src import backend\n from keras.src import tree\n from keras.src.api_export import keras_export\n from keras.src.backend.common import global_state\n@@ -226,10 +227,7 @@ class Sequential(Model):\n             outputs = layer(inputs, **kwargs)\n             inputs = outputs\n \n-            def _get_mask_from_keras_tensor(kt):\n-                return getattr(kt, \"_keras_mask\", None)\n-\n-            mask = tree.map_structure(_get_mask_from_keras_tensor, outputs)\n+            mask = tree.map_structure(backend.get_keras_mask, outputs)\n         return outputs\n \n     @property\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8896051e213c3234f8bb33a86ef45d1e68c57af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 24 | Churn Cumulative: 24 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,24 @@\n+import weakref\n+\n+from keras.src.backend.common import global_state\n+\n+\n+def set_keras_mask(x, mask):\n+    try:\n+        x._keras_mask = mask\n+    except AttributeError:\n+        if mask is None:\n+            return\n+        mask_dict = global_state.get_global_attribute(\"keras_mask_dict\")\n+        if mask_dict is None:\n+            mask_dict = weakref.WeakValueDictionary()\n+            global_state.set_global_attribute(\"keras_mask_dict\", mask_dict)\n+        mask_dict[id(x)] = mask\n+\n+\n+def get_keras_mask(x):\n+    if not hasattr(x, \"_keras_mask\"):\n+        mask_dict = global_state.get_global_attribute(\"keras_mask_dict\")\n+        if mask_dict is not None:\n+            return mask_dict.get(id(x), None)\n+    return getattr(x, \"_keras_mask\", None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#076ab315a7d1939d2ec965dc097946c53ef1d539", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 123 | Lines Deleted: 7 | Files Changed: 6 | Hunks: 24 | Methods Changed: 18 | Complexity Δ (Sum/Max): 25/18 | Churn Δ: 130 | Churn Cumulative: 1819 | Contributors (this commit): 14 | Commits (past 90d): 30 | Contributors (cumulative): 31 | DMM Complexity: 0.6933333333333334\n\nDIFF:\n@@ -0,0 +1,29 @@\n+import tensorflow as tf\n+from absl.testing import parameterized\n+\n+import keras\n+from keras.src.testing import test_case\n+\n+\n+class MyDenseLayer(keras.layers.Layer):\n+    def __init__(self, num_outputs):\n+        super(MyDenseLayer, self).__init__()\n+        self.num_outputs = num_outputs\n+\n+    def build(self, input_shape):\n+        self.kernel = self.add_weight(\n+            shape=[int(input_shape[-1]), self.num_outputs],\n+        )\n+\n+    def call(self, inputs):\n+        kernel = tf.cast(self.kernel, tf.complex64)\n+        return tf.matmul(inputs, kernel)\n+\n+\n+# Custom layer test with complex input\n+class TestDenseLayer(test_case.TestCase, parameterized.TestCase):\n+    def test_layer_output_shape(self):\n+        input = tf.zeros([10, 5], dtype=tf.complex64)\n+        layer = MyDenseLayer(10)\n+        output = layer(input)\n+        self.assertAllEqual(output.shape, (10, 10))\n\n@@ -17,6 +17,7 @@ INT_TYPES = (\n )\n FLOAT_TYPES = (\"bfloat16\", \"float16\", \"float32\", \"float64\")\n WEAK_TYPES = (\"int\", \"float\")\n+COMPLEX_TYPES = (\"complex64\", \"complex128\")\n # We need to separate float8 from float because there are no implicit\n # conversions from float8 dtypes to other dtypes.\n # Ref: https://github.com/google/jax/issues/16705\n@@ -40,6 +41,8 @@ ALLOWED_DTYPES = (\n     \"string\",\n     \"float8_e4m3fn\",\n     \"float8_e5m2\",\n+    \"complex64\",\n+    \"complex128\",\n )\n PYTHON_DTYPES_MAP = {\n     bool: \"bool\",\n@@ -48,6 +51,7 @@ PYTHON_DTYPES_MAP = {\n     str: \"string\",\n     # special case for string value\n     \"int\": \"int64\" if config.backend() == \"tensorflow\" else \"int32\",\n+    complex: \"complex128\" if config.backend() == \"tensorflow\" else \"complex64\",\n }\n \n # We adapted the type promotion lattice from JAX. Ref:\n@@ -63,13 +67,14 @@ def _type_promotion_lattice():\n     (u1, u2, u4, u8, i1, i2, i4, i8) = INT_TYPES\n     bf, f2, f4, f8 = FLOAT_TYPES\n     i_, f_ = WEAK_TYPES\n+    c64, c128 = COMPLEX_TYPES\n     out = {\n         b1: [i_],\n         u1: [i2, u2],\n         u2: [i4, u4],\n         u4: [i8, u8],\n         u8: [f_],\n-        i_: [u1, i1],\n+        i_: [u1, i1, c64],\n         i1: [i2],\n         i2: [i4],\n         i4: [i8],\n@@ -77,8 +82,10 @@ def _type_promotion_lattice():\n         f_: [bf, f2],\n         bf: [f4],\n         f2: [f4],\n-        f4: [f8],\n-        f8: [],\n+        f4: [f8, c64],\n+        f8: [c128],\n+        c64: [c128],\n+        c128: [],\n     }\n     return out\n \n@@ -186,6 +193,8 @@ def _respect_weak_type(dtype, weak_type):\n             return \"float\"\n         elif \"int\" in dtype:\n             return \"int\"\n+        elif \"complex\" in dtype:\n+            return \"complex\"\n         else:\n             raise ValueError(\n                 \"Invalid value for argument `dtype`. Expected one of \"\n@@ -295,6 +304,11 @@ def result_type(*dtypes):\n     >>> y = keras.ops.ones((1,), dtype=\"float32\")\n     >>> keras.backend.result_type(x.dtype, y.dtype)\n     \"float32\"\n+\n+    >>> z= keras.ops.ones((1,),dtype='complex64')\n+    >>> keras.backend.result_type(z.dtype, int)\n+    \"float64\"\n+\n     \"\"\"\n     if len(dtypes) == 0:\n         # If no dtypes provided, default to floatx, this matches\n\n@@ -91,6 +91,16 @@ class DtypesTest(test_case.TestCase):\n             dtypes._resolve_weak_type(\"bfloat16\", precision=\"64\"), \"float64\"\n         )\n \n+    def test_respect_weak_type_for_complex64(self):\n+        self.assertAllEqual(\n+            dtypes._respect_weak_type(\"complex64\", True), \"complex\"\n+        )\n+\n+    def test_respect_weak_type_for_complex128(self):\n+        self.assertAllEqual(\n+            dtypes._respect_weak_type(\"complex128\", True), \"complex\"\n+        )\n+\n     def test_invalid_dtype_for_keras_promotion(self):\n         with self.assertRaisesRegex(\n             ValueError, \"is not a valid dtype for Keras type promotion.\"\n\n@@ -159,7 +159,13 @@ class VariablePropertiesTest(test_case.TestCase):\n         self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n \n     @parameterized.parameters(\n-        *((dtype for dtype in dtypes.ALLOWED_DTYPES if dtype != \"string\"))\n+        *(\n+            (\n+                dtype\n+                for dtype in dtypes.ALLOWED_DTYPES\n+                if dtype not in [\"string\", \"complex64\", \"complex28\"]\n+            )\n+        )\n     )\n     def test_standardize_dtype(self, dtype):\n         \"\"\"Tests standardize_dtype for all ALLOWED_DTYPES except string.\"\"\"\n@@ -167,9 +173,17 @@ class VariablePropertiesTest(test_case.TestCase):\n             \"uint16\",\n             \"uint32\",\n             \"uint64\",\n+            \"complex64\",\n+            \"complex128\",\n         ):\n             self.skipTest(f\"torch backend does not support dtype {dtype}\")\n \n+        if backend.backend() == \"jax\" and dtype in (\n+            \"complex64\",\n+            \"complex128\",\n+        ):\n+            self.skipTest(f\"JAX backend does not support dtype {dtype}\")\n+\n         if backend.backend() == \"jax\":\n             import jax\n \n@@ -680,7 +694,10 @@ class VariableOpsDTypeTest(test_case.TestCase):\n     if backend.backend() == \"torch\":\n         # TODO: torch doesn't support uint16, uint32 and uint64\n         ALL_DTYPES = [\n-            x for x in ALL_DTYPES if x not in [\"uint16\", \"uint32\", \"uint64\"]\n+            x\n+            for x in ALL_DTYPES\n+            if x\n+            not in [\"uint16\", \"uint32\", \"uint64\", \"complex128\", \"complex64\"]\n         ]\n         INT_DTYPES = [\n             x for x in INT_DTYPES if x not in [\"uint16\", \"uint32\", \"uint64\"]\n@@ -736,6 +753,11 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n+        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n+            \"complex64\",\n+            \"complex128\",\n+        ]:\n+            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -751,6 +773,11 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n+        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n+            \"complex64\",\n+            \"complex128\",\n+        ]:\n+            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -766,6 +793,11 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n+        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n+            \"complex64\",\n+            \"complex128\",\n+        ]:\n+            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -781,6 +813,12 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n+        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n+            \"complex64\",\n+            \"complex128\",\n+        ]:\n+            self.skipTest(\"Skipped test\")\n+\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -830,6 +868,12 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n+        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n+            \"complex64\",\n+            \"complex128\",\n+        ]:\n+            self.skipTest(\"Skipped test\")\n+\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -851,6 +895,11 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         # the expected dtype from 64 bit to 32 bit when using jax backend.\n         with jax.experimental.disable_x64():\n             dtype1, dtype2 = dtypes\n+            if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n+                \"complex64\",\n+                \"complex128\",\n+            ]:\n+                self.skipTest(\"Skipped test\")\n             x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n             x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n             x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -873,6 +922,11 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n+        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n+            \"complex64\",\n+            \"complex128\",\n+        ]:\n+            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -891,6 +945,11 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n+        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n+            \"complex64\",\n+            \"complex128\",\n+        ]:\n+            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n\n@@ -870,7 +870,9 @@ class CoreOpsDtypeTest(testing.TestCase):\n     # resulting in different behavior between JAX and Keras. Currently, we\n     # are skipping the test for uint64\n     ALL_DTYPES = [\n-        x for x in dtypes.ALLOWED_DTYPES if x not in [\"string\", \"uint64\"]\n+        x\n+        for x in dtypes.ALLOWED_DTYPES\n+        if x not in [\"string\", \"uint64\", \"complex64\", \"complex128\"]\n     ] + [None]\n \n     if backend.backend() == \"torch\":\n\n@@ -5145,7 +5145,9 @@ class NumpyDtypeTest(testing.TestCase):\n     # resulting in different behavior between JAX and Keras. Currently, we\n     # are skipping the test for uint64\n     ALL_DTYPES = [\n-        x for x in dtypes.ALLOWED_DTYPES if x not in [\"string\", \"uint64\"]\n+        x\n+        for x in dtypes.ALLOWED_DTYPES\n+        if x not in [\"string\", \"uint64\", \"complex64\", \"complex128\"]\n     ] + [None]\n     INT_DTYPES = [x for x in dtypes.INT_TYPES if x != \"uint64\"]\n     FLOAT_DTYPES = dtypes.FLOAT_TYPES\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9dbbc72adef6332a1742742b3c4cb7b7ba2d47af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 90 | Files Changed: 4 | Hunks: 25 | Methods Changed: 17 | Complexity Δ (Sum/Max): -17/4 | Churn Δ: 131 | Churn Cumulative: 1400 | Contributors (this commit): 12 | Commits (past 90d): 25 | Contributors (cumulative): 23 | DMM Complexity: 0.07692307692307693\n\nDIFF:\n@@ -1,29 +0,0 @@\n-import tensorflow as tf\n-from absl.testing import parameterized\n-\n-import keras\n-from keras.src.testing import test_case\n-\n-\n-class MyDenseLayer(keras.layers.Layer):\n-    def __init__(self, num_outputs):\n-        super(MyDenseLayer, self).__init__()\n-        self.num_outputs = num_outputs\n-\n-    def build(self, input_shape):\n-        self.kernel = self.add_weight(\n-            shape=[int(input_shape[-1]), self.num_outputs],\n-        )\n-\n-    def call(self, inputs):\n-        kernel = tf.cast(self.kernel, tf.complex64)\n-        return tf.matmul(inputs, kernel)\n-\n-\n-# Custom layer test with complex input\n-class TestDenseLayer(test_case.TestCase, parameterized.TestCase):\n-    def test_layer_output_shape(self):\n-        input = tf.zeros([10, 5], dtype=tf.complex64)\n-        layer = MyDenseLayer(10)\n-        output = layer(input)\n-        self.assertAllEqual(output.shape, (10, 10))\n\n@@ -178,13 +178,9 @@ class VariablePropertiesTest(test_case.TestCase):\n         ):\n             self.skipTest(f\"torch backend does not support dtype {dtype}\")\n \n-        if backend.backend() == \"jax\" and dtype in (\n-            \"complex64\",\n-            \"complex128\",\n-        ):\n-            self.skipTest(f\"JAX backend does not support dtype {dtype}\")\n-\n         if backend.backend() == \"jax\":\n+            if dtype in (\"complex128\",):\n+                self.skipTest(f\"jax backend does not support dtype {dtype}\")\n             import jax\n \n             if not jax.config.x64_enabled and \"64\" in dtype:\n@@ -679,9 +675,6 @@ class VariableOpsBehaviorTest(test_case.TestCase):\n             float(v)\n \n \n-class VariableOpsDTypeTest(test_case.TestCase):\n-    \"\"\"Test the dtype to verify that the behavior matches JAX.\"\"\"\n-\n # TODO: Using uint64 will lead to weak type promotion (`float`),\n # resulting in different behavior between JAX and Keras. Currently, we\n # are skipping the test for uint64\n@@ -690,20 +683,25 @@ class VariableOpsDTypeTest(test_case.TestCase):\n ] + [None]\n INT_DTYPES = [x for x in dtypes.INT_TYPES if x != \"uint64\"]\n FLOAT_DTYPES = dtypes.FLOAT_TYPES\n+COMPLEX_DTYPES = [\"complex32\", \"complex64\", \"complex128\"]\n \n if backend.backend() == \"torch\":\n-        # TODO: torch doesn't support uint16, uint32 and uint64\n+    # TODO: torch doesn't support uint16, uint32 and uint64, complex\n     ALL_DTYPES = [\n         x\n         for x in ALL_DTYPES\n-            if x\n-            not in [\"uint16\", \"uint32\", \"uint64\", \"complex128\", \"complex64\"]\n+        if x not in [\"uint16\", \"uint32\", \"uint64\", \"complex128\", \"complex64\"]\n     ]\n     INT_DTYPES = [\n         x for x in INT_DTYPES if x not in [\"uint16\", \"uint32\", \"uint64\"]\n     ]\n # Remove float8 dtypes for the following tests\n ALL_DTYPES = [x for x in ALL_DTYPES if x not in dtypes.FLOAT8_TYPES]\n+NON_COMPLEX_DTYPES = [x for x in ALL_DTYPES if x and x not in COMPLEX_DTYPES]\n+\n+\n+class VariableOpsDTypeTest(test_case.TestCase):\n+    \"\"\"Test the dtype to verify that the behavior matches JAX.\"\"\"\n \n     def setUp(self):\n         from jax.experimental import enable_x64\n@@ -747,17 +745,12 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         self.assertDType(x1 != x2, expected_dtype)\n \n     @parameterized.named_parameters(\n-        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+        named_product(dtypes=itertools.combinations(NON_COMPLEX_DTYPES, 2))\n     )\n     def test_lt(self, dtypes):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n-            \"complex64\",\n-            \"complex128\",\n-        ]:\n-            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -767,17 +760,12 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         self.assertDType(x1 < x2, expected_dtype)\n \n     @parameterized.named_parameters(\n-        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+        named_product(dtypes=itertools.combinations(NON_COMPLEX_DTYPES, 2))\n     )\n     def test_le(self, dtypes):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n-            \"complex64\",\n-            \"complex128\",\n-        ]:\n-            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -787,17 +775,12 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         self.assertDType(x1 <= x2, expected_dtype)\n \n     @parameterized.named_parameters(\n-        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+        named_product(dtypes=itertools.combinations(NON_COMPLEX_DTYPES, 2))\n     )\n     def test_gt(self, dtypes):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n-            \"complex64\",\n-            \"complex128\",\n-        ]:\n-            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -807,18 +790,12 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         self.assertDType(x1 > x2, expected_dtype)\n \n     @parameterized.named_parameters(\n-        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+        named_product(dtypes=itertools.combinations(NON_COMPLEX_DTYPES, 2))\n     )\n     def test_ge(self, dtypes):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n-            \"complex64\",\n-            \"complex128\",\n-        ]:\n-            self.skipTest(\"Skipped test\")\n-\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -868,12 +845,6 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n-            \"complex64\",\n-            \"complex128\",\n-        ]:\n-            self.skipTest(\"Skipped test\")\n-\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -884,7 +855,7 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         self.assertDType(x1.__rmul__(x2), expected_dtype)\n \n     @parameterized.named_parameters(\n-        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+        named_product(dtypes=itertools.combinations(NON_COMPLEX_DTYPES, 2))\n     )\n     def test_truediv(self, dtypes):\n         import jax.experimental\n@@ -895,11 +866,6 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         # the expected dtype from 64 bit to 32 bit when using jax backend.\n         with jax.experimental.disable_x64():\n             dtype1, dtype2 = dtypes\n-            if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n-                \"complex64\",\n-                \"complex128\",\n-            ]:\n-                self.skipTest(\"Skipped test\")\n             x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n             x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n             x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -916,17 +882,12 @@ class VariableOpsDTypeTest(test_case.TestCase):\n             self.assertDType(x1.__rtruediv__(x2), expected_dtype)\n \n     @parameterized.named_parameters(\n-        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+        named_product(dtypes=itertools.combinations(NON_COMPLEX_DTYPES, 2))\n     )\n     def test_floordiv(self, dtypes):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n-            \"complex64\",\n-            \"complex128\",\n-        ]:\n-            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n@@ -939,17 +900,12 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         self.assertDType(x1.__rfloordiv__(x2), expected_dtype)\n \n     @parameterized.named_parameters(\n-        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+        named_product(dtypes=itertools.combinations(NON_COMPLEX_DTYPES, 2))\n     )\n     def test_mod(self, dtypes):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        if dtype1 in [\"complex64\", \"complex128\"] or dtype2 in [\n-            \"complex64\",\n-            \"complex128\",\n-        ]:\n-            self.skipTest(\"Skipped test\")\n         x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n         x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n\n@@ -46,6 +46,9 @@ TORCH_DTYPES = {\n     \"bool\": torch.bool,\n     \"float8_e4m3fn\": torch.float8_e4m3fn,\n     \"float8_e5m2\": torch.float8_e5m2,\n+    \"complex32\": torch.complex32,\n+    \"complex64\": torch.complex64,\n+    \"complex128\": torch.complex128,\n }\n \n \n\n@@ -1328,3 +1328,24 @@ class LayerTest(testing.TestCase):\n         layer2.build(None)\n         layer2_names = list(pname for pname, _ in layer2.named_parameters())\n         self.assertListEqual(layer1_names, layer2_names)\n+\n+    def test_complex_dtype_support(self):\n+\n+        class MyDenseLayer(layers.Layer):\n+            def __init__(self, num_outputs):\n+                super(MyDenseLayer, self).__init__()\n+                self.num_outputs = num_outputs\n+\n+            def build(self, input_shape):\n+                self.kernel = self.add_weight(\n+                    shape=[int(input_shape[-1]), self.num_outputs],\n+                )\n+\n+            def call(self, inputs):\n+                kernel = ops.cast(self.kernel, \"complex64\")\n+                return ops.matmul(inputs, kernel)\n+\n+        inputs = ops.zeros([10, 5], dtype=\"complex64\")\n+        layer = MyDenseLayer(10)\n+        output = layer(inputs)\n+        self.assertAllEqual(output.shape, (10, 10))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bae21acd992685f88372f0d603d89a46d1dd2bf8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/1 | Churn Δ: 37 | Churn Cumulative: 956 | Contributors (this commit): 8 | Commits (past 90d): 30 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -162,6 +162,13 @@ class Functional(Function, Model):\n                 layers.append(operation)\n         return layers\n \n+    @layers.setter\n+    def layers(self, _):\n+        raise AttributeError(\n+            \"`Model.layers` attribute is reserved and should not be used. \"\n+            \"Please use another name.\"\n+        )\n+\n     def call(self, inputs, training=None, mask=None):\n         # Add support for training, masking\n         inputs = self._standardize_inputs(inputs)\n\n@@ -530,3 +530,12 @@ class FunctionalTest(testing.TestCase):\n     def test_add_loss(self):\n         # TODO\n         pass\n+\n+    def test_layers_setter(self):\n+        inputs = Input(shape=(3,), batch_size=2, name=\"input\")\n+        outputs = layers.Dense(5)(inputs)\n+        model = Functional(inputs, outputs)\n+        with self.assertRaisesRegex(\n+            AttributeError, \"`Model.layers` attribute is reserved\"\n+        ):\n+            model.layers = [layers.Dense(4)]\n\n@@ -876,3 +876,10 @@ class ModelTest(testing.TestCase):\n             \"The following variable path is found twice in the model\",\n         ):\n             model.get_state_tree()\n+\n+    def test_layers_setter(self):\n+        model = Model()\n+        with self.assertRaisesRegex(\n+            AttributeError, \"`Model.layers` attribute is reserved\"\n+        ):\n+            model.layers = [layers.Dense(4)]\n\n@@ -240,6 +240,13 @@ class Sequential(Model):\n             return layers[1:]\n         return layers[:]\n \n+    @layers.setter\n+    def layers(self, _):\n+        raise AttributeError(\n+            \"`Sequential.layers` attribute is reserved and should not be used. \"\n+            \"Use `add()` and `pop()` to change the layers in this model.\"\n+        )\n+\n     def compute_output_spec(self, inputs, training=None, mask=None):\n         if self._functional:\n             return self._functional.compute_output_spec(\n\n@@ -378,3 +378,10 @@ class SequentialTest(testing.TestCase):\n         self.assertTrue(hasattr(model, \"output_shape\"))\n         self.assertTrue(hasattr(model, \"inputs\"))\n         self.assertTrue(hasattr(model, \"outputs\"))\n+\n+    def test_layers_setter(self):\n+        model = Sequential()\n+        with self.assertRaisesRegex(\n+            AttributeError, \"Use `add\\(\\)` and `pop\\(\\)`\"\n+        ):\n+            model.layers = [layers.Dense(4)]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d35031cd4a03ce07feea5f1e8f3a709c435b7057", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 165 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,7 @@ class RandomBrightnessTest(testing.TestCase):\n         output = layer(inputs, training=False)\n         self.assertAllClose(inputs, output)\n \n-    def test_output(self):\n+    def test_correctness(self):\n         seed = 2390\n \n         # Always scale up, but randomly between 0 ~ 255\n\n@@ -382,6 +382,6 @@ class SequentialTest(testing.TestCase):\n     def test_layers_setter(self):\n         model = Sequential()\n         with self.assertRaisesRegex(\n-            AttributeError, \"Use `add\\(\\)` and `pop\\(\\)`\"\n+            AttributeError, r\"Use `add\\(\\)` and `pop\\(\\)`\"\n         ):\n             model.layers = [layers.Dense(4)]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ac9d8cad13a99cf23eeb6e38f080ec032dd3ea9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 217 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 19 | Complexity Δ (Sum/Max): 43/43 | Churn Δ: 217 | Churn Cumulative: 217 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.7909604519774012\n\nDIFF:\n@@ -0,0 +1,217 @@\n+import json\n+import zipfile\n+\n+import h5py\n+\n+from keras.src.saving import deserialize_keras_object\n+from keras.src.saving.saving_lib import H5IOStore\n+\n+try:\n+    import IPython as ipython\n+except ImportError:\n+    ipython = None\n+\n+\n+def check_ipython():\n+    return ipython is not None\n+\n+\n+_CONFIG_FILENAME = \"config.json\"\n+_METADATA_FILENAME = \"metadata.json\"\n+_VARS_FNAME = \"model.weights\"\n+\n+\n+class KerasFileEditor:\n+    def __init__(self, filepath, reference_model=None, custom_objects=None):\n+        self.filepath = filepath\n+        self.custom_objects = custom_objects\n+        self.metadata = None\n+        self.reference_model = None\n+        self.config = None\n+\n+        if filepath.endswith(\".keras\"):\n+            self._init_for_keras_format(\n+                custom_objects, filepath, reference_model\n+            )\n+            weights_store = H5IOStore(\n+                _VARS_FNAME + \".h5\",\n+                archive=zipfile.ZipFile(filepath, \"r\"),\n+                mode=\"r\",\n+            )\n+        elif filepath.endswith(\".weights.h5\"):\n+            weights_store = H5IOStore(filepath, mode=\"r\")\n+        else:\n+            raise ValueError(\n+                \"Invalid filename: \"\n+                \"expected a `.keras` `.weights.h5` extension. \"\n+                f\"Received: filepath={filepath}\"\n+            )\n+\n+        def _extract_values(data):\n+            result = {}\n+            for key in data.keys():\n+                value = data[key]\n+                if isinstance(value, h5py.Group) and len(value) == 0:\n+                    continue\n+                if hasattr(value, \"keys\"):\n+                    if \"vars\" in value.keys():\n+                        result[key] = _extract_values(value[\"vars\"])\n+                    else:\n+                        result[key] = _extract_values(value)\n+                else:\n+                    result[key] = value\n+            return result\n+\n+        self.nested_dict = _extract_values(weights_store.h5_file)\n+\n+    def _init_for_keras_format(self, custom_objects, filepath, reference_model):\n+        with zipfile.ZipFile(filepath, \"r\") as zf:\n+            with zf.open(_CONFIG_FILENAME, \"r\") as f:\n+                self.config = json.loads(f.read())\n+            if reference_model is None:\n+                self.reference_model = deserialize_keras_object(\n+                    self.config, custom_objects=custom_objects\n+                )\n+\n+            with zf.open(_METADATA_FILENAME, \"r\") as f:\n+                self.metadata = json.loads(f.read())\n+\n+    def _generate_filepath_info(self):\n+        output = f\"Keras model file '{self.filepath}'\"\n+        return output\n+\n+    def _generate_config_info(self):\n+        output = (\n+            f\"Model: {self.config['class_name']} \"\n+            + f\"'name='{self.config['config']['name']}'\"\n+        )\n+        return output\n+\n+    def _generate_metadata_info(self):\n+        output = [\n+            f\"Saved with Keras {self.metadata['keras_version']}\",\n+            f\"Date saved: {self.metadata['date_saved']}\",\n+        ]\n+        return output\n+\n+    def list_layers_for_cli(self):\n+        def _print_filepath():\n+            print(self._generate_filepath_info())\n+\n+        def _print_config():\n+            print(self._generate_config_info())\n+\n+        def _print_metadata():\n+            for meta_info in self._generate_metadata_info():\n+                print(meta_info)\n+\n+        def _print_layer_structure(\n+            dictionary, indent=0, is_last=True, prefix=\"\"\n+        ):\n+            for idx, (key, value) in enumerate(dictionary.items()):\n+                is_last_item = idx == len(dictionary) - 1\n+                connector = \"└─ \" if is_last_item else \"├─ \"\n+\n+                if isinstance(value, dict):\n+                    print(f\"{prefix}{connector}{key}\")\n+                    new_prefix = prefix + (\"    \" if is_last_item else \"│   \")\n+                    _print_layer_structure(\n+                        value,\n+                        indent + 1,\n+                        is_last=is_last_item,\n+                        prefix=new_prefix,\n+                    )\n+                else:\n+                    if isinstance(value, h5py.Dataset):\n+                        print(\n+                            f\"{prefix}{connector}{key}:\"\n+                            + f\" shape={value.shape}, dtype={value.dtype}\"\n+                        )\n+                    else:\n+                        print(f\"{prefix}{connector}{key}: {value}\")\n+\n+        def _print_layer():\n+            print(\"Layers\")\n+            _print_layer_structure(self.nested_dict[\"layers\"], prefix=\" \" * 2)\n+\n+        _print_filepath()\n+\n+        if self.config is not None:\n+            _print_config()\n+\n+        if self.metadata is not None:\n+            _print_metadata()\n+\n+        _print_layer()\n+\n+    def list_layers_for_html(self):\n+        if not check_ipython():\n+            message = (\n+                \"You must install ipython (`pip install ipython`) for\"\n+                + \"KerasFileEditor to work.\"\n+            )\n+            raise ImportError(message)\n+\n+        def _generate_html_filepath():\n+            output = f\"<h2>{self._generate_filepath_info()}</h2>\"\n+            return output\n+\n+        def _generate_html_config():\n+            output = f\"<p>{self._generate_config_info()}</p>\"\n+            return output\n+\n+        def _generate_html_metadata():\n+            output = [\n+                f\"<p>{meta_info}</p>\"\n+                for meta_info in self._generate_metadata_info()\n+            ]\n+            output = \"\".join(output)\n+            return output\n+\n+        def _generate_html_weight(dictionary, margin_left=0, font_size=20):\n+            html = \"\"\n+            for key, value in dictionary.items():\n+                if isinstance(value, dict) and value:\n+                    html += (\n+                        f'<details style=\"margin-left: {margin_left}px;\">'\n+                        + f'<summary style=\"font-size: {font_size}px;\">'\n+                        + f\"{key}</summary>\"\n+                        + _generate_html_weight(\n+                            value, margin_left + 20, font_size - 1\n+                        )\n+                        + \"</details>\"\n+                    )\n+                else:\n+                    if isinstance(value, h5py.Dataset):\n+                        html += (\n+                            f'<details style=\"margin-left: {margin_left}px;\">'\n+                            + f'<summary style=\"font-size: {font_size}px;\">'\n+                            + f\"{key} : shape={value.shape}\"\n+                            + f\", dtype={value.dtype}</summary>\"\n+                            + \"</details>\"\n+                        )\n+                    else:\n+                        html += (\n+                            f'<details style=\"margin-left: {margin_left}px;\">'\n+                            + f'<summary style=\"font-size: {font_size}px;\">'\n+                            + f\"{key} </summary>\"\n+                            + \"</details>\"\n+                        )\n+            return html\n+\n+        def _generate_html_layer():\n+            output = \"<p>Layers</p>\"\n+            output += _generate_html_weight(self.nested_dict[\"layers\"])\n+            return output\n+\n+        output = _generate_html_filepath()\n+\n+        if self.config is not None:\n+            output += _generate_html_config()\n+\n+        if self.metadata is not None:\n+            output += _generate_html_metadata()\n+\n+        output += _generate_html_layer()\n+\n+        ipython.display.display(ipython.display.HTML(output))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
