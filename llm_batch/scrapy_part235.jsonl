{"custom_id": "scrapy#766b2c84539d58ee871e1f301df1ad0ae0d44079", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 120 | Lines Deleted: 28 | Files Changed: 4 | Hunks: 32 | Methods Changed: 12 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 148 | Churn Cumulative: 1322 | Contributors (this commit): 39 | Commits (past 90d): 15 | Contributors (cumulative): 59 | DMM Complexity: 0.978021978021978\n\nDIFF:\n@@ -36,7 +36,7 @@ class BaseItemExporter(object):\n         self.encoding = options.pop('encoding', None)\n         self.fields_to_export = options.pop('fields_to_export', None)\n         self.export_empty_fields = options.pop('export_empty_fields', False)\n-        self.indent_width = options.pop('indent_width', None)\n+        self.indent = options.pop('indent', None)\n         if not dont_fail and options:\n             raise TypeError(\"Unexpected options: %s\" % ', '.join(options.keys()))\n \n@@ -100,20 +100,28 @@ class JsonItemExporter(BaseItemExporter):\n         self._configure(kwargs, dont_fail=True)\n         self.file = file\n         kwargs.setdefault('ensure_ascii', not self.encoding)\n-        self.encoder = ScrapyJSONEncoder(indent=self.indent_width, **kwargs)\n+        kwargs.setdefault('indent', self.indent)\n+        self.encoder = ScrapyJSONEncoder(**kwargs)\n         self.first_item = True\n \n+    def _beautify_newline(self):\n+        if self.indent is not None:\n+            self.file.write(b'\\n')\n+\n     def start_exporting(self):\n-        self.file.write(b\"[\\n\")\n+        self.file.write(b\"[\")\n+        self._beautify_newline()\n \n     def finish_exporting(self):\n-        self.file.write(b\"\\n]\")\n+        self._beautify_newline()\n+        self.file.write(b\"]\")\n \n     def export_item(self, item):\n         if self.first_item:\n             self.first_item = False\n         else:\n-            self.file.write(b',\\n')\n+            self.file.write(b',')\n+            self._beautify_newline()\n         itemdict = dict(self._get_serialized_fields(item))\n         data = self.encoder.encode(itemdict)\n         self.file.write(to_bytes(data, self.encoding))\n@@ -130,12 +138,12 @@ class XmlItemExporter(BaseItemExporter):\n         self.xg = XMLGenerator(file, encoding=self.encoding)\n \n     def _beautify_newline(self):\n-        if self.indent_width:\n+        if self.indent is not None:\n             self._xg_characters('\\n')\n \n     def _beautify_indent(self, depth=1):\n-        if self.indent_width:\n-            self._xg_characters(' ' * self.indent_width * depth)\n+        if self.indent:\n+            self._xg_characters(' ' * self.indent * depth)\n \n     def start_exporting(self):\n         self.xg.startDocument()\n\n@@ -172,7 +172,9 @@ class FeedExporter(object):\n         self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n         self._exporting = False\n         self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n-        self.indent_width = settings.getint('FEED_EXPORT_INDENT_WIDTH') or None\n+        self.indent = None\n+        if settings.get('FEED_EXPORT_INDENT') is not None:\n+            self.indent = settings.getint('FEED_EXPORT_INDENT')\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n \n@@ -189,7 +191,7 @@ class FeedExporter(object):\n         storage = self._get_storage(uri)\n         file = storage.open(spider)\n         exporter = self._get_exporter(file, fields_to_export=self.export_fields,\n-            encoding=self.export_encoding, indent_width=self.indent_width)\n+            encoding=self.export_encoding, indent=self.indent)\n         if self.store_empty:\n             exporter.start_exporting()\n             self._exporting = True\n\n@@ -161,7 +161,7 @@ FEED_EXPORTERS_BASE = {\n     'marshal': 'scrapy.exporters.MarshalItemExporter',\n     'pickle': 'scrapy.exporters.PickleItemExporter',\n }\n-FEED_EXPORT_INDENT_WIDTH = None\n+FEED_EXPORT_INDENT = None\n \n FILES_STORE_S3_ACL = 'private'\n \n\n@@ -319,7 +319,7 @@ class FeedExportTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n         formats = (\n-            ('json', b'[\\n\\n]'),\n+            ('json', b'[]'),\n             ('jsonlines', b''),\n             ('xml', b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items></items>'),\n             ('csv', b''),\n@@ -425,25 +425,25 @@ class FeedExportTest(unittest.TestCase):\n         header = ['foo']\n \n         formats = {\n-            'json': u'[\\n{\"foo\": \"Test\\\\u00d6\"}\\n]'.encode('utf-8'),\n+            'json': u'[{\"foo\": \"Test\\\\u00d6\"}]'.encode('utf-8'),\n             'jsonlines': u'{\"foo\": \"Test\\\\u00d6\"}\\n'.encode('utf-8'),\n             'xml': u'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode('utf-8'),\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('utf-8'),\n         }\n \n         for format, expected in formats.items():\n-            settings = {'FEED_FORMAT': format, 'FEED_EXPORT_INDENT_WIDTH': None}\n+            settings = {'FEED_FORMAT': format, 'FEED_EXPORT_INDENT': None}\n             data = yield self.exported_data(items, settings)\n             self.assertEqual(expected, data)\n \n         formats = {\n-            'json': u'[\\n{\"foo\": \"Test\\xd6\"}\\n]'.encode('latin-1'),\n+            'json': u'[{\"foo\": \"Test\\xd6\"}]'.encode('latin-1'),\n             'jsonlines': u'{\"foo\": \"Test\\xd6\"}\\n'.encode('latin-1'),\n             'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode('latin-1'),\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('latin-1'),\n         }\n \n-        settings = {'FEED_EXPORT_INDENT_WIDTH': None, 'FEED_EXPORT_ENCODING': 'latin-1'}\n+        settings = {'FEED_EXPORT_INDENT': None, 'FEED_EXPORT_ENCODING': 'latin-1'}\n         for format, expected in formats.items():\n             settings['FEED_FORMAT'] = format\n             data = yield self.exported_data(items, settings)\n@@ -451,48 +451,89 @@ class FeedExportTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_export_indentation(self):\n-        items = [dict({'foo': ['bar']})]\n+        items = [dict({'foo': ['bar']}), dict({'key': 'value'})]\n \n         output = [\n             # JSON\n             {\n                 'format': 'json',\n-                'indent_width': None,\n-                'expected': b'[\\n{\"foo\": [\"bar\"]}\\n]',\n+                'indent': None,\n+                'expected': b'[{\"foo\": [\"bar\"]},{\"key\": \"value\"}]',\n             },\n             {\n                 'format': 'json',\n-                'indent_width': 2,\n+                'indent': -1,\n                 'expected': b\"\"\"\n [\n {\n \"foo\": [\n \"bar\"\n ]\n+},\n+{\n+\"key\": \"value\"\n+}\n+]\n+\"\"\",\n+            },\n+            {\n+                'format': 'json',\n+                'indent': 0,\n+                'expected': b\"\"\"\n+[\n+{\n+\"foo\": [\n+\"bar\"\n+]\n+},\n+{\n+\"key\": \"value\"\n+}\n+]\n+\"\"\",\n+            },\n+            {\n+                'format': 'json',\n+                'indent': 2,\n+                'expected': b\"\"\"\n+[\n+{\n+  \"foo\": [\n+    \"bar\"\n+  ]\n+},\n+{\n+  \"key\": \"value\"\n }\n ]\"\"\",\n             },\n             {\n                 'format': 'json',\n-                'indent_width': 4,\n+                'indent': 4,\n                 'expected': b\"\"\"\n [\n {\n     \"foo\": [\n         \"bar\"\n     ]\n+},\n+{\n+    \"key\": \"value\"\n }\n ]\"\"\",\n             },\n             {\n                 'format': 'json',\n-                'indent_width': 5,\n+                'indent': 5,\n                 'expected': b\"\"\"\n [\n {\n      \"foo\": [\n           \"bar\"\n      ]\n+},\n+{\n+     \"key\": \"value\"\n }\n ]\"\"\",\n             },\n@@ -500,12 +541,12 @@ class FeedExportTest(unittest.TestCase):\n             # XML\n             {\n                 'format': 'xml',\n-                'indent_width': None,\n-                'expected': b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo><value>bar</value></foo></item></items>',\n+                'indent': None,\n+                'expected': b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo><value>bar</value></foo></item><item><key>value</key></item></items>',\n             },\n             {\n                 'format': 'xml',\n-                'indent_width': 2,\n+                'indent': -1,\n                 'expected': b\"\"\"\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n@@ -514,11 +555,14 @@ class FeedExportTest(unittest.TestCase):\n <value>bar</value>\n </foo>\n </item>\n+<item>\n+<key>value</key>\n+</item>\n </items>\"\"\",\n             },\n             {\n                 'format': 'xml',\n-                'indent_width': 4,\n+                'indent': 0,\n                 'expected': b\"\"\"\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n@@ -527,11 +571,14 @@ class FeedExportTest(unittest.TestCase):\n <value>bar</value>\n </foo>\n </item>\n+<item>\n+<key>value</key>\n+</item>\n </items>\"\"\",\n             },\n             {\n                 'format': 'xml',\n-                'indent_width': 5,\n+                'indent': 2,\n                 'expected': b\"\"\"\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n@@ -540,11 +587,46 @@ class FeedExportTest(unittest.TestCase):\n       <value>bar</value>\n     </foo>\n   </item>\n+  <item>\n+    <key>value</key>\n+  </item>\n+</items>\"\"\",\n+            },\n+            {\n+                'format': 'xml',\n+                'indent': 4,\n+                'expected': b\"\"\"\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<items>\n+    <item>\n+        <foo>\n+            <value>bar</value>\n+        </foo>\n+    </item>\n+    <item>\n+        <key>value</key>\n+    </item>\n+</items>\"\"\",\n+            },\n+            {\n+                'format': 'xml',\n+                'indent': 5,\n+                'expected': b\"\"\"\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<items>\n+     <item>\n+          <foo>\n+               <value>bar</value>\n+          </foo>\n+     </item>\n+     <item>\n+          <key>value</key>\n+     </item>\n </items>\"\"\",\n             },\n         ]\n \n         for row in output:\n-            settings = {'FEED_FORMAT': row['format'], 'FEED_EXPORT_INDENT_WIDTH': row['indent_width']}\n+            settings = {'FEED_FORMAT': row['format'], 'FEED_EXPORT_INDENT': row['indent']}\n             data = yield self.exported_data(items, settings)\n             self.assertEqual(row['expected'].strip(), data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7bb2fa8ce2633d92a7ec2840f84b174a5494428", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 43 | Files Changed: 3 | Hunks: 16 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 72 | Churn Cumulative: 1305 | Contributors (this commit): 38 | Commits (past 90d): 16 | Contributors (cumulative): 50 | DMM Complexity: 0.0\n\nDIFF:\n@@ -99,8 +99,12 @@ class JsonItemExporter(BaseItemExporter):\n     def __init__(self, file, **kwargs):\n         self._configure(kwargs, dont_fail=True)\n         self.file = file\n+        # there is a small difference between the behaviour or JsonItemExporter.indent\n+        # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n+        # the addition of newlines everywhere\n+        json_indent = self.indent if self.indent is not None and self.indent > 0 else None\n+        kwargs.setdefault('indent', json_indent)\n         kwargs.setdefault('ensure_ascii', not self.encoding)\n-        kwargs.setdefault('indent', self.indent)\n         self.encoder = ScrapyJSONEncoder(**kwargs)\n         self.first_item = True\n \n@@ -137,8 +141,8 @@ class XmlItemExporter(BaseItemExporter):\n             self.encoding = 'utf-8'\n         self.xg = XMLGenerator(file, encoding=self.encoding)\n \n-    def _beautify_newline(self):\n-        if self.indent is not None:\n+    def _beautify_newline(self, new_item=False):\n+        if self.indent is not None and (self.indent > 0 or new_item):\n             self._xg_characters('\\n')\n \n     def _beautify_indent(self, depth=1):\n@@ -148,7 +152,7 @@ class XmlItemExporter(BaseItemExporter):\n     def start_exporting(self):\n         self.xg.startDocument()\n         self.xg.startElement(self.root_element, {})\n-        self._beautify_newline()\n+        self._beautify_newline(new_item=True)\n \n     def export_item(self, item):\n         self._beautify_indent(depth=1)\n@@ -158,7 +162,7 @@ class XmlItemExporter(BaseItemExporter):\n             self._export_xml_field(name, value, depth=2)\n         self._beautify_indent(depth=1)\n         self.xg.endElement(self.item_element)\n-        self._beautify_newline()\n+        self._beautify_newline(new_item=True)\n \n     def finish_exporting(self):\n         self.xg.endElement(self.root_element)\n\n@@ -161,7 +161,7 @@ FEED_EXPORTERS_BASE = {\n     'marshal': 'scrapy.exporters.MarshalItemExporter',\n     'pickle': 'scrapy.exporters.PickleItemExporter',\n }\n-FEED_EXPORT_INDENT = None\n+FEED_EXPORT_INDENT = 0\n \n FILES_STORE_S3_ACL = 'private'\n \n\n@@ -326,7 +326,7 @@ class FeedExportTest(unittest.TestCase):\n         )\n \n         for fmt, expctd in formats:\n-            settings = {'FEED_FORMAT': fmt, 'FEED_STORE_EMPTY': True}\n+            settings = {'FEED_FORMAT': fmt, 'FEED_STORE_EMPTY': True, 'FEED_EXPORT_INDENT': None}\n             data = yield self.exported_no_data(settings)\n             self.assertEqual(data, expctd)\n \n@@ -451,9 +451,12 @@ class FeedExportTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_export_indentation(self):\n-        items = [dict({'foo': ['bar']}), dict({'key': 'value'})]\n+        items = [\n+            {'foo': ['bar']},\n+            {'key': 'value'},\n+        ]\n \n-        output = [\n+        test_cases = [\n             # JSON\n             {\n                 'format': 'json',\n@@ -465,14 +468,8 @@ class FeedExportTest(unittest.TestCase):\n                 'indent': -1,\n                 'expected': b\"\"\"\n [\n-{\n-\"foo\": [\n-\"bar\"\n-]\n-},\n-{\n-\"key\": \"value\"\n-}\n+{\"foo\": [\"bar\"]},\n+{\"key\": \"value\"}\n ]\n \"\"\",\n             },\n@@ -481,14 +478,8 @@ class FeedExportTest(unittest.TestCase):\n                 'indent': 0,\n                 'expected': b\"\"\"\n [\n-{\n-\"foo\": [\n-\"bar\"\n-]\n-},\n-{\n-\"key\": \"value\"\n-}\n+{\"foo\": [\"bar\"]},\n+{\"key\": \"value\"}\n ]\n \"\"\",\n             },\n@@ -542,7 +533,9 @@ class FeedExportTest(unittest.TestCase):\n             {\n                 'format': 'xml',\n                 'indent': None,\n-                'expected': b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo><value>bar</value></foo></item><item><key>value</key></item></items>',\n+                'expected': b\"\"\"\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<items><item><foo><value>bar</value></foo></item><item><key>value</key></item></items>\"\"\",\n             },\n             {\n                 'format': 'xml',\n@@ -550,14 +543,8 @@ class FeedExportTest(unittest.TestCase):\n                 'expected': b\"\"\"\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n-<item>\n-<foo>\n-<value>bar</value>\n-</foo>\n-</item>\n-<item>\n-<key>value</key>\n-</item>\n+<item><foo><value>bar</value></foo></item>\n+<item><key>value</key></item>\n </items>\"\"\",\n             },\n             {\n@@ -566,14 +553,8 @@ class FeedExportTest(unittest.TestCase):\n                 'expected': b\"\"\"\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n <items>\n-<item>\n-<foo>\n-<value>bar</value>\n-</foo>\n-</item>\n-<item>\n-<key>value</key>\n-</item>\n+<item><foo><value>bar</value></foo></item>\n+<item><key>value</key></item>\n </items>\"\"\",\n             },\n             {\n@@ -626,7 +607,8 @@ class FeedExportTest(unittest.TestCase):\n             },\n         ]\n \n-        for row in output:\n+        for row in test_cases:\n             settings = {'FEED_FORMAT': row['format'], 'FEED_EXPORT_INDENT': row['indent']}\n             data = yield self.exported_data(items, settings)\n+            print(row['format'], row['indent'])\n             self.assertEqual(row['expected'].strip(), data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7be773e14ae27501914c3f3922f5504a7ac72f48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 7 | Files Changed: 8 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 31 | Churn Cumulative: 1444 | Contributors (this commit): 47 | Commits (past 90d): 25 | Contributors (cumulative): 81 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,8 @@ from scrapy.commands import ScrapyCommand\n class Command(ScrapyCommand):\n \n     requires_project = True\n-    default_settings = {'LOG_ENABLED': False}\n+    default_settings = {'LOG_ENABLED': False,\n+                        'SPIDER_LOADER_WARN_ONLY': True}\n \n     def short_desc(self):\n         return \"List available spiders\"\n\n@@ -28,6 +28,7 @@ def _import_file(filepath):\n class Command(ScrapyCommand):\n \n     requires_project = False\n+    default_settings = {'SPIDER_LOADER_WARN_ONLY': True}\n \n     def syntax(self):\n         return \"[options] <spider_file>\"\n\n@@ -7,7 +7,8 @@ from scrapy.settings import BaseSettings\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'LOG_ENABLED': False}\n+    default_settings = {'LOG_ENABLED': False,\n+                        'SPIDER_LOADER_WARN_ONLY': True}\n \n     def syntax(self):\n         return \"[options]\"\n\n@@ -26,7 +26,8 @@ IGNORE = ignore_patterns('*.pyc', '.svn')\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'LOG_ENABLED': False}\n+    default_settings = {'LOG_ENABLED': False,\n+                        'SPIDER_LOADER_WARN_ONLY': True}\n \n     def syntax(self):\n         return \"<project_name> [project_dir]\"\n\n@@ -11,7 +11,8 @@ from scrapy.commands import ScrapyCommand\n \n class Command(ScrapyCommand):\n \n-    default_settings = {'LOG_ENABLED': False}\n+    default_settings = {'LOG_ENABLED': False,\n+                        'SPIDER_LOADER_WARN_ONLY': True}\n \n     def syntax(self):\n         return \"[-v]\"\n\n@@ -250,6 +250,7 @@ SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'\n SCHEDULER_PRIORITY_QUEUE = 'queuelib.PriorityQueue'\n \n SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader'\n+SPIDER_LOADER_WARN_ONLY = False\n \n SPIDER_MIDDLEWARES = {}\n \n\n@@ -19,6 +19,7 @@ class SpiderLoader(object):\n     \"\"\"\n     def __init__(self, settings):\n         self.spider_modules = settings.getlist('SPIDER_MODULES')\n+        self.warn_only = settings.getbool('SPIDER_LOADER_WARN_ONLY')\n         self._spiders = {}\n         self._found = defaultdict(list)\n         self._load_all_spiders()\n@@ -46,10 +47,13 @@ class SpiderLoader(object):\n                 for module in walk_modules(name):\n                     self._load_spiders(module)\n             except ImportError as e:\n+                if self.warn_only:\n                     msg = (\"\\n{tb}Could not load spiders from module '{modname}'. \"\n-                       \"Check SPIDER_MODULES setting\".format(\n+                           \"See above traceback for details.\".format(\n                                 modname=name, tb=traceback.format_exc()))\n                     warnings.warn(msg, RuntimeWarning)\n+                else:\n+                    raise\n         self._check_name_duplicates()\n \n     @classmethod\n\n@@ -91,18 +91,25 @@ class SpiderLoaderTest(unittest.TestCase):\n         self.assertTrue(issubclass(crawler.spidercls, scrapy.Spider))\n         self.assertEqual(crawler.spidercls.name, 'spider1')\n \n+    def test_bad_spider_modules_exception(self):\n+\n+        module = 'tests.test_spiderloader.test_spiders.doesnotexist'\n+        settings = Settings({'SPIDER_MODULES': [module]})\n+        with self.assertRaises(ImportError):\n+            SpiderLoader.from_settings(settings)\n+\n     def test_bad_spider_modules_warning(self):\n \n         with warnings.catch_warnings(record=True) as w:\n             module = 'tests.test_spiderloader.test_spiders.doesnotexist'\n-            settings = Settings({'SPIDER_MODULES': [module]})\n+            settings = Settings({'SPIDER_MODULES': [module],\n+                                 'SPIDER_LOADER_WARN_ONLY': True})\n             spider_loader = SpiderLoader.from_settings(settings)\n             self.assertIn(\"Could not load spiders from module\", str(w[0].message))\n \n             spiders = spider_loader.list()\n             self.assertEqual(spiders, [])\n \n-\n class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n \n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4090cc3990636337964a6e157679d6be15ba6f3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 42 | Churn Cumulative: 382 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,16 +9,20 @@ from scrapy.spiders import Spider\n from scrapy.item import Item, Field\n from scrapy.http import Request\n from scrapy.utils.test import get_crawler\n+from tests.mockserver import MockServer\n \n \n class TestItem(Item):\n     value = Field()\n \n \n+class LocalhostSpider(Spider):\n+    start_urls = ['http://localhost:8998']  # tests.mockserver.MockServer\n+\n+\n # ================================================================================\n # exceptions from a spider's parse method\n-class BaseExceptionFromParseMethodSpider(Spider):\n-    start_urls = [\"http://example.com/\"]\n+class BaseExceptionFromParseMethodSpider(LocalhostSpider):\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {'tests.test_spidermiddleware.CatchExceptionMiddleware': 540}\n     }\n@@ -65,8 +69,7 @@ class CatchExceptionMiddleware(object):\n # exception from a previous middleware's process_spider_input method\n # process_spider_input is not expected to return an iterable, so there are no\n # separate tests for generator/non-generator implementations\n-class FromPreviousMiddlewareInputSpider(Spider):\n-    start_urls = [\"http://example.com/\"]\n+class FromPreviousMiddlewareInputSpider(LocalhostSpider):\n     name = 'not_a_generator_from_previous_middleware_input'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n@@ -88,8 +91,7 @@ class RaiseExceptionOnInputMiddleware(object):\n \n # ================================================================================\n # exception from a previous middleware's process_spider_output method (not a generator)\n-class NotAGeneratorFromPreviousMiddlewareOutputSpider(Spider):\n-    start_urls = [\"http://example.com/\"]\n+class NotAGeneratorFromPreviousMiddlewareOutputSpider(LocalhostSpider):\n     name = 'not_a_generator_from_previous_middleware_output'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n@@ -111,8 +113,7 @@ class RaiseExceptionOnOutputNotAGeneratorMiddleware(object):\n \n # ================================================================================\n # exception from a previous middleware's process_spider_output method (generator)\n-class GeneratorFromPreviousMiddlewareOutputSpider(Spider):\n-    start_urls = [\"http://example.com/\"]\n+class GeneratorFromPreviousMiddlewareOutputSpider(LocalhostSpider):\n     name = 'generator_from_previous_middleware_output'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n@@ -136,8 +137,7 @@ class RaiseExceptionOnOutputGeneratorMiddleware(object):\n \n # ================================================================================\n # do something useful from the exception handler\n-class DoSomethingSpider(Spider):\n-    start_urls = [\"http://example.com\"]\n+class DoSomethingSpider(LocalhostSpider):\n     name = 'do_something'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n@@ -155,13 +155,12 @@ class DoSomethingSpider(Spider):\n \n class DoSomethingMiddleware(object):\n     def process_spider_exception(self, response, exception, spider):\n-        return [Request('http://example.org'), {'value': 10}, TestItem(value='asdf')]\n+        return [Request('http://localhost:8998?processed=true'), {'value': 10}, TestItem(value='asdf')]\n \n \n # ================================================================================\n # don't catch InvalidOutput from scrapy's spider middleware manager\n-class InvalidReturnValueFromPreviousMiddlewareInputSpider(Spider):\n-    start_urls = [\"http://example.com/\"]\n+class InvalidReturnValueFromPreviousMiddlewareInputSpider(LocalhostSpider):\n     name = 'invalid_return_value_from_previous_middleware_input'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n@@ -181,8 +180,7 @@ class InvalidReturnValueInputMiddleware(object):\n         return 1.0  # <type 'float'>, not None\n \n \n-class InvalidReturnValueFromPreviousMiddlewareOutputSpider(Spider):\n-    start_urls = [\"http://example.com/\"]\n+class InvalidReturnValueFromPreviousMiddlewareOutputSpider(LocalhostSpider):\n     name = 'invalid_return_value_from_previous_middleware_output'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n@@ -205,8 +203,7 @@ class InvalidReturnValueOutputMiddleware(object):\n # ================================================================================\n # make sure only non already called process_spider_output methods\n # are called if process_spider_exception returns an iterable\n-class ExecutionChainSpider(Spider):\n-    start_urls = [\"http://example.com\"]\n+class ExecutionChainSpider(LocalhostSpider):\n     name = 'execution_chain'\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n@@ -258,6 +255,13 @@ class ThirdMiddleware(object):\n \n class TestSpiderMiddleware(TestCase):\n \n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n     @defer.inlineCallbacks\n     def test_process_spider_exception_from_parse_method(self):\n         # non-generator return value\n@@ -309,8 +313,8 @@ class TestSpiderMiddleware(TestCase):\n         self.assertIn(\"ImportError exception caught\", str(log))\n         self.assertIn(\"{'value': 10}\", str(log))\n         self.assertIn(\"{'value': 'asdf'}\", str(log))\n-        self.assertIn(\"{'value': 'http://example.com'}\", str(log))\n-        self.assertIn(\"{'value': 'http://example.org'}\", str(log))\n+        self.assertIn(\"{'value': 'http://localhost:8998'}\", str(log))\n+        self.assertIn(\"{'value': 'http://localhost:8998?processed=true'}\", str(log))\n \n     @defer.inlineCallbacks\n     def test_process_spider_exception_invalid_return_value_previous_middleware(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9cfe9ae0989069b1a7a2ae0af916ffccb5a8bcee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 202 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -95,8 +95,7 @@ class SpiderLoaderTest(unittest.TestCase):\n \n         module = 'tests.test_spiderloader.test_spiders.doesnotexist'\n         settings = Settings({'SPIDER_MODULES': [module]})\n-        with self.assertRaises(ImportError):\n-            SpiderLoader.from_settings(settings)\n+        self.assertRaises(ImportError, SpiderLoader.from_settings, settings)\n \n     def test_bad_spider_modules_warning(self):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f2ac24eb7b353d1140729ba7d8c81ad9635d5899", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 48 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -4,8 +4,7 @@ from scrapy.commands import ScrapyCommand\n class Command(ScrapyCommand):\n \n     requires_project = True\n-    default_settings = {'LOG_ENABLED': False,\n-                        'SPIDER_LOADER_WARN_ONLY': True}\n+    default_settings = {'LOG_ENABLED': False}\n \n     def short_desc(self):\n         return \"List available spiders\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c256cf693d73e854d409d717854b3f354b5e0a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 629 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n import six\n from twisted.python.failure import Failure\n-from scrapy.exceptions import InvalidOutput\n+from scrapy.exceptions import _InvalidOutput\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.conf import build_component_list\n@@ -42,7 +42,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 try:\n                     result = method(response=response, spider=spider)\n                     if result is not None:\n-                        raise InvalidOutput('Middleware {} must return None or raise ' \\\n+                        raise _InvalidOutput('Middleware {} must return None or raise ' \\\n                             'an exception, got {}'.format(fname(method), type(result)))\n                 except:\n                     return scrape_func(Failure(), request, spider)\n@@ -50,13 +50,13 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n         def process_spider_exception(_failure):\n             exception = _failure.value\n-            # don't handle InvalidOutput exception\n-            if isinstance(exception, InvalidOutput):\n+            # don't handle _InvalidOutput exception\n+            if isinstance(exception, _InvalidOutput):\n                 return _failure\n             for method in self.methods['process_spider_exception']:\n                 result = method(response=response, exception=exception, spider=spider)\n                 if result is not None and not _isiterable(result):\n-                    raise InvalidOutput('Middleware {} must return None or an iterable ' \\\n+                    raise _InvalidOutput('Middleware {} must return None or an iterable ' \\\n                         'object, got {}'.format(fname(method), type(result)))\n                 # stop exception handling by handing control over to the\n                 # process_spider_output chain if an iterable has been returned\n@@ -80,7 +80,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 if _isiterable(result):\n                     result = wrapper(result)\n                 else:\n-                    raise InvalidOutput('Middleware {} must return an iterable object, ' \\\n+                    raise _InvalidOutput('Middleware {} must return an iterable object, ' \\\n                         'got {}'.format(fname(method), type(result)))\n             return result\n \n\n@@ -11,9 +11,11 @@ class NotConfigured(Exception):\n     \"\"\"Indicates a missing configuration situation\"\"\"\n     pass\n \n-class InvalidOutput(TypeError):\n-    \"\"\"Indicates an invalid value has been returned\n-    by a middleware's processing method\"\"\"\n+class _InvalidOutput(TypeError):\n+    \"\"\"\n+    Indicates an invalid value has been returned by a middleware's processing method.\n+    Internal and undocumented, it should not be raised or caught by user code.\n+    \"\"\"\n     pass\n \n # HTTP and crawling\n\n@@ -159,7 +159,7 @@ class DoSomethingMiddleware(object):\n \n \n # ================================================================================\n-# don't catch InvalidOutput from scrapy's spider middleware manager\n+# don't catch _InvalidOutput from scrapy's spider middleware manager\n class InvalidReturnValueFromPreviousMiddlewareInputSpider(LocalhostSpider):\n     name = 'invalid_return_value_from_previous_middleware_input'\n     custom_settings = {\n@@ -318,19 +318,19 @@ class TestSpiderMiddleware(TestCase):\n \n     @defer.inlineCallbacks\n     def test_process_spider_exception_invalid_return_value_previous_middleware(self):\n-        \"\"\" don't catch InvalidOutput from middleware \"\"\"\n+        \"\"\" don't catch _InvalidOutput from middleware \"\"\"\n         # on middleware's input\n         crawler1 = get_crawler(InvalidReturnValueFromPreviousMiddlewareInputSpider)\n         with LogCapture() as log1:\n             yield crawler1.crawl()\n-        self.assertNotIn(\"InvalidOutput exception caught\", str(log1))\n-        self.assertIn(\"'spider_exceptions/InvalidOutput'\", str(log1))\n+        self.assertNotIn(\"_InvalidOutput exception caught\", str(log1))\n+        self.assertIn(\"'spider_exceptions/_InvalidOutput'\", str(log1))\n         # on middleware's output\n         crawler2 = get_crawler(InvalidReturnValueFromPreviousMiddlewareOutputSpider)\n         with LogCapture() as log2:\n             yield crawler2.crawl()\n-        self.assertNotIn(\"InvalidOutput exception caught\", str(log2))\n-        self.assertIn(\"'spider_exceptions/InvalidOutput'\", str(log2))\n+        self.assertNotIn(\"_InvalidOutput exception caught\", str(log2))\n+        self.assertIn(\"'spider_exceptions/_InvalidOutput'\", str(log2))\n \n     @defer.inlineCallbacks\n     def test_process_spider_exception_execution_chain(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cfbe8204480214b65d48caaf080feda30fe91ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 19 | Churn Cumulative: 183 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ import six\n \n from twisted.internet import defer\n \n+from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Request, Response\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n@@ -35,9 +36,9 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         def process_request(request):\n             for method in self.methods['process_request']:\n                 response = yield method(request=request, spider=spider)\n-                assert response is None or isinstance(response, (Response, Request)), \\\n-                        'Middleware %s.process_request must return None, Response or Request, got %s' % \\\n-                        (six.get_method_self(method).__class__.__name__, response.__class__.__name__)\n+                if response is not None and not isinstance(response, (Response, Request)):\n+                    raise _InvalidOutput('Middleware %s.process_request must return None, Response or Request, got %s' % \\\n+                                         (six.get_method_self(method).__class__.__name__, response.__class__.__name__))\n                 if response:\n                     defer.returnValue(response)\n             defer.returnValue((yield download_func(request=request,spider=spider)))\n@@ -51,9 +52,9 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n             for method in self.methods['process_response']:\n                 response = yield method(request=request, response=response,\n                                         spider=spider)\n-                assert isinstance(response, (Response, Request)), \\\n-                    'Middleware %s.process_response must return Response or Request, got %s' % \\\n-                    (six.get_method_self(method).__class__.__name__, type(response))\n+                if not isinstance(response, (Response, Request)):\n+                    raise _InvalidOutput('Middleware %s.process_response must return Response or Request, got %s' % \\\n+                                         (six.get_method_self(method).__class__.__name__, type(response)))\n                 if isinstance(response, Request):\n                     defer.returnValue(response)\n             defer.returnValue(response)\n@@ -64,9 +65,9 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n             for method in self.methods['process_exception']:\n                 response = yield method(request=request, exception=exception,\n                                         spider=spider)\n-                assert response is None or isinstance(response, (Response, Request)), \\\n-                    'Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n-                    (six.get_method_self(method).__class__.__name__, type(response))\n+                if response is not None and not isinstance(response, (Response, Request)):\n+                    raise _InvalidOutput('Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n+                                         (six.get_method_self(method).__class__.__name__, type(response)))\n                 if response:\n                     defer.returnValue(response)\n             defer.returnValue(_failure)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7dcc86e61adf37fdfb77b00375040fe25e7ad832", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 180 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,13 +5,17 @@ from subprocess import Popen, PIPE\n \n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n+from twisted.web.static import File\n from twisted.web.test.test_webclient import PayloadResource\n from twisted.web.server import GzipEncoderFactory\n from twisted.web.resource import EncodingResourceWrapper\n+from twisted.web.util import redirectTo\n from twisted.internet import reactor, ssl\n from twisted.internet.task import deferLater\n \n+\n from scrapy.utils.python import to_bytes, to_unicode\n+from tests import tests_datadir\n \n \n def getarg(request, name, default=None, type=None):\n@@ -120,6 +124,16 @@ class Echo(LeafResource):\n         return to_bytes(json.dumps(output))\n \n \n+class RedirectTo(LeafResource):\n+\n+    def render(self, request):\n+        goto = getarg(request, b'goto', b'/')\n+        # we force the body content, otherwise Twisted redirectTo()\n+        # returns HTML with <meta http-equiv=\"refresh\"\n+        redirectTo(goto, request)\n+        return b'redirecting...'\n+\n+\n class Partial(LeafResource):\n \n     def render_GET(self, request):\n@@ -160,6 +174,8 @@ class Root(Resource):\n         self.putChild(b\"echo\", Echo())\n         self.putChild(b\"payload\", PayloadResource())\n         self.putChild(b\"xpayload\", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n+        self.putChild(b\"files\", File(os.path.join(tests_datadir, 'test_site/files/')))\n+        self.putChild(b\"redirect-to\", RedirectTo())\n \n     def getChild(self, name, request):\n         return self\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#708f1b009b9b23971d73bfc7bc09163969ab6e00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 163 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 12 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 163 | Churn Cumulative: 163 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,163 @@\n+# -*- coding: utf-8 -*-\n+import os\n+import shutil\n+\n+from testfixtures import LogCapture\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+from w3lib.url import add_or_replace_parameter\n+\n+from scrapy.crawler import CrawlerRunner\n+from scrapy import signals\n+from tests.mockserver import MockServer\n+from tests.spiders import SimpleSpider\n+\n+\n+class MediaDownloadSpider(SimpleSpider):\n+    name = 'mediadownload'\n+\n+    def _process_url(self, url):\n+        return url\n+\n+    def parse(self, response):\n+        self.logger.info(response.headers)\n+        self.logger.info(response.text)\n+        item = {\n+            'images': [],\n+            'image_urls': [\n+                self._process_url(response.urljoin(href))\n+                    for href in response.xpath('''\n+                        //table[thead/tr/th=\"Filename\"]\n+                            /tbody//a/@href\n+                        ''').extract()],\n+        }\n+        yield item\n+\n+\n+class BrokenLinksMediaDownloadSpider(MediaDownloadSpider):\n+    name = 'brokenmedia'\n+\n+    def _process_url(self, url):\n+        return url + '.foo'\n+\n+\n+class RedirectedMediaDownloadSpider(MediaDownloadSpider):\n+    name = 'redirectedmedia'\n+\n+    def _process_url(self, url):\n+        return add_or_replace_parameter(\n+                    'http://localhost:8998/redirect-to',\n+                    'goto', url)\n+\n+\n+class MediaDownloadCrawlTestCase(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+\n+        # prepare a directory for storing files\n+        self.tmpmediastore = self.mktemp()\n+        os.mkdir(self.tmpmediastore)\n+        self.settings = {\n+            'ITEM_PIPELINES': {'scrapy.pipelines.images.ImagesPipeline': 1},\n+            'IMAGES_STORE': self.tmpmediastore,\n+        }\n+        self.runner = CrawlerRunner(self.settings)\n+        self.items = []\n+        # these are the checksums for images in test_site/files/images\n+        # - scrapy.png\n+        # - python-powered-h-50x65.png\n+        # - python-logo-master-v3-TM-flattened.png\n+        self.expected_checksums = set([\n+            'a7020c30837f971084834e603625af58',\n+            'acac52d42b63cf2c3b05832641f3a53c',\n+            '195672ac5888feb400fbf7b352553afe'])\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpmediastore)\n+        self.items = []\n+        self.mockserver.__exit__(None, None, None)\n+\n+    def _on_item_scraped(self, item):\n+        self.items.append(item)\n+\n+    def _create_crawler(self, spider_class):\n+        crawler = self.runner.create_crawler(spider_class)\n+        crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n+        return crawler\n+\n+    def _assert_files_downloaded(self, items, logs):\n+        self.assertEqual(len(items), 1)\n+        self.assertIn('images', items[0])\n+\n+        # check that logs show the expected number of successful file downloads\n+        file_dl_success = 'File (downloaded): Downloaded file from'\n+        self.assertEqual(logs.count(file_dl_success), 3)\n+\n+        # check that the images checksums are what we know they should be\n+        checksums = set(\n+            i['checksum']\n+                for item in items\n+                    for i in item['images'])\n+        self.assertEqual(checksums, self.expected_checksums)\n+\n+        # check that the image files where actually written to the media store\n+        for item in items:\n+            for i in item['images']:\n+                self.assertTrue(\n+                    os.path.exists(\n+                        os.path.join(self.tmpmediastore, i['path'])))\n+\n+    def _assert_files_download_failure(self, crawler, items, code, logs):\n+\n+        # check that the item does NOT have the \"images\" field populated\n+        self.assertEqual(len(items), 1)\n+        self.assertIn('images', items[0])\n+        self.assertFalse(items[0]['images'])\n+\n+        # check that there was 1 successful fetch and 3 other responses with non-200 code\n+        self.assertEqual(crawler.stats.get_value('downloader/request_method_count/GET'), 4)\n+        self.assertEqual(crawler.stats.get_value('downloader/response_count'), 4)\n+        self.assertEqual(crawler.stats.get_value('downloader/response_status_count/200'), 1)\n+        self.assertEqual(crawler.stats.get_value('downloader/response_status_count/%d' % code), 3)\n+\n+        # check that logs do show the failure on the file downloads\n+        file_dl_failure = 'File (code: %d): Error downloading file from' % code\n+        self.assertEqual(logs.count(file_dl_failure), 3)\n+\n+        # check that no files were written to the media store\n+        self.assertEqual(os.listdir(self.tmpmediastore), [])\n+\n+    @defer.inlineCallbacks\n+    def test_download_media(self):\n+        crawler = self._create_crawler(MediaDownloadSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\"http://localhost:8998/files/images/\")\n+        self._assert_files_downloaded(self.items, str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_download_media_wrong_urls(self):\n+        crawler = self._create_crawler(BrokenLinksMediaDownloadSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\"http://localhost:8998/files/images/\")\n+        self._assert_files_download_failure(crawler, self.items, 404, str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_download_media_redirected_default_failure(self):\n+        crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\"http://localhost:8998/files/images/\")\n+        self._assert_files_download_failure(crawler, self.items, 302, str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_download_media_redirected_allowed(self):\n+        settings = dict(self.settings)\n+        settings.update({'MEDIA_ALLOW_REDIRECTS': True})\n+        self.runner = CrawlerRunner(settings)\n+\n+        crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\"http://localhost:8998/files/images/\")\n+        self._assert_files_downloaded(self.items, str(log))\n+        self.assertEqual(crawler.stats.get_value('downloader/response_status_count/302'), 3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#810658bcc5b1897d57c0882a0f7ab4a0d264a778", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 41 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,6 +63,9 @@ class RetryMiddleware(object):\n     def _retry(self, request, reason, spider):\n         retries = request.meta.get('retry_times', 0) + 1\n \n+        if 'max_retry_times' in request.meta:\n+            self.max_retry_times = request.meta['max_retry_times']\n+\n         stats = spider.crawler.stats\n         if retries <= self.max_retry_times:\n             logger.debug(\"Retrying %(request)s (failed %(retries)d times): %(reason)s\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#871134ee22653f7f074fbfb8f3c393ba3555a6d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 16 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 71 | Churn Cumulative: 234 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,8 +23,8 @@ class MediaDownloadSpider(SimpleSpider):\n         self.logger.info(response.headers)\n         self.logger.info(response.text)\n         item = {\n-            'images': [],\n-            'image_urls': [\n+            self.media_key: [],\n+            self.media_urls_key: [\n                 self._process_url(response.urljoin(href))\n                     for href in response.xpath('''\n                         //table[thead/tr/th=\"Filename\"]\n@@ -50,7 +50,15 @@ class RedirectedMediaDownloadSpider(MediaDownloadSpider):\n                     'goto', url)\n \n \n-class MediaDownloadCrawlTestCase(TestCase):\n+class FileDownloadCrawlTestCase(TestCase):\n+    pipeline_class = 'scrapy.pipelines.files.FilesPipeline'\n+    store_setting_key = 'FILES_STORE'\n+    media_key = 'files'\n+    media_urls_key = 'file_urls'\n+    expected_checksums = set([\n+        '5547178b89448faf0015a13f904c936e',\n+        'c2281c83670e31d8aaab7cb642b824db',\n+        'ed3f6538dc15d4d9179dae57319edc5f'])\n \n     def setUp(self):\n         self.mockserver = MockServer()\n@@ -60,19 +68,11 @@ class MediaDownloadCrawlTestCase(TestCase):\n         self.tmpmediastore = self.mktemp()\n         os.mkdir(self.tmpmediastore)\n         self.settings = {\n-            'ITEM_PIPELINES': {'scrapy.pipelines.images.ImagesPipeline': 1},\n-            'IMAGES_STORE': self.tmpmediastore,\n+            'ITEM_PIPELINES': {self.pipeline_class: 1},\n+            self.store_setting_key: self.tmpmediastore,\n         }\n         self.runner = CrawlerRunner(self.settings)\n         self.items = []\n-        # these are the checksums for images in test_site/files/images\n-        # - scrapy.png\n-        # - python-powered-h-50x65.png\n-        # - python-logo-master-v3-TM-flattened.png\n-        self.expected_checksums = set([\n-            'a7020c30837f971084834e603625af58',\n-            'acac52d42b63cf2c3b05832641f3a53c',\n-            '195672ac5888feb400fbf7b352553afe'])\n \n     def tearDown(self):\n         shutil.rmtree(self.tmpmediastore)\n@@ -82,39 +82,40 @@ class MediaDownloadCrawlTestCase(TestCase):\n     def _on_item_scraped(self, item):\n         self.items.append(item)\n \n-    def _create_crawler(self, spider_class):\n-        crawler = self.runner.create_crawler(spider_class)\n+    def _create_crawler(self, spider_class, **kwargs):\n+        crawler = self.runner.create_crawler(spider_class, **kwargs)\n         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n         return crawler\n \n     def _assert_files_downloaded(self, items, logs):\n         self.assertEqual(len(items), 1)\n-        self.assertIn('images', items[0])\n+        self.assertIn(self.media_key, items[0])\n \n         # check that logs show the expected number of successful file downloads\n         file_dl_success = 'File (downloaded): Downloaded file from'\n         self.assertEqual(logs.count(file_dl_success), 3)\n \n-        # check that the images checksums are what we know they should be\n+        # check that the images/files checksums are what we know they should be\n+        if self.expected_checksums is not None:\n             checksums = set(\n                 i['checksum']\n                     for item in items\n-                    for i in item['images'])\n+                        for i in item[self.media_key])\n             self.assertEqual(checksums, self.expected_checksums)\n \n         # check that the image files where actually written to the media store\n         for item in items:\n-            for i in item['images']:\n+            for i in item[self.media_key]:\n                 self.assertTrue(\n                     os.path.exists(\n                         os.path.join(self.tmpmediastore, i['path'])))\n \n     def _assert_files_download_failure(self, crawler, items, code, logs):\n \n-        # check that the item does NOT have the \"images\" field populated\n+        # check that the item does NOT have the \"images/files\" field populated\n         self.assertEqual(len(items), 1)\n-        self.assertIn('images', items[0])\n-        self.assertFalse(items[0]['images'])\n+        self.assertIn(self.media_key, items[0])\n+        self.assertFalse(items[0][self.media_key])\n \n         # check that there was 1 successful fetch and 3 other responses with non-200 code\n         self.assertEqual(crawler.stats.get_value('downloader/request_method_count/GET'), 4)\n@@ -133,21 +134,27 @@ class MediaDownloadCrawlTestCase(TestCase):\n     def test_download_media(self):\n         crawler = self._create_crawler(MediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(\"http://localhost:8998/files/images/\")\n+            yield crawler.crawl(\"http://localhost:8998/files/images/\",\n+                media_key=self.media_key,\n+                media_urls_key=self.media_urls_key)\n         self._assert_files_downloaded(self.items, str(log))\n \n     @defer.inlineCallbacks\n     def test_download_media_wrong_urls(self):\n         crawler = self._create_crawler(BrokenLinksMediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(\"http://localhost:8998/files/images/\")\n+            yield crawler.crawl(\"http://localhost:8998/files/images/\",\n+                media_key=self.media_key,\n+                media_urls_key=self.media_urls_key)\n         self._assert_files_download_failure(crawler, self.items, 404, str(log))\n \n     @defer.inlineCallbacks\n     def test_download_media_redirected_default_failure(self):\n         crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(\"http://localhost:8998/files/images/\")\n+            yield crawler.crawl(\"http://localhost:8998/files/images/\",\n+                media_key=self.media_key,\n+                media_urls_key=self.media_urls_key)\n         self._assert_files_download_failure(crawler, self.items, 302, str(log))\n \n     @defer.inlineCallbacks\n@@ -158,6 +165,18 @@ class MediaDownloadCrawlTestCase(TestCase):\n \n         crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(\"http://localhost:8998/files/images/\")\n+            yield crawler.crawl(\"http://localhost:8998/files/images/\",\n+                media_key=self.media_key,\n+                media_urls_key=self.media_urls_key)\n         self._assert_files_downloaded(self.items, str(log))\n         self.assertEqual(crawler.stats.get_value('downloader/response_status_count/302'), 3)\n+\n+\n+class ImageDownloadCrawlTestCase(FileDownloadCrawlTestCase):\n+    pipeline_class = 'scrapy.pipelines.images.ImagesPipeline'\n+    store_setting_key = 'IMAGES_STORE'\n+    media_key = 'images'\n+    media_urls_key = 'image_urls'\n+\n+    # somehow checksums for images are different for Python 3.3\n+    expected_checksums = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
