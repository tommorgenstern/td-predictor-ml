{"custom_id": "keras#2d7c1492361efee782786b1cf6ef7181b74a0e85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 136 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 138 | Churn Cumulative: 1072 | Contributors (this commit): 11 | Commits (past 90d): 24 | Contributors (cumulative): 17 | DMM Complexity: 0.7211538461538461\n\nDIFF:\n@@ -81,8 +81,8 @@ def _build_attention_equation(rank, attn_axes):\n         + [source_notation[i] for i in attn_axes]\n     )\n     dot_product_equation = \"%s,%s->%s\" % (\n-        source_notation,\n         target_notation,\n+        source_notation,\n         product_notation,\n     )\n     attn_scores_rank = len(product_notation)\n@@ -522,7 +522,7 @@ class MultiHeadAttention(Layer):\n \n         # Take the dot product between \"query\" and \"key\" to get the raw\n         # attention scores.\n-        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n+        attention_scores = tf.einsum(self._dot_product_equation, query, key)\n \n         attention_scores = self._masked_softmax(\n             attention_scores, attention_mask\n\n@@ -14,11 +14,16 @@\n # ==============================================================================\n \"\"\"Tests for the MultiHeadAttention layer.\"\"\"\n \n+import math\n+import string\n+\n import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n+from keras.layers import activation\n+from keras.layers import regularization\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -392,6 +397,135 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         )\n \n \n+class AttentionWithTranspose(keras.layers.MultiHeadAttention):\n+    def _build_attention_equation(self, rank, attn_axes):\n+        _CHR_IDX = string.ascii_lowercase\n+        target_notation = _CHR_IDX[:rank]\n+        # `batch_dims` includes the head dim.\n+        batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n+        letter_offset = rank\n+        source_notation = \"\"\n+        for i in range(rank):\n+            if i in batch_dims or i == rank - 1:\n+                source_notation += target_notation[i]\n+            else:\n+                source_notation += _CHR_IDX[letter_offset]\n+                letter_offset += 1\n+\n+        product_notation = \"\".join(\n+            [target_notation[i] for i in batch_dims]\n+            + [target_notation[i] for i in attn_axes]\n+            + [source_notation[i] for i in attn_axes]\n+        )\n+        dot_product_equation = \"%s,%s->%s\" % (\n+            source_notation,\n+            target_notation,\n+            product_notation,\n+        )\n+        attn_scores_rank = len(product_notation)\n+        combine_equation = \"%s,%s->%s\" % (\n+            product_notation,\n+            source_notation,\n+            target_notation,\n+        )\n+        return dot_product_equation, combine_equation, attn_scores_rank\n+\n+    def _build_attention(self, rank):\n+        if self._attention_axes is None:\n+            self._attention_axes = tuple(range(1, rank - 2))\n+        else:\n+            self._attention_axes = tuple(self._attention_axes)\n+        (\n+            self._dot_product_equation,\n+            self._combine_equation,\n+            attn_scores_rank,\n+        ) = self._build_attention_equation(rank, attn_axes=self._attention_axes)\n+        norm_axes = tuple(\n+            range(\n+                attn_scores_rank - len(self._attention_axes), attn_scores_rank\n+            )\n+        )\n+        self._softmax = activation.Softmax(axis=norm_axes)\n+        self._dropout_layer = regularization.Dropout(rate=self._dropout)\n+\n+    def _compute_attention(\n+        self, query, key, value, attention_mask=None, training=None\n+    ):\n+        query = tf.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n+        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n+        attention_scores = self._masked_softmax(\n+            attention_scores, attention_mask\n+        )\n+        attention_scores_dropout = self._dropout_layer(\n+            attention_scores, training=training\n+        )\n+        attention_output = tf.einsum(\n+            self._combine_equation, attention_scores_dropout, value\n+        )\n+        return attention_output, attention_scores\n+\n+\n+@test_combinations.run_all_keras_modes\n+class AttentionTransposeTest(test_combinations.TestCase):\n+    def test_transpose(self):\n+        \"\"\"Test that removing transpose (i.e., changing key query multiplication\n+\n+        to query key multiplication) does not change attention outputs and\n+        attention scores.\n+        \"\"\"\n+\n+        input_tensor_1 = tf.random.uniform((32, 4, 8))\n+        input_tensor_2 = tf.random.uniform((32, 4, 8))\n+\n+        # Instantiate layer and call with inputs to build.\n+        orig_layer = AttentionWithTranspose(num_heads=2, key_dim=2)\n+        _ = orig_layer(input_tensor_1, input_tensor_2)\n+        opt_layer = keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n+        _ = opt_layer(input_tensor_1, input_tensor_2)\n+\n+        # Set the weights of the two layers to be the same.\n+        query_dense_weights = np.random.uniform(size=(8, 2, 2))\n+        query_dense_bias = np.random.uniform(size=(2, 2))\n+        key_dense_weights = np.random.uniform(size=(8, 2, 2))\n+        key_dense_bias = np.random.uniform(size=(2, 2))\n+        value_dense_weights = np.random.uniform(size=(8, 2, 2))\n+        value_dense_bias = np.random.uniform(size=(2, 2))\n+        attention_output_dense_weights = np.random.uniform(size=(2, 2, 8))\n+        attention_output_dense_bias = np.random.uniform(size=(8,))\n+\n+        orig_layer._query_dense.set_weights(\n+            [query_dense_weights, query_dense_bias]\n+        )\n+        orig_layer._key_dense.set_weights([key_dense_weights, key_dense_bias])\n+        orig_layer._value_dense.set_weights(\n+            [value_dense_weights, value_dense_bias]\n+        )\n+        orig_layer._output_dense.set_weights(\n+            [attention_output_dense_weights, attention_output_dense_bias]\n+        )\n+\n+        opt_layer._query_dense.set_weights(\n+            [query_dense_weights, query_dense_bias]\n+        )\n+        opt_layer._key_dense.set_weights([key_dense_weights, key_dense_bias])\n+        opt_layer._value_dense.set_weights(\n+            [value_dense_weights, value_dense_bias]\n+        )\n+        opt_layer._output_dense.set_weights(\n+            [attention_output_dense_weights, attention_output_dense_bias]\n+        )\n+\n+        # Calculate two sets of attention outputs and scores and compare.\n+        orig_attn_output, orig_attn_score = orig_layer(\n+            input_tensor_1, input_tensor_2, return_attention_scores=True\n+        )\n+        opt_attn_output, opt_attn_score = opt_layer(\n+            input_tensor_1, input_tensor_2, return_attention_scores=True\n+        )\n+        self.assertAllClose(orig_attn_output, opt_attn_output)\n+        self.assertAllClose(orig_attn_score, opt_attn_score)\n+\n+\n class SubclassAttention(keras.layers.MultiHeadAttention):\n     def _build_attention(self, qkv_rank):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f541a8821227d622f678ff85d01d7035700f9884", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 136 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 138 | Churn Cumulative: 1210 | Contributors (this commit): 11 | Commits (past 90d): 26 | Contributors (cumulative): 17 | DMM Complexity: 0.27884615384615385\n\nDIFF:\n@@ -81,8 +81,8 @@ def _build_attention_equation(rank, attn_axes):\n         + [source_notation[i] for i in attn_axes]\n     )\n     dot_product_equation = \"%s,%s->%s\" % (\n-        target_notation,\n         source_notation,\n+        target_notation,\n         product_notation,\n     )\n     attn_scores_rank = len(product_notation)\n@@ -522,7 +522,7 @@ class MultiHeadAttention(Layer):\n \n         # Take the dot product between \"query\" and \"key\" to get the raw\n         # attention scores.\n-        attention_scores = tf.einsum(self._dot_product_equation, query, key)\n+        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n \n         attention_scores = self._masked_softmax(\n             attention_scores, attention_mask\n\n@@ -14,16 +14,11 @@\n # ==============================================================================\n \"\"\"Tests for the MultiHeadAttention layer.\"\"\"\n \n-import math\n-import string\n-\n import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.layers import activation\n-from keras.layers import regularization\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -397,135 +392,6 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         )\n \n \n-class AttentionWithTranspose(keras.layers.MultiHeadAttention):\n-    def _build_attention_equation(self, rank, attn_axes):\n-        _CHR_IDX = string.ascii_lowercase\n-        target_notation = _CHR_IDX[:rank]\n-        # `batch_dims` includes the head dim.\n-        batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n-        letter_offset = rank\n-        source_notation = \"\"\n-        for i in range(rank):\n-            if i in batch_dims or i == rank - 1:\n-                source_notation += target_notation[i]\n-            else:\n-                source_notation += _CHR_IDX[letter_offset]\n-                letter_offset += 1\n-\n-        product_notation = \"\".join(\n-            [target_notation[i] for i in batch_dims]\n-            + [target_notation[i] for i in attn_axes]\n-            + [source_notation[i] for i in attn_axes]\n-        )\n-        dot_product_equation = \"%s,%s->%s\" % (\n-            source_notation,\n-            target_notation,\n-            product_notation,\n-        )\n-        attn_scores_rank = len(product_notation)\n-        combine_equation = \"%s,%s->%s\" % (\n-            product_notation,\n-            source_notation,\n-            target_notation,\n-        )\n-        return dot_product_equation, combine_equation, attn_scores_rank\n-\n-    def _build_attention(self, rank):\n-        if self._attention_axes is None:\n-            self._attention_axes = tuple(range(1, rank - 2))\n-        else:\n-            self._attention_axes = tuple(self._attention_axes)\n-        (\n-            self._dot_product_equation,\n-            self._combine_equation,\n-            attn_scores_rank,\n-        ) = self._build_attention_equation(rank, attn_axes=self._attention_axes)\n-        norm_axes = tuple(\n-            range(\n-                attn_scores_rank - len(self._attention_axes), attn_scores_rank\n-            )\n-        )\n-        self._softmax = activation.Softmax(axis=norm_axes)\n-        self._dropout_layer = regularization.Dropout(rate=self._dropout)\n-\n-    def _compute_attention(\n-        self, query, key, value, attention_mask=None, training=None\n-    ):\n-        query = tf.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n-        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n-        attention_scores = self._masked_softmax(\n-            attention_scores, attention_mask\n-        )\n-        attention_scores_dropout = self._dropout_layer(\n-            attention_scores, training=training\n-        )\n-        attention_output = tf.einsum(\n-            self._combine_equation, attention_scores_dropout, value\n-        )\n-        return attention_output, attention_scores\n-\n-\n-@test_combinations.run_all_keras_modes\n-class AttentionTransposeTest(test_combinations.TestCase):\n-    def test_transpose(self):\n-        \"\"\"Test that removing transpose (i.e., changing key query multiplication\n-\n-        to query key multiplication) does not change attention outputs and\n-        attention scores.\n-        \"\"\"\n-\n-        input_tensor_1 = tf.random.uniform((32, 4, 8))\n-        input_tensor_2 = tf.random.uniform((32, 4, 8))\n-\n-        # Instantiate layer and call with inputs to build.\n-        orig_layer = AttentionWithTranspose(num_heads=2, key_dim=2)\n-        _ = orig_layer(input_tensor_1, input_tensor_2)\n-        opt_layer = keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n-        _ = opt_layer(input_tensor_1, input_tensor_2)\n-\n-        # Set the weights of the two layers to be the same.\n-        query_dense_weights = np.random.uniform(size=(8, 2, 2))\n-        query_dense_bias = np.random.uniform(size=(2, 2))\n-        key_dense_weights = np.random.uniform(size=(8, 2, 2))\n-        key_dense_bias = np.random.uniform(size=(2, 2))\n-        value_dense_weights = np.random.uniform(size=(8, 2, 2))\n-        value_dense_bias = np.random.uniform(size=(2, 2))\n-        attention_output_dense_weights = np.random.uniform(size=(2, 2, 8))\n-        attention_output_dense_bias = np.random.uniform(size=(8,))\n-\n-        orig_layer._query_dense.set_weights(\n-            [query_dense_weights, query_dense_bias]\n-        )\n-        orig_layer._key_dense.set_weights([key_dense_weights, key_dense_bias])\n-        orig_layer._value_dense.set_weights(\n-            [value_dense_weights, value_dense_bias]\n-        )\n-        orig_layer._output_dense.set_weights(\n-            [attention_output_dense_weights, attention_output_dense_bias]\n-        )\n-\n-        opt_layer._query_dense.set_weights(\n-            [query_dense_weights, query_dense_bias]\n-        )\n-        opt_layer._key_dense.set_weights([key_dense_weights, key_dense_bias])\n-        opt_layer._value_dense.set_weights(\n-            [value_dense_weights, value_dense_bias]\n-        )\n-        opt_layer._output_dense.set_weights(\n-            [attention_output_dense_weights, attention_output_dense_bias]\n-        )\n-\n-        # Calculate two sets of attention outputs and scores and compare.\n-        orig_attn_output, orig_attn_score = orig_layer(\n-            input_tensor_1, input_tensor_2, return_attention_scores=True\n-        )\n-        opt_attn_output, opt_attn_score = opt_layer(\n-            input_tensor_1, input_tensor_2, return_attention_scores=True\n-        )\n-        self.assertAllClose(orig_attn_output, opt_attn_output)\n-        self.assertAllClose(orig_attn_score, opt_attn_score)\n-\n-\n class SubclassAttention(keras.layers.MultiHeadAttention):\n     def _build_attention(self, qkv_rank):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c62468522af20f9a4c97a98d1d6abc4ac35fa2d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 19 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 50 | Churn Cumulative: 4362 | Contributors (this commit): 13 | Commits (past 90d): 46 | Contributors (cumulative): 24 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -747,7 +747,8 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 for grad, _ in grads_and_vars:\n                     outer_self.assertIsInstance(grad, tf.Tensor)\n                 return super().apply_gradients(\n-                    grads_and_vars, skip_gradients_aggregation\n+                    grads_and_vars,\n+                    skip_gradients_aggregation=skip_gradients_aggregation,\n                 )\n \n         with create_mirrored_strategy().scope() as strategy:\n\n@@ -480,14 +480,16 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         grads_and_vars = self.compute_gradients(loss, var_list, tape)\n         self.apply_gradients(grads_and_vars)\n \n-    def apply_gradients(self, grads_and_vars):\n+    def apply_gradients(self, grads_and_vars, name=None):\n         \"\"\"Apply gradients to variables.\n \n         Args:\n-          grads_and_vars: List of (gradient, variable) pairs.\n+          grads_and_vars: List of `(gradient, variable)` pairs.\n+          name: string, defaults to None. The name of the namescope to\n+            use when creating variables. If None, `self.name` will be used.\n \n         Returns:\n-          None\n+          A `tf.Variable`, representing the current iteration.\n \n         Raises:\n           TypeError: If `grads_and_vars` is malformed.\n@@ -514,7 +516,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n             # `apply_gradients` is a no-op.\n             return\n         grads, trainable_variables = zip(*grads_and_vars)\n-        scope_name = self.name or \"optimizer\"\n+        scope_name = name or self.name or \"optimizer\"\n         with tf.name_scope(scope_name):\n             with tf.init_scope():\n                 # Lift variable creation to init scope to avoid environment\n@@ -523,11 +525,13 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         grads = self._clip_gradients(grads)\n         grads = self._deduplicate_sparse_grad(grads)\n         grads_and_vars = list(zip(grads, trainable_variables))\n-        self._internal_apply_gradients(grads_and_vars)\n+        iteration = self._internal_apply_gradients(grads_and_vars)\n \n+        # Apply variable constraints after applying gradients.\n         for variable in trainable_variables:\n             if variable.constraint is not None:\n                 variable.assign(variable.constraint(variable))\n+        return iteration\n \n     def _internal_apply_gradients(self, grads_and_vars):\n         \"\"\"Helper function of apply gradients.\n@@ -543,8 +547,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         else:\n             for grad, var in grads_and_vars:\n                 self._update_step(grad, var)\n-\n-        self.iterations.assign_add(1)\n+        return self.iterations.assign_add(1)\n \n     def _update_model_variables_moving_average(self, var_list):\n         \"\"\"Update the stored moving average using the latest value.\"\"\"\n@@ -923,17 +926,21 @@ class Optimizer(_BaseOptimizer):\n         \"\"\"\n         return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n \n-    def apply_gradients(self, grads_and_vars, skip_gradients_aggregation=False):\n+    def apply_gradients(\n+        self, grads_and_vars, name=None, skip_gradients_aggregation=False\n+    ):\n         \"\"\"Apply gradients to variables.\n \n         Args:\n-          grads_and_vars: List of (gradient, variable) pairs.\n+          grads_and_vars: List of `(gradient, variable)` pairs.\n+          name: string, defaults to None. The name of the namescope to\n+            use when creating variables. If None, `self.name` will be used.\n           skip_gradients_aggregation: If true, gradients aggregation will not be\n             performed inside optimizer. Usually this arg is set to True when you\n             write custom code aggregating gradients outside the optimizer.\n \n         Returns:\n-          None\n+          A `tf.Variable`, representing the current iteration.\n \n         Raises:\n           TypeError: If `grads_and_vars` is malformed.\n@@ -941,10 +948,10 @@ class Optimizer(_BaseOptimizer):\n         \"\"\"\n         if not skip_gradients_aggregation:\n             grads_and_vars = self.aggregate_gradients(grads_and_vars)\n-        super().apply_gradients(grads_and_vars)\n+        return super().apply_gradients(grads_and_vars, name=name)\n \n     def _internal_apply_gradients(self, grads_and_vars):\n-        tf.__internal__.distribute.interim.maybe_merge_call(\n+        return tf.__internal__.distribute.interim.maybe_merge_call(\n             self._distributed_apply_gradients_fn,\n             self._distribution_strategy,\n             grads_and_vars,\n@@ -997,7 +1004,6 @@ class Optimizer(_BaseOptimizer):\n             distribution.extended.update(\n                 var, apply_grad_to_update_var, args=(grad,), group=False\n             )\n-        self.iterations.assign_add(1)\n \n         if self.use_ema:\n             _, var_list = zip(*grads_and_vars)\n@@ -1006,8 +1012,8 @@ class Optimizer(_BaseOptimizer):\n                 # Only when self.ema_overwrite_frequency is not None, we\n                 # overwrite the model variables.\n                 should_overwrite_model_vars = (\n-                    self.iterations % self.ema_overwrite_frequency == 0\n-                )\n+                    self.iterations + 1\n+                ) % self.ema_overwrite_frequency == 0\n                 tf.cond(\n                     tf.cast(should_overwrite_model_vars, tf.bool),\n                     true_fn=lambda: self._overwrite_model_variables_with_average_value(  # noqa: E501\n@@ -1015,6 +1021,7 @@ class Optimizer(_BaseOptimizer):\n                     ),\n                     false_fn=lambda: None,\n                 )\n+        return self.iterations.assign_add(1)\n \n \n class RestoredOptimizer(Optimizer):\n\n@@ -231,7 +231,8 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertEqual(optimizer.iterations, 2)\n         var_list = [tf.Variable(2.0), tf.Variable(2.0)]\n         grads = tf.convert_to_tensor([1.0, 1.0])\n-        optimizer.apply_gradients(zip(grads, var_list))\n+        iterations = optimizer.apply_gradients(zip(grads, var_list))\n+        self.assertEqual(iterations, 3)\n         self.assertEqual(optimizer.iterations, 3)\n         with self.assertRaisesRegex(RuntimeError, \"Cannot set*\"):\n             optimizer.iterations = 2\n@@ -250,6 +251,13 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         optimizer = adam_new.Adam(jit_compile=False)\n         optimizer.apply_gradients(zip([], []))\n \n+    def testApplyGradientsNameArg(self):\n+        optimizer = adam_new.Adam(jit_compile=False)\n+        var_list = [tf.Variable(2.0), tf.Variable(2.0)]\n+        grads = tf.convert_to_tensor([1.0, 1.0])\n+        optimizer.apply_gradients(zip(grads, var_list), name=\"dummy\")\n+        self.assertIn(\"dummy\", optimizer._velocities[0].name)\n+\n     def testPassingMissingWDError(self):\n         with self.assertRaises(ValueError):\n             _ = adamw_new.AdamW(0.01, weight_decay=None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2808585a99bf1a73fd61dad2795f17de935e1a17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 6193 | Contributors (this commit): 11 | Commits (past 90d): 44 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3031,7 +3031,6 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n             )\n             model.compile(optimizer=\"adam\", loss=\"mse\")\n             model.build([None, 1])  # create weights.\n-            self.assertEmpty(model.optimizer.variables())\n             return model\n \n         model = create_model()\n\n@@ -83,9 +83,9 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n                 f\"global_clipnorm={self.global_clipnorm}.\"\n             )\n \n+        self._variables = []\n         self._create_iteration_variable()\n         self._process_kwargs(kwargs)\n-        self._variables = []\n \n     def _create_iteration_variable(self):\n         \"\"\"Create the iterations counter variable.\"\"\"\n@@ -95,6 +95,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n             self._iterations = tf.Variable(\n                 0, name=\"iteration\", dtype=tf.int64, trainable=False\n             )\n+        self._variables.append(self._iterations)\n \n     def _process_kwargs(self, kwargs):\n         kwargs.pop(\"is_legacy_optimizer\", None)\n@@ -330,6 +331,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         self.learning_rate = learning_rate\n \n     def _build_learning_rate(self, learning_rate):\n+        with tf.init_scope():\n             if isinstance(\n                 learning_rate, learning_rate_schedule.LearningRateSchedule\n             ):\n@@ -341,6 +343,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n                     trainable=False,\n                 )\n                 return learning_rate\n+\n             return tf.Variable(\n                 learning_rate,\n                 name=\"learning_rate\",\n\n@@ -178,10 +178,11 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         optimizer.apply_gradients(zip([grads], [x]))\n         optimizer_variables = optimizer.variables()\n         all_names = [var._shared_name for var in optimizer_variables]\n-        self.assertLen(optimizer_variables, 2)\n+        self.assertLen(optimizer_variables, 3)\n         self.assertCountEqual(\n             all_names,\n             [\n+                \"iteration\",\n                 \"Adam/m/Variable\",\n                 \"Adam/v/Variable\",\n             ],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#59bc90a4a8bd5f4dce2328487d977d07269d84c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/2 | Churn Δ: 37 | Churn Cumulative: 3447 | Contributors (this commit): 14 | Commits (past 90d): 18 | Contributors (cumulative): 25 | DMM Complexity: 0.8620689655172413\n\nDIFF:\n@@ -1399,6 +1399,32 @@ class TestWholeModelSaving(test_combinations.TestCase):\n         model.save(saved_model_dir)\n         keras.models.load_model(saved_model_dir)\n \n+    @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n+    @test_utils.run_v2_only\n+    def test_save_functional_with_constant_string_input(self):\n+        input1 = keras.Input(shape=[2], dtype=tf.string)\n+        input2 = tf.constant([[\"単\", \"に\"]])\n+        outputs = keras.layers.Concatenate()([input1, input2])\n+        model = keras.Model(input1, outputs)\n+        saved_model_dir = self._save_model_dir()\n+        model.save(saved_model_dir)\n+        loaded_model = keras.models.load_model(saved_model_dir)\n+        x = tf.constant([[\"a\", \"b\"]])\n+        self.assertAllEqual(model(x), loaded_model(x))\n+\n+    @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n+    @test_utils.run_v2_only\n+    def test_save_functional_with_ragged_constant_string_input(self):\n+        input1 = keras.Input(shape=[1], dtype=tf.string)\n+        input2 = tf.ragged.constant([[\"単\", \"に\"], [\"単\"]])\n+        outputs = keras.layers.Concatenate(axis=0)([input1, input2])\n+        model = keras.Model(input1, outputs)\n+        saved_model_dir = self._save_model_dir()\n+        model.save(saved_model_dir)\n+        loaded_model = keras.models.load_model(saved_model_dir)\n+        x = tf.constant([[\"a\"]])\n+        self.assertAllEqual(model(x), loaded_model(x))\n+\n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     @test_utils.run_v2_only\n     def test_save_inputs_spec_with_composite_tensor_names(self):\n\n@@ -136,6 +136,8 @@ def _decode_helper(\n                 )\n             except ValueError:\n                 pass\n+        elif obj[\"class_name\"] == \"__bytes__\":\n+            return obj[\"value\"].encode(\"utf-8\")\n     return obj\n \n \n@@ -218,6 +220,9 @@ def get_json_type(obj):\n     if isinstance(obj, enum.Enum):\n         return obj.value\n \n+    if isinstance(obj, bytes):\n+        return {\"class_name\": \"__bytes__\", \"value\": obj.decode(\"utf-8\")}\n+\n     raise TypeError(\n         f\"Unable to serialize {obj} to JSON. Unrecognized type {type(obj)}.\"\n     )\n\n@@ -96,6 +96,12 @@ class JsonUtilsTest(test_combinations.TestCase):\n         loaded = json_utils.decode(string)\n         self.assertAllEqual(loaded, x)\n \n+    def test_encode_decode_bytes(self):\n+        b_string = b\"abc\"\n+        json_string = json_utils.Encoder().encode(b_string)\n+        loaded = json_utils.decode(json_string)\n+        self.assertAllEqual(b_string, loaded)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1968c6e6a35e29c291ea6d0dff9d005c8d5b9ae1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 1315 | Contributors (this commit): 8 | Commits (past 90d): 38 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -930,7 +930,11 @@ class Optimizer(_BaseOptimizer):\n         return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n \n     def apply_gradients(\n-        self, grads_and_vars, name=None, skip_gradients_aggregation=False\n+        self,\n+        grads_and_vars,\n+        name=None,\n+        skip_gradients_aggregation=False,\n+        **kwargs,\n     ):\n         \"\"\"Apply gradients to variables.\n \n@@ -941,6 +945,7 @@ class Optimizer(_BaseOptimizer):\n           skip_gradients_aggregation: If true, gradients aggregation will not be\n             performed inside optimizer. Usually this arg is set to True when you\n             write custom code aggregating gradients outside the optimizer.\n+          **kwargs: keyword arguments only used for backward compatibility.\n \n         Returns:\n           A `tf.Variable`, representing the current iteration.\n@@ -949,7 +954,13 @@ class Optimizer(_BaseOptimizer):\n           TypeError: If `grads_and_vars` is malformed.\n           RuntimeError: If called in a cross-replica context.\n         \"\"\"\n-        if not skip_gradients_aggregation:\n+        # `experimental_aggregate_gradients` is an arg in `apply_gradients` of\n+        # v2 optimizer -- the reverse of `skip_gradients_aggregation`.\n+        # We read it from kwargs for backward compatibility.\n+        experimental_aggregate_gradients = kwargs.pop(\n+            \"experimental_aggregate_gradients\", True\n+        )\n+        if not skip_gradients_aggregation and experimental_aggregate_gradients:\n             grads_and_vars = self.aggregate_gradients(grads_and_vars)\n         return super().apply_gradients(grads_and_vars, name=name)\n \n\n@@ -610,7 +610,8 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n                         )(labels, output_1)\n                     grads_1 = tape.gradient(loss_1, model_1.trainable_variables)\n                     optimizer_1.apply_gradients(\n-                        zip(grads_1, model_1.trainable_variables)\n+                        zip(grads_1, model_1.trainable_variables),\n+                        skip_gradients_aggregation=False,\n                     )\n \n                     with tf.GradientTape() as tape:\n@@ -620,7 +621,8 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n                         )(labels, output_2)\n                     grads_2 = tape.gradient(loss_2, model_2.trainable_variables)\n                     optimizer_2.apply_gradients(\n-                        zip(grads_2, model_2.trainable_variables)\n+                        zip(grads_2, model_2.trainable_variables),\n+                        experimental_aggregate_gradients=True,\n                     )\n \n                 strategy.run(replica_fn, args=(next(iter(ds)),))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e6c11160ecd8e4e2c0bd8a10332c8bd494880aa8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 4 | Churn Cumulative: 19786 | Contributors (this commit): 134 | Commits (past 90d): 32 | Contributors (cumulative): 134 | DMM Complexity: None\n\nDIFF:\n@@ -653,8 +653,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               strings 'accuracy' or 'acc', we convert this to one of\n               `tf.keras.metrics.BinaryAccuracy`,\n               `tf.keras.metrics.CategoricalAccuracy`,\n-              `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n-              function used and the model output shape. We do a similar\n+              `tf.keras.metrics.SparseCategoricalAccuracy` based on the shapes\n+              of the targets and of the model output. We do a similar\n               conversion for the strings 'crossentropy' and 'ce' as well.\n               The metrics passed here are evaluated without sample weighting; if\n               you would like sample weighting to apply, you can specify your\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb3596ed3b223e3ac60fcf3fb7a8dcad3e190bce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 30 | Churn Cumulative: 1345 | Contributors (this commit): 8 | Commits (past 90d): 40 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,6 +118,16 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n                     \" for `optimizer_experimental.Optimizer`.\"\n                 )\n \n+    def _create_or_restore_slot_variable(self, **kwargs):\n+        raise ValueError(\n+            \"You are trying to restore a checkpoint from a legacy Keras \"\n+            \"optimizer into a v2.11+ Optimizer, which can cause \"\n+            \"errors. Please update the optimizer referenced in your code \"\n+            \"to be an instance of \"\n+            \"`tf.keras.optimizers.legacy.Optimizer`, e.g.: \"\n+            \"`tf.keras.optimizer.legacy.Adam`.\"\n+        )\n+\n     def _var_key(self, variable):\n         \"\"\"Get a unique identifier of the given variable.\"\"\"\n         # Get the distributed variable if it exists.\n\n@@ -382,6 +382,26 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n             model.optimizer.iterations.numpy(),\n         )\n \n+    def testRestoreOldOptimizerCheckpoint(self):\n+        inputs = keras.layers.Input(shape=(1,))\n+        outputs = keras.layers.Dense(1)(inputs)\n+        model = keras.Model(inputs=inputs, outputs=outputs)\n+        optimizer = adam_old.Adam()\n+        x = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n+        y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n+        model.compile(loss=\"mse\", optimizer=optimizer)\n+        path = os.path.join(self.get_temp_dir(), \"ckpt\")\n+        checkpoint_callback = keras.callbacks.ModelCheckpoint(path)\n+        model.fit(x, y, callbacks=[checkpoint_callback])\n+\n+        new_model = keras.Model(inputs=inputs, outputs=outputs)\n+        new_optimizer = adam_new.Adam()\n+        new_model.compile(loss=\"mse\", optimizer=new_optimizer)\n+        with self.assertRaisesRegex(\n+            ValueError, \"You are trying to restore a checkpoint*\"\n+        ):\n+            new_model.load_weights(path)\n+\n     @parameterized.product(optimizer_fn=OPTIMIZER_FN)\n     def testSaveAndLoadOptimizerWithModel(self, optimizer_fn):\n         inputs = keras.layers.Input(shape=(1,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f61e9f1dc77ac3af9fa16dc3f1e31c8ded23c6e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 30 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 30 | Churn Cumulative: 1375 | Contributors (this commit): 8 | Commits (past 90d): 42 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -118,16 +118,6 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n                     \" for `optimizer_experimental.Optimizer`.\"\n                 )\n \n-    def _create_or_restore_slot_variable(self, **kwargs):\n-        raise ValueError(\n-            \"You are trying to restore a checkpoint from a legacy Keras \"\n-            \"optimizer into a v2.11+ Optimizer, which can cause \"\n-            \"errors. Please update the optimizer referenced in your code \"\n-            \"to be an instance of \"\n-            \"`tf.keras.optimizers.legacy.Optimizer`, e.g.: \"\n-            \"`tf.keras.optimizer.legacy.Adam`.\"\n-        )\n-\n     def _var_key(self, variable):\n         \"\"\"Get a unique identifier of the given variable.\"\"\"\n         # Get the distributed variable if it exists.\n\n@@ -382,26 +382,6 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n             model.optimizer.iterations.numpy(),\n         )\n \n-    def testRestoreOldOptimizerCheckpoint(self):\n-        inputs = keras.layers.Input(shape=(1,))\n-        outputs = keras.layers.Dense(1)(inputs)\n-        model = keras.Model(inputs=inputs, outputs=outputs)\n-        optimizer = adam_old.Adam()\n-        x = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n-        y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n-        model.compile(loss=\"mse\", optimizer=optimizer)\n-        path = os.path.join(self.get_temp_dir(), \"ckpt\")\n-        checkpoint_callback = keras.callbacks.ModelCheckpoint(path)\n-        model.fit(x, y, callbacks=[checkpoint_callback])\n-\n-        new_model = keras.Model(inputs=inputs, outputs=outputs)\n-        new_optimizer = adam_new.Adam()\n-        new_model.compile(loss=\"mse\", optimizer=new_optimizer)\n-        with self.assertRaisesRegex(\n-            ValueError, \"You are trying to restore a checkpoint*\"\n-        ):\n-            new_model.load_weights(path)\n-\n     @parameterized.product(optimizer_fn=OPTIMIZER_FN)\n     def testSaveAndLoadOptimizerWithModel(self, optimizer_fn):\n         inputs = keras.layers.Input(shape=(1,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7dd67a94e22da6e6fc5aca8ae64a5ed58a0c4316", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 11 | Churn Cumulative: 8757 | Contributors (this commit): 24 | Commits (past 90d): 31 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2173,7 +2173,8 @@ class TrainingTest(test_combinations.TestCase):\n \n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     def test_ema_overwrite(self):\n-\n+        if not tf.__internal__.tf2.enabled():\n+            self.skipTest(\"EMA optimizer is only available in TF2.\")\n         model = sequential.Sequential()\n         model.add(input_layer.Input(shape=(4,)))\n         model.add(layers_module.Dense(1, activation=\"relu\"))\n\n@@ -251,10 +251,16 @@ def get(identifier, **kwargs):\n         (\n             Optimizer,\n             base_optimizer_v2.OptimizerV2,\n-            optimizer_experimental.Optimizer,\n         ),\n     ):\n         return identifier\n+    elif isinstance(identifier, optimizer_experimental.Optimizer):\n+        if tf.__internal__.tf2.enabled():\n+            return identifier\n+        else:\n+            # If TF2 is disabled, we convert to the legacy optimizer.\n+            return convert_to_legacy_optimizer(identifier)\n+\n     # Wrap legacy TF optimizer instances\n     elif isinstance(identifier, tf.compat.v1.train.Optimizer):\n         opt = TFOptimizer(identifier)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5e27ac75ca3c76c7a2127032bcc2862b7c862e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 50 | Files Changed: 2 | Hunks: 18 | Methods Changed: 7 | Complexity Δ (Sum/Max): -2/1 | Churn Δ: 80 | Churn Cumulative: 1450 | Contributors (this commit): 6 | Commits (past 90d): 13 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,7 +84,7 @@ class TimeDistributed(Wrapper):\n             layer\n         ) and not getattr(layer, \"stateful\", False)\n \n-    def _get_shape_tuple(self, init_tuple, tensor, start_idx, int_shape=None):\n+    def _get_shape_tuple(self, init_tuple, tensor, start_idx):\n         \"\"\"Finds non-specific dimensions in the static shapes.\n \n         The static shapes are replaced with the corresponding dynamic shapes of\n@@ -95,25 +95,19 @@ class TimeDistributed(Wrapper):\n             as the last part of the output shape\n           start_idx: int, which indicate the first dimension to take from\n             the static shape of the tensor\n-          int_shape: an alternative static shape to take as the last part\n-            of the output shape\n         Returns:\n-          The new int_shape with the first part from init_tuple\n-          and the last part from either `int_shape` (if provided)\n-          or `tensor.shape`, where every `None` is replaced by\n-          the corresponding dimension from `tf.shape(tensor)`.\n+          The new shape with the first part from `init_tuple` and the last part\n+          from or `tensor.shape`, where every `None` is replaced by the\n+          corresponding dimension from `tf.shape(tensor)`.\n         \"\"\"\n         # replace all None in int_shape by backend.shape\n-        if int_shape is None:\n         int_shape = backend.int_shape(tensor)[start_idx:]\n-        if isinstance(int_shape, tf.TensorShape):\n-            int_shape = int_shape.as_list()\n-        if not any(not s for s in int_shape):\n-            return init_tuple + tuple(int_shape)\n+        if not any(s is None for s in int_shape):\n+            return init_tuple + int_shape\n         shape = backend.shape(tensor)\n         int_shape = list(int_shape)\n         for i, s in enumerate(int_shape):\n-            if not s:\n+            if s is None:\n                 int_shape[i] = shape[start_idx + i]\n         return init_tuple + tuple(int_shape)\n \n@@ -251,29 +245,20 @@ class TimeDistributed(Wrapper):\n \n                 y = self.layer(inputs, **kwargs)\n \n-                # Shape: (num_samples, timesteps, ...)\n-                output_shape = self.compute_output_shape(input_shape)\n-\n+                # Reconstruct the output shape by re-splitting the 0th dimension\n+                # back into (num_samples, timesteps, ...)\n+                # We use batch_size when available so that the 0th dimension is\n+                # set in the static shape of the reshaped output\n+                reshape_batch_size = batch_size if batch_size else -1\n                 output_shape = tf.nest.map_structure(\n-                    lambda tensor, int_shape: self._get_shape_tuple(\n-                        (-1, input_length), tensor, 1, int_shape[2:]\n+                    lambda tensor: self._get_shape_tuple(\n+                        (reshape_batch_size, input_length), tensor, 1\n                     ),\n                     y,\n-                    output_shape,\n                 )\n                 y = tf.__internal__.nest.map_structure_up_to(\n                     y, tf.reshape, y, output_shape\n                 )\n-                if not tf.executing_eagerly():\n-                    # Set the static shape for the result since it might be lost\n-                    # during array_ops reshape, eg, some `None` dim in the\n-                    # result could be inferred.\n-                    tf.__internal__.nest.map_structure_up_to(\n-                        y,\n-                        lambda tensor, shape: tensor.set_shape(shape),\n-                        y,\n-                        self.compute_output_shape(input_shape),\n-                    )\n \n         return y\n \n@@ -359,21 +344,9 @@ class TimeDistributed(Wrapper):\n                     lambda x: backend.shape(x)[1], inputs\n                 )\n                 input_length = tf.nest.flatten(input_length)[0]\n-            output_mask_int_shape = backend.int_shape(output_mask)\n-            if output_mask_int_shape is None:\n-                # if the output_mask does not have a static shape,\n-                # its shape must be the same as mask's\n-                if mask is not None:\n-                    output_mask_int_shape = backend.int_shape(mask)\n-                else:\n-                    input_shape = generic_utils.to_list(\n-                        tf.nest.flatten(input_shape)\n-                    )[0]\n-                    output_mask_int_shape = backend.compute_output_shape(\n-                        input_shape\n-                    )[:-1]\n+            reshape_batch_size = batch_size if batch_size else -1\n             output_mask_shape = self._get_shape_tuple(\n-                (-1, input_length), output_mask, 1, output_mask_int_shape[1:]\n+                (reshape_batch_size, input_length), output_mask, 1\n             )\n             output_mask = backend.reshape(output_mask, output_mask_shape)\n         return output_mask\n\n@@ -308,16 +308,23 @@ class TimeDistributedTest(test_combinations.TestCase):\n         td3 = keras.layers.TimeDistributed(NoReshapeLayer())\n         self.assertFalse(td3._always_use_reshape)\n \n-    @test_combinations.generate(\n-        test_combinations.combine(mode=[\"graph\", \"eager\"])\n+    @test_combinations.run_all_keras_modes\n+    @parameterized.named_parameters(\n+        (\"fully_defined\", [3, 2, 4], [3, 2, 8]),\n+        (\"dynamic_batch_size\", [None, 2, 4], [None, 2, 8]),\n+        (\"two_dynamic_dims\", [None, None, 4], [None, None, 8]),\n+        (\"rank_only\", [None, None, None], [None, None, None]),\n     )\n-    def test_TimeDistributed_output_shape_return_types(self):\n+    def test_TimeDistributed_output_shape_return_types(\n+        self, input_shape, expected_output_shape\n+    ):\n         class TestLayer(keras.layers.Layer):\n             def call(self, inputs):\n                 return tf.concat([inputs, inputs], axis=-1)\n \n             def compute_output_shape(self, input_shape):\n                 output_shape = tf.TensorShape(input_shape).as_list()\n+                if output_shape[-1] is not None:\n                     output_shape[-1] = output_shape[-1] * 2\n                 output_shape = tf.TensorShape(output_shape)\n                 return output_shape\n@@ -336,12 +343,12 @@ class TimeDistributedTest(test_combinations.TestCase):\n         test_layers = [TestLayer, TestListLayer, TestTupleLayer]\n         for layer in test_layers:\n             input_layer = keras.layers.TimeDistributed(layer())\n-            inputs = keras.backend.placeholder(shape=(None, 2, 4))\n+            inputs = keras.backend.placeholder(shape=input_shape)\n             output = input_layer(inputs)\n-            self.assertEqual(output.shape.as_list(), [None, 2, 8])\n+            self.assertEqual(output.shape.as_list(), expected_output_shape)\n             self.assertEqual(\n-                input_layer.compute_output_shape([None, 2, 4]).as_list(),\n-                [None, 2, 8],\n+                input_layer.compute_output_shape(input_shape).as_list(),\n+                expected_output_shape,\n             )\n \n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a5f965de22a4c85633d45ce1d287343879f01163", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 750 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -374,7 +374,7 @@ def load_img(\n \n     Usage:\n \n-    ```\n+    ```python\n     image = tf.keras.utils.load_img(image_path)\n     input_arr = tf.keras.utils.img_to_array(image)\n     input_arr = np.array([input_arr])  # Convert single image to a batch.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
