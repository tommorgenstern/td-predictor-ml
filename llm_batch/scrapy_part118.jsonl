{"custom_id": "scrapy#85890a5092e0694a71ea3f532a50703b21cd7cf3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 153 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -100,6 +100,6 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.deferred.callback(self)\n \n     def log(self, msg):\n-        msg += \"project=%r spider=%r job=%r pid=%r egg=%r\" % (self.project, \\\n-            self.spider, self.job, self.pid, self.eggfile)\n+        msg += \"project=%r spider=%r job=%r pid=%r egg=%r log=%r\" % (self.project, \\\n+            self.spider, self.job, self.pid, self.eggfile, self.logfile)\n         log.msg(msg, system=\"Launcher\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c02d6db6a34e3a135b2a00e6c6bebd0507ec4d56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 67 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,9 @@\n from scrapy.utils.misc import load_object\n from .config import Config\n \n-def get_application():\n+def get_application(config=None):\n+    if config is None:\n         config = Config()\n     apppath = config.get('application', 'scrapyd.app.application')\n     appfunc = load_object(apppath)\n-    return appfunc()\n+    return appfunc(config)\n\n@@ -11,9 +11,8 @@ from .environ import Environment\n from .website import Root\n from .config import Config\n \n-def application():\n+def application(config):\n     app = Application(\"Scrapyd\")\n-    config = Config()\n     http_port = config.getint('http_port', 6800)\n \n     poller = QueuePoller(config)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5a46ce47ee9ce1444187f6be19aacd45ab7d79ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 72 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import glob\n from cStringIO import StringIO\n from ConfigParser import SafeConfigParser, NoSectionError, NoOptionError\n \n+from scrapy.utils.conf import closest_scrapy_cfg\n from scrapy.utils.py26 import get_data\n \n __package__ = 'scrapyd' # required for compatibility with python 2.5\n@@ -12,13 +13,15 @@ class Config(object):\n \n     SECTION = 'scrapyd'\n \n-    def __init__(self, values=None):\n+    def __init__(self, values=None, extra_sources=()):\n         if values is None:\n             sources = self._getsources()\n             default_config = get_data(__package__, 'default_scrapyd.conf')\n             self.cp = SafeConfigParser()\n             self.cp.readfp(StringIO(default_config))\n             self.cp.read(sources)\n+            for fp in extra_sources:\n+                self.cp.readfp(fp)\n         else:\n             self.cp = SafeConfigParser(values)\n             self.cp.add_section(self.SECTION)\n@@ -27,6 +30,7 @@ class Config(object):\n         sources = ['/etc/scrapyd/scrapyd.conf', r'c:\\scrapyd\\scrapyd.conf']\n         sources += sorted(glob.glob('/etc/scrapyd/conf.d/*'))\n         sources += ['scrapyd.conf']\n+        sources += closest_scrapy_cfg()\n         return sources\n \n     def _getany(self, method, option, default):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b84591ea9dd4bf1c66649b0a863c3d5312e1ae5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 62 | Churn Cumulative: 62 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,22 @@\n+from __future__ import absolute_import\n+\n+from scrapy.command import ScrapyCommand\n+from scrapy.exceptions import UsageError\n+\n+class Command(ScrapyCommand):\n+\n+    requires_project = True\n+\n+    def short_desc(self):\n+        return \"Start Scrapyd server for this project\"\n+\n+    def long_desc(self):\n+        return \"Start Scrapyd server for this project, which can be referred \" \\\n+            \"from the JSON API with the name 'default'\"\n+\n+    def run(self, args, opts):\n+        try:\n+            from scrapyd.script import execute\n+            execute()\n+        except ImportError:\n+            raise UsageError(\"Scrapyd is not available in this system\")\n\n@@ -0,0 +1,40 @@\n+\"\"\"This module can be used to execute Scrapyd from a Scrapy command\"\"\"\n+\n+import sys\n+import os\n+from cStringIO import StringIO\n+\n+from twisted.python import log\n+from twisted.internet import reactor\n+from twisted.application import app\n+\n+from scrapy.utils.project import project_data_dir\n+\n+from scrapyd import get_application\n+from scrapyd.config import Config\n+\n+def _get_config():\n+    datadir = os.path.join(project_data_dir(), '.scrapy', 'scrapyd')\n+    conf = {\n+        'eggs_dir': os.path.join(datadir, 'eggs'),\n+        'logs_dir': os.path.join(datadir, 'logs'),\n+        'dbs_dir': os.path.join(datadir, 'dbs'),\n+    }\n+    for k in ['eggs_dir', 'logs_dir', 'dbs_dir']: # create dirs\n+        d = conf[k]\n+        if not os.path.exists(d):\n+            os.makedirs(d)\n+    scrapyd_conf = \"\"\"\n+[scrapyd]\n+eggs_dir = %(eggs_dir)s\n+logs_dir = %(logs_dir)s\n+dbs_dir  = %(dbs_dir)s\n+    \"\"\" % conf\n+    return Config(extra_sources=[StringIO(scrapyd_conf)])\n+\n+def execute():\n+    config = _get_config()\n+    log.startLogging(sys.stderr)\n+    application = get_application(config)\n+    app.startApplication(application, False)\n+    reactor.run()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#823fd9822ca3544ba38ba3cd822f9f1221c9fa49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 76 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,9 @@ class Config(object):\n         sources = ['/etc/scrapyd/scrapyd.conf', r'c:\\scrapyd\\scrapyd.conf']\n         sources += sorted(glob.glob('/etc/scrapyd/conf.d/*'))\n         sources += ['scrapyd.conf']\n-        sources += closest_scrapy_cfg()\n+        scrapy_cfg = closest_scrapy_cfg()\n+        if scrapy_cfg:\n+            sources.append(scrapy_cfg)\n         return sources\n \n     def _getany(self, method, option, default):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a3d30c35fe08a28eb93845e69701ed23014e7cbd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 56 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n from twisted.application.service import Application\n from twisted.application.internet import TimerService, TCPServer\n from twisted.web import server\n+from twisted.python import log\n \n from .interfaces import IEggStorage, IPoller, ISpiderScheduler, IEnvironment\n from .launcher import Launcher\n@@ -28,6 +29,7 @@ def application(config):\n     launcher = Launcher(config, app)\n     timer = TimerService(5, poller.poll)\n     webservice = TCPServer(http_port, server.Site(Root(config, app)))\n+    log.msg(\"Scrapyd web console available at http://localhost:%s/\" % http_port)\n \n     launcher.setServiceParent(app)\n     timer.setServiceParent(app)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#831dc818d6c2359afc3879aee29d6c28d48cb542", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 123 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,7 @@ class Root(resource.Resource):\n         self.eggrunner = config.get('egg_runner')\n         logsdir = config.get('logs_dir')\n         self.app = app\n-        self.putChild('', Home())\n+        self.putChild('', Home(self))\n         self.putChild('schedule.json', webservice.Schedule(self))\n         self.putChild('addversion.json', webservice.AddVersion(self))\n         self.putChild('listprojects.json', webservice.ListProjects(self))\n@@ -50,20 +50,38 @@ class Root(resource.Resource):\n \n class Home(resource.Resource):\n \n+    def __init__(self, root):\n+        resource.Resource.__init__(self)\n+        self.root = root\n+\n     def render_GET(self, txrequest):\n+        vars = {\n+            'projects': ', '.join(self.root.scheduler.list_projects()),\n+        }\n         return \"\"\"\n <html>\n <head><title>Scrapyd</title></head>\n <body>\n <h1>Scrapyd</h1>\n+<p>Available projects: <b>%(projects)s</b></p>\n <ul>\n <li><a href=\"/procmon\">Process monitor</a></li>\n <li><a href=\"/logs/\">Logs</li>\n <li><a href=\"http://doc.scrapy.org/topics/scrapyd.html\">Documentation</a></li>\n </ul>\n+\n+<h2>How to schedule a spider?</h2>\n+\n+<p>To schedule a spider you need to use the API (this web UI is only for\n+monitoring)</p>\n+\n+<p>Example using <a href=\"http://curl.haxx.se/\">curl</a>:</p>\n+<p><code>curl http://localhost:6800/schedule.json -d project=default -d spider=somespider</code></p>\n+\n+<p>For more information about the API, see the <a href=\"http://doc.scrapy.org/topics/scrapyd.html\">Scrapyd documentation</a></p>\n </body>\n </html>\n-\"\"\"\n+\"\"\" % vars\n \n \n class ProcessMonitor(resource.Resource):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a1b69c93f45fdda8adf1781947b7a69adf29cc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 141 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ class Command(runserver.Command):\n         return \"[options] <list|clear|count|add spider1 ..>\"\n \n     def short_desc(self):\n-        return \"Control the spider queue\"\n+        return \"Deprecated command. See Scrapyd documentation.\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n@@ -7,7 +7,7 @@ class Command(ScrapyCommand):\n     default_settings = {'KEEP_ALIVE': True}\n \n     def short_desc(self):\n-        return \"Start Scrapy in server mode\"\n+        return \"Deprecated command. Use 'server' command instead\"\n \n     def run(self, args, opts):\n         self.crawler.start()\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02ccca01ebd4d407fd39ad1053395346fe669baa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 347 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -266,6 +266,11 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\", keep_fragments=True),\n                                           u\"http://user:pass@www.example.com/do?a=1#frag\")\n \n+        # dont convert safe characters to percent encoding representation\n+        self.assertEqual(canonicalize_url(\n+            \"http://www.simplybedrooms.com/White-Bedroom-Furniture/Bedroom-Mirror:-Josephine-Cheval-Mirror.html\"),\n+            \"http://www.simplybedrooms.com/White-Bedroom-Furniture/Bedroom-Mirror:-Josephine-Cheval-Mirror.html\")\n+\n         # urllib.quote uses a mapping cache of encoded characters. when parsing\n         # an already percent-encoded url, it will fail if that url was not\n         # percent-encoded as utf-8, that's why canonicalize_url must always\n\n@@ -152,7 +152,7 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n     keyvals = cgi.parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urllib.urlencode(keyvals)\n-    path = urllib.quote(urllib.unquote(path))\n+    path = safe_url_string(urllib.unquote(path))\n     fragment = '' if not keep_fragments else fragment\n     return urlparse.urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b19ff21acd4f93f0fa6cc3c53b81987ce1a7f8c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 60 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n from zope.interface import implements\n-from twisted.internet.defer import DeferredQueue\n+from twisted.internet.defer import DeferredQueue, inlineCallbacks, maybeDeferred, returnValue\n \n from .utils import get_spider_queues\n from .interfaces import IPoller\n@@ -13,13 +13,15 @@ class QueuePoller(object):\n         self.update_projects()\n         self.dq = DeferredQueue(size=1)\n \n+    @inlineCallbacks\n     def poll(self):\n         if self.dq.pending:\n             return\n         for p, q in self.queues.iteritems():\n-            if q.count():\n-                msg = q.pop()\n-                return self.dq.put(self._message(msg, p))\n+            c = yield maybeDeferred(q.count)\n+            if c:\n+                msg = yield maybeDeferred(q.pop)\n+                returnValue(self.dq.put(self._message(msg, p)))\n \n     def next(self):\n         return self.dq.get()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f2cea4775f4567d13cd68647c501e452b9365a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 148 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,10 @@ class Command(runserver.Command):\n             raise UsageError()\n         cmd = args[0]\n \n+        import warnings\n+        warnings.warn(\"Scrapy queue command is deprecated - use Scrapyd instead.\", \\\n+            DeprecationWarning)\n+\n         q = self.crawler.queue._queue\n \n         if cmd == 'add' and len(args) < 2:\n\n@@ -10,4 +10,7 @@ class Command(ScrapyCommand):\n         return \"Deprecated command. Use 'server' command instead\"\n \n     def run(self, args, opts):\n+        import warnings\n+        warnings.warn(\"Scrapy queue command is deprecated - use 'server' command instead.\", \\\n+            DeprecationWarning)\n         self.crawler.start()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#119fd20e91d21d6ea2ee7068d6b2fd3bff526e7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 19 | Churn Cumulative: 29 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,29 @@\n+import sys\n+import platform\n+\n+import twisted\n+\n import scrapy\n from scrapy.command import ScrapyCommand\n \n class Command(ScrapyCommand):\n \n+    def syntax(self):\n+        return \"[-v]\"\n+\n     def short_desc(self):\n         return \"Print Scrapy version\"\n \n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--verbose\", \"-v\", dest=\"verbose\", action=\"store_true\",\n+            help=\"also display twisted/python/platform info (useful for bug reports)\")\n+\n     def run(self, args, opts):\n+        if opts.verbose:\n+            print \"Scrapy  : %s\" % scrapy.__version__\n+            print \"Twisted : %s\" % twisted.version.short()\n+            print \"Python  : %s\" % sys.version.replace(\"\\n\", \"- \")\n+            print \"Platform: %s\" % platform.platform()\n+        else:\n             print \"Scrapy %s\" % scrapy.__version__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97a790d6597b466d3ed5e18b6c79531d06e0f5eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 266 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 11, 0, 'dev')\n+version_info = (0, 11, 0)\n __version__ = \"0.11.0\"\n \n import sys, os, warnings\n\n@@ -2,6 +2,9 @@\n #\n # It doesn't depend on setuptools, but if setuptools is available it'll use\n # some of its features, like package dependencies.\n+\n+from __future__ import with_statement\n+\n from distutils.command.install_data import install_data\n from distutils.command.install import INSTALL_SCHEMES\n from subprocess import Popen, PIPE\n@@ -79,10 +82,11 @@ scripts = ['bin/scrapy']\n if os.name == 'nt':\n     scripts.append('extras/scrapy.bat')\n \n-version = __import__('scrapy').__version__\n if os.environ.get('SCRAPY_VERSION_FROM_HG'):\n     rev = Popen([\"hg\", \"tip\", \"--template\", \"{rev}\"], stdout=PIPE).communicate()[0]\n-    version = \"%s.r%s\" % (version, rev)\n+    with open('scrapy/__init__.py', 'a') as f:\n+        f.write(\"\\n__version__ = '.'.join(map(str, version_info)) + '.%s'\" % rev)\n+version = __import__('scrapy').__version__\n \n setup_args = {\n     'name': 'Scrapy',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#60bdb7dc0a0853ca6fc7187b1d15600efa6d6950", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 31 | Files Changed: 5 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 1188 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -14,14 +14,6 @@ if sys.version_info < (2,5):\n # ignore noisy twisted deprecation warnings\n warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n \n-# prevents noisy (and innocent) dropin.cache errors when loading spiders from\n-# egg files using the old Spider Manager. TODO: Remove for Scrapy 0.11\n-try:\n-    from twisted.python.zippath import ZipPath\n-    ZipPath.setContent = lambda x, y: None\n-except ImportError: # to avoid making setup.py depend on Twisted\n-    pass\n-\n # monkey patches to fix external library issues\n from scrapy.xlib import twisted_250_monkeypatches, urlparse_monkeypatches\n \n\n@@ -1,5 +0,0 @@\n-from scrapy.exceptions import *\n-\n-import warnings\n-warnings.warn(\"scrapy.core.exceptions is deprecated and will be removed in Scrapy 0.11, use scrapy.exceptions instead\", \\\n-    DeprecationWarning, stacklevel=2)\n\n@@ -1,6 +0,0 @@\n-import warnings\n-warnings.warn(\"scrapy.core.manager.scrapymanager is deprecated and will be removed in Scrapy 0.11, use scrapy.project.crawler instead\", \\\n-    DeprecationWarning, stacklevel=2)\n-\n-from scrapy.project import crawler\n-scrapymanager = crawler\n\n@@ -1,5 +0,0 @@\n-from scrapy.queue import *\n-\n-import warnings\n-warnings.warn(\"scrapy.core.queue is deprecated and will be removed in Scrapy 0.11, use scrapy.queue instead\", \\\n-    DeprecationWarning, stacklevel=2)\n\n@@ -1,7 +0,0 @@\n-from scrapy.signals import *\n-\n-import warnings\n-warnings.warn(\"scrapy.core.signals is deprecated and will be removed in Scrapy 0.11, use scrapy.signals instead\", \\\n-    DeprecationWarning, stacklevel=2)\n-\n-request_uploaded = response_downloaded\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#55ec7a05542deb8e0bfd185bec1965ccbe89b3e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 297 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,12 +82,11 @@ class Command(ScrapyCommand):\n         tmpdir = None\n         if opts.egg:\n             _log(\"Using egg: %s\" % opts.egg)\n-            egg = open(opts.egg, 'rb')\n+            egg = opts.egg\n         else:\n             _log(\"Building egg of %s-%s\" % (project, version))\n             egg, tmpdir = _build_egg()\n         _upload_egg(target, egg, project, version)\n-        egg.close()\n         if tmpdir:\n             shutil.rmtree(tmpdir)\n \n@@ -145,11 +144,13 @@ def _get_version(target, opts):\n     else:\n         return str(int(time.time()))\n \n-def _upload_egg(target, eggfile, project, version):\n+def _upload_egg(target, eggpath, project, version):\n+    with open(eggpath, 'rb') as f:\n+        eggdata = f.read()\n     data = {\n         'project': project,\n         'version': version,\n-        'egg': ('project.egg', eggfile.read()),\n+        'egg': ('project.egg', eggdata),\n     }\n     body, boundary = encode_multipart(data)\n     url = _url(target, 'addversion.json')\n@@ -193,9 +194,9 @@ def _build_egg():\n         _create_default_setup_py(settings=settings)\n     d = tempfile.mkdtemp()\n     f = tempfile.TemporaryFile(dir=d)\n-    check_call([sys.executable, 'setup.py', 'bdist_egg', '-d', d], stdout=f)\n+    check_call([sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=f)\n     egg = glob.glob(os.path.join(d, '*.egg'))[0]\n-    return open(egg, 'rb'), d\n+    return egg, d\n \n def _create_default_setup_py(**kwargs):\n     with open('setup.py', 'w') as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#63c97f65f4bd1fd2d6bf33fc94996ef65ba9143f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 250 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ from twisted.trial import unittest\n \n from scrapy.utils.iterators import csviter, xmliter\n from scrapy.contrib_exp.iterators import xmliter_lxml\n-from scrapy.http import XmlResponse, TextResponse\n+from scrapy.http import XmlResponse, TextResponse, Response\n from scrapy.tests import get_testdata\n \n \n@@ -132,6 +132,18 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n+    def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):\n+        body = get_testdata('feeds', 'feed-sample3.csv').replace(',', '\\t')\n+        response = Response(url=\"http://example.com/\", body=body)\n+        csv = csviter(response, delimiter='\\t')\n+\n+        self.assertEqual([row for row in csv],\n+                         [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n+                          {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n+                          {u'id': u'4', u'name': u'empty',   u'value': u''}])\n+\n+\n     def test_csviter_headers(self):\n         sample = get_testdata('feeds', 'feed-sample3.csv').splitlines()\n         headers, body = sample[0].split(','), '\\n'.join(sample[1:])\n\n@@ -1,7 +1,7 @@\n import re, csv\n from cStringIO import StringIO\n \n-from scrapy.http import Response\n+from scrapy.http import TextResponse\n from scrapy.selector import XmlXPathSelector\n from scrapy import log\n from scrapy.utils.python import re_rsearch, str_to_unicode\n@@ -45,7 +45,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n     headers is an iterable that when provided offers the keys\n     for the returned dictionaries, if not the first row is used.\n     \"\"\"\n-    encoding = obj.encoding if isinstance(obj, Response) else encoding or 'utf-8'\n+    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n     def _getrow(csv_r):\n         return [str_to_unicode(field, encoding) for field in csv_r.next()]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fff22c269a92f3e3c85c14cab78869534f4cf8f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 44 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,12 +6,16 @@ based on different criterias.\n \n from os.path import abspath, dirname, join\n from mimetypes import MimeTypes\n+from cStringIO import StringIO\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import isbinarytext\n+from scrapy.utils.py26 import get_data\n from scrapy.conf import settings\n \n+__package__ = 'scrapy.core.downloader.responsetypes' # required for python 2.5\n+\n class ResponseTypes(object):\n \n     CLASSES = {\n@@ -31,8 +35,9 @@ class ResponseTypes(object):\n     def __init__(self):\n         self.CLASSES.update(settings.get('RESPONSE_CLASSES', {}))\n         self.classes = {}\n-        mimefile = join(abspath(dirname(__file__)), 'mime.types')\n-        self.mimetypes = MimeTypes([mimefile])\n+        self.mimetypes = MimeTypes()\n+        mimedata = get_data(__package__, 'mime.types')\n+        self.mimetypes.readfp(StringIO(mimedata))\n         for mimetype, cls in self.CLASSES.iteritems():\n             self.classes[mimetype] = load_object(cls)\n \n\n@@ -75,5 +75,9 @@ class ResponseTypesTest(unittest.TestCase):\n             retcls = responsetypes.from_args(**source)\n             assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n \n+    def test_custom_mime_types_loaded(self):\n+        # check that mime.types files shipped with scrapy are loaded\n+        self.assertEqual(responsetypes.mimetypes.guess_type('x.scrapytest')[0], 'x-scrapy/test')\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db07a9a938e3437b01d06f561cdf83c5b5003cf5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 36 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ master_doc = 'index'\n \n # General information about the project.\n project = u'Scrapy'\n-copyright = u'2008-2010, Insophia'\n+copyright = u'2008-2011, Insophia'\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n@@ -51,12 +51,11 @@ copyright = u'2008-2010, Insophia'\n # The short X.Y version.\n try:\n     import scrapy\n-    version = scrapy.__version__\n+    version = '.'.join(map(str, scrapy.version_info[:2]))\n+    release = scrapy.__version__\n except ImportError:\n     version = ''\n-\n-# The full version, including alpha/beta/rc tags.\n-release = version\n+    release = ''\n \n # The language for content autogenerated by Sphinx. Refer to documentation\n # for a list of supported languages.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d9a3df45c600224f75bc235fd663faf684a8ed0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 172 | Files Changed: 6 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 176 | Churn Cumulative: 3033 | Contributors (this commit): 4 | Commits (past 90d): 18 | Contributors (cumulative): 17 | DMM Complexity: 0.5555555555555556\n\nDIFF:\n@@ -19,8 +19,7 @@ Main departures from the original algorithm:\n from operator import itemgetter\n from .regionextract import build_extraction_tree\n from .pageparsing import parse_template, parse_extraction_page\n-from .pageobjects import TokenDict, AnnotationText\n-from .similarity import common_prefix\n+from .pageobjects import TokenDict\n \n class InstanceBasedLearningExtractor(object):\n     \"\"\"Implementation of the instance based learning algorithm to \n@@ -50,35 +49,6 @@ class InstanceBasedLearningExtractor(object):\n                in parsed_plus_templates if _annotation_count(p)]\n         parsed_templates = map(itemgetter(0), parsed_plus_epages)\n         \n-        # calculate common text prefixes for annotations of same field across all templates\n-        extracted_text = {}\n-        extraction_pages = map(itemgetter(1), parsed_plus_epages)\n-        for i, parsed in enumerate(parsed_templates):\n-            for annot in parsed.annotations:\n-                if annot.match_common_prefix:\n-                    field = annot.surrounds_attribute\n-                    if field is not None:\n-                        descriptor = type_descriptor.attribute_map.get(field) if type_descriptor else None\n-                        allow_markup = descriptor.allow_markup if descriptor else False\n-                        start, end = annot.start_index, annot.end_index\n-                        if allow_markup:\n-                            text = extraction_pages[i].html_between_tokens(start, end)\n-                        else:\n-                            text = extraction_pages[i].text_between_tokens(start, end)\n-                        extracted_text.setdefault(field, []).append(text)\n-        common_prefixes = {}\n-        for field, data in extracted_text.iteritems():\n-            if len(data) > 1:\n-                cprefix = common_prefix(*data)\n-                if cprefix:\n-                    common_prefixes[field] = \"\".join(cprefix).strip()\n-        # now apply common prefixes to annotations\n-        for i, parsed in enumerate(parsed_templates):\n-            for annot in parsed.annotations:\n-                for field, prefix in common_prefixes.iteritems():\n-                    if annot.surrounds_attribute == field and not annot.annotation_text:\n-                        annot.annotation_text = AnnotationText(prefix)\n-        \n         # templates with more attributes are considered first\n         sorted_templates = sorted(parsed_templates, key=_annotation_count, reverse=True)\n         self.extraction_trees = [build_extraction_tree(t, type_descriptor, \n\n@@ -189,23 +189,20 @@ class AnnotationTag(object):\n         tag_attributes - list of (tag attribute, extracted attribute) tuples\n                          for each item to be extracted from a tag attribute\n         annotation_text - text prefix and suffix for the attribute to be extracted\n-        match_common_prefix - use this annotation for calculating across-template prefixes\n         metadata - dict with annotation data not used by IBL extractor\n     \"\"\"\n     __slots__ = ('surrounds_attribute', 'start_index', 'end_index',\n             'tag_attributes', 'annotation_text', 'variant_id', \n-            'match_common_prefix', 'metadata')\n+            'metadata')\n     \n     def __init__(self, start_index, end_index, surrounds_attribute=None, \n-            annotation_text=None, tag_attributes=None, variant_id=None,\n-            match_common_prefix=False):\n+            annotation_text=None, tag_attributes=None, variant_id=None):\n         self.start_index = start_index\n         self.end_index = end_index\n         self.surrounds_attribute = surrounds_attribute\n         self.annotation_text = annotation_text\n         self.tag_attributes = tag_attributes or []\n         self.variant_id = variant_id\n-        self.match_common_prefix = match_common_prefix\n         self.metadata = {}\n \n     def __str__(self):\n\n@@ -133,8 +133,6 @@ class TemplatePageParser(InstanceLearningParser):\n                 else:\n                     annotation.tag_attributes.append((extract_attribute, tag_value))\n             self.annotations.append(annotation)\n-            if jannotation.pop('common_prefix', False):\n-                annotation.match_common_prefix = True\n \n             self.extra_required_attrs.extend(jannotation.pop('required', []))\n             annotation.metadata = jannotation\n@@ -196,9 +194,6 @@ class TemplatePageParser(InstanceLearningParser):\n                 ignored = self.ignored_regions.pop()\n                 self.ignored_regions.append((ignored[0]-1, ignored[1]))\n                 \n-        if jannotation.pop('common_prefix', False):\n-            annotation.match_common_prefix = True\n-                \n         self.extra_required_attrs.extend(jannotation.pop('required', []))\n         \n         attribute_annotations = jannotation.pop('annotations', {}).items()\n\n@@ -156,9 +156,6 @@ class BasicTypeExtractor(object):\n         \"\"\"Create a basic type extractor for the annotation\"\"\"\n         text_region = annotation.annotation_text\n         if text_region is not None:\n-            if annotation.match_common_prefix:\n-                region_extract = TextPrefixRegionDataExtractor(text_region.start_text).extract\n-            else:\n             region_extract = TextRegionDataExtractor(text_region.start_text, \n                 text_region.follow_text).extract\n             # copy attribute_descriptors and add the text extractor\n@@ -627,25 +624,4 @@ class TextRegionDataExtractor(object):\n             return None\n         return text[pref_index:pref_index + sidx]\n \n-class TextPrefixRegionDataExtractor(object):\n-    \"\"\"\n-    Data extractor for extracting text fragment from within a\n-    larger body of text, based on a fixed prefix.\n-    >>> extractor = TextPrefixRegionDataExtractor(\"&pounds;\")\n-    >>> extractor.extract(\"&pounds; 17.00\")\n-    ' 17.00'\n-    >>> extractor.extract(\"&euro; 17.00\") is None\n-    True\n-    >>> extractor.extract(\"$ 17.00\") is None\n-    True\n-    >>> extractor.extract(\" &pounds; 17.00 \")\n-    ' 17.00 '\n-    \"\"\"\n-    def __init__(self, prefix):\n-        self.prefix = prefix\n-    def extract(self, text):\n-        text = text.lstrip()\n-        # attempt to extract a substring from the text\n-        if text.startswith(self.prefix):\n-            return text.replace(self.prefix, '')\n    \n\n@@ -238,70 +238,6 @@ EXTRACT_PAGE9 = u\"\"\"\n </body></html>\n \"\"\"\n \n-ANNOTATED_PAGE10a = u\"\"\"\n-<html><body>\n-<table><tbody>\n-<tr data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;common_prefix&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;site_id&quot;}}\">\n-<td>SKU</td><td>L345</td>\n-</tr>\n-<tr data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;common_prefix&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;dimensions&quot;}}\">\n-<td>Size</td><td>10cmx20cm</td>\n-</tr>\n-<tr data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;common_prefix&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">\n-<td>Price</td><td>&pounds;99.00</td>\n-</tr>\n-</tbody></table>\n-</body></html>\n-\"\"\"\n-\n-ANNOTATED_PAGE10b = u\"\"\"\n-<html><body>\n-<table><tbody>\n-<tr data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;common_prefix&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;site_id&quot;}}\">\n-<td>SKU</td><td>S220</td>\n-</tr>\n-<tr data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;common_prefix&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;dimensions&quot;}}\">\n-<td>Size</td><td>20cmx20cm</td>\n-</tr>\n-<tr data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;common_prefix&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">\n-<td>Price</td><td>&pounds;85.00</td>\n-</tr>\n-</tbody></table>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE10a = u\"\"\"\n-<html><body>\n-<table><tbody>\n-<tr>\n-<td>Offer</td><td>From $2500.00</td>\n-</tr>\n-<tr>\n-<td>Description</td><td>Electrorheological Cyborgs</td>\n-</tr>\n-<tr>\n-<td>Series:</td><td>T2000</td>\n-</tr>\n-</tbody></table>\n-</body></html>\n-\"\"\"\n-\n-EXTRACT_PAGE10b = u\"\"\"\n-<html><body>\n-<table><tbody>\n-<tr>\n-<td>SKU</td><td>K80</td>\n-</tr>\n-<tr>\n-<td>Size</td><td>50cm</td>\n-</tr>\n-<tr>\n-<td>Price</td><td>&euros;85.00</td>\n-</tr>\n-</tbody></table>\n-</body></html>\n-\"\"\"\n-\n ANNOTATED_PAGE11 = u\"\"\"\n <html><body>\n <p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;description&quot;}}\">\n@@ -868,35 +804,6 @@ TEST_DATA = [\n              'price': [u'\\n12.00\\n(VAT exc.)'],\n           }\n     ),\n-    # detection of common prefixes across templates, all templates marked\n-    (# wrong extraction\n-    'without_match_common_prefix', [ANNOTATED_PAGE10a], EXTRACT_PAGE10a, None,\n-          {\n-             'price': [u'\\n Series:  T2000 \\n'],\n-             'dimensions': [u'\\n Description  Electrorheological Cyborgs \\n'],\n-             'site_id': [u'\\n Offer  From $2500.00 \\n'],\n-          }\n-    ),\n-    (# right extraction\n-    'with_match_common_prefix', [ANNOTATED_PAGE10a, ANNOTATED_PAGE10b], EXTRACT_PAGE10a, None,\n-          {}\n-    ),\n-    (# another example\n-    'match_common_prefix', [ANNOTATED_PAGE10a, ANNOTATED_PAGE10b], EXTRACT_PAGE10b, None,\n-          {\n-             'dimensions': [u'50cm'],\n-             'site_id': [u'K80'],\n-          }\n-    ),\n-    (# common_prefix with allow_markup attribute\n-    'common_prefix_allow_markup', [ANNOTATED_PAGE10a, ANNOTATED_PAGE10b], EXTRACT_PAGE10b,\n-        ItemDescriptor('test', 'product test',\n-            [A('dimensions', \"something about dimensions\", allow_markup=True)]),\n-          {\n-             'dimensions': [u'50cm</td>'],\n-             'site_id': [u'K80'],\n-          }\n-    ),\n     (# special case with partial annotations\n     'special_partial_annotation', [ANNOTATED_PAGE11], EXTRACT_PAGE11, None,\n           {\n\n@@ -34,7 +34,7 @@ LABELLED_PAGE1 = u\"\"\"\n This is such a nice item<br/>\n Everybody likes it.\n </p>\n-<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;common_prefix&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\"/>\n+<p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\"/>\n \\xa310.00\n <br/>\n <p data-scrapy-annotate=\"{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;short_description&quot;}}\">\n@@ -220,19 +220,6 @@ class TestPageParsing(TestCase):\n         self.assertEqual(len(lp.annotations), 5)\n         self._validate_annotation(lp, lp.annotations[0], \n                 'name', '<h1>', '</h1>')\n-        self.assertEqual(lp.annotations[0].match_common_prefix, False)\n-        self._validate_annotation(lp, lp.annotations[1], \n-                'description', '<p>', '</p>')\n-        self.assertEqual(lp.annotations[1].match_common_prefix, False)\n-        self._validate_annotation(lp, lp.annotations[2],\n-                'price', '<p/>', '<p>')\n-        self.assertEqual(lp.annotations[2].match_common_prefix, True)\n-        self._validate_annotation(lp, lp.annotations[3],\n-                'short_description', '<p>', '<p>')\n-        self.assertEqual(lp.annotations[3].match_common_prefix, False)\n-        self._validate_annotation(lp, lp.annotations[4],\n-                'short_description', '<p>', '<p>')\n-        self.assertEqual(lp.annotations[4].match_common_prefix, False)\n         \n         # all tags were closed\n         self.assertEqual(len(lp.labelled_tag_stacks), 0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c8d74eb5b304a9fa26f5e69d4ab296471797236", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 77 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,7 @@ class Environment(object):\n     def get_environment(self, message, slot, eggpath):\n         project = message['_project']\n         env = self.initenv.copy()\n+        env['SCRAPY_SLOT'] = str(slot)\n         env['SCRAPY_PROJECT'] = project\n         if eggpath:\n             env['SCRAPY_EGGFILE'] = eggpath\n\n@@ -26,6 +26,7 @@ class EggStorageTest(unittest.TestCase):\n         slot = 3\n         env = self.environ.get_environment(msg, slot, '/path/to/file.egg')\n         self.assertEqual(env['SCRAPY_PROJECT'], 'mybot')\n+        self.assertEqual(env['SCRAPY_SLOT'], '3')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n         self.assert_(env['SCRAPY_LOG_FILE'].endswith('/mybot/myspider/ID.log'))\n         self.assert_(env['SCRAPY_EGGFILE'].endswith('/path/to/file.egg'))\n@@ -36,6 +37,7 @@ class EggStorageTest(unittest.TestCase):\n         slot = 3\n         env = self.environ.get_environment(msg, slot, None)\n         self.assertEqual(env['SCRAPY_PROJECT'], 'newbot')\n+        self.assertEqual(env['SCRAPY_SLOT'], '3')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('newbot.db'))\n         self.assert_(env['SCRAPY_LOG_FILE'].endswith('/newbot/myspider/ID.log'))\n         self.assertEqual(env['SCRAPY_SETTINGS_MODULE'], 'newbot.settings')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9cd649b3a04e5eed2acaca9af17d735de55adee7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 83 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,6 +23,8 @@ class Environment(object):\n         env = self.initenv.copy()\n         env['SCRAPY_SLOT'] = str(slot)\n         env['SCRAPY_PROJECT'] = project\n+        env['SCRAPY_SPIDER'] = message['_spider']\n+        env['SCRAPY_JOB'] = message['_job']\n         if eggpath:\n             env['SCRAPY_EGGFILE'] = eggpath\n         elif project in self.settings:\n\n@@ -27,6 +27,8 @@ class EggStorageTest(unittest.TestCase):\n         env = self.environ.get_environment(msg, slot, '/path/to/file.egg')\n         self.assertEqual(env['SCRAPY_PROJECT'], 'mybot')\n         self.assertEqual(env['SCRAPY_SLOT'], '3')\n+        self.assertEqual(env['SCRAPY_SPIDER'], 'myspider')\n+        self.assertEqual(env['SCRAPY_JOB'], 'ID')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n         self.assert_(env['SCRAPY_LOG_FILE'].endswith('/mybot/myspider/ID.log'))\n         self.assert_(env['SCRAPY_EGGFILE'].endswith('/path/to/file.egg'))\n@@ -38,6 +40,8 @@ class EggStorageTest(unittest.TestCase):\n         env = self.environ.get_environment(msg, slot, None)\n         self.assertEqual(env['SCRAPY_PROJECT'], 'newbot')\n         self.assertEqual(env['SCRAPY_SLOT'], '3')\n+        self.assertEqual(env['SCRAPY_SPIDER'], 'myspider')\n+        self.assertEqual(env['SCRAPY_JOB'], 'ID')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('newbot.db'))\n         self.assert_(env['SCRAPY_LOG_FILE'].endswith('/newbot/myspider/ID.log'))\n         self.assertEqual(env['SCRAPY_SETTINGS_MODULE'], 'newbot.settings')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fa644f7a5ed6099e9e88b05ec6be1aef1c5bd3ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 107 | Files Changed: 12 | Hunks: 35 | Methods Changed: 24 | Complexity Δ (Sum/Max): 0/5 | Churn Δ: 217 | Churn Cumulative: 1060 | Contributors (this commit): 1 | Commits (past 90d): 47 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +0,0 @@\n-import os\n-\n-from scrapyd.eggutils import activate_egg\n-\n-eggpath = os.environ.get('SCRAPY_EGGFILE')\n-if eggpath:\n-    activate_egg(eggpath)\n-from scrapy.cmdline import execute\n-execute()\n\n@@ -1,24 +1,4 @@\n-from __future__ import with_statement\n-\n-import os, sys, shutil, pkg_resources\n-from subprocess import Popen, PIPE\n-from tempfile import NamedTemporaryFile\n-\n-def get_spider_list_from_eggfile(eggfile, project, eggrunner='scrapyd.eggrunner'):\n-    with NamedTemporaryFile(suffix='.egg') as f:\n-        shutil.copyfileobj(eggfile, f)\n-        f.flush()\n-        eggfile.seek(0)\n-        pargs = [sys.executable, '-m', eggrunner, 'list']\n-        env = os.environ.copy()\n-        env['SCRAPY_PROJECT'] = project\n-        env['SCRAPY_EGGFILE'] = f.name\n-        proc = Popen(pargs, stdout=PIPE, stderr=PIPE, env=env)\n-        out, err = proc.communicate()\n-        if proc.returncode:\n-            msg = err or out or 'unknown error'\n-            raise RuntimeError(msg.splitlines()[-1])\n-        return out.splitlines()\n+import os, pkg_resources\n \n def activate_egg(eggpath):\n     \"\"\"Activate a Scrapy egg file. This is meant to be used from egg runners\n\n@@ -18,16 +18,14 @@ class Environment(object):\n             self.settings = {}\n         self.initenv = initenv\n \n-    def get_environment(self, message, slot, eggpath):\n+    def get_environment(self, message, slot):\n         project = message['_project']\n         env = self.initenv.copy()\n         env['SCRAPY_SLOT'] = str(slot)\n         env['SCRAPY_PROJECT'] = project\n         env['SCRAPY_SPIDER'] = message['_spider']\n         env['SCRAPY_JOB'] = message['_job']\n-        if eggpath:\n-            env['SCRAPY_EGGFILE'] = eggpath\n-        elif project in self.settings:\n+        if project in self.settings:\n             env['SCRAPY_SETTINGS_MODULE'] = self.settings[project]\n         dbpath = os.path.join(self.dbs_dir, '%s.db' % project)\n         env['SCRAPY_SQLITE_DB'] = dbpath\n\n@@ -64,13 +64,9 @@ class ISpiderScheduler(Interface):\n class IEnvironment(Interface):\n     \"\"\"A component to generate the environment of crawler processes\"\"\"\n \n-    def get_environment(message, slot, eggpath):\n+    def get_environment(message, slot):\n         \"\"\"Return the environment variables to use for running the process.\n \n         `message` is the message received from the IPoller.next() method\n         `slot` is the Launcher slot where the process will be running.\n-        `eggpath` is the path to an eggfile that contains the project code. The\n-           `eggpath` may be `None` if no egg was found for the project, in\n-           which case the project must be on the python path and its settings\n-           defined in scrapyd.conf [settings] section\n         \"\"\"\n\n@@ -1,6 +1,4 @@\n import sys, os\n-from shutil import copyfileobj\n-from tempfile import mkstemp\n from datetime import datetime\n \n from twisted.internet import reactor, defer, protocol, error\n@@ -10,7 +8,7 @@ from twisted.python import log\n from scrapy.utils.py26 import cpu_count\n from scrapy.utils.python import stringify_dict\n from scrapyd.utils import get_crawl_args\n-from .interfaces import IPoller, IEggStorage, IEnvironment\n+from .interfaces import IPoller, IEnvironment\n \n class Launcher(Service):\n \n@@ -21,57 +19,41 @@ class Launcher(Service):\n         self.max_proc = config.getint('max_proc', 0)\n         if not self.max_proc:\n             self.max_proc = cpu_count() * config.getint('max_proc_per_cpu', 4)\n-        self.egg_runner = config.get('egg_runner', 'scrapyd.eggrunner')\n+        self.runner = config.get('runner', 'scrapyd.runner')\n         self.app = app\n \n     def startService(self):\n         for slot in range(self.max_proc):\n             self._wait_for_project(slot)\n-        log.msg(\"%s started: max_proc=%r, egg_runner=%r\" % (self.parent.name, \\\n-            self.max_proc, self.egg_runner), system=\"Launcher\")\n+        log.msg(\"%s started: max_proc=%r, runner=%r\" % (self.parent.name, \\\n+            self.max_proc, self.runner), system=\"Launcher\")\n \n     def _wait_for_project(self, slot):\n         poller = self.app.getComponent(IPoller)\n         poller.next().addCallback(self._spawn_process, slot)\n \n-    def _get_eggpath(self, project):\n-        eggstorage = self.app.getComponent(IEggStorage)\n-        version, eggf = eggstorage.get(project)\n-        if eggf is None:\n-            return\n-        prefix = '%s-%s-' % (project, version)\n-        fd, eggpath = mkstemp(prefix=prefix, suffix='.egg')\n-        lf = os.fdopen(fd, 'wb')\n-        copyfileobj(eggf, lf)\n-        lf.close()\n-        return eggpath\n-\n     def _spawn_process(self, message, slot):\n         msg = stringify_dict(message, keys_only=False)\n         project = msg['_project']\n-        eggpath = self._get_eggpath(project)\n-        args = [sys.executable, '-m', self.egg_runner, 'crawl']\n+        args = [sys.executable, '-m', self.runner, 'crawl']\n         args += get_crawl_args(msg)\n         e = self.app.getComponent(IEnvironment)\n-        env = e.get_environment(msg, slot, eggpath)\n+        env = e.get_environment(msg, slot)\n         env = stringify_dict(env, keys_only=False)\n-        pp = ScrapyProcessProtocol(eggpath, slot, project, msg['_spider'], \\\n+        pp = ScrapyProcessProtocol(slot, project, msg['_spider'], \\\n             msg['_job'], env)\n-        pp.deferred.addBoth(self._process_finished, eggpath, slot)\n+        pp.deferred.addBoth(self._process_finished, slot)\n         reactor.spawnProcess(pp, sys.executable, args=args, env=env)\n         self.processes[slot] = pp\n \n-    def _process_finished(self, _, eggpath, slot):\n-        if eggpath:\n-            os.remove(eggpath)\n+    def _process_finished(self, _, slot):\n         self.processes.pop(slot)\n         self._wait_for_project(slot)\n \n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n-    def __init__(self, eggfile, slot, project, spider, job, env):\n-        self.eggfile = eggfile\n+    def __init__(self, slot, project, spider, job, env):\n         self.slot = slot\n         self.pid = None\n         self.project = project\n@@ -100,6 +82,6 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.deferred.callback(self)\n \n     def log(self, msg):\n-        msg += \"project=%r spider=%r job=%r pid=%r egg=%r log=%r\" % (self.project, \\\n-            self.spider, self.job, self.pid, self.eggfile, self.logfile)\n+        msg += \"project=%r spider=%r job=%r pid=%r log=%r\" % (self.project, \\\n+            self.spider, self.job, self.pid, self.logfile)\n         log.msg(msg, system=\"Launcher\")\n\n@@ -0,0 +1,37 @@\n+import os\n+import shutil\n+import tempfile\n+from contextlib import contextmanager\n+\n+from scrapyd import get_application\n+from scrapyd.interfaces import IEggStorage\n+from scrapyd.eggutils import activate_egg\n+\n+@contextmanager\n+def project_environment(project):\n+    app = get_application()\n+    eggstorage = app.getComponent(IEggStorage)\n+    version, eggfile = eggstorage.get(project)\n+    if eggfile:\n+        prefix = '%s-%s-' % (project, version)\n+        fd, eggpath = tempfile.mkstemp(prefix=prefix, suffix='.egg')\n+        lf = os.fdopen(fd, 'wb')\n+        shutil.copyfileobj(eggfile, lf)\n+        lf.close()\n+        activate_egg(eggpath)\n+    else:\n+        eggpath = None\n+    try:\n+        yield\n+    finally:\n+        if eggpath:\n+            os.remove(eggpath)\n+\n+def main():\n+    project = os.environ['SCRAPY_PROJECT']\n+    with project_environment(project):\n+        from scrapy.cmdline import execute\n+        execute()\n+\n+if __name__ == '__main__':\n+    main()\n\n@@ -1,14 +0,0 @@\n-import unittest\n-from cStringIO import StringIO\n-\n-from scrapyd.eggutils import get_spider_list_from_eggfile\n-from scrapy.utils.py26 import get_data\n-\n-__package__ = 'scrapyd.tests' # required for compatibility with python 2.5\n-\n-class EggUtilsTest(unittest.TestCase):\n-\n-    def test_get_spider_list_from_eggfile(self):\n-        eggfile = StringIO(get_data(__package__, 'mybot.egg'))\n-        spiders = get_spider_list_from_eggfile(eggfile, 'mybot')\n-        self.assertEqual(set(spiders), set(['spider1', 'spider2']))\n\n@@ -8,7 +8,7 @@ from scrapyd.interfaces import IEnvironment\n from scrapyd.config import Config\n from scrapyd.environ import Environment\n \n-class EggStorageTest(unittest.TestCase):\n+class EnvironmentTest(unittest.TestCase):\n \n     def setUp(self):\n         d = self.mktemp()\n@@ -24,25 +24,11 @@ class EggStorageTest(unittest.TestCase):\n     def test_get_environment_with_eggfile(self):\n         msg = {'_project': 'mybot', '_spider': 'myspider', '_job': 'ID'}\n         slot = 3\n-        env = self.environ.get_environment(msg, slot, '/path/to/file.egg')\n+        env = self.environ.get_environment(msg, slot)\n         self.assertEqual(env['SCRAPY_PROJECT'], 'mybot')\n         self.assertEqual(env['SCRAPY_SLOT'], '3')\n         self.assertEqual(env['SCRAPY_SPIDER'], 'myspider')\n         self.assertEqual(env['SCRAPY_JOB'], 'ID')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n         self.assert_(env['SCRAPY_LOG_FILE'].endswith('/mybot/myspider/ID.log'))\n-        self.assert_(env['SCRAPY_EGGFILE'].endswith('/path/to/file.egg'))\n         self.failIf('SCRAPY_SETTINGS_MODULE' in env)\n-\n-    def test_get_environment_without_eggfile(self):\n-        msg = {'_project': 'newbot', '_spider': 'myspider', '_job': 'ID'}\n-        slot = 3\n-        env = self.environ.get_environment(msg, slot, None)\n-        self.assertEqual(env['SCRAPY_PROJECT'], 'newbot')\n-        self.assertEqual(env['SCRAPY_SLOT'], '3')\n-        self.assertEqual(env['SCRAPY_SPIDER'], 'myspider')\n-        self.assertEqual(env['SCRAPY_JOB'], 'ID')\n-        self.assert_(env['SCRAPY_SQLITE_DB'].endswith('newbot.db'))\n-        self.assert_(env['SCRAPY_LOG_FILE'].endswith('/newbot/myspider/ID.log'))\n-        self.assertEqual(env['SCRAPY_SETTINGS_MODULE'], 'newbot.settings')\n-        self.failIf('SCRAPY_EGGFILE' in env)\n\n@@ -1,6 +1,16 @@\n-import unittest\n+from __future__ import with_statement\n \n-from scrapyd.utils import get_crawl_args\n+import os\n+from cStringIO import StringIO\n+\n+from twisted.trial import unittest\n+\n+from scrapy.utils.py26 import get_data\n+from scrapyd.interfaces import IEggStorage\n+from scrapyd.utils import get_crawl_args, get_spider_list\n+from scrapyd import get_application\n+\n+__package__ = 'scrapyd.tests' # required for compatibility with python 2.5\n \n class UtilsTest(unittest.TestCase):\n \n@@ -11,3 +21,27 @@ class UtilsTest(unittest.TestCase):\n         cargs = get_crawl_args(msg)\n         self.assertEqual(cargs, ['lala', '-a', 'arg1=val1'])\n         assert all(isinstance(x, str) for x in cargs), cargs\n+\n+class GetSpiderListTest(unittest.TestCase):\n+\n+    def test_get_spider_list(self):\n+        path = self.mktemp()\n+        j = os.path.join\n+        eggs_dir = j(path, 'eggs')\n+        os.makedirs(eggs_dir)\n+        dbs_dir = j(path, 'dbs')\n+        os.makedirs(dbs_dir)\n+        logs_dir = j(path, 'logs')\n+        os.makedirs(logs_dir)\n+        os.chdir(path)\n+        with open('scrapyd.conf', 'w') as f:\n+            f.write(\"[scrapyd]\\n\")\n+            f.write(\"eggs_dir = %s\\n\" % eggs_dir)\n+            f.write(\"dbs_dir = %s\\n\" % dbs_dir)\n+            f.write(\"logs_dir = %s\\n\" % logs_dir)\n+        app = get_application()\n+        eggstorage = app.getComponent(IEggStorage)\n+        eggfile = StringIO(get_data(__package__, 'mybot.egg'))\n+        eggstorage.put(eggfile, 'mybot', 'r1')\n+        self.assertEqual(sorted(get_spider_list('mybot')), ['spider1', 'spider2'])\n+\n\n@@ -1,4 +1,6 @@\n+import sys\n import os\n+from subprocess import Popen, PIPE\n from ConfigParser import NoSectionError\n \n from scrapy.spiderqueue import SqliteSpiderQueue\n@@ -41,3 +43,16 @@ def get_crawl_args(message):\n         args += ['-a']\n         args += ['%s=%s' % (k, v)]\n     return args\n+\n+def get_spider_list(project, runner='scrapyd.runner'):\n+    \"\"\"Return the spider list from the given project, using the given runner\"\"\"\n+    env = os.environ.copy()\n+    env['SCRAPY_PROJECT'] = project\n+    pargs = [sys.executable, '-m', runner, 'list']\n+    proc = Popen(pargs, stdout=PIPE, stderr=PIPE, env=env)\n+    out, err = proc.communicate()\n+    if proc.returncode:\n+        msg = err or out or 'unknown error'\n+        raise RuntimeError(msg.splitlines()[-1])\n+    return out.splitlines()\n+\n\n@@ -4,7 +4,7 @@ import uuid\n from cStringIO import StringIO\n \n from scrapy.utils.txweb import JsonResource\n-from .eggutils import get_spider_list_from_eggfile\n+from .utils import get_spider_list\n \n class WsResource(JsonResource):\n \n@@ -42,8 +42,8 @@ class AddVersion(WsResource):\n         project = d['project'][0]\n         version = d['version'][0]\n         eggf = StringIO(d['egg'][0])\n-        spiders = get_spider_list_from_eggfile(eggf, project)\n         self.root.eggstorage.put(eggf, project, version)\n+        spiders = get_spider_list(project)\n         self.root.update_projects()\n         return {\"status\": \"ok\", \"project\": project, \"version\": version, \\\n             \"spiders\": len(spiders)}\n@@ -65,9 +65,7 @@ class ListSpiders(WsResource):\n \n     def render_GET(self, txrequest):\n         project = txrequest.args['project'][0]\n-        _, eggf = self.root.eggstorage.get(project)\n-        spiders = get_spider_list_from_eggfile(eggf, project, \\\n-            eggrunner=self.root.egg_runner)\n+        spiders = get_spider_list(project, runner=self.root.runner)\n         return {\"status\": \"ok\", \"spiders\": spiders}\n \n class DeleteProject(WsResource):\n\n@@ -11,7 +11,7 @@ class Root(resource.Resource):\n     def __init__(self, config, app):\n         resource.Resource.__init__(self)\n         self.debug = config.getboolean('debug', False)\n-        self.eggrunner = config.get('egg_runner')\n+        self.runner = config.get('runner')\n         logsdir = config.get('logs_dir')\n         self.app = app\n         self.putChild('', Home(self))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
