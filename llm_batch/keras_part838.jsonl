{"custom_id": "keras#6adc2bf7d8d548f92667f8337d27be6b1674ddf5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 8 | Churn Cumulative: 59 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -197,6 +197,14 @@ def image_dataset_from_directory(\n             f\"Received: color_mode={color_mode}\"\n         )\n \n+    if isinstance(image_size, int):\n+        image_size = (image_size, image_size)\n+    elif not isinstance(image_size, (list, tuple)) or not len(image_size) == 2:\n+        raise ValueError(\n+            \"Invalid `image_size` value. Expected a tuple of 2 integers. \"\n+            f\"Received: image_size={image_size}\"\n+        )\n+\n     interpolation = interpolation.lower()\n     supported_interpolations = (\n         \"bilinear\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#def3c0ee7f1a96e204ec85a2ebdce91c32f81fde", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 101 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,5 @@\n import hashlib\n import os\n-import pathlib\n import re\n import shutil\n import tarfile\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a9be826be85197a1c4a8b80a1521593d7a39a46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 28 | Churn Cumulative: 210 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -905,6 +905,8 @@ def _is_tpu_strategy_class(clz):\n def convert_to_np_if_not_ragged(x):\n     if isinstance(x, tf.RaggedTensor):\n         return x\n+    elif isinstance(x, tf.SparseTensor):\n+        return x\n     return x.numpy()\n \n \n\n@@ -0,0 +1,26 @@\n+\"\"\"Tests for trainer functionality under tf implementation.\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras.src import testing\n+from keras.src.backend.tensorflow.trainer import convert_to_np_if_not_ragged\n+\n+\n+class TrainerTest(testing.TestCase):\n+    def test_convert_to_np_if_not_ragged__ragged_input_should_return_ragged(self):\n+        rg_input = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n+        output = convert_to_np_if_not_ragged(rg_input)\n+        self.assertIsInstance(output, tf.RaggedTensor)\n+\n+    def test_convert_to_np_if_not_ragged__sparse_input_should_return_sparse(self):\n+        sp_input = tf.sparse.SparseTensor(indices=[[0, 3], [2, 4]],\n+                                          values=[10, 20],\n+                                          dense_shape=[3, 10])\n+        output = convert_to_np_if_not_ragged(sp_input)\n+        self.assertIsInstance(output, tf.SparseTensor)\n+\n+    def test_convert_to_np_if_not_ragged__tftensor_input_should_return_numpy(self):\n+        tf_input = tf.constant([[3, 1, 4, 1], [5, 9, 2, 6]])\n+        output = convert_to_np_if_not_ragged(tf_input)\n+        self.assertIsInstance(output, np.ndarray)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3cc4d44df0d9d2cc0766c9a36ddc9cc0f20e9f3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 26 | Churn Cumulative: 52 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,26 +0,0 @@\n-\"\"\"Tests for trainer functionality under tf implementation.\"\"\"\n-\n-import numpy as np\n-import tensorflow as tf\n-\n-from keras.src import testing\n-from keras.src.backend.tensorflow.trainer import convert_to_np_if_not_ragged\n-\n-\n-class TrainerTest(testing.TestCase):\n-    def test_convert_to_np_if_not_ragged__ragged_input_should_return_ragged(self):\n-        rg_input = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n-        output = convert_to_np_if_not_ragged(rg_input)\n-        self.assertIsInstance(output, tf.RaggedTensor)\n-\n-    def test_convert_to_np_if_not_ragged__sparse_input_should_return_sparse(self):\n-        sp_input = tf.sparse.SparseTensor(indices=[[0, 3], [2, 4]],\n-                                          values=[10, 20],\n-                                          dense_shape=[3, 10])\n-        output = convert_to_np_if_not_ragged(sp_input)\n-        self.assertIsInstance(output, tf.SparseTensor)\n-\n-    def test_convert_to_np_if_not_ragged__tftensor_input_should_return_numpy(self):\n-        tf_input = tf.constant([[3, 1, 4, 1], [5, 9, 2, 6]])\n-        output = convert_to_np_if_not_ragged(tf_input)\n-        self.assertIsInstance(output, np.ndarray)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb233fa91f91caba33e3ab63e74a1e29519d7ae5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 47 | Churn Cumulative: 412 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -168,7 +168,7 @@ class Functional(Function, Model):\n         if mask is None:\n             masks = [None] * len(inputs)\n         else:\n-            masks = self._flatten_to_reference_inputs(mask)\n+            masks = tree.flatten(mask)\n             for x, mask in zip(inputs, masks):\n                 if mask is not None:\n                     x._keras_mask = mask\n@@ -205,8 +205,21 @@ class Functional(Function, Model):\n     def _assert_input_compatibility(self, *args):\n         return super(Model, self)._assert_input_compatibility(*args)\n \n-    def _flatten_to_reference_inputs(self, inputs):\n-        return tree.flatten(inputs)\n+    def _maybe_warn_inputs_struct_mismatch(self, inputs):\n+        try:\n+            tree.assert_same_structure(\n+                inputs, self._inputs_struct, check_types=False\n+            )\n+        except:\n+            model_inputs_struct = tree.map_structure(\n+                lambda x: x.name, self._inputs_struct\n+            )\n+            inputs_struct = tree.map_structure(lambda x: \"*\", inputs)\n+            warnings.warn(\n+                \"The structure of `inputs` doesn't match the expected \"\n+                f\"structure: {model_inputs_struct}. \"\n+                f\"Received: the structure of inputs={inputs_struct}\"\n+            )\n \n     def _convert_inputs_to_tensors(self, flat_inputs):\n         converted = []\n@@ -254,7 +267,8 @@ class Functional(Function, Model):\n         return adjusted\n \n     def _standardize_inputs(self, inputs):\n-        flat_inputs = self._flatten_to_reference_inputs(inputs)\n+        self._maybe_warn_inputs_struct_mismatch(inputs)\n+        flat_inputs = tree.flatten(inputs)\n         flat_inputs = self._convert_inputs_to_tensors(flat_inputs)\n         return self._adjust_input_rank(flat_inputs)\n \n\n@@ -1,5 +1,4 @@\n import os\n-import warnings\n \n import numpy as np\n import pytest\n@@ -162,9 +161,8 @@ class FunctionalTest(testing.TestCase, parameterized.TestCase):\n \n         model = Functional({\"a\": input_a}, outputs)\n \n+        with pytest.warns() as record:\n             # Eager call\n-        with warnings.catch_warnings():\n-            warnings.simplefilter(\"error\")\n             in_val = {\n                 \"a\": np.random.random((2, 3)),\n                 \"b\": np.random.random((2, 1)),\n@@ -172,14 +170,17 @@ class FunctionalTest(testing.TestCase, parameterized.TestCase):\n             out_val = model(in_val)\n             self.assertEqual(out_val.shape, (2, 3))\n \n-        with warnings.catch_warnings():\n-            warnings.simplefilter(\"error\")\n             # Symbolic call\n             input_a_2 = Input(shape=(3,), batch_size=2)\n             input_b_2 = Input(shape=(1,), batch_size=2)\n             in_val = {\"a\": input_a_2, \"b\": input_b_2}\n             out_val = model(in_val)\n             self.assertEqual(out_val.shape, (2, 3))\n+        self.assertLen(record, 1)\n+        self.assertStartsWith(\n+            str(record[0].message),\n+            r\"The structure of `inputs` doesn't match the expected structure:\",\n+        )\n \n     @parameterized.named_parameters(\n         (\"list\", list),\n@@ -495,6 +496,20 @@ class FunctionalTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(out, np.ones((2, 2)))\n         # Note: it's not intended to work in symbolic mode (yet).\n \n+    def test_warning_for_mismatched_inputs_structure(self):\n+        i1 = Input((2,))\n+        i2 = Input((2,))\n+        outputs = layers.Add()([i1, i2])\n+        model = Model({\"i1\": i1, \"i2\": i2}, outputs)\n+\n+        with pytest.warns() as record:\n+            model([np.ones((2, 2)), np.zeros((2, 2))])\n+        self.assertLen(record, 1)\n+        self.assertStartsWith(\n+            str(record[0].message),\n+            r\"The structure of `inputs` doesn't match the expected structure:\",\n+        )\n+\n     def test_for_functional_in_sequential(self):\n         # Test for a v3.4.1 regression.\n         if backend.image_data_format() == \"channels_first\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3de251dcb06de705b0f16a9c8165c311b0b5603b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 811 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -769,7 +769,7 @@ def argmax(x, axis=None, keepdims=False):\n     _x = x\n     if axis is None:\n         x = tf.reshape(x, [-1])\n-    y = tf.cast(tf.argmax(x, axis=axis), dtype=\"int32\")\n+    y = tf.argmax(x, axis=axis, output_type=\"int32\")\n     if keepdims:\n         y = _keepdims(_x, y, axis)\n     return y\n@@ -779,7 +779,7 @@ def argmin(x, axis=None, keepdims=False):\n     _x = x\n     if axis is None:\n         x = tf.reshape(x, [-1])\n-    y = tf.cast(tf.argmin(x, axis=axis), dtype=\"int32\")\n+    y = tf.argmin(x, axis=axis, output_type=\"int32\")\n     if keepdims:\n         y = _keepdims(_x, y, axis)\n     return y\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dcea7f27009976801561d0537cab3f115e1788c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 50 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 137 | Churn Cumulative: 410 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.8666666666666667\n\nDIFF:\n@@ -372,8 +372,12 @@ def load_weights_from_hdf5_group(f, model):\n                 f\"Layer expects {len(symbolic_weights)} weight(s). Received \"\n                 f\"{len(weight_values)} saved weight(s)\"\n             )\n-        for ref_v, val in zip(symbolic_weights, weight_values):\n-            ref_v.assign(val)\n+        _set_weights(\n+            layer,\n+            symbolic_weights,\n+            weight_values,\n+            name=f\"layer #{k} (named {layer.name})\",\n+        )\n \n     if \"top_level_model_weights\" in f:\n         symbolic_weights = list(\n@@ -392,8 +396,57 @@ def load_weights_from_hdf5_group(f, model):\n                 f\"Model expects {len(symbolic_weights)} top-level weight(s). \"\n                 f\"Received {len(weight_values)} saved top-level weight(s)\"\n             )\n-        for ref_v, val in zip(symbolic_weights, weight_values):\n-            ref_v.assign(val)\n+        _set_weights(\n+            model,\n+            symbolic_weights,\n+            weight_values,\n+            name=\"top-level model\",\n+        )\n+\n+\n+def _set_weights(\n+    instance, symbolic_weights, weight_values, name, skip_mismatch=False\n+):\n+    \"\"\"Safely set weights into a model or a layer,\n+       calling load_own_variables if implemented.\n+    Args:\n+        instance: Model or layer instance,\n+        symbolic_weights: symbolic tensors representing\n+                        the weights of the variables to load,\n+        weight_values: values of the weights to load,\n+        skip_mismatch: Boolean, whether to skip loading of weights\n+            where there is a mismatch in the shape of the weights,\n+        name: name used to identify the group.\n+    Raises:\n+        ValueError: in case of mismatch between provided\n+            model/layer and weights.\n+    \"\"\"\n+    for i, weight_value in enumerate(weight_values):\n+        expected_shape = symbolic_weights[i].shape\n+        received_shape = weight_value.shape\n+        if expected_shape != received_shape:\n+            if skip_mismatch:\n+                warnings.warn(\n+                    f\"Skipping loading weights for {name}\"\n+                    f\"due to mismatch in shape for \"\n+                    f\"weight {symbolic_weights[i].path}. \"\n+                    f\"Weight expects shape {expected_shape}. \"\n+                    \"Received saved weight \"\n+                    f\"with shape {received_shape}\",\n+                    stacklevel=2,\n+                )\n+                continue\n+            raise ValueError(\n+                f\"Shape mismatch in {name}\"\n+                f\"for weight {symbolic_weights[i].path}. \"\n+                f\"Weight expects shape {expected_shape}. \"\n+                \"Received saved weight \"\n+                f\"with shape {received_shape}\"\n+            )\n+        symbolic_weights[i].assign(weight_value)\n+\n+    if hasattr(instance, \"finalize_state\") and symbolic_weights:\n+        instance.finalize_state()\n \n \n def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n@@ -456,30 +509,13 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                     f\"Received {len(weight_values)} saved weight(s)\"\n                 )\n             # Set values.\n-            for i in range(len(weight_values)):\n-                expected_shape = symbolic_weights[i].shape\n-                received_shape = weight_values[i].shape\n-                if expected_shape != received_shape:\n-                    if skip_mismatch:\n-                        warnings.warn(\n-                            f\"Skipping loading weights for layer #{k} (named \"\n-                            f\"{layer.name}) due to mismatch in shape for \"\n-                            f\"weight {symbolic_weights[i].path}. \"\n-                            f\"Weight expects shape {expected_shape}. \"\n-                            \"Received saved weight \"\n-                            f\"with shape {received_shape}\",\n-                            stacklevel=2,\n+            _set_weights(\n+                layer,\n+                symbolic_weights,\n+                weight_values,\n+                skip_mismatch=skip_mismatch,\n+                name=f\"layer #{k} (named {layer.name})\",\n             )\n-                        continue\n-                    raise ValueError(\n-                        f\"Shape mismatch in layer #{k} (named {layer.name}) \"\n-                        f\"for weight {symbolic_weights[i].path}. \"\n-                        f\"Weight expects shape {expected_shape}. \"\n-                        \"Received saved weight \"\n-                        f\"with shape {received_shape}\"\n-                    )\n-                else:\n-                    symbolic_weights[i].assign(weight_values[i])\n \n     if \"top_level_model_weights\" in f:\n         symbolic_weights = model.trainable_weights + model.non_trainable_weights\n@@ -505,30 +541,13 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                     f\"Received {len(weight_values)} saved top-level weight(s)\"\n                 )\n         else:\n-            for i in range(len(weight_values)):\n-                expected_shape = symbolic_weights[i].shape\n-                received_shape = weight_values[i].shape\n-                if expected_shape != received_shape:\n-                    if skip_mismatch:\n-                        warnings.warn(\n-                            \"Skipping loading top-level weight for model due \"\n-                            \"to mismatch in shape for \"\n-                            f\"weight {symbolic_weights[i].path}. \"\n-                            f\"Weight expects shape {expected_shape}. \"\n-                            \"Received saved weight \"\n-                            f\"with shape {received_shape}\",\n-                            stacklevel=2,\n+            _set_weights(\n+                model,\n+                symbolic_weights,\n+                weight_values,\n+                skip_mismatch=skip_mismatch,\n+                name=\"top-level model\",\n             )\n-                    else:\n-                        raise ValueError(\n-                            \"Shape mismatch in model for top-level weight \"\n-                            f\"{symbolic_weights[i].path}. \"\n-                            f\"Weight expects shape {expected_shape}. \"\n-                            \"Received saved weight \"\n-                            f\"with shape {received_shape}\"\n-                        )\n-                else:\n-                    symbolic_weights[i].assign(weight_values[i])\n \n \n def load_subset_weights_from_hdf5_group(f):\n\n@@ -959,6 +959,24 @@ class SavingBattleTest(testing.TestCase):\n             model_a.dense.kernel.numpy(), model_b.dense.kernel.numpy()\n         )\n \n+    def test_normalization_legacy_h5_format(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"custom_model.h5\")\n+\n+        inputs = keras.Input((32,))\n+        normalization = keras.layers.Normalization()\n+        outputs = normalization(inputs)\n+\n+        model = keras.Model(inputs, outputs)\n+\n+        x = np.random.random((1, 32))\n+        normalization.adapt(x)\n+        ref_out = model(x)\n+\n+        model.save(temp_filepath)\n+        new_model = keras.saving.load_model(temp_filepath)\n+        out = new_model(x)\n+        self.assertAllClose(ref_out, out, atol=1e-6)\n+\n     def test_legacy_h5_format(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"custom_model.h5\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4a51168bfedf69a9aae7ddff289277972dfd85d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 142 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -407,8 +407,8 @@ def load_weights_from_hdf5_group(f, model):\n def _set_weights(\n     instance, symbolic_weights, weight_values, name, skip_mismatch=False\n ):\n-    \"\"\"Safely set weights into a model or a layer,\n-       calling load_own_variables if implemented.\n+    \"\"\"Safely set weights into a model or a layer.\n+\n     Args:\n         instance: Model or layer instance,\n         symbolic_weights: symbolic tensors representing\n@@ -417,6 +417,7 @@ def _set_weights(\n         skip_mismatch: Boolean, whether to skip loading of weights\n             where there is a mismatch in the shape of the weights,\n         name: name used to identify the group.\n+\n     Raises:\n         ValueError: in case of mismatch between provided\n             model/layer and weights.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2797475aa89b35c3dafc11bc95365ff71d31ce5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 9 | Churn Cumulative: 67 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -190,6 +190,14 @@ def make_node(layer, **kwargs):\n     return node\n \n \n+def remove_unused_edges(dot):\n+    nodes = [v.get_name() for v in dot.get_nodes()]\n+    for edge in dot.get_edges():\n+        if edge.get_destination() not in nodes:\n+            dot.del_edge(edge.get_source(), edge.get_destination())\n+    return dot\n+\n+\n @keras_export(\"keras.utils.model_to_dot\")\n def model_to_dot(\n     model,\n@@ -460,6 +468,7 @@ def plot_model(\n     to_file = str(to_file)\n     if dot is None:\n         return\n+    dot = remove_unused_edges(dot)\n     _, extension = os.path.splitext(to_file)\n     if not extension:\n         extension = \"png\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#19ba3d1dd6079c4b72cc68b8ae969f5da3e09ea5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 20 | Files Changed: 36 | Hunks: 70 | Methods Changed: 53 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 105 | Churn Cumulative: 844 | Contributors (this commit): 9 | Commits (past 90d): 54 | Contributors (cumulative): 83 | DMM Complexity: 0.8166666666666667\n\nDIFF:\n@@ -8,6 +8,12 @@ from keras.src.ops.operation import Operation\n \n \n class TorchLayer(torch.nn.Module):\n+    @property\n+    def torch_params(self):\n+        if not hasattr(self, \"_torch_params\"):\n+            self._track_variables()\n+        return self._torch_params\n+\n     def _post_build(self):\n         # Do not track variables when in a stateless scope.\n         # The variables are not initialized.\n@@ -18,7 +24,7 @@ class TorchLayer(torch.nn.Module):\n     def _track_variables(self):\n         # set torch_params attribute will have module automatically track\n         # parameters.\n-        self.torch_params = torch.nn.ParameterDict(\n+        self._torch_params = torch.nn.ParameterDict(\n             {variable.path: variable.value for variable in self.variables}\n         )\n \n@@ -28,7 +34,7 @@ class TorchLayer(torch.nn.Module):\n         recurse: bool = True,\n         remove_duplicate: bool = True,\n     ) -> Iterator[Tuple[str, torch.nn.Parameter]]:\n-        if not hasattr(self, \"torch_params\"):\n+        if not hasattr(self, \"_torch_params\"):\n             self._track_variables()\n         return torch.nn.Module.named_parameters(\n             self, prefix, recurse, remove_duplicate\n@@ -43,7 +49,7 @@ class TorchLayer(torch.nn.Module):\n         if (\n             isinstance(value, torch.nn.Module)\n             and not isinstance(value, Layer)\n-            and not name == \"torch_params\"\n+            and not name == \"_torch_params\"\n         ):\n             from keras.src.utils.torch_utils import TorchModuleWrapper\n \n@@ -52,11 +58,11 @@ class TorchLayer(torch.nn.Module):\n         return name, value\n \n     def _post_track_variable(self, variable):\n-        if hasattr(self, \"torch_params\"):\n+        if hasattr(self, \"_torch_params\"):\n             if variable.path not in self.torch_params:\n                 self.torch_params[variable.path] = variable.value\n \n     def _post_untrack_variable(self, variable):\n-        if hasattr(self, \"torch_params\"):\n+        if hasattr(self, \"_torch_params\"):\n             if variable.path in self.torch_params:\n                 self.torch_params.pop(variable.path)\n\n@@ -26,6 +26,7 @@ class Activation(Layer):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n         self.activation = activations.get(activation)\n+        self.built = True\n \n     def call(self, inputs):\n         return self.activation(inputs)\n\n@@ -20,6 +20,7 @@ class ActivationTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n         self.run_layer_test(\n             layers.Activation,\n@@ -33,4 +34,5 @@ class ActivationTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n\n@@ -23,6 +23,7 @@ class ELU(Layer):\n         super().__init__(**kwargs)\n         self.alpha = alpha\n         self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs):\n         return activations.elu(inputs, alpha=self.alpha)\n\n@@ -17,6 +17,7 @@ class ELUTest(testing.TestCase):\n             init_kwargs={},\n             input_shape=(2, 3, 4),\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_correctness(self):\n\n@@ -49,8 +49,9 @@ class LeakyReLU(Layer):\n                 \"cannot be None or negative value. Expected a float.\"\n                 f\" Received: negative_slope={negative_slope}\"\n             )\n-        self.supports_masking = True\n         self.negative_slope = negative_slope\n+        self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs):\n         return activations.leaky_relu(\n\n@@ -15,6 +15,7 @@ class LeakyReLUTest(testing.TestCase):\n             },\n             input_shape=(2, 3, 4),\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_leaky_relu_correctness(self):\n\n@@ -57,10 +57,11 @@ class ReLU(Layer):\n                 f\"value. Received: threshold={threshold}\"\n             )\n \n-        self.supports_masking = True\n         self.max_value = max_value\n         self.negative_slope = negative_slope\n         self.threshold = threshold\n+        self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs):\n         return activations.relu(\n\n@@ -17,6 +17,7 @@ class ReLUTest(testing.TestCase):\n             },\n             input_shape=(2, 3, 4),\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_normal_relu_correctness(self):\n\n@@ -44,8 +44,9 @@ class Softmax(Layer):\n \n     def __init__(self, axis=-1, **kwargs):\n         super().__init__(**kwargs)\n-        self.supports_masking = True\n         self.axis = axis\n+        self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs, mask=None):\n         if mask is not None:\n\n@@ -13,6 +13,7 @@ class SoftmaxTest(testing.TestCase):\n             init_kwargs={},\n             input_shape=(2, 3, 4),\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_softmax_correctness(self):\n\n@@ -15,6 +15,7 @@ class Identity(Layer):\n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs):\n         return inputs\n\n@@ -30,4 +30,5 @@ class IdentityTest(testing.TestCase, parameterized.TestCase):\n             expected_num_losses=0,\n             run_training_check=not sparse,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n\n@@ -45,8 +45,9 @@ class Masking(Layer):\n \n     def __init__(self, mask_value=0.0, **kwargs):\n         super().__init__(**kwargs)\n-        self.supports_masking = True\n         self.mask_value = mask_value\n+        self.supports_masking = True\n+        self.built = True\n \n     def compute_mask(self, inputs, mask=None):\n         return ops.any(ops.not_equal(inputs, self.mask_value), axis=-1)\n\n@@ -19,6 +19,7 @@ class MaskingTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     @pytest.mark.requires_trainable_backend\n\n@@ -1102,7 +1102,7 @@ class LayerTest(testing.TestCase, parameterized.TestCase):\n         self.assertTrue(\"inner_inner_layer/inner\" in variable_paths)\n         if backend.backend() == \"torch\":\n             parameter_names = set(\n-                param_name.replace(\"torch_params.\", \"\")\n+                param_name.replace(\"_torch_params.\", \"\")\n                 for param_name, _ in layer.named_parameters()\n             )\n             self.assertSetEqual(variable_paths, parameter_names)\n@@ -1159,7 +1159,7 @@ class LayerTest(testing.TestCase, parameterized.TestCase):\n         )\n         if backend.backend() == \"torch\":\n             parameter_names = set(\n-                param_name.replace(\"torch_params.\", \"\")\n+                param_name.replace(\"_torch_params.\", \"\")\n                 for param_name, _ in layer.named_parameters()\n             )\n             self.assertSetEqual(variable_paths, parameter_names)\n@@ -1212,7 +1212,7 @@ class LayerTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(len(parameter_names), 1)\n             self.assertEqual(\n                 parameter_names[0],\n-                \"torch_params.training_layer/post_build_modify_layer/var\",\n+                \"_torch_params.training_layer/post_build_modify_layer/var\",\n             )\n \n         layer.post_build_modify_layer.post_build_add()\n@@ -1234,7 +1234,7 @@ class LayerTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(len(parameter_names), 1)\n             self.assertEqual(\n                 parameter_names[0],\n-                \"torch_params.training_layer/post_build_modify_layer/var\",\n+                \"_torch_params.training_layer/post_build_modify_layer/var\",\n             )\n \n             parameter_names = [\n@@ -1244,11 +1244,11 @@ class LayerTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(len(parameter_names), 2)\n             self.assertEqual(\n                 parameter_names[0],\n-                \"torch_params.training_layer/post_build_modify_layer/var\",\n+                \"_torch_params.training_layer/post_build_modify_layer/var\",\n             )\n             self.assertEqual(\n                 parameter_names[1],\n-                \"torch_params.training_layer/post_build_modify_layer/var2\",\n+                \"_torch_params.training_layer/post_build_modify_layer/var2\",\n             )\n \n         layer.post_build_modify_layer.post_build_remove()\n@@ -1267,12 +1267,12 @@ class LayerTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(len(parameter_names), 2)\n             self.assertEqual(\n                 parameter_names[0],\n-                \"post_build_modify_layer.torch_params.training_layer/\"\n+                \"post_build_modify_layer._torch_params.training_layer/\"\n                 \"post_build_modify_layer/var2\",\n             )\n             self.assertEqual(\n                 parameter_names[1],\n-                \"torch_params.training_layer/post_build_modify_layer/var\",\n+                \"_torch_params.training_layer/post_build_modify_layer/var\",\n             )\n \n             parameter_names = [\n@@ -1282,7 +1282,7 @@ class LayerTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(len(parameter_names), 1)\n             self.assertEqual(\n                 parameter_names[0],\n-                \"torch_params.training_layer/post_build_modify_layer/var2\",\n+                \"_torch_params.training_layer/post_build_modify_layer/var2\",\n             )\n \n     @pytest.mark.skipif(backend.backend() != \"torch\", reason=\"Torch only test.\")\n\n@@ -37,8 +37,6 @@ class UnitNormalization(Layer):\n                 f\"Received: axis={axis}\"\n             )\n         self.supports_masking = True\n-\n-    def build(self, input_shape):\n         self.built = True\n \n     def call(self, inputs):\n\n@@ -20,6 +20,7 @@ class UnitNormalizationTest(testing.TestCase):\n             input_shape=(2, 3),\n             expected_output_shape=(2, 3),\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n         self.run_layer_test(\n             layers.UnitNormalization,\n@@ -27,6 +28,7 @@ class UnitNormalizationTest(testing.TestCase):\n             input_shape=(1, 3, 3),\n             expected_output_shape=(1, 3, 3),\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_invalid_axis(self):\n\n@@ -164,6 +164,7 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            assert_built_after_instantiation=True,\n         )\n \n     @parameterized.parameters(\n@@ -197,6 +198,7 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            assert_built_after_instantiation=True,\n         )\n \n     @parameterized.parameters(\n@@ -236,6 +238,7 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n             # Incomplete op support on tensorflow.\n             run_mixed_precision_check=False,\n+            assert_built_after_instantiation=True,\n         )\n \n \n\n@@ -14,6 +14,7 @@ class BaseGlobalPooling(Layer):\n         self.data_format = backend.standardize_data_format(data_format)\n         self.keepdims = keepdims\n         self.input_spec = InputSpec(ndim=pool_dimensions + 2)\n+        self.built = True\n \n     def call(self, inputs):\n         raise NotImplementedError\n\n@@ -34,6 +34,7 @@ class BasePooling(Layer):\n         self.data_format = backend.standardize_data_format(data_format)\n \n         self.input_spec = InputSpec(ndim=pool_dimensions + 2)\n+        self.built = True\n \n     def call(self, inputs):\n         if self.pool_mode == \"max\":\n\n@@ -32,6 +32,7 @@ class GlobalAveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     @parameterized.parameters(\n@@ -58,6 +59,7 @@ class GlobalAveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            assert_built_after_instantiation=True,\n         )\n \n     @parameterized.parameters(\n@@ -84,6 +86,7 @@ class GlobalAveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            assert_built_after_instantiation=True,\n         )\n \n \n\n@@ -32,6 +32,7 @@ class GlobalMaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            assert_built_after_instantiation=True,\n         )\n \n     @parameterized.parameters(\n@@ -58,6 +59,7 @@ class GlobalMaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            assert_built_after_instantiation=True,\n         )\n \n     @parameterized.parameters(\n@@ -84,6 +86,7 @@ class GlobalMaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            assert_built_after_instantiation=True,\n         )\n \n \n\n@@ -163,6 +163,7 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            assert_built_after_instantiation=True,\n         )\n \n     @parameterized.parameters(\n@@ -193,6 +194,7 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            assert_built_after_instantiation=True,\n         )\n \n     @parameterized.parameters(\n@@ -232,6 +234,7 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n             # Incomplete op support on tensorflow.\n             run_mixed_precision_check=False,\n+            assert_built_after_instantiation=True,\n         )\n \n \n\n@@ -27,6 +27,7 @@ class ActivityRegularization(Layer):\n         self.supports_masking = True\n         self.l1 = l1\n         self.l2 = l2\n+        self.built = True\n \n     def call(self, inputs):\n         return inputs\n\n@@ -25,4 +25,5 @@ class ActivityRegularizationTest(test_case.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=1,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n\n@@ -46,6 +46,7 @@ class AlphaDropout(Layer):\n         if rate > 0:\n             self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs, training=False):\n         if training and self.rate > 0:\n\n@@ -22,6 +22,7 @@ class AlphaDropoutTest(testing.TestCase):\n             expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_alpha_dropout_correctness(self):\n\n@@ -52,6 +52,7 @@ class Dropout(Layer):\n         if rate > 0:\n             self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs, training=False):\n         if training and self.rate > 0:\n\n@@ -22,6 +22,7 @@ class DropoutTest(testing.TestCase):\n             expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_dropout_rescaling(self):\n\n@@ -37,6 +37,7 @@ class GaussianDropout(layers.Layer):\n         if rate > 0:\n             self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs, training=False):\n         if training and self.rate > 0:\n\n@@ -22,6 +22,7 @@ class GaussianDropoutTest(testing.TestCase):\n             expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_gaussian_dropout_correctness(self):\n\n@@ -38,6 +38,7 @@ class GaussianNoise(layers.Layer):\n         if stddev > 0:\n             self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n+        self.built = True\n \n     def call(self, inputs, training=False):\n         if training and self.stddev > 0:\n\n@@ -22,6 +22,7 @@ class GaussianNoiseTest(testing.TestCase):\n             expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_gaussian_noise_correctness(self):\n\n@@ -14,6 +14,7 @@ class SpatialDropoutTest(test_case.TestCase):\n             init_kwargs={\"rate\": 0.5},\n             call_kwargs={\"training\": True},\n             input_shape=(2, 3, 4),\n+            assert_built_after_instantiation=True,\n         )\n \n         self.run_layer_test(\n@@ -21,6 +22,7 @@ class SpatialDropoutTest(test_case.TestCase):\n             init_kwargs={\"rate\": 0.5},\n             call_kwargs={\"training\": False},\n             input_shape=(2, 3, 4),\n+            assert_built_after_instantiation=True,\n         )\n \n     @pytest.mark.requires_trainable_backend\n@@ -30,6 +32,7 @@ class SpatialDropoutTest(test_case.TestCase):\n             init_kwargs={\"rate\": 0.5},\n             call_kwargs={\"training\": True},\n             input_shape=(2, 3, 4, 5),\n+            assert_built_after_instantiation=True,\n         )\n \n         self.run_layer_test(\n@@ -37,6 +40,7 @@ class SpatialDropoutTest(test_case.TestCase):\n             init_kwargs={\"rate\": 0.5, \"data_format\": \"channels_first\"},\n             call_kwargs={\"training\": True},\n             input_shape=(2, 3, 4, 5),\n+            assert_built_after_instantiation=True,\n         )\n \n     @pytest.mark.requires_trainable_backend\n@@ -46,6 +50,7 @@ class SpatialDropoutTest(test_case.TestCase):\n             init_kwargs={\"rate\": 0.5},\n             call_kwargs={\"training\": True},\n             input_shape=(2, 3, 4, 4, 5),\n+            assert_built_after_instantiation=True,\n         )\n \n         self.run_layer_test(\n@@ -53,6 +58,7 @@ class SpatialDropoutTest(test_case.TestCase):\n             init_kwargs={\"rate\": 0.5, \"data_format\": \"channels_first\"},\n             call_kwargs={\"training\": True},\n             input_shape=(2, 3, 4, 4, 5),\n+            assert_built_after_instantiation=True,\n         )\n \n     def test_spatial_dropout_1D_dynamic(self):\n\n@@ -174,6 +174,7 @@ class TestCase(unittest.TestCase):\n         custom_objects=None,\n         run_training_check=True,\n         run_mixed_precision_check=True,\n+        assert_built_after_instantiation=False,\n     ):\n         \"\"\"Run basic checks on a layer.\n \n@@ -216,6 +217,8 @@ class TestCase(unittest.TestCase):\n                 (if an input shape or input data was provided).\n             run_mixed_precision_check: Whether to test the layer with a mixed\n                 precision dtype policy.\n+            assert_built_after_instantiation: Whether to assert `built=True`\n+                after the layer's instantiation.\n         \"\"\"\n         if input_shape is not None and input_data is not None:\n             raise ValueError(\n@@ -551,6 +554,17 @@ class TestCase(unittest.TestCase):\n                 output_mask = layer.compute_mask(keras_tensor_inputs)\n                 self.assertEqual(expected_mask_shape, output_mask.shape)\n \n+            # The stateless layers should be built after instantiation.\n+            if assert_built_after_instantiation:\n+                layer = layer_cls(**init_kwargs)\n+                self.assertTrue(\n+                    layer.built,\n+                    msg=(\n+                        f\"{type(layer)} is stateless, so it should be built \"\n+                        \"after instantiation.\"\n+                    ),\n+                )\n+\n         # Eager call test and compiled training test.\n         if input_data is not None or input_shape is not None:\n             if input_data is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f6d71565bd319a22eeb9e0a6eb9369193c97afc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 189 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,11 +13,11 @@ from keras.src.layers.layer import Layer\n \n @keras_export(\"keras.layers.Embedding\")\n class Embedding(Layer):\n-    \"\"\"Turns positive integers (indexes) into dense vectors of fixed size.\n+    \"\"\"Turns nonnegative integers (indexes) into dense vectors of fixed size.\n \n     e.g. `[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]`\n \n-    This layer can only be used on positive integer inputs of a fixed range.\n+    This layer can only be used on nonnegative integer inputs of a fixed range.\n \n     Example:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c71314cfa51e462a3a7ebcbd27dc52d8b788bc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 27 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,12 +60,12 @@ def load_data():\n     assert y_test.shape == (10000, 1)\n     ```\n     \"\"\"\n-    dirname = \"cifar-10-batches-py\"\n+    dirname = \"cifar-10-batches-py-target\"\n     origin = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n     path = get_file(\n         fname=dirname,\n         origin=origin,\n-        untar=True,\n+        extract=True,\n         file_hash=(  # noqa: E501\n             \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n         ),\n@@ -76,6 +76,8 @@ def load_data():\n     x_train = np.empty((num_train_samples, 3, 32, 32), dtype=\"uint8\")\n     y_train = np.empty((num_train_samples,), dtype=\"uint8\")\n \n+    # batches are within an inner folder\n+    path = os.path.join(path, \"cifar-10-batches-py\")\n     for i in range(1, 6):\n         fpath = os.path.join(path, \"data_batch_\" + str(i))\n         (\n\n@@ -58,17 +58,18 @@ def load_data(label_mode=\"fine\"):\n             f\"Received: label_mode={label_mode}.\"\n         )\n \n-    dirname = \"cifar-100-python\"\n+    dirname = \"cifar-100-python-target\"\n     origin = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n     path = get_file(\n         fname=dirname,\n         origin=origin,\n-        untar=True,\n+        extract=True,\n         file_hash=(  # noqa: E501\n             \"85cd44d02ba6437773c5bbd22e183051d648de2e7d6b014e1ef29b855ba677a7\"\n         ),\n     )\n \n+    path = os.path.join(path, \"cifar-100-python\")\n     fpath = os.path.join(path, \"train\")\n     x_train, y_train = load_batch(fpath, label_key=label_mode + \"_labels\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#62472e7a7d659c25ebedc0ba1a178fc022f30bc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 34 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -470,7 +470,7 @@ Args:\n             be applied.\n     classes: optional number of classes to classify images\n         into, only to be specified if `include_top` is `True`, and\n-        if no `weights` argument is specified.\n+        if no `weights` argument is specified. Defaults to 1000.\n     classifier_activation: A `str` or callable.\n         The activation function to use\n         on the \"top\" layer. Ignored unless `include_top=True`. Set\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
