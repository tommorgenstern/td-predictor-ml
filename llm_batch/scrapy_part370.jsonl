{"custom_id": "scrapy#7001193c802029612542ab7a30fa8c0e147a1894", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 20 | Churn Cumulative: 2933 | Contributors (this commit): 30 | Commits (past 90d): 10 | Contributors (cumulative): 50 | DMM Complexity: 0.0\n\nDIFF:\n@@ -143,21 +143,13 @@ class RequestFingerprinter:\n         else:\n             implementation = \"SENTINEL\"\n \n-        if implementation == \"SENTINEL\":\n-            self._fingerprint = fingerprint\n-        elif implementation == \"2.7\":\n+        if implementation != \"SENTINEL\":\n             message = (\n                 \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' is a deprecated setting.\\n\"\n                 \"And it will be removed in future version of Scrapy.\"\n             )\n             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n         self._fingerprint = fingerprint\n-        else:\n-            raise ValueError(\n-                f\"Got an invalid value on setting \"\n-                f\"'REQUEST_FINGERPRINTER_IMPLEMENTATION': \"\n-                f\"{implementation!r}. Valid values are '2.7' and 'SENTINEL'.\"\n-            )\n \n     def fingerprint(self, request: Request) -> bytes:\n         return self._fingerprint(request)\n\n@@ -76,6 +76,7 @@ class TestSpider(Spider):\n def get_crawler(\n     spidercls: Optional[Type[Spider]] = None,\n     settings_dict: Optional[Dict[str, Any]] = None,\n+    prevent_warnings: bool = True,\n ) -> Crawler:\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n@@ -85,6 +86,8 @@ def get_crawler(\n \n     # Set by default settings that prevent deprecation warnings.\n     settings: Dict[str, Any] = {}\n+    if prevent_warnings:\n+        pass\n     settings.update(settings_dict or {})\n     runner = CrawlerRunner(settings)\n     crawler = runner.create_crawler(spidercls or TestSpider)\n\n@@ -277,13 +277,6 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n         )\n         self.assertTrue(logged_warnings)\n \n-    def test_unknown_implementation(self):\n-        settings = {\n-            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.5\",\n-        }\n-        with self.assertRaises(ValueError):\n-            get_crawler(settings_dict=settings)\n-\n \n class CustomRequestFingerprinterTestCase(unittest.TestCase):\n     def test_include_headers(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53ccf0016d99e54adec6f98236cce37ede835f63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 510 | Contributors (this commit): 20 | Commits (past 90d): 3 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -86,8 +86,6 @@ def get_crawler(\n \n     # Set by default settings that prevent deprecation warnings.\n     settings: Dict[str, Any] = {}\n-    if prevent_warnings:\n-        pass\n     settings.update(settings_dict or {})\n     runner = CrawlerRunner(settings)\n     crawler = runner.create_crawler(spidercls or TestSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c5dad41190551578c2973c34520952f26f75dc7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 65 | Churn Cumulative: 967 | Contributors (this commit): 19 | Commits (past 90d): 8 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,6 +74,18 @@ def response_httprepr(response: Response) -> bytes:\n     return b\"\".join(values)\n \n \n+def _remove_html_comments(body):\n+    start = body.find(b\"<!--\")\n+    while start != -1:\n+        end = body.find(b\"-->\", start + 1)\n+        if end == -1:\n+            return body[:start]\n+        else:\n+            body = body[:start] + body[end + 3 :]\n+            start = body.find(b\"<!--\")\n+    return body\n+\n+\n def open_in_browser(\n     response: Union[\n         \"scrapy.http.response.html.HtmlResponse\",\n@@ -103,8 +115,8 @@ def open_in_browser(\n     body = response.body\n     if isinstance(response, HtmlResponse):\n         if b\"<base\" not in body:\n+            _remove_html_comments(body)\n             repl = rf'\\0<base href=\"{response.url}\">'\n-            body = re.sub(b\"(?s)<!--.*?(?:-->|$)\", b\"\", body)\n             body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)\n         ext = \".html\"\n     elif isinstance(response, TextResponse):\n\n@@ -8,9 +8,9 @@ import pytest\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, Response, TextResponse\n-from scrapy.settings.default_settings import DOWNLOAD_MAXSIZE\n from scrapy.utils.python import to_bytes\n from scrapy.utils.response import (\n+    _remove_html_comments,\n     get_base_url,\n     get_meta_refresh,\n     open_in_browser,\n@@ -203,14 +203,13 @@ class ResponseUtilsTest(unittest.TestCase):\n             r5, _openfunc=check_base_url\n         ), \"Inject unique base url with conditional comment\"\n \n-    @pytest.mark.slow\n     def test_open_in_browser_redos_comment(self):\n-        MAX_CPU_TIME = 30\n+        MAX_CPU_TIME = 0.001\n \n         # Exploit input from\n         # https://makenowjust-labs.github.io/recheck/playground/\n         # for /<!--.*?-->/ (old pattern to remove comments).\n-        body = b\"-><!--\\x00\" * (int(DOWNLOAD_MAXSIZE / 7) - 10) + b\"->\\n<!---->\"\n+        body = b\"-><!--\\x00\" * 25_000 + b\"->\\n<!---->\"\n \n         response = HtmlResponse(\"https://example.com\", body=body)\n \n@@ -221,14 +220,13 @@ class ResponseUtilsTest(unittest.TestCase):\n         end_time = process_time()\n         self.assertLess(end_time - start_time, MAX_CPU_TIME)\n \n-    @pytest.mark.slow\n     def test_open_in_browser_redos_head(self):\n-        MAX_CPU_TIME = 15\n+        MAX_CPU_TIME = 0.001\n \n         # Exploit input from\n         # https://makenowjust-labs.github.io/recheck/playground/\n         # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).\n-        body = b\"<head\\t\" * int(DOWNLOAD_MAXSIZE / 6)\n+        body = b\"<head\\t\" * 8_000\n \n         response = HtmlResponse(\"https://example.com\", body=body)\n \n@@ -238,3 +236,42 @@ class ResponseUtilsTest(unittest.TestCase):\n \n         end_time = process_time()\n         self.assertLess(end_time - start_time, MAX_CPU_TIME)\n+\n+\n+@pytest.mark.parametrize(\n+    \"input_body,output_body\",\n+    (\n+        (\n+            b\"a<!--\",\n+            b\"a\",\n+        ),\n+        (\n+            b\"a<!---->b\",\n+            b\"ab\",\n+        ),\n+        (\n+            b\"a<!--b-->c\",\n+            b\"ac\",\n+        ),\n+        (\n+            b\"a<!--b-->c<!--\",\n+            b\"ac\",\n+        ),\n+        (\n+            b\"a<!--b-->c<!--d\",\n+            b\"ac\",\n+        ),\n+        (\n+            b\"a<!--b-->c<!---->d\",\n+            b\"acd\",\n+        ),\n+        (\n+            b\"a<!--b--><!--c-->d\",\n+            b\"ad\",\n+        ),\n+    ),\n+)\n+def test_remove_html_comments(input_body, output_body):\n+    assert (\n+        _remove_html_comments(input_body) == output_body\n+    ), f\"{_remove_html_comments(input_body)=} == {output_body=}\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f1d345e74d19e33016f9e69fcda0bda9afb568d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 28 | Churn Cumulative: 1486 | Contributors (this commit): 21 | Commits (past 90d): 13 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -402,6 +402,10 @@ class HttpCompressionTest(TestCase):\n         self._test_compression_bomb_setting(\"gzip\")\n \n     def test_compression_bomb_setting_zstd(self):\n+        try:\n+            import zstandard  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no zstd support (zstandard)\")\n         self._test_compression_bomb_setting(\"zstd\")\n \n     def _test_compression_bomb_spider_attr(self, compression_id):\n@@ -436,6 +440,10 @@ class HttpCompressionTest(TestCase):\n         self._test_compression_bomb_spider_attr(\"gzip\")\n \n     def test_compression_bomb_spider_attr_zstd(self):\n+        try:\n+            import zstandard  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no zstd support (zstandard)\")\n         self._test_compression_bomb_spider_attr(\"zstd\")\n \n     def _test_compression_bomb_request_meta(self, compression_id):\n@@ -468,6 +476,10 @@ class HttpCompressionTest(TestCase):\n         self._test_compression_bomb_request_meta(\"gzip\")\n \n     def test_compression_bomb_request_meta_zstd(self):\n+        try:\n+            import zstandard  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no zstd support (zstandard)\")\n         self._test_compression_bomb_request_meta(\"zstd\")\n \n     def _test_download_warnsize_setting(self, compression_id):\n@@ -510,6 +522,10 @@ class HttpCompressionTest(TestCase):\n         self._test_download_warnsize_setting(\"gzip\")\n \n     def test_download_warnsize_setting_zstd(self):\n+        try:\n+            import zstandard  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no zstd support (zstandard)\")\n         self._test_download_warnsize_setting(\"zstd\")\n \n     def _test_download_warnsize_spider_attr(self, compression_id):\n@@ -554,6 +570,10 @@ class HttpCompressionTest(TestCase):\n         self._test_download_warnsize_spider_attr(\"gzip\")\n \n     def test_download_warnsize_spider_attr_zstd(self):\n+        try:\n+            import zstandard  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no zstd support (zstandard)\")\n         self._test_download_warnsize_spider_attr(\"zstd\")\n \n     def _test_download_warnsize_request_meta(self, compression_id):\n@@ -596,6 +616,10 @@ class HttpCompressionTest(TestCase):\n         self._test_download_warnsize_request_meta(\"gzip\")\n \n     def test_download_warnsize_request_meta_zstd(self):\n+        try:\n+            import zstandard  # noqa: F401\n+        except ImportError:\n+            raise SkipTest(\"no zstd support (zstandard)\")\n         self._test_download_warnsize_request_meta(\"zstd\")\n \n \n\n@@ -204,7 +204,7 @@ class ResponseUtilsTest(unittest.TestCase):\n         ), \"Inject unique base url with conditional comment\"\n \n     def test_open_in_browser_redos_comment(self):\n-        MAX_CPU_TIME = 0.001\n+        MAX_CPU_TIME = 0.02\n \n         # Exploit input from\n         # https://makenowjust-labs.github.io/recheck/playground/\n@@ -221,7 +221,7 @@ class ResponseUtilsTest(unittest.TestCase):\n         self.assertLess(end_time - start_time, MAX_CPU_TIME)\n \n     def test_open_in_browser_redos_head(self):\n-        MAX_CPU_TIME = 0.001\n+        MAX_CPU_TIME = 0.02\n \n         # Exploit input from\n         # https://makenowjust-labs.github.io/recheck/playground/\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bccb4cf18ba38c8bf09d61d19e0ffabaf15554b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 418 | Contributors (this commit): 20 | Commits (past 90d): 1 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -248,5 +248,5 @@ class LxmlLinkExtractor:\n             links = self._extract_links(doc, response.url, response.encoding, base_url)\n             all_links.extend(self._process_links(links))\n         if self.link_extractor.unique:\n-            return unique_list(all_links)\n+            return unique_list(all_links, key=self.link_extractor.link_key)\n         return all_links\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9bb973dc54766a0f8d10eca0947d11f195c1a1be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 106 | Churn Cumulative: 135 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,10 @@ logger = logging.getLogger(__name__)\n \n \n class LogStats:\n-    \"\"\"Log basic scraping stats periodically\"\"\"\n+    \"\"\"Log basic scraping stats periodically like:\n+    * RPM - Requests per Minute\n+    * IPM - Items per Minute\n+    \"\"\"\n \n     def __init__(self, stats, interval=60.0):\n         self.stats = stats\n@@ -35,24 +38,45 @@ class LogStats:\n         self.task.start(self.interval)\n \n     def log(self, spider):\n-        items = self.stats.get_value(\"item_scraped_count\", 0)\n-        pages = self.stats.get_value(\"response_received_count\", 0)\n-        irate = (items - self.itemsprev) * self.multiplier\n-        prate = (pages - self.pagesprev) * self.multiplier\n-        self.pagesprev, self.itemsprev = pages, items\n+        self.calculate_stats()\n \n         msg = (\n             \"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n             \"scraped %(items)d items (at %(itemrate)d items/min)\"\n         )\n         log_args = {\n-            \"pages\": pages,\n-            \"pagerate\": prate,\n-            \"items\": items,\n-            \"itemrate\": irate,\n+            \"pages\": self.pages,\n+            \"pagerate\": self.prate,\n+            \"items\": self.items,\n+            \"itemrate\": self.irate,\n         }\n         logger.info(msg, log_args, extra={\"spider\": spider})\n \n+    def calculate_stats(self):\n+        self.items = self.stats.get_value(\"item_scraped_count\", 0)\n+        self.pages = self.stats.get_value(\"response_received_count\", 0)\n+        self.irate = (self.items - self.itemsprev) * self.multiplier\n+        self.prate = (self.pages - self.pagesprev) * self.multiplier\n+        self.pagesprev, self.itemsprev = self.pages, self.items\n+\n     def spider_closed(self, spider, reason):\n         if self.task and self.task.running:\n             self.task.stop()\n+\n+        rpm_final, ipm_final = self.calculate_final_stats(spider)\n+        self.stats.set_value(\"responses_per_minute\", rpm_final)\n+        self.stats.set_value(\"items_per_minute\", ipm_final)\n+\n+    def calculate_final_stats(self, spider):\n+        start_time = self.stats.get_value(\"start_time\")\n+        finished_time = self.stats.get_value(\"finished_time\")\n+\n+        if not start_time or not finished_time:\n+            return None, None\n+\n+        mins_elapsed = (finished_time - start_time).seconds / 60\n+\n+        items = self.stats.get_value(\"item_scraped_count\", 0)\n+        pages = self.stats.get_value(\"response_received_count\", 0)\n+\n+        return (pages / mins_elapsed), (items / mins_elapsed)\n\n@@ -0,0 +1,62 @@\n+import unittest\n+from datetime import datetime\n+\n+from scrapy.extensions.logstats import LogStats\n+from scrapy.utils.test import get_crawler\n+from tests.spiders import SimpleSpider\n+\n+\n+class TestLogStats(unittest.TestCase):\n+    def setUp(self):\n+        self.crawler = get_crawler(SimpleSpider)\n+        self.spider = self.crawler._create_spider(\"spidey\")\n+        self.stats = self.crawler.stats\n+\n+        self.stats.set_value(\"response_received_count\", 4802)\n+        self.stats.set_value(\"item_scraped_count\", 3201)\n+\n+    def test_stats_calculations(self):\n+        logstats = LogStats.from_crawler(self.crawler)\n+\n+        with self.assertRaises(AttributeError):\n+            logstats.pagesprev\n+            logstats.itemsprev\n+\n+        logstats.spider_opened(self.spider)\n+        self.assertEqual(logstats.pagesprev, 4802)\n+        self.assertEqual(logstats.itemsprev, 3201)\n+\n+        logstats.calculate_stats()\n+        self.assertEqual(logstats.items, 3201)\n+        self.assertEqual(logstats.pages, 4802)\n+        self.assertEqual(logstats.irate, 0.0)\n+        self.assertEqual(logstats.prate, 0.0)\n+        self.assertEqual(logstats.pagesprev, 4802)\n+        self.assertEqual(logstats.itemsprev, 3201)\n+\n+        # Simulate what happens after a minute\n+        self.stats.set_value(\"response_received_count\", 5187)\n+        self.stats.set_value(\"item_scraped_count\", 3492)\n+        logstats.calculate_stats()\n+        self.assertEqual(logstats.items, 3492)\n+        self.assertEqual(logstats.pages, 5187)\n+        self.assertEqual(logstats.irate, 291.0)\n+        self.assertEqual(logstats.prate, 385.0)\n+        self.assertEqual(logstats.pagesprev, 5187)\n+        self.assertEqual(logstats.itemsprev, 3492)\n+\n+        # Simulate when spider closes after running for 30 mins\n+        self.stats.set_value(\"start_time\", datetime.fromtimestamp(1655100172))\n+        self.stats.set_value(\"finished_time\", datetime.fromtimestamp(1655101972))\n+        logstats.spider_closed(self.spider, \"test reason\")\n+        self.assertEqual(self.stats.get_value(\"responses_per_minute\"), 172.9)\n+        self.assertEqual(self.stats.get_value(\"items_per_minute\"), 116.4)\n+\n+    def test_stats_calculations_no_time(self):\n+        \"\"\"The stat values should be None since the start and finish time are\n+        not available.\n+        \"\"\"\n+        logstats = LogStats.from_crawler(self.crawler)\n+        logstats.spider_closed(self.spider, \"test reason\")\n+        self.assertIsNone(self.stats.get_value(\"responses_per_minute\"))\n+        self.assertIsNone(self.stats.get_value(\"items_per_minute\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e51417a485f296354e9639f85fb0b51a4a3e533", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 114 | Churn Cumulative: 2202 | Contributors (this commit): 22 | Commits (past 90d): 3 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -153,7 +153,7 @@ class LxmlLinkExtractor:\n             unique=unique,\n             process=process_value,\n             strip=strip,\n-            canonicalized=canonicalize,\n+            canonicalized=not canonicalize,\n         )\n         self.allow_res = [\n             x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)\n\n@@ -745,6 +745,118 @@ class Base:\n             lx = self.extractor_cls()\n             self.assertIsInstance(pickle.loads(pickle.dumps(lx)), self.extractor_cls)\n \n+        def test_link_extractor_aggregation(self):\n+            \"\"\"When a parameter like restrict_css is used, the underlying\n+            implementation calls its internal link extractor once per selector\n+            matching the specified restrictions, and then aggregates the\n+            extracted links.\n+\n+            Test that aggregation respects the unique and canonicalize\n+            parameters.\n+            \"\"\"\n+            # unique=True (default), canonicalize=False (default)\n+            lx = self.extractor_cls(restrict_css=(\"div\",))\n+            response = HtmlResponse(\n+                \"https://example.com\",\n+                body=b\"\"\"\n+                    <div>\n+                        <a href=\"/a\">a1</a>\n+                        <a href=\"/b?a=1&b=2\">b1</a>\n+                    </div>\n+                    <div>\n+                        <a href=\"/a\">a2</a>\n+                        <a href=\"/b?b=2&a=1\">b2</a>\n+                    </div>\n+                \"\"\",\n+            )\n+            actual = lx.extract_links(response)\n+            self.assertEqual(\n+                actual,\n+                [\n+                    Link(url=\"https://example.com/a\", text=\"a1\"),\n+                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n+                    Link(url=\"https://example.com/b?b=2&a=1\", text=\"b2\"),\n+                ],\n+            )\n+\n+            # unique=True (default), canonicalize=True\n+            lx = self.extractor_cls(restrict_css=(\"div\",), canonicalize=True)\n+            response = HtmlResponse(\n+                \"https://example.com\",\n+                body=b\"\"\"\n+                    <div>\n+                        <a href=\"/a\">a1</a>\n+                        <a href=\"/b?a=1&b=2\">b1</a>\n+                    </div>\n+                    <div>\n+                        <a href=\"/a\">a2</a>\n+                        <a href=\"/b?b=2&a=1\">b2</a>\n+                    </div>\n+                \"\"\",\n+            )\n+            actual = lx.extract_links(response)\n+            self.assertEqual(\n+                actual,\n+                [\n+                    Link(url=\"https://example.com/a\", text=\"a1\"),\n+                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n+                ],\n+            )\n+\n+            # unique=False, canonicalize=False (default)\n+            lx = self.extractor_cls(restrict_css=(\"div\",), unique=False)\n+            response = HtmlResponse(\n+                \"https://example.com\",\n+                body=b\"\"\"\n+                    <div>\n+                        <a href=\"/a\">a1</a>\n+                        <a href=\"/b?a=1&b=2\">b1</a>\n+                    </div>\n+                    <div>\n+                        <a href=\"/a\">a2</a>\n+                        <a href=\"/b?b=2&a=1\">b2</a>\n+                    </div>\n+                \"\"\",\n+            )\n+            actual = lx.extract_links(response)\n+            self.assertEqual(\n+                actual,\n+                [\n+                    Link(url=\"https://example.com/a\", text=\"a1\"),\n+                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n+                    Link(url=\"https://example.com/a\", text=\"a2\"),\n+                    Link(url=\"https://example.com/b?b=2&a=1\", text=\"b2\"),\n+                ],\n+            )\n+\n+            # unique=False, canonicalize=True\n+            lx = self.extractor_cls(\n+                restrict_css=(\"div\",), unique=False, canonicalize=True\n+            )\n+            response = HtmlResponse(\n+                \"https://example.com\",\n+                body=b\"\"\"\n+                    <div>\n+                        <a href=\"/a\">a1</a>\n+                        <a href=\"/b?a=1&b=2\">b1</a>\n+                    </div>\n+                    <div>\n+                        <a href=\"/a\">a2</a>\n+                        <a href=\"/b?b=2&a=1\">b2</a>\n+                    </div>\n+                \"\"\",\n+            )\n+            actual = lx.extract_links(response)\n+            self.assertEqual(\n+                actual,\n+                [\n+                    Link(url=\"https://example.com/a\", text=\"a1\"),\n+                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b1\"),\n+                    Link(url=\"https://example.com/a\", text=\"a2\"),\n+                    Link(url=\"https://example.com/b?a=1&b=2\", text=\"b2\"),\n+                ],\n+            )\n+\n \n class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n     extractor_cls = LxmlLinkExtractor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee1189512f652fae72f013c9d4759976b8b69994", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 18 | Files Changed: 6 | Hunks: 19 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 39 | Churn Cumulative: 5050 | Contributors (this commit): 43 | Commits (past 90d): 6 | Contributors (cumulative): 82 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,6 @@ import logging\n from enum import Enum\n from io import BytesIO\n from typing import TYPE_CHECKING, Dict, List, Optional, Tuple\n-from urllib.parse import urlparse\n \n from h2.errors import ErrorCodes\n from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n@@ -15,6 +14,7 @@ from twisted.web.client import ResponseFailed\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils.httpobj import urlparse_cached\n \n if TYPE_CHECKING:\n     from scrapy.core.http2.protocol import H2ClientProtocol\n@@ -185,7 +185,7 @@ class Stream:\n \n     def check_request_url(self) -> bool:\n         # Make sure that we are sending the request to the correct URL\n-        url = urlparse(self._request.url)\n+        url = urlparse_cached(self._request)\n         return (\n             url.netloc == str(self._protocol.metadata[\"uri\"].host, \"utf-8\")\n             or url.netloc == str(self._protocol.metadata[\"uri\"].netloc, \"utf-8\")\n@@ -194,7 +194,7 @@ class Stream:\n         )\n \n     def _get_request_headers(self) -> List[Tuple[str, str]]:\n-        url = urlparse(self._request.url)\n+        url = urlparse_cached(self._request)\n \n         path = url.path\n         if url.query:\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n from typing import TYPE_CHECKING, Any, List, Union, cast\n-from urllib.parse import urljoin, urlparse\n+from urllib.parse import urljoin\n \n from w3lib.url import safe_url_string\n \n@@ -125,7 +125,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n         assert response.headers[\"Location\"] is not None\n         location = safe_url_string(response.headers[\"Location\"])\n         if response.headers[\"Location\"].startswith(b\"//\"):\n-            request_scheme = urlparse(request.url).scheme\n+            request_scheme = urlparse_cached(request).scheme\n             location = request_scheme + \"://\" + location.lstrip(\"/\")\n \n         redirected_url = urljoin(request.url, location)\n\n@@ -9,6 +9,7 @@ from twisted.python.runtime import platform\n \n from scrapy import Request, Spider\n from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import configure_logging\n from tests.mockserver import MockDNSServer, MockServer\n \n@@ -30,7 +31,7 @@ class LocalhostSpider(Spider):\n         yield Request(self.url)\n \n     def parse(self, response):\n-        netloc = urlparse(response.url).netloc\n+        netloc = urlparse_cached(response).netloc\n         host = netloc.split(\":\")[0]\n         self.logger.info(f\"Host: {host}\")\n         self.logger.info(f\"Type: {type(response.ip_address)}\")\n\n@@ -1,8 +1,8 @@\n from unittest import TestCase\n-from urllib.parse import urlparse\n \n from scrapy.http import Request, Response\n from scrapy.http.cookies import WrappedRequest, WrappedResponse\n+from scrapy.utils.httpobj import urlparse_cached\n \n \n class WrappedRequestTest(TestCase):\n@@ -17,12 +17,12 @@ class WrappedRequestTest(TestCase):\n         self.assertEqual(self.wrapped.full_url, self.request.url)\n \n     def test_get_host(self):\n-        self.assertEqual(self.wrapped.get_host(), urlparse(self.request.url).netloc)\n-        self.assertEqual(self.wrapped.host, urlparse(self.request.url).netloc)\n+        self.assertEqual(self.wrapped.get_host(), urlparse_cached(self.request).netloc)\n+        self.assertEqual(self.wrapped.host, urlparse_cached(self.request).netloc)\n \n     def test_get_type(self):\n-        self.assertEqual(self.wrapped.get_type(), urlparse(self.request.url).scheme)\n-        self.assertEqual(self.wrapped.type, urlparse(self.request.url).scheme)\n+        self.assertEqual(self.wrapped.get_type(), urlparse_cached(self.request).scheme)\n+        self.assertEqual(self.wrapped.type, urlparse_cached(self.request).scheme)\n \n     def test_is_unverifiable(self):\n         self.assertFalse(self.wrapped.is_unverifiable())\n\n@@ -5,7 +5,7 @@ import warnings\n import xmlrpc.client\n from typing import Any, Dict, List\n from unittest import mock\n-from urllib.parse import parse_qs, unquote_to_bytes, urlparse\n+from urllib.parse import parse_qs, unquote_to_bytes\n \n from scrapy.http import (\n     FormRequest,\n@@ -16,6 +16,7 @@ from scrapy.http import (\n     XmlRpcRequest,\n )\n from scrapy.http.request import NO_CALLBACK\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n@@ -617,8 +618,8 @@ class FormRequestTest(RequestTest):\n             method=\"GET\",\n             formdata=((\"foo\", \"bar\"), (\"foo\", \"baz\")),\n         )\n-        self.assertEqual(urlparse(req.url).hostname, \"www.example.com\")\n-        self.assertEqual(urlparse(req.url).query, \"foo=bar&foo=baz\")\n+        self.assertEqual(urlparse_cached(req).hostname, \"www.example.com\")\n+        self.assertEqual(urlparse_cached(req).query, \"foo=bar&foo=baz\")\n \n     def test_from_response_override_duplicate_form_key(self):\n         response = _buildresponse(\n@@ -666,8 +667,8 @@ class FormRequestTest(RequestTest):\n             response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n         )\n         self.assertEqual(r1.method, \"GET\")\n-        self.assertEqual(urlparse(r1.url).hostname, \"www.example.com\")\n-        self.assertEqual(urlparse(r1.url).path, \"/this/get.php\")\n+        self.assertEqual(urlparse_cached(r1).hostname, \"www.example.com\")\n+        self.assertEqual(urlparse_cached(r1).path, \"/this/get.php\")\n         fs = _qs(r1)\n         self.assertEqual(set(fs[b\"test\"]), {b\"val1\", b\"val2\"})\n         self.assertEqual(set(fs[b\"one\"]), {b\"two\", b\"three\"})\n\n@@ -1,6 +1,6 @@\n from typing import Dict, Optional\n from unittest import TestCase\n-from urllib.parse import urljoin, urlparse\n+from urllib.parse import urljoin\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -9,6 +9,7 @@ from twisted.trial.unittest import TestCase as TwistedTestCase\n from scrapy.core.scheduler import BaseScheduler\n from scrapy.http import Request\n from scrapy.spiders import Spider\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.request import fingerprint\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n@@ -57,7 +58,7 @@ class TestSpider(Spider):\n         self.start_urls = map(mockserver.url, PATHS)\n \n     def parse(self, response):\n-        return {\"path\": urlparse(response.url).path}\n+        return {\"path\": urlparse_cached(response).path}\n \n \n class InterfaceCheckMixin:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f096f17fa4ac1307fa1c81ae082bb52e9f86653a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 55 | Churn Cumulative: 215 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,8 @@\n+import sys\n+from io import StringIO\n+from unittest.mock import Mock, PropertyMock, patch\n+\n+from scrapy.commands.check import Command\n from tests.test_commands import CommandTest\n \n \n@@ -94,3 +99,53 @@ class CheckSpider(scrapy.Spider):\n             raise Exception('SCRAPY_CHECK not set')\n         \"\"\"\n         self._test_contract(parse_def=parse_def)\n+\n+    @patch(\"scrapy.commands.check.ContractsManager\")\n+    def test_run_with_opts_list_prints_spider(self, cm_cls_mock):\n+        output = StringIO()\n+        sys.stdout = output\n+        cmd = Command()\n+        cmd.settings = Mock(getwithbase=Mock(return_value={}))\n+        cm_cls_mock.return_value = cm_mock = Mock()\n+        spider_loader_mock = Mock()\n+        cmd.crawler_process = Mock(spider_loader=spider_loader_mock)\n+        spider_name = \"FakeSpider\"\n+        spider_cls_mock = Mock()\n+        type(spider_cls_mock).name = PropertyMock(return_value=spider_name)\n+        spider_loader_mock.load.side_effect = lambda x: {spider_name: spider_cls_mock}[\n+            x\n+        ]\n+        tested_methods = [\"fakeMethod1\", \"fakeMethod2\"]\n+        cm_mock.tested_methods_from_spidercls.side_effect = lambda x: {\n+            spider_cls_mock: tested_methods\n+        }[x]\n+\n+        cmd.run([spider_name], Mock(list=True))\n+\n+        self.assertEqual(\n+            \"FakeSpider\\n  * fakeMethod1\\n  * fakeMethod2\\n\", output.getvalue()\n+        )\n+        sys.stdout = sys.__stdout__\n+\n+    @patch(\"scrapy.commands.check.ContractsManager\")\n+    def test_run_without_opts_list_does_not_crawl_spider_with_no_tested_methods(\n+        self, cm_cls_mock\n+    ):\n+        cmd = Command()\n+        cmd.settings = Mock(getwithbase=Mock(return_value={}))\n+        cm_cls_mock.return_value = cm_mock = Mock()\n+        spider_loader_mock = Mock()\n+        cmd.crawler_process = Mock(spider_loader=spider_loader_mock)\n+        spider_name = \"FakeSpider\"\n+        spider_cls_mock = Mock()\n+        spider_loader_mock.load.side_effect = lambda x: {spider_name: spider_cls_mock}[\n+            x\n+        ]\n+        tested_methods = []\n+        cm_mock.tested_methods_from_spidercls.side_effect = lambda x: {\n+            spider_cls_mock: tested_methods\n+        }[x]\n+\n+        cmd.run([spider_name], Mock(list=False))\n+\n+        cmd.crawler_process.crawl.assert_not_called()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8e6d28479a0479361cd3de0fb854c243ed684b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 34 | Churn Cumulative: 1816 | Contributors (this commit): 16 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import pickle\n import re\n import unittest\n from typing import Optional\n+from unittest.mock import Mock\n \n from packaging.version import Version\n from pytest import mark\n@@ -851,3 +852,36 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n                 ),\n             ],\n         )\n+\n+    def test_link_allowed_is_false_with_empty_url(self):\n+        mock_link = Mock()\n+        mock_link.url = \"\"\n+        expected = False\n+\n+        actual = LxmlLinkExtractor()._link_allowed(mock_link)\n+\n+        self.assertEqual(expected, actual)\n+\n+    def test_link_allowed_is_false_with_bad_url_prefix(self):\n+        mock_link = Mock()\n+        mock_link.url = \"htp://should_be_http.com\"\n+        expected = False\n+\n+        actual = LxmlLinkExtractor()._link_allowed(mock_link)\n+\n+        self.assertEqual(expected, actual)\n+\n+    def test_link_allowed_is_false_with_missing_url_prefix(self):\n+        mock_link = Mock()\n+        mock_link.url = \"should_have_prefix.com\"\n+        expected = False\n+\n+        actual = LxmlLinkExtractor()._link_allowed(mock_link)\n+\n+        self.assertEqual(expected, actual)\n+\n+    def test_link_allowed_raises_with_none_url(self):\n+        mock_link = Mock()\n+        mock_link.url = None\n+\n+        self.assertRaises(AttributeError, LxmlLinkExtractor()._link_allowed, mock_link)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e27d320c3cde3c965e61a674e8880043943cf17a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 56 | Churn Cumulative: 2691 | Contributors (this commit): 31 | Commits (past 90d): 2 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1642,6 +1642,62 @@ class JsonRequestTest(RequestTest):\n             self.assertEqual(kwargs[\"ensure_ascii\"], True)\n             self.assertEqual(kwargs[\"allow_nan\"], True)\n \n+    def test_form_response_with_invalid_formdata_type_error(self):\n+        \"\"\"Test that a form response with invalid form data throws a type error\"\"\"\n+        response = _buildresponse(\n+            \"\"\"<html><body>\n+            <form action=\"/submit\" method=\"post\">\n+                <input type=\"text\" name=\"test\" value=\"value\">\n+            </form>\n+            </body></html>\"\"\"\n+        )\n+        with self.assertRaises(ValueError) as context:\n+            FormRequest.from_response(response, formdata=123)\n+\n+        self.assertIn(\n+            \"formdata should be a dict or iterable of tuples\", str(context.exception)\n+        )\n+\n+    def test_form_response_with_custom_invalid_formdata_value_error(self):\n+        \"\"\"Test that a form response with invalid form data throws a value error\"\"\"\n+        response = _buildresponse(\n+            \"\"\"<html><body>\n+                <form action=\"/submit\" method=\"post\">\n+                    <input type=\"text\" name=\"test\" value=\"value\">\n+                </form>\n+            </body></html>\"\"\"\n+        )\n+\n+        class CustomFormdata:\n+            def __iter__(self):\n+                raise ValueError(\"Custom iteration error for testing\")\n+\n+        with self.assertRaises(ValueError) as context:\n+            FormRequest.from_response(response, formdata=CustomFormdata())\n+\n+        self.assertIn(\n+            \"formdata should be a dict or iterable of tuples\", str(context.exception)\n+        )\n+\n+    def test_get_form_with_xpath_no_form_parent(self):\n+        \"\"\"Test that _get_from raised a ValueError when an XPath selects an element\n+        not nested within a <form> and no <form> parent is found\"\"\"\n+        response = _buildresponse(\n+            \"\"\"<html><body>\n+                <div id=\"outside-form\">\n+                    <p>This paragraph is not inside a form.</p>\n+                </div>\n+                <form action=\"/submit\" method=\"post\">\n+                    <input type=\"text\" name=\"inside-form\" value=\"\">\n+                </form>\n+            </body></html>\"\"\"\n+        )\n+\n+        with self.assertRaises(ValueError) as context:\n+            FormRequest.from_response(response, formxpath='//div[@id=\"outside-form\"]/p')\n+\n+        self.assertIn(\"No <form> element found with\", str(context.exception))\n+\n     def tearDown(self):\n         warnings.resetwarnings()\n         super().tearDown()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2a0c85f1167c7c32219eaf095c1818b2c74702c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2695 | Contributors (this commit): 31 | Commits (past 90d): 3 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -1643,7 +1643,7 @@ class JsonRequestTest(RequestTest):\n             self.assertEqual(kwargs[\"allow_nan\"], True)\n \n     def test_form_response_with_invalid_formdata_type_error(self):\n-        \"\"\"Test that a form response with invalid form data throws a type error\"\"\"\n+        \"\"\"Test that a ValueError is raised for non-iterable and non-dict formdata input\"\"\"\n         response = _buildresponse(\n             \"\"\"<html><body>\n             <form action=\"/submit\" method=\"post\">\n@@ -1659,7 +1659,7 @@ class JsonRequestTest(RequestTest):\n         )\n \n     def test_form_response_with_custom_invalid_formdata_value_error(self):\n-        \"\"\"Test that a form response with invalid form data throws a value error\"\"\"\n+        \"\"\"Test that a ValueError is raised for fault-inducing iterable formdata input\"\"\"\n         response = _buildresponse(\n             \"\"\"<html><body>\n                 <form action=\"/submit\" method=\"post\">\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12b4417c56d8aa76cbe3a36c026962612453ee6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 20 | Churn Cumulative: 2715 | Contributors (this commit): 31 | Commits (past 90d): 4 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1642,6 +1642,26 @@ class JsonRequestTest(RequestTest):\n             self.assertEqual(kwargs[\"ensure_ascii\"], True)\n             self.assertEqual(kwargs[\"allow_nan\"], True)\n \n+    def test_replacement_both_body_and_data_warns(self):\n+        \"\"\"Test that we can get a warning if both body and data are passed for branch coverage\"\"\"\n+        body1 = None\n+        body2 = b\"body\"\n+        data1 = {\n+            \"name1\": \"value1\",\n+        }\n+        data2 = {\n+            \"name2\": \"value2\",\n+        }\n+        r1 = self.request_class(url=\"http://www.example.com/\", data=data1, body=body1)\n+\n+        with mock.patch(\"warnings.warn\") as mock_warn:\n+            r1.replace(data=data2, body=body2)\n+            mock_warn.assert_called_once()\n+            (warning_message,), _ = mock_warn.call_args\n+            self.assertIn(\n+                \"Both body and data passed. data will be ignored\", warning_message\n+            )\n+\n     def tearDown(self):\n         warnings.resetwarnings()\n         super().tearDown()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc036542a82ec054dd6a36b4b928a9bc6ae48e63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 56 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 112 | Churn Cumulative: 2827 | Contributors (this commit): 31 | Commits (past 90d): 5 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -1426,6 +1426,62 @@ class FormRequestTest(RequestTest):\n             r = self.request_class.from_response(response)\n             self.assertEqual(r.method, expected)\n \n+    def test_form_response_with_invalid_formdata_type_error(self):\n+        \"\"\"Test that a ValueError is raised for non-iterable and non-dict formdata input\"\"\"\n+        response = _buildresponse(\n+            \"\"\"<html><body>\n+            <form action=\"/submit\" method=\"post\">\n+                <input type=\"text\" name=\"test\" value=\"value\">\n+            </form>\n+            </body></html>\"\"\"\n+        )\n+        with self.assertRaises(ValueError) as context:\n+            FormRequest.from_response(response, formdata=123)\n+\n+        self.assertIn(\n+            \"formdata should be a dict or iterable of tuples\", str(context.exception)\n+        )\n+\n+    def test_form_response_with_custom_invalid_formdata_value_error(self):\n+        \"\"\"Test that a ValueError is raised for fault-inducing iterable formdata input\"\"\"\n+        response = _buildresponse(\n+            \"\"\"<html><body>\n+                <form action=\"/submit\" method=\"post\">\n+                    <input type=\"text\" name=\"test\" value=\"value\">\n+                </form>\n+            </body></html>\"\"\"\n+        )\n+\n+        class CustomFormdata:\n+            def __iter__(self):\n+                raise ValueError(\"Custom iteration error for testing\")\n+\n+        with self.assertRaises(ValueError) as context:\n+            FormRequest.from_response(response, formdata=CustomFormdata())\n+\n+        self.assertIn(\n+            \"formdata should be a dict or iterable of tuples\", str(context.exception)\n+        )\n+\n+    def test_get_form_with_xpath_no_form_parent(self):\n+        \"\"\"Test that _get_from raised a ValueError when an XPath selects an element\n+        not nested within a <form> and no <form> parent is found\"\"\"\n+        response = _buildresponse(\n+            \"\"\"<html><body>\n+                <div id=\"outside-form\">\n+                    <p>This paragraph is not inside a form.</p>\n+                </div>\n+                <form action=\"/submit\" method=\"post\">\n+                    <input type=\"text\" name=\"inside-form\" value=\"\">\n+                </form>\n+            </body></html>\"\"\"\n+        )\n+\n+        with self.assertRaises(ValueError) as context:\n+            FormRequest.from_response(response, formxpath='//div[@id=\"outside-form\"]/p')\n+\n+        self.assertIn(\"No <form> element found with\", str(context.exception))\n+\n \n def _buildresponse(body, **kwargs):\n     kwargs.setdefault(\"body\", body)\n@@ -1642,62 +1698,6 @@ class JsonRequestTest(RequestTest):\n             self.assertEqual(kwargs[\"ensure_ascii\"], True)\n             self.assertEqual(kwargs[\"allow_nan\"], True)\n \n-    def test_form_response_with_invalid_formdata_type_error(self):\n-        \"\"\"Test that a ValueError is raised for non-iterable and non-dict formdata input\"\"\"\n-        response = _buildresponse(\n-            \"\"\"<html><body>\n-            <form action=\"/submit\" method=\"post\">\n-                <input type=\"text\" name=\"test\" value=\"value\">\n-            </form>\n-            </body></html>\"\"\"\n-        )\n-        with self.assertRaises(ValueError) as context:\n-            FormRequest.from_response(response, formdata=123)\n-\n-        self.assertIn(\n-            \"formdata should be a dict or iterable of tuples\", str(context.exception)\n-        )\n-\n-    def test_form_response_with_custom_invalid_formdata_value_error(self):\n-        \"\"\"Test that a ValueError is raised for fault-inducing iterable formdata input\"\"\"\n-        response = _buildresponse(\n-            \"\"\"<html><body>\n-                <form action=\"/submit\" method=\"post\">\n-                    <input type=\"text\" name=\"test\" value=\"value\">\n-                </form>\n-            </body></html>\"\"\"\n-        )\n-\n-        class CustomFormdata:\n-            def __iter__(self):\n-                raise ValueError(\"Custom iteration error for testing\")\n-\n-        with self.assertRaises(ValueError) as context:\n-            FormRequest.from_response(response, formdata=CustomFormdata())\n-\n-        self.assertIn(\n-            \"formdata should be a dict or iterable of tuples\", str(context.exception)\n-        )\n-\n-    def test_get_form_with_xpath_no_form_parent(self):\n-        \"\"\"Test that _get_from raised a ValueError when an XPath selects an element\n-        not nested within a <form> and no <form> parent is found\"\"\"\n-        response = _buildresponse(\n-            \"\"\"<html><body>\n-                <div id=\"outside-form\">\n-                    <p>This paragraph is not inside a form.</p>\n-                </div>\n-                <form action=\"/submit\" method=\"post\">\n-                    <input type=\"text\" name=\"inside-form\" value=\"\">\n-                </form>\n-            </body></html>\"\"\"\n-        )\n-\n-        with self.assertRaises(ValueError) as context:\n-            FormRequest.from_response(response, formxpath='//div[@id=\"outside-form\"]/p')\n-\n-        self.assertIn(\"No <form> element found with\", str(context.exception))\n-\n     def tearDown(self):\n         warnings.resetwarnings()\n         super().tearDown()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
