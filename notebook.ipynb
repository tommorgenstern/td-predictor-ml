{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62126a22",
   "metadata": {},
   "source": [
    "# Bachelorarbeit – Vorhersage technischer Schuld mittels maschinellem Lernen auf Basis von Code-Metriken und Commit-Historien\n",
    "\n",
    "(Zur vollständigen Reproduktion die hier importierten Module installieren, die Repos klonen und deren Pfade ggf. konfigurieren sowie die Umgebungsvariable OPENAI_API_KEY setzen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ba605c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, math, asyncio, datetime as dt, collections, hashlib\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "from pydriller import Repository\n",
    "from pydriller.domain.commit import ModificationType as MT\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_DIR = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "REPOS = {\n",
    "    'requests':  {'path': 'repos/requests',        'branch': 'main'},\n",
    "    'fastapi':  {'path': 'repos/fastapi',        'branch': 'master'},\n",
    "    'scrapy':   {'path': 'repos/scrapy',         'branch': 'master'},\n",
    "    'flask':    {'path': 'repos/flask',          'branch': 'main'},\n",
    "    'keras':    {'path': 'repos/keras',          'branch': 'master'},\n",
    "}\n",
    "\n",
    "CUTOFF=dt.datetime(2025,6,15,23,59,59, tzinfo=dt.timezone.utc)\n",
    "SATD=re.compile(r'\\b(TODO|FIXME|BUG|HACK|XXX|WORKAROUND|TEMP|KLUDGE|UGLY|DIRTY|BROKEN|FIX)\\b',re.I)\n",
    "\n",
    "LLM_FRAC=.25\n",
    "LLM_MODEL='gpt-4o'\n",
    "\n",
    "OUT=Path('data'); OUT.mkdir(exist_ok=True)\n",
    "SPLIT=Path('splits'); SPLIT.mkdir(exist_ok=True)\n",
    "LLM_DIR=Path('llm_batch'); LLM_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f40c2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_comment_or_docstring(line):\n",
    "    line = line.strip()\n",
    "    return (\n",
    "        line.startswith('#') or\n",
    "        line.startswith('\"\"\"') or\n",
    "        line.startswith(\"'''\") or\n",
    "        line.endswith('\"\"\"') or\n",
    "        line.endswith(\"'''\")\n",
    "    )\n",
    "\n",
    "def satd_delta(mod):\n",
    "    add = rem = 0\n",
    "    for _, line in mod.diff_parsed['added']:\n",
    "        if is_comment_or_docstring(line) and SATD.search(line):\n",
    "            add += 1\n",
    "    for _, line in mod.diff_parsed['deleted']:\n",
    "        if is_comment_or_docstring(line) and SATD.search(line):\n",
    "            rem += 1\n",
    "    return add - rem\n",
    "\n",
    "def is_py(m):\n",
    "    fp=m.new_path or m.old_path or ''\n",
    "    return fp.endswith('.py')\n",
    "\n",
    "def quick_hunks_count(mod):\n",
    "    diff = mod.diff.splitlines()\n",
    "    state = False\n",
    "    h = 0\n",
    "    for line in diff:\n",
    "        if line.startswith(('+', '-')):\n",
    "            if not state:\n",
    "                state = True\n",
    "                h += 1\n",
    "        else:\n",
    "            state = False\n",
    "    return h\n",
    "\n",
    "#def diff_snippet(txt,max_lines=3000):\n",
    "#    return '\\n'.join(txt.split('\\n')[:max_lines])\n",
    "\n",
    "#def summary(r):\n",
    "#    return (f\"adds {r['lines_added']} LOC ({r['lines_deleted']} del) across {r['files_changed']} py‑files; \"\n",
    "#            f\"ΔCCmax {r['cc_delta_max']}; methods {r['n_methods_changed']}; \"\n",
    "#            f\"commits90d {r['n_commits_file_past90d']}; authors tot {r['n_authors_till_now']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0affa1e8",
   "metadata": {},
   "source": [
    "## Mining & Feature‑Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdeb5f-febc-48fe-9e48-995253848fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from pydriller import Repository\n",
    "from pydriller.metrics.process.commits_count import CommitsCount\n",
    "from pydriller.metrics.process.contributors_experience import ContributorsExperience\n",
    "from pydriller.metrics.process.history_complexity import HistoryComplexity\n",
    "\n",
    "# LLM Batch Setup\n",
    "GENERATE_LLM_BATCH = True\n",
    "MAX_REQ = 50_000\n",
    "MAX_MB = 180\n",
    "\n",
    "# Konstanten\n",
    "COLS = [\n",
    "    'repo_id', 'commit_hash', 'commit_uid', 'commit_date',\n",
    "    'lines_added', 'lines_deleted', 'files_changed', 'hunks',\n",
    "    'n_methods_changed', 'cc_delta_sum', 'cc_delta_max',\n",
    "    'complexity_current_sum', 'churn_delta', 'churn_cum',\n",
    "    'contributors_count', 'contributors_cum',\n",
    "    'n_authors_till_now', 'n_commits_file_past90d',\n",
    "    'commits_count_file', 'contributors_experience',\n",
    "    'history_complexity',\n",
    "    'dmm_unit_complexity', 'dmm_unit_size', 'dmm_unit_interfacing',\n",
    "    'satd_delta', 'label_td_satd'\n",
    "]\n",
    "\n",
    "# Hilfsfunktionen\n",
    "def prepare_llm_prompt(row, diff_text):\n",
    "    def summary_llm(r):\n",
    "        fields = [\n",
    "            f\"Lines Added: {r['lines_added']}\",\n",
    "            f\"Lines Deleted: {r['lines_deleted']}\",\n",
    "            f\"Files Changed: {r['files_changed']}\",\n",
    "            f\"Hunks: {r['hunks']}\",\n",
    "            f\"Methods Changed: {r['n_methods_changed']}\",\n",
    "            f\"Complexity Δ (Sum/Max): {r['cc_delta_sum']}/{r['cc_delta_max']}\",\n",
    "            f\"Churn Δ: {r['churn_delta']}\",\n",
    "            f\"Churn Cumulative: {r['churn_cum']}\",\n",
    "            f\"Contributors (this commit): {r['contributors_count']}\",\n",
    "            f\"Commits (past 90d): {r['n_commits_file_past90d']}\",\n",
    "            f\"Contributors (cumulative): {r['contributors_cum']}\",\n",
    "            f\"DMM Complexity: {r['dmm_unit_complexity']}\"\n",
    "        ]\n",
    "        return \" | \".join(fields)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a senior reviewer.\\n\\n\"\n",
    "        \"Commit Summary:\\n\"\n",
    "        f\"{summary_llm(row)}\\n\\n\"\n",
    "        \"DIFF:\\n\"\n",
    "        f\"{diff_text}\\n\\n\"\n",
    "        \"Question: Does this commit introduce technical debt? Answer yes or no.\"\n",
    "    )\n",
    "\n",
    "    rec = {\n",
    "        'custom_id': row['commit_uid'],\n",
    "        'method': 'POST',\n",
    "        'url': '/v1/chat/completions',\n",
    "        'body': {\n",
    "            'model': LLM_MODEL,\n",
    "            'messages': [{'role': 'user', 'content': prompt}],\n",
    "            'max_tokens': 1,\n",
    "            'temperature': 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return json.dumps(rec, ensure_ascii=False) + '\\n'\n",
    "\n",
    "\n",
    "# Verarbeitung\n",
    "for repo, cfg in tqdm(REPOS.items(), desc=\"Repos\"):\n",
    "    path = (BASE_DIR / cfg['path']).expanduser().resolve()\n",
    "    if not path.is_dir():\n",
    "        raise FileNotFoundError(f\"{path} existiert nicht – REPOS-Eintrag prüfen!\")\n",
    "\n",
    "    file_prev_cc = collections.defaultdict(int)\n",
    "    file_current_cc = collections.defaultdict(int)\n",
    "    file_churn_cum = collections.defaultdict(int)\n",
    "    file_authors = collections.defaultdict(set)\n",
    "    file_times = collections.defaultdict(list)\n",
    "    file_contributors = collections.defaultdict(set)\n",
    "\n",
    "    START_DATE = dt.datetime(1970, 1, 1, tzinfo=dt.timezone.utc)\n",
    "    print(f\"[{repo}] Berechne Prozessmetriken...\")\n",
    "\n",
    "    commits_count_dict = CommitsCount(str(path), since=START_DATE, to=CUTOFF).count()\n",
    "    contrib_exp_dict = ContributorsExperience(str(path), since=START_DATE, to=CUTOFF).count()\n",
    "    hist_complexity_dict = HistoryComplexity(str(path), since=START_DATE, to=CUTOFF).count()\n",
    "\n",
    "    print(f\"[{repo}] Prozessmetriken berechnet.\")\n",
    "\n",
    "    csv_file = OUT / f'features_{repo}.csv'\n",
    "    writer = csv.DictWriter(csv_file.open('w', newline=''), fieldnames=COLS)\n",
    "    writer.writeheader()\n",
    "\n",
    "    part = 1\n",
    "    cur_size = 0\n",
    "    handle = None\n",
    "\n",
    "    commits = Repository(\n",
    "        path_to_repo=str(path),\n",
    "        only_in_branch=cfg['branch'],\n",
    "        to=CUTOFF,\n",
    "        only_modifications_with_file_types=['.py'],\n",
    "        num_workers=64,\n",
    "        skip_whitespaces=True,\n",
    "        histogram_diff=True\n",
    "    ).traverse_commits()\n",
    "\n",
    "    for c in tqdm(commits, desc=repo, leave=False):\n",
    "        py = [m for m in c.modified_files if is_py(m)]\n",
    "        if not py:\n",
    "            continue\n",
    "\n",
    "        la = sum(m.added_lines for m in py)\n",
    "        ld = sum(m.deleted_lines for m in py)\n",
    "        files_changed = len(py)\n",
    "        hunks = sum(quick_hunks_count(m) for m in py)\n",
    "        n_methods = sum(len(m.changed_methods) for m in py)\n",
    "\n",
    "        cc_delta_sum = 0\n",
    "        cc_delta_max = 0\n",
    "        churn_delta = 0\n",
    "        complexity_current_sum = 0\n",
    "        churn_cum_sum = 0\n",
    "        contributors_in_commit = set()\n",
    "\n",
    "        for m in py:\n",
    "            fp = m.new_path or m.old_path\n",
    "            delta_cc = (m.complexity or 0) - file_prev_cc[fp]\n",
    "            cc_delta_sum += delta_cc\n",
    "            cc_delta_max = max(cc_delta_max, delta_cc)\n",
    "            file_prev_cc[fp] = m.complexity or 0\n",
    "            file_current_cc[fp] = m.complexity or 0\n",
    "            complexity_current_sum += file_current_cc[fp]\n",
    "            churn_this = m.added_lines + m.deleted_lines\n",
    "            churn_delta += churn_this\n",
    "            file_churn_cum[fp] += churn_this\n",
    "            churn_cum_sum += file_churn_cum[fp]\n",
    "            file_contributors[fp].add(c.author.email)\n",
    "            contributors_in_commit.update(file_contributors[fp])\n",
    "            file_authors[fp].add(c.author.email)\n",
    "            file_times[fp].append(c.author_date)\n",
    "\n",
    "        cutoff90 = c.author_date - dt.timedelta(days=90)\n",
    "        n_commits90 = sum(\n",
    "            len([t for t in ts if t >= cutoff90])\n",
    "            for fp, ts in file_times.items()\n",
    "            if fp in [m.new_path or m.old_path for m in py]\n",
    "        )\n",
    "\n",
    "        commits_count_file = sum(commits_count_dict.get(m.new_path or m.old_path, 0) for m in py)\n",
    "        contributors_experience = sum(contrib_exp_dict.get(m.new_path or m.old_path, 0) for m in py)\n",
    "        history_complexity = sum(hist_complexity_dict.get(m.new_path or m.old_path, 0) for m in py)\n",
    "        satd = sum(satd_delta(m) for m in py)\n",
    "        label_td_satd = 1 if satd > 0 else 0\n",
    "\n",
    "        row_dict = {\n",
    "            'repo_id': repo,\n",
    "            'commit_hash': c.hash,\n",
    "            'commit_uid': f'{repo}#{c.hash}',\n",
    "            'commit_date': c.author_date.isoformat(),\n",
    "            'lines_added': la,\n",
    "            'lines_deleted': ld,\n",
    "            'files_changed': files_changed,\n",
    "            'hunks': hunks,\n",
    "            'n_methods_changed': n_methods,\n",
    "            'cc_delta_sum': cc_delta_sum,\n",
    "            'cc_delta_max': cc_delta_max,\n",
    "            'complexity_current_sum': complexity_current_sum,\n",
    "            'churn_delta': churn_delta,\n",
    "            'churn_cum': churn_cum_sum,\n",
    "            'contributors_count': len(contributors_in_commit),\n",
    "            'contributors_cum': sum(len(file_contributors[fp]) for m in py for fp in [m.new_path or m.old_path]),\n",
    "            'n_authors_till_now': len({a for s in file_authors.values() for a in s}),\n",
    "            'n_commits_file_past90d': n_commits90,\n",
    "            'commits_count_file': commits_count_file,\n",
    "            'contributors_experience': contributors_experience,\n",
    "            'history_complexity': history_complexity,\n",
    "            'dmm_unit_complexity': c.dmm_unit_complexity,\n",
    "            'dmm_unit_size': c.dmm_unit_size,\n",
    "            'dmm_unit_interfacing': c.dmm_unit_interfacing,\n",
    "            'satd_delta': satd,\n",
    "            'label_td_satd': label_td_satd\n",
    "        }\n",
    "\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "        if GENERATE_LLM_BATCH and row_dict['satd_delta'] <= 0:\n",
    "            diff_text = '\\n'.join([diff_snippet(m.diff) for m in py])\n",
    "            jsonl_line = prepare_llm_prompt(row_dict, diff_text)\n",
    "\n",
    "            if handle is None:\n",
    "                f = LLM_DIR / f\"{repo}_part{part}.jsonl\"\n",
    "                handle = f.open('w', encoding='utf-8')\n",
    "                cur_size = 0\n",
    "\n",
    "            if cur_size + len(jsonl_line.encode('utf-8')) > MAX_MB * 1_000_000 or (handle.tell() // 1) > MAX_REQ:\n",
    "                handle.close()\n",
    "                part += 1\n",
    "                f = LLM_DIR / f\"{repo}_part{part}.jsonl\"\n",
    "                handle = f.open('w', encoding='utf-8')\n",
    "                cur_size = 0\n",
    "\n",
    "            handle.write(jsonl_line)\n",
    "            cur_size += len(jsonl_line.encode('utf-8'))\n",
    "\n",
    "    if handle:\n",
    "        handle.close()\n",
    "\n",
    "    print(f\"[{repo}] abgeschlossen.\")\n",
    "\n",
    "print(\"Alle Repos verarbeitet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4df27",
   "metadata": {},
   "source": [
    "## Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a91c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# CSV-Dateien zusammenfügen\n",
    "csv_files = list((OUT).glob('features_*.csv'))\n",
    "df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# Splits erstellen\n",
    "df['commit_dt'] = pd.to_datetime(df.commit_date, utc=True)\n",
    "train_idx = []\n",
    "test_idx = []\n",
    "\n",
    "# Zeitbasiertes Splitten je Projekt (dann aggregieren)\n",
    "for repo, g in df.groupby('repo_id'):\n",
    "    g = g.sort_values('commit_dt')\n",
    "    n = int(0.7 * len(g))\n",
    "    train_idx += list(g.index[:n]) # first 70%\n",
    "    test_idx += list(g.index[n:]) #last 30%\n",
    "\n",
    "# Speichern\n",
    "SPLIT.joinpath('time_train.csv').write_text(df.loc[train_idx].to_csv(index=False))\n",
    "SPLIT.joinpath('time_test.csv').write_text(df.loc[test_idx].to_csv(index=False))\n",
    "\n",
    "# LOPO-Splits erstellen\n",
    "for repo in df.repo_id.unique():\n",
    "    SPLIT.joinpath(f'lopo_train_excl_{repo}.csv').write_text(df[df.repo_id != repo].to_csv(index=False))\n",
    "    SPLIT.joinpath(f'lopo_test_{repo}.csv').write_text(df[df.repo_id == repo].to_csv(index=False))\n",
    "\n",
    "print('Globale Splits erstellt.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3bc4a-db79-45de-a052-908593196cbc",
   "metadata": {},
   "source": [
    "## Erste Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11a6bd-c770-431d-9659-aee6b05dc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Daten laden\n",
    "SPLIT = Path(\"splits\")\n",
    "train_df = pd.read_csv(SPLIT / 'time_train.csv')\n",
    "test_df = pd.read_csv(SPLIT / 'time_test.csv')\n",
    "\n",
    "# Funktionen\n",
    "def prepare_xy(df):\n",
    "    X = df.drop(columns=[\n",
    "        'repo_id', 'commit_hash', 'commit_uid', 'commit_date', 'commit_dt', # irrelevant für Training / keine Features\n",
    "        'satd_delta',  # direktes SATD-Delta wird nicht als Feature genutzt\n",
    "        'label_td_satd'  # Zielvariable wird ebenfalls entfernt\n",
    "    ])\n",
    "    y = df['label_td_satd']\n",
    "    return X, y\n",
    "\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\n{name} – Klassifikationsbericht:\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    print(f\"Confusion Matrix ({name}):\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(f\"ROC AUC ({name}): {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "\n",
    "    return y_pred, y_proba\n",
    "\n",
    "# Daten vorbereiten\n",
    "X_train, y_train = prepare_xy(train_df)\n",
    "X_test, y_test = prepare_xy(test_df)\n",
    "\n",
    "MODELS_DIR = Path(\"trained_models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=250, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "joblib.dump(rf, MODELS_DIR / \"rf_model.joblib\")\n",
    "evaluate_model(\"Random Forest\", rf, X_test, y_test)\n",
    "\n",
    "# LightGBM\n",
    "lgbm = lgb.LGBMClassifier(n_estimators=250, random_state=42, n_jobs=-1)\n",
    "lgbm.fit(X_train, y_train)\n",
    "joblib.dump(lgbm, MODELS_DIR / \"lgbm_model.joblib\")\n",
    "evaluate_model(\"LightGBM\", lgbm, X_test, y_test)\n",
    "\n",
    "# XGBoost\n",
    "xgbm = xgb.XGBClassifier(n_estimators=250, random_state=42, n_jobs=-1, use_label_encoder=False)\n",
    "xgbm.fit(X_train, y_train)\n",
    "joblib.dump(xgbm, MODELS_DIR / \"xgb_model.joblib\")\n",
    "evaluate_model(\"XGBoost\", xgbm, X_test, y_test)\n",
    "\n",
    "print(\"Alle Modelle trainiert und gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c2f51-62dd-4adc-a256-59f24153a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOPO-Demonstration für alle Repos und Modelle\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Ordner mit Splits\n",
    "SPLIT = Path(\"splits\")\n",
    "\n",
    "# Repos aus LOPO-Splits ableiten:\n",
    "repos = sorted([p.name.replace(\"lopo_test_\", \"\").replace(\".csv\", \"\") \n",
    "                for p in SPLIT.glob(\"lopo_test_*.csv\")])\n",
    "\n",
    "# Funktion zur Aufbereitung\n",
    "def prepare_xy(df):\n",
    "    X = df.drop(columns=[\n",
    "        'repo_id', 'commit_hash', 'commit_uid', 'commit_date', 'commit_dt',\n",
    "        'satd_delta', 'label_td_satd'\n",
    "    ])\n",
    "    y = df['label_td_satd']\n",
    "    return X, y\n",
    "\n",
    "# Funktion zur Evaluation\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\n{name} – LOPO-Ergebnisse:\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    print(f\"Confusion Matrix ({name}):\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f\"ROC AUC ({name}): {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "\n",
    "# LOPO über alle Projekte\n",
    "for repo in repos:\n",
    "    print(f\"\\n=== LOPO: {repo} excluded ===\")\n",
    "    \n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(SPLIT / f'lopo_train_excl_{repo}.csv')\n",
    "    test_df = pd.read_csv(SPLIT / f'lopo_test_{repo}.csv')\n",
    "\n",
    "    # Aufbereiten\n",
    "    X_train, y_train = prepare_xy(train_df)\n",
    "    X_test, y_test = prepare_xy(test_df)\n",
    "\n",
    "    # Modelle trainieren und evaluieren\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=250, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    evaluate_model(f\"Random Forest (LOPO: {repo})\", rf, X_test, y_test)\n",
    "\n",
    "    lgbm = lgb.LGBMClassifier(n_estimators=250, random_state=42, n_jobs=-1)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    evaluate_model(f\"LightGBM (LOPO: {repo})\", lgbm, X_test, y_test)\n",
    "\n",
    "    xgbm = xgb.XGBClassifier(n_estimators=250, random_state=42, n_jobs=-1, use_label_encoder=False)\n",
    "    xgbm.fit(X_train, y_train)\n",
    "    evaluate_model(f\"XGBoost (LOPO: {repo})\", xgbm, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5cc92-ba64-4743-bfcb-6006af6576c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TreeExplainer vorbereiten (für Baum-Modelle wie RF, LightGBM, XGBoost)\n",
    "explainer_rf = shap.TreeExplainer(rf)\n",
    "explainer_lgbm = shap.TreeExplainer(lgbm)\n",
    "explainer_xgbm = shap.TreeExplainer(xgbm)\n",
    "\n",
    "# SHAP-Werte für das Test-Set berechnen\n",
    "shap_values_rf = explainer_rf.shap_values(X_test)\n",
    "shap_values_lgbm = explainer_lgbm.shap_values(X_test)\n",
    "shap_values_xgbm = explainer_xgbm.shap_values(X_test)\n",
    "\n",
    "# SHAP Summary Plot für Random Forest\n",
    "shap.summary_plot(shap_values_rf, X_test, show=False)\n",
    "plt.title(\"SHAP Summary – Random Forest\")\n",
    "plt.savefig(\"shap_rf_summary.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# SHAP Summary Plot für LightGBM\n",
    "shap.summary_plot(shap_values_lgbm, X_test, show=False)\n",
    "plt.title(\"SHAP Summary – LightGBM\")\n",
    "plt.savefig(\"shap_lgbm_summary.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# SHAP Summary Plot für XGBoost\n",
    "shap.summary_plot(shap_values_xgbm, X_test, show=False)\n",
    "plt.title(\"SHAP Summary – XGBoost\")\n",
    "plt.savefig(\"shap_xgbm_summary.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"SHAP-Analyse abgeschlossen. Plots als PNG gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb0fcf0-bf16-4aff-bb3d-48c3896a9255",
   "metadata": {},
   "source": [
    "## LLM-as-Judge OpenAI GPT-API Batching (Upload & Batch Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3643bf4-435b-42d8-9011-0ccbfdb56df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API-Key aus Umgebungsvariable laden\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "assert client.api_key, \"OPENAI_API_KEY Umgebungsvariable ist nicht gesetzt.\"\n",
    "\n",
    "LLM_DIR = Path(\"llm_batch\")\n",
    "BATCH_INFO_FILE = LLM_DIR / \"batch_metadata.json\"\n",
    "\n",
    "batch_metadata = {}\n",
    "\n",
    "jsonl_files = sorted(LLM_DIR.glob(\"*.jsonl\"))\n",
    "\n",
    "for jsonl_file in tqdm(jsonl_files, desc=\"Batch uploads + creations\"):\n",
    "    # Datei-Upload\n",
    "    with open(jsonl_file, \"rb\") as f:\n",
    "        batch_input_file = client.files.create(\n",
    "            file=f,\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "\n",
    "    file_id = batch_input_file.id\n",
    "    print(f\"File {jsonl_file.name} uploaded: {file_id}\")\n",
    "\n",
    "    # Batch starten\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"description\": f\"TD Detection Batch for {jsonl_file.name}\"}\n",
    "    )\n",
    "\n",
    "    batch_id = batch.id\n",
    "    print(f\"Batch gestartet für {jsonl_file.name}: {batch_id}\")\n",
    "\n",
    "    # Metadaten speichern\n",
    "    batch_metadata[jsonl_file.name] = {\n",
    "        \"file_id\": file_id,\n",
    "        \"batch_id\": batch_id,\n",
    "        \"status\": \"submitted\"\n",
    "    }\n",
    "\n",
    "# Metadaten-JSON sichern\n",
    "with open(BATCH_INFO_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(batch_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Alle Batches gestartet. Metadaten gespeichert in {BATCH_INFO_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657614dd-4953-4441-97be-9ca0201042ea",
   "metadata": {},
   "source": [
    "## LLM-as-Judge OpenAI GPT-API Batching (Status & ggf. Download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ea816-41de-4078-89f6-449b5868417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API-Key laden\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Verzeichnisse und Dateien\n",
    "LLM_DIR = Path(\"llm_batch\")\n",
    "BATCH_INFO_FILE = LLM_DIR / \"batch_metadata.json\"\n",
    "BATCH_STATUS_FILE = LLM_DIR / \"batch_status.json\"\n",
    "\n",
    "# Batch-Metadaten laden\n",
    "with open(BATCH_INFO_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    batch_metadata = json.load(f)\n",
    "\n",
    "# Status für alle bekannten Batches abfragen\n",
    "batch_status_results = {}\n",
    "\n",
    "for name, meta in tqdm(batch_metadata.items(), desc=\"Batch Status Check\"):\n",
    "    batch_id = meta[\"batch_id\"]\n",
    "\n",
    "    # Batch-Status vom OpenAI-Server abrufen\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    status_info = {\n",
    "        \"status\": batch.status,\n",
    "        \"input_file_id\": batch.input_file_id,\n",
    "        \"output_file_id\": batch.output_file_id,\n",
    "        \"error_file_id\": batch.error_file_id,\n",
    "        \"request_counts\": batch.request_counts\n",
    "    }\n",
    "\n",
    "    # Download output- und error-file\n",
    "    if batch.output_file_id:\n",
    "        output_path = LLM_DIR / f\"{name}_output.jsonl\"\n",
    "        with open(output_path, \"wb\") as out_f:\n",
    "            content = client.files.content(batch.output_file_id)\n",
    "            out_f.write(content.read())\n",
    "\n",
    "    if batch.error_file_id:\n",
    "        error_path = LLM_DIR / f\"{name}_errors.jsonl\"\n",
    "        with open(error_path, \"wb\") as err_f:\n",
    "            content = client.files.content(batch.error_file_id)\n",
    "            err_f.write(content.read())\n",
    "\n",
    "    batch_status_results[name] = status_info\n",
    "\n",
    "# Ergebnisse sichern\n",
    "def safe_json(obj):\n",
    "    try:\n",
    "        json.dumps(obj)\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        return str(obj)\n",
    "\n",
    "with open(BATCH_STATUS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: {kk: safe_json(vv) for kk, vv in v.items()} for k, v in batch_status_results.items()}, f, indent=2)\n",
    "\n",
    "print(f\"Batch-Status und Ergebnisse gespeichert in {BATCH_STATUS_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8b7e3-5d30-4ee9-a80d-558fd69140af",
   "metadata": {},
   "source": [
    "## Zweite Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e37c12-658c-4cea-936f-ff45e086248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# === Pfade und Setup ===\n",
    "LLM_DIR = Path(\"llm_batch\")\n",
    "OUT = Path(\"data\")\n",
    "SPLIT = Path(\"splits\")\n",
    "SPLIT.mkdir(exist_ok=True)\n",
    "\n",
    "# === 1. LLM-Judgement einlesen ===\n",
    "llm_labels = {}\n",
    "\n",
    "jsonl_files = sorted(LLM_DIR.glob(\"*_output.jsonl\"))\n",
    "\n",
    "for file in tqdm(jsonl_files, desc=\"LLM Judgements einlesen\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            commit_uid = data.get(\"custom_id\")\n",
    "            choice = data.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [{}])[0]\n",
    "            content = choice.get(\"message\", {}).get(\"content\", \"\").strip().lower()\n",
    "            if content in [\"yes\", \"no\"]:\n",
    "                llm_labels[commit_uid] = 1 if content == \"yes\" else 0\n",
    "\n",
    "print(f\"LLM labels geladen: {len(llm_labels)}\")\n",
    "\n",
    "# === 2. CSV-Dateien aktualisieren und splits neu erstellen ===\n",
    "\n",
    "csv_files = list((OUT).glob('features_*.csv'))\n",
    "dfs = []\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"CSV-Dateien verarbeiten\"):\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"label_llm\"] = df[\"commit_uid\"].map(llm_labels).fillna(0).astype(int)\n",
    "    df[\"label_td_combined\"] = ((df[\"label_td_satd\"] == 1) | (df[\"label_llm\"] == 1)).astype(int)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Splits erstellen\n",
    "df_all['commit_dt'] = pd.to_datetime(df_all.commit_date, utc=True)\n",
    "train_idx, test_idx = [], []\n",
    "\n",
    "for repo, g in df_all.groupby('repo_id'):\n",
    "    g = g.sort_values('commit_dt')\n",
    "    n = int(0.7 * len(g))\n",
    "    train_idx += list(g.index[:n])\n",
    "    test_idx += list(g.index[n:])\n",
    "\n",
    "SPLIT.joinpath('time_train_iteration-2.csv').write_text(df_all.loc[train_idx].to_csv(index=False))\n",
    "SPLIT.joinpath('time_test_iteration-2.csv').write_text(df_all.loc[test_idx].to_csv(index=False))\n",
    "\n",
    "for repo in df_all.repo_id.unique():\n",
    "    SPLIT.joinpath(f'lopo_train_excl_{repo}_iteration-2.csv').write_text(df_all[df_all.repo_id != repo].to_csv(index=False))\n",
    "    SPLIT.joinpath(f'lopo_test_{repo}_iteration-2.csv').write_text(df_all[df_all.repo_id == repo].to_csv(index=False))\n",
    "\n",
    "print(\"Neue Splits erstellt (Iteration 2)\")\n",
    "\n",
    "# === 3. Training, Test, LOPO, SHAP wie Iteration 1 ===\n",
    "\n",
    "# Hilfsfunktionen\n",
    "def prepare_xy(df):\n",
    "    X = df.drop(columns=[\n",
    "        'repo_id', 'commit_hash', 'commit_uid', 'commit_date', 'commit_dt',\n",
    "        'satd_delta', 'label_td_satd', 'label_llm'\n",
    "    ])\n",
    "    y = df['label_td_combined']\n",
    "    return X, y\n",
    "\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    print(f\"\\n{name} – Evaluation:\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    print(f\"Confusion Matrix ({name}):\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"ROC AUC ({name}): {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "\n",
    "# Laden\n",
    "train_df = pd.read_csv(SPLIT / 'time_train_iteration-2.csv')\n",
    "test_df = pd.read_csv(SPLIT / 'time_test_iteration-2.csv')\n",
    "\n",
    "X_train, y_train = prepare_xy(train_df)\n",
    "X_test, y_test = prepare_xy(test_df)\n",
    "\n",
    "# Modelle\n",
    "rf = RandomForestClassifier(n_estimators=250, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "evaluate_model(\"Random Forest (Iteration 2)\", rf, X_test, y_test)\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(n_estimators=250, random_state=42, n_jobs=-1)\n",
    "lgbm.fit(X_train, y_train)\n",
    "evaluate_model(\"LightGBM (Iteration 2)\", lgbm, X_test, y_test)\n",
    "\n",
    "xgbm = xgb.XGBClassifier(n_estimators=250, random_state=42, n_jobs=-1, use_label_encoder=False)\n",
    "xgbm.fit(X_train, y_train)\n",
    "evaluate_model(\"XGBoost (Iteration 2)\", xgbm, X_test, y_test)\n",
    "\n",
    "joblib.dump(rf, \"trained_rf_iteration2.joblib\")\n",
    "joblib.dump(lgbm, \"trained_lgbm_iteration2.joblib\")\n",
    "joblib.dump(xgbm, \"trained_xgbm_iteration2.joblib\")\n",
    "\n",
    "# LOPO\n",
    "repos = df_all.repo_id.unique()\n",
    "\n",
    "for repo in repos:\n",
    "    print(f\"\\n=== LOPO: {repo} excluded ===\")\n",
    "    train_df = pd.read_csv(SPLIT / f'lopo_train_excl_{repo}_iteration-2.csv')\n",
    "    test_df = pd.read_csv(SPLIT / f'lopo_test_{repo}_iteration-2.csv')\n",
    "    X_train, y_train = prepare_xy(train_df)\n",
    "    X_test, y_test = prepare_xy(test_df)\n",
    "    rf.fit(X_train, y_train)\n",
    "    evaluate_model(f\"Random Forest (LOPO {repo})\", rf, X_test, y_test)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    evaluate_model(f\"LightGBM (LOPO {repo})\", lgbm, X_test, y_test)\n",
    "    xgbm.fit(X_train, y_train)\n",
    "    evaluate_model(f\"XGBoost (LOPO {repo})\", xgbm, X_test, y_test)\n",
    "\n",
    "# SHAP\n",
    "explainer = shap.TreeExplainer(lgbm)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, show=False)\n",
    "plt.title(\"SHAP Summary – LightGBM – Iteration 2\")\n",
    "plt.savefig(\"shap_lgbm_iteration2.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"Iteration 2 abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63472b27-8aa6-4035-9e2d-c55e69a3b1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-td)",
   "language": "python",
   "name": "venv-td"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
